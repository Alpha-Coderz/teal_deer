{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR and LDA on Lit Review Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdfminer as pdf\n",
    "import os\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim\n",
    "import unicodedata\n",
    "import re, string\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk import corpus\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a blob of the pdf filenames, and turn them into a list\n",
    "files = glob.glob('/Users/fm61/Documents/LitReviews/chatbot/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/fm61/Documents/LitReviews/chatbot/1408.6762.pdf',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot/1511.04108-2.pdf',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot/1605.00090.pdf',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot/1606.01541.pdf',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot/1606.08340.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1408.6762.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1408.6762.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1511.04108-2.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1511.04108-2.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1605.00090.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1605.00090.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1606.01541.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1606.01541.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1606.08340.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1606.08340.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1609.04285.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1609.04285.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1609.06479.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1609.06479.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1611.00483.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1611.00483.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1611.03799.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1611.03799.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1611.08669.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1611.08669.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1612.01627.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1612.01627.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1701.07149.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1701.07149.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1701.07274.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1701.07274.pdf\n",
      "pdf2txt.py -o /Users/fm61/Documents/LitReviews/chatbot_text/1702.03334.txt -t text /Users/fm61/Documents/LitReviews/chatbot/1702.03334.pdf\n"
     ]
    }
   ],
   "source": [
    "for x in files:\n",
    "    dest_loc = x.replace('.pdf','.txt').replace('chatbot/','chatbot_text/')\n",
    "    ocr_cmd = 'pdf2txt.py -o ' + dest_loc + ' -t text ' + x\n",
    "    print ocr_cmd\n",
    "    os.system(ocr_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_files = glob.glob('/Users/fm61/Documents/LitReviews/chatbot_text/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/fm61/Documents/LitReviews/chatbot_text/1408.6762.txt',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot_text/1511.04108-2.txt',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot_text/1605.00090.txt',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot_text/1606.01541.txt',\n",
       " '/Users/fm61/Documents/LitReviews/chatbot_text/1606.08340.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, now pull these text files in and lets do some lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for textpath in text_files:\n",
    "    # Set an open string to place document text into\n",
    "    all_doc_string = ''\n",
    "    \n",
    "    # Open the file and save contents to variable 'file_text'\n",
    "    file_text = open(textpath, 'rb')\n",
    "    \n",
    "    # Read the lines of the tester into a variable. \n",
    "    file_lines = file_text.readlines()\n",
    "    \n",
    "    # Iterate through the lines, and append them to the string to make one big text blob\n",
    "    for line in file_lines:\n",
    "        all_doc_string = all_doc_string + line\n",
    "        \n",
    "    # Append the new string to the list of all files\n",
    "    all_texts.append(all_doc_string)\n",
    "    \n",
    "    # String will reset next; close the file and flush the file buffer\n",
    "    file_text.flush()\n",
    "    file_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\nNikolaos Polatidis \\n\\n \\n\\nSupervisor: Dr Peter Hancox \\n\\n \\n \\n\\nChatbot for admissions \\n\\n \\n \\n\\nSubmitted for the degree of \\n\\n \\n\\nMSc in Internet Software Systems \\n\\n \\n\\nSchool of Computer Science \\n\\n \\n\\nUniversity of Birmingham \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n1 \\n\\n\\x0cAbstract \\n\\n \\nThe communication of potential students with a university department is performed manually and it \\nis a very time consuming procedure. The opportunity to communicate with on a one-to-one basis is \\nhighly  valued.  However  with  many  hundreds  of  applications  each  year,  one-to-one  conversations \\nare  not  feasible  in  most  cases.  The  communication  will  require  a  member  of  academic  staff  to \\nexpend several hours to find suitable answers and contact each student. It would be useful to reduce \\nhis costs and time. \\n \\nThe  project  aims  to  reduce  the  burden  on  the  head  of  admissions,  and  potentially  other  users,  by \\ndeveloping a convincing chatbot. A suitable algorithm must be devised to search through the set of \\ndata and find a potential answer. The program then replies to the user and provides a relevant web \\nlink if the user is not satisfied by the answer. Furthermore a web interface is provided for both users \\nand an administrator.    \\n \\nThe achievements of the project  can be summarised as follows. To prepare the background of the \\nproject  a  literature  review  was  undertaken,  together  with  an  investigation  of  existing  tools,  and \\nconsultation  with  the  head  of  admissions. The  requirements  of  the  system  were  established  and  a \\nrange  of  algorithms  and  tools  were  investigated,  including  keyword  and  template  matching.  An \\nalgorithm  that  combines  keyword  matching  with  string  similarity  has  been  developed.  A  usable \\nsystem using the proposed algorithm has been implemented.  The system was evaluated by keeping \\nlogs of questions and answers and by feedback received by potential students that used it.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n2 \\n\\n\\x0cAcknowledgements \\n\\n \\nI  would  like  to  thank  Dr  Peter  Hancox  for  his  immeasurable  help  and  support  throughout  this \\nproject. I also need to express my thanks to the computer support team for their excellent help and \\ninstructions. \\n \\nFinally, I feel the need to acknowledge the constant support offered by my parents. \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n3 \\n\\n\\x0cTable of Contents\\n\\nChapter 1 ....................................................................................................................... 9 \\nIntroduction ................................................................................................................... 9 \\n1.1 Context .................................................................................................................... 9 \\n1.2 Aims and Objectives ................................................................................................ 9 \\n1.3 Achievements ........................................................................................................ 10 \\n1.4 Methodology.......................................................................................................... 10 \\n1.5 Contributions ......................................................................................................... 11 \\n1.6 Outline of Dissertation .......................................................................................... 11 \\nChapter 2 ..................................................................................................................... 12 \\nBackground Research .................................................................................................. 12 \\n2.1 Chatbots ................................................................................................................. 12 \\n2.2 Existing applications ............................................................................................. 12 \\n2.2.1 ELIZA ................................................................................................................. 12 \\n2.2.2 A.L.I.C.E ............................................................................................................. 12 \\n2.3 Learning chatbots ................................................................................................ 123 \\n2.4 Interaction with humans ........................................................................................ 14 \\n2.5 Web applications .................................................................................................... 14 \\n2.6 Web application technologies ................................................................................ 15 \\n2.6.1 Java ..................................................................................................................... 15 \\n2.6.2 PostgreSQL......................................................................................................... 15 \\n2.6.3 Hibernate ............................................................................................................ 15 \\n2.6.4 Spring ................................................................................................................. 15 \\n2.6.5 Apache Wicket .................................................................................................... 16 \\n2.6.6 Javascript ............................................................................................................ 16 \\n2.6.7 HTML and CSS .................................................................................................. 16 \\n2.6.8 Third party libraries ............................................................................................ 16 \\n2.7 Other ideas ............................................................................................................. 16 \\n2.8 Conclusion ............................................................................................................. 16 \\nChapter 3 ..................................................................................................................... 18 \\nAnalysis and Specification .......................................................................................... 18 \\n3.1 Overview ............................................................................................................... 18 \\n3.2 Functional Requirements ....................................................................................... 18 \\n3.3 Non-Functional Requirements .............................................................................. 19 \\n3.4 Use case model ...................................................................................................... 20 \\n3.4.1 Actor documentation .......................................................................................... 20 \\nChapter 4 ..................................................................................................................... 22 \\nDesign .......................................................................................................................... 22 \\n4.1 Database design ..................................................................................................... 22 \\n4.2 Algorithms ............................................................................................................. 23 \\n4.2.1 Keyword matching ............................................................................................. 22 \\n4.2.2 String similarity .................................................................................................. 22 \\n\\n4 \\n\\n\\x0c4.2.3 Combination of algorithms ................................................................................. 23 \\n4.2.4 Spell checker ...................................................................................................... 24 \\n4.2.5 Natural language parser ...................................................................................... 24 \\n4.2.6 Google search ..................................................................................................... 24 \\n4.3 Business logic ........................................................................................................ 26 \\n4.4 Web Interface ......................................................................................................... 28 \\nChapter 5 ..................................................................................................................... 30 \\nImplementation ............................................................................................................ 30 \\n5.1 Overview ............................................................................................................... 30 \\n5.2 Database layer implementation ............................................................................. 30 \\n5.2.1 Hibernate mapping ............................................................................................. 29 \\n5.2.2 Hibernate configuration ...................................................................................... 30 \\n5.2.3 Data access objects ............................................................................................. 31 \\n5.3 Business logic layer implementation ..................................................................... 33 \\n5.3.1 Business analysis ................................................................................................ 32 \\n5.4 Presentation layer implementation ........................................................................ 35 \\n5.4.1 Presentation analysis .......................................................................................... 34 \\n5.5 User Interface ........................................................................................................ 35 \\n5.5.1 Chatbot interface ................................................................................................ 34 \\n5.5.2 Leave feedback ................................................................................................... 35 \\n5.5.3 Add information ................................................................................................. 36 \\n5.5.4 View information ................................................................................................ 36 \\n5.5.5 View feedback .................................................................................................... 37 \\n5.5.6 View logs ............................................................................................................ 37 \\nChapter 6 ..................................................................................................................... 39 \\nTesting and Evaluation ................................................................................................ 39 \\n6.1 Testing ................................................................................................................... 39 \\n6.1.1 Overview ............................................................................................................ 38 \\n6.1.2 Unit and integration testing ................................................................................ 38 \\n6.1.3 Performance testing ............................................................................................ 39 \\n6.1.4 Security testing ................................................................................................... 40 \\n6.2 Evaluation .............................................................................................................. 42 \\n6.2.1 Overview ............................................................................................................ 41 \\n6.2.2 Question categories ............................................................................................ 41 \\n6.2.3 User evaluation ................................................................................................... 42 \\n6.2.4 Survey ................................................................................................................. 42 \\n6.2.4.1 Initial results .................................................................................................... 43 \\n6.2.4.2 Final results ..................................................................................................... 44 \\nChapter 7 ..................................................................................................................... 47 \\nSummary ...................................................................................................................... 47 \\n7.1 Achievements ........................................................................................................ 47 \\n7.2 Limitations ............................................................................................................. 47 \\n7.3 Future work ........................................................................................................... 48 \\n\\n5 \\n\\n\\x0c7.4 Conclusions ........................................................................................................... 49 \\nReferences ................................................................................................................... 50 \\nAppendix A Project proposal ....................................................................................... 52 \\nAppendix B How to run the system ............................................................................ 54 \\nAppendix C Example Conversation ............................................................................ 56 \\nAppendix D Feedback ................................................................................................. 57 \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n6 \\n\\n\\x0cFigures\\n\\nFigure 1.1 Waterfall model .................................................................................................................. 8 \\nFigure 2.1 A sample conversation with ELIZA ................................................................................. 11 \\nFigure 2.2 A sample conversation with ALICE ................................................................................. 12 \\nFigure 2.3 Three Tier Architecture ..................................................................................................... 14 \\nFigure 3.1 Use case diagram .............................................................................................................. 18 \\nFigure 4.1 Entity Relationship diagram ............................................................................................. 20 \\nFigure 4.2 Jaro-Winkler code walk through ...................................................................................... 24 \\nFigure 4.3 Business Logic Implementation ....................................................................................... 25 \\nFigure 4.4 Business Logic Interface .................................................................................................. 28 \\nFigure 4.5 Web interface .................................................................................................................... 26 \\nFigure 4.6 Web interfaces .................................................................................................................. 27 \\nFigure 5.1 Java persistent object ........................................................................................................ 29 \\nFigure 5.2 XML mapping .................................................................................................................. 29 \\nFigure 5.3 Hibernate mapping configuration ..................................................................................... 30 \\nFigure 5.4 Hibernate database configuration ..................................................................................... 30 \\nFigure 5.5 Feedback data access object interface .............................................................................. 31 \\nFigure 5.6 Feedback data access object implementation ................................................................... 31 \\nFigure 5.7 Chatbot service interface .................................................................................................. 32 \\nFigure 5.8 Sentence parse partial implementation ............................................................................. 32 \\nFigure 5.9 Spring security .................................................................................................................. 33 \\nFigure 5.10 Spring Integration with Wicket....................................................................................... 33 \\nFigure 5.11 Chatbot ............................................................................................................................ 34 \\nFigure 5.12 Feedback ......................................................................................................................... 34 \\nFigure 5.13 Add information .............................................................................................................. 35 \\nFigure 5.14 View information ............................................................................................................ 35 \\nFigure 5.15 View feedback ................................................................................................................ 36 \\nFigure 5.16 View logs ........................................................................................................................ 36 \\nFigure 6.1 Performance testing stages ............................................................................................... 38 \\nFigure 6.2 Initial log results ............................................................................................................... 42 \\nFigure 6.3 Final log results ................................................................................................................ 43\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n7 \\n\\n\\x0cTables\\n\\nTable 3.1 Actor documentation  ......................................................................................................... 19 \\nTable 4.1 Keyword matching ............................................................................................................. 21 \\nTable 4.2 String similarity .................................................................................................................. 21 \\nTable 4.3 Combination of keyword matching and string similarity .................................................. 22 \\nTable 6.1 Unit and Integration testing ................................................................................................ 38 \\nTable 6.2 Security testing ................................................................................................................... 39 \\nTable 6.3 Initial logs ........................................................................................................................... 42 \\nTable 6.4 Final logs ............................................................................................................................ 43\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n8 \\n\\n\\x0cChapter 1 \\n \\n\\nIntroduction \\n \\nThis chapter covers an introduction to the project including the context, a description of aims and \\nobjectives, a description of what has been achieved, contributions and the structure of the report. \\n \\n\\n1.1 Context \\n\\nAlthough the admissions process works properly as it is, it is very difficult and time consuming to \\ncontact a member of staff of the university. However, the problem would be partially solved if the \\napplicant  could  talk  to  a  convincing  chatbot,  able  to  respond  to  their  concerns  with  information \\nabout  admissions,  booking  accommodation,  paying  fees  in  instalments  and  what  pre-sessional \\ncourses are on offer.  \\n \\nThe chatbot should be able to communicate with a user in a way similar to the following: \\n \\nChatBot: Hello how can I help you? \\nUser: What is the minimum score of IELTS required for entry into MSc Computer Science? \\nChatBot: The minimum requirement is a total score of 6.0 with no less that 5.5 in any section. \\nUser: What other test is accepted by the department? \\nChatBot: We also accept TOEFL and the certificate of Proficiency of the University of Cambridge. \\nUser: What are the entry requirements for the MSc in Computer Security? \\nChatBot: To enter the programme, you need to have at least an Upper Second Class (2.1) degree or \\nan  international  equivalent  in  Computer  Science  or  a  closely  related  discipline  and  a  solid \\nfoundation in programming. \\n \\n\\n1.2 Aims and Objectives \\n\\nThe  aim  of  this  project  is  to  contribute  to  the  solution  of  the  problem  of  direct  communication \\nbetween applicants and the university. \\n \\nThe main objectives of the project are as follows: \\n \\nDatabase:  To  develop  a  database  were  all  the  relevant  information  about  questions,  answers, \\nkeywords, logs and feedback will be stored. \\n \\nAlgorithm: To develop a keyword matching algorithm and a string distance comparison algorithm \\nand combine them in order to retrieve the best possible answer. \\n \\nInterface: To develop a web interface which aims to give the ability to potential students and their \\nfamilies to submit questions in a chatbot and get convincing replies. The web system can be hosted \\non any computer that supports an operating system on which the Java Virtual machine will be able \\nto  load.  Furthermore  the  Hibernate,  Spring,  Apache  Wicket,  Lingpipe,  the  Jazzy  spelling,  the \\nStanford NLP parser and Google search libraries should be available.  \\n\\n9 \\n\\n\\x0c1.3 Achievements \\n\\nThe following goals were achieved: \\n \\n\\n\\xef\\x83\\xb1  Investigated a range of algorithms for solving the problem of direct communication with the \\nuniversity  and  developed  an  algorithm  that  combines  the  use  of  keyword  matching  with \\nstring distance. \\n\\n\\xef\\x83\\xb1  Designed  and  implemented  a  usable  chat  system,  which  has  been  extended  after  being \\n\\nevaluated by users. \\n\\n\\xef\\x83\\xb1  Evaluated the extended system with users. The system was used by potential students of the \\n\\nuniversity. \\n\\n\\xef\\x83\\xb1  Completed  a  literature  research  in  order  to  learn  more  about  natural  language  processing, \\n\\ninvestigated existing tools and established the requirements. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n1.4 Methodology \\n\\nThe design and development of the system followed the waterfall model as described below. \\n\\nFigure 1.1 Waterfall model \\n\\n \\nThe waterfall model follows a series of processes, which are used during development. Usually the \\nstages will require the gathering of requirements and their analysis. The design of the system is the \\nnext  stage,  followed  by  coding  the  actual  system.  Then  evaluation,  testing  and  debugging,  if \\nnecessary,  is  the  next  step.  Finally  the  system  will  either  be  accepted  and  therefore  maintained  or \\nrejected. It is vital to move to the next process of the waterfall model if the previous step has been \\ncompleted. \\n \\n\\n10 \\n\\n\\x0c1.5 Contributions \\n\\nA number of novel concepts have been incorporated to the system. A keyword matching algorithm \\nhas  been  used  in  combination  with  a  string  similarity  algorithm  to  provide  a  better  reply  and  is \\ndescribed in  sections 4.2.1, 4.2.2 and 4.2.3. Also the Stanford parser has  been used to  analyse the \\nstructure of the given sentence as described in section 4.2.5. Finally the system has been evaluated \\nusing feedback and logs from potential students that used it, described in section 6.2.  \\n \\n\\n1.6 Outline of Dissertation \\n\\nThis report is divided into several chapters. \\n \\nChapter two covers the background research, which  gives an overview of chatbots, interactions of \\nchatter-bots  with  humans,  matching  algorithms  and  current  systems.  Chapter  three  gives  and \\noverview and covers the requirements of the system. Chapter four covers the design of the system. \\nIt includes the database design, an explanation of the algorithms and how they work. It goes on to \\nexplain  the  functionality  provided  by  the  system  for  the  users  and  the  administrator.  Chapter  five \\ncovers  the  implementation  of  the  system.  The  system  is  described  with  further  details  on  the \\ntechnologies used. Chapter six contains the evaluation and testing of the final system. Chapter seven \\nis the conclusion of the project.  It  contains a summary, the main achievements, its limitations and \\nfuture work. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n11 \\n\\n\\x0cChapter 2 \\n \\n\\nBackground Research \\n \\nThis  chapter  contains  the  background  research  about  topics  relevant  to  the  admissions \\ncommunication. It presents an overview of chatbots and their interaction with humans. Furthermore \\nthe keyword matching and the template matching algorithms are mentioned, followed by real world \\nexamples of their use. \\n \\n\\n2.1 Chatbots \\n\\nA  chatbot  is  a  program  that  is  used  to  participate  in  conversations  with  humans.  It  uses  an \\nappropriate interface for input and output and with the  use of AI techniques it can  provide realistic \\nanswers  so  the  user  will  think  that  the  communication  taking  place  is  with  another  human.  The \\nimplementation of such systems varies from using keyword matching, string similarity or complex \\nnatural  language  processing  techniques.  More  sophisticated  chatbots  could  learn  from  the  user \\ninput.  Nowadays  chat  bots  are  used  widely  in  web  applications  in  order  to  provide  help  or \\ninformation when it is asked by the users.  \\n\\n\\xe2\\x80\\x9cChatbots are computer programs that interact with users using natural languages[21].\\xe2\\x80\\x9d \\n\\n  \\n\\n2.2 Existing applications \\n\\nChatbots are used in many organisations instead of humans when this is possible. Most of the time \\nsystems like that are based on the ways ELIZA or ALICE communicates. \\n \\n\\n2.2.1 ELIZA \\n\\nThe  first  chatbot  developed  was  ELIZA  [21].  It  was  developed  by  Joseph  Weizenbaum  using  a \\nkeyword matching technique. The idea was to read the user input and search for certain keywords, \\nif a keyword was found then the answer was retrieved.  If a keyword was not  present  then ELIZA \\nwould  try,  according  to  specified  rules,  to  get  more  information  from  the  user  to  keep  the \\nconversation going. Consider the following example to understand how this would work. \\n \\nUser: How can I get in touch with my mother? \\n \\nThen the chatbot would identify the keyword mother and would find a connection between mother \\nand family and it would respond with a question. \\n \\nELIZA: Tell me more about your family \\n \\nThe chatbot doesn\\'t understand what it is saying. It only produces results according to rules and in \\nthis  case  there  was  a  rule  connecting  mother  and  family.  Then  it  had  to  produce  an  answer  that \\nwould encourage the user to submit another question with the hope of finding a keyword. However \\nmost of the time there are some standard replies that ELIZA would use if a keyword is not found. \\nSome examples would be \\xe2\\x80\\x9cVery interesting, please go on\\xe2\\x80\\x9d or \\xe2\\x80\\x9cCan you think of a special example?\\xe2\\x80\\x9d \\n \\n\\n12 \\n\\n\\x0cThe following figure gives a good example of chatting with ELIZA. \\n \\n\\nFigure 2.1 A sample conversation with ELIZA (Jurafsky and Martin, 2000) \\n\\n \\n\\n2.2.2 A.L.I.C.E \\n\\n<pattern>What is your name</pattern> \\n<template>My name is Nick</template> \\n\\nALICE was implemented by Richard Wallace in 1995[21]. It uses pattern matching and stores the \\ninformation in Artificial Intelligence Mark-up Language, or else known as AIML, files. An AIML \\nfile is similar to an XML file that was developed to store pattern knowledge for chatbots. There are \\nthree  types  of  AIML  categories,  atomic  categories,  default  categories  and  recursive  categories, \\nwhich are described below. \\n \\nAtomic categories: Is a type of AIML category where there is an exact match. \\n \\n<category> \\n \\n \\n</category> \\n \\nIn the above example if the user submits \\xe2\\x80\\x9cWhat is your name\\xe2\\x80\\x9d then the chatbot will reply with \\xe2\\x80\\x9cMy \\nname is Nick\\xe2\\x80\\x9d. \\n \\nDefault categories: Is a type of AIML category where the use of a wild character such as * is used to \\nmatch any input.  \\n \\n<category> \\n \\n \\n</category> \\n \\nIf ALICE does not find a solution using the first category it will proceed to the second. In this case \\nif a user submits \\xe2\\x80\\x9cWhat is a name\\xe2\\x80\\x9d  then ALICE will do a check until the wild char and if there is a \\nmatch it will come with the answer. \\n \\nRecursive  categories:  Is  a  type  of  an  AIML  category  where  special  tags  are  used  to  refer  to  a \\nrecursion to convince the user to be more specific. \\n \\n\\n<pattern>What is *</pattern> \\n<template>It is my name</template> \\n\\n13 \\n\\n\\x0c<pattern>Do you know what the * is</pattern> \\n<template>It is my name \\n<srai>What is <star/></srai> \\n\\n<category> \\n \\n \\n \\n</template> \\n</category> \\n \\nIn this case the wild character * is used to reduce the input submitted by the user. If the user submits \\n\\xe2\\x80\\x9cDo you know what the time is\\xe2\\x80\\x9d then this becomes \\xe2\\x80\\x9cWhat is the time\\xe2\\x80\\x9d. \\n \\nThe following figure gives an example of a conversation between a human and ALICE. \\n \\n\\nFigure 2.2 A sample conversation with ALICE (Shawar and Atwell, 2007) \\n\\n \\n \\nWhen  a  user  submits  a  question  then  any  relevant  punctuation  is  removed  and  the  sentence  goes \\nthrough  the  categories  to  find  the  longest  pattern  match. The  question  \\xe2\\x80\\x9cCan  you  tell  me  the  time, \\nusing hours, minutes and seconds?\\xe2\\x80\\x9d will become\\xc2\\xbb Can you tell me the time using hours minutes and \\nseconds\\xe2\\x80\\x9d and then it will search to find the longest match. When the match is found then it will be \\nforwarded to the output. However ALICE does not have the ability to learn and can only come with \\ndata that exist in its database. \\n \\n\\n2.3 Learning chatbots \\n\\nLearning chatbots have the ability to learn from user input. The intelligence of the chatbot is based \\non specified rules and by assuming that the user will give a correct answer. There are two types of \\nlearning and both use the same learning rules. The first one will understand what the user said, will \\nstore it for the current session and will reply with that answer if the user asks about. The second one \\nwill store the information in its database and will reply with that answer for any user. Certain rules \\ncan be specified to understand from previous user input if the possibility of a user saying the truth is \\nhigh or not and decide to store the information for future users. Also questions and answers among \\nusers can be saved in the database and compared to identify the possibility of something being true.  \\n \\n\\n14 \\n\\n\\x0cThe following conversation gives an example of the learning procedure. \\n \\nUser: How many lecturers are there in the department of computer science? \\nChatbot: I am sorry but I don\\'t know the answer. Perhaps you can help me learn \\nUser: There are 28 lecturers at the department \\nChatbot: Is this the correct answer? \\nUser: Yes \\nChatbot: Thank you for helping \\nUser: How many lecturers are there in the department of computer science? \\nChatbot: There are 28 lecturers at the department \\n \\n\\n2.4 Interaction with humans \\n\\nHumans believe that the concept of a chatbot is not realistic and that a chatbot will not be able to \\nprovide satisfying answers. Most of the time chatbots are being developed to serve certain purposes \\nand if a human tries to trick the chatbot it is likely that he will succeed. Moreover humans do not \\nthink that a computer program will be able to satisfy their needs and they do not even try to interact. \\nAlso if a human is aware that a chatbot has extensive knowledge then he will be more demanding. \\nIn  the  case  that  we  have  to  convince  a  human  that  the  conversation  is  taking  place  with  another \\nhuman  then  an  extensive  database  has  to  be  used  along  with  certain  rules  such  as  answering \\nmultiple questions, occurrence of words and sentence parsing. \\n \\nThe  Loebner  prize  is  a  chatbot  competition  with  a  number  of  judges  communicating  with  both \\nhumans and chatbots at the same time and is based on a standard Turing test. There are a number of \\nrounds  in  which  a  human  judge  is  communicating  with  a  chatbot  and  with  a  human  that  use  a \\ncomputer  interface.  Based  on  the  answers  the  judge  will  have  to  decide  which  one  is  the  chatbot \\nand which one is the human, within a five minute limit. \\n \\nAnaphora resolution is a very important aspect for a human like chatbot. It is  a way of finding to \\nwhat an expression refers.  \\n \\nWe found a room under the university accommodation, but it didn\\'t have an en-suite bathroom. \\n \\nIt is clear for a human that the bathroom refers to the room. However it would be more difficult for \\na  chatbot  to  understand.  However  there  are  cases  that  an  expression  can  become  complex  and \\ntherefore difficult to understand. \\n \\nI accepted my offer to the university because it was unconditional. \\n \\nI accepted my offer to the university because it was one of the top.   \\n \\nIn the first case the expression refers to the offer, whereas in the second case refers to the university. \\n \\n\\n2.5 Web applications \\n\\nThe classic architecture is the three tier architecture as  described in Figure 2.3. The advantages of \\nusing this type of model is that the functionality is distributed across three independent systems and \\nalso  that  any  of  the  tier  can  be  replaced  or  upgraded  independently  if  the  requirements  or  the \\ntechnology change. \\n\\n15 \\n\\n\\x0c \\n\\n \\n\\nFigure 2.3 Three Tier Architecture \\n\\n \\n\\n2.6 Web application technologies \\n\\nA  number  of  technologies  and  libraries  were  investigated  for  the  purpose  of  development  of  this \\nproject and the following were identified as the ones to be used for creating the system. \\n \\n\\n2.6.1 Java \\n\\nJava is  an object  oriented programming language that can be used to  develop  software that it can \\nrun on a variety of operating systems without modification. Moreover it is simple to use language, \\nwith a large number of APIs, sufficient documentation and it is free.  \\n\\n2.6.2 PostgreSQL \\n\\nPostgreSQL is an open source relational database management system. It has been chosen because \\nit is supported by a variety of operating systems, it works efficiently with increasing load and has \\nmany advanced features. Furthermore there are a number of command line, web and GUI front end \\ntools that can be used to manipulate databases.  \\n\\n2.6.3 Hibernate \\n\\nHibernate  is  an  open  source  object-relational  mapping  set  of  APIs  for  the  Java  language.  The \\npurpose of Hibernate is to map the problem of mismatch between relational database systems and \\nobject.  The  idea  is  to  have  a  Java  class  for  each  table  and  manipulate  the  objects  created  by  the \\nclasses that map the tables. Moreover its thorough documentation and the fact that it is lightweight \\nmake it a wise choice.  \\n\\n2.6.4 Spring \\n\\nSpring  is  a  lightweight  open  source  application  framework  that  can  be  easily  used  with  other \\nframeworks.  It  has  a  number  of  features  that  make  it  essential  to  use.  There  is  a  centralized \\nconfiguration file where Java beans can be created, transactions can be handled, without having to \\nmanage each transaction separately and security can be managed. Data access objects can be used \\non  top  of  the  Hibernate,  which  reduces  errors  and  simplifies  the  development  process.  Testing \\napplication code written using Spring is easier than other similar technologies. \\n\\n16 \\n\\n\\x0c2.6.5 Apache Wicket \\n\\nApache  Wicket  is  a  Java,  component  based,  web  application  framework  that  is  easy  to  integrate \\nwith Spring and Hibernate. It provides a clear separation between static HTML and dynamic Java \\nparts  of  the  presentation  layer.  It  allows  the  creation  of  reusable  components.  Wicket  provides \\nauthorization roles that can be easily placed on top of each page. Also errors are identified during \\nthe compilation process. \\n\\n2.6.6 Javascript \\n\\nJavascript  is  a  scripting,  client-side,  programming  language.  It  can  be  used  to  provide  encase  the \\nfunctionality  of  HTML  pages.  It  can  be  used  for  various  purposes  such  as  setting  character \\nlimitations on text areas.  \\n\\n2.6.7 HTML and CSS \\n\\nHTML  is  the  standard  mark-up  language  used  for  the  development  of  web  interfaces.  It  must  be \\nused in combination with CSS, which is responsible for the layout of the HTML components. \\n \\nThe CSS files have been based on the CSS files developed by Mike Stenhouse[12]. \\n\\n2.6.8 Third party libraries \\n\\nThe Jazzy spelling Java API has been used and requires an interaction with a local dictionary file in \\norder to provide a spelling correction if necessary. The Google search Java API has been used and \\nrequires  communication  with  the  Google  search  server  on  user  request.  Also  the  lingpipe  Java \\nnatural  language  processing API  has  been  used  for  calculating  the  string  similarity.  The  Stanford \\nnatural language parser has been used for the analysis of a sentence and validity checking. \\n\\n2.7 Other ideas \\n\\nA  combination  of  the  keyword  matching  algorithm  with  the  string  similarity  algorithm  should  be \\nused.  The  keyword  matching  will  start  identifying  keywords  and  retrieve  the  answer  that  has  the \\nmost keywords. In the case that the same number of keywords is found in more than one answer  or \\nthere are no keywords then the String similarity algorithm will run to identify the which question is \\nmost similar to the one entered and retrieve the answer.    \\n\\n2.8 Conclusion \\n\\nFrom the information gathered from the research the strongest points and weaknesses of each have \\nbeen identified and a list of desired features that should be in the system has been created. Finally \\nthe conclusion was that the system should contain a combination of the keyword matching with the \\nString  similarity  algorithm,  a  customized  google  search  option  and  a  spell  checker,  log  and \\nfeedback  functionality.  Furthermore  an  administration  part  should  be  created  where  the \\nadministrator will be able to add, update and delete information related to the system. \\n \\n \\n \\n \\n \\n \\n \\n\\n17 \\n\\n\\x0cChapter 3 \\n \\n\\nAnalysis and Specification \\n \\nThis  chapter  contains  an  overview,  a  system  proposal  and  a  thorough  analysis  of  functional,  non-\\nfunctional requirements and users of the system. \\n\\n3.1 Overview \\n\\nThe role of the system is to  provide a  chatbot that  will be able to  answer questions related to  the \\nadmission procedure. It will provide a web interface for the users to interact with the system and an \\nadministration interface. \\n \\nA  user  is  anyone  who  would  like  to  visit  the  website  and  engage  in  a  conversation.  As  well  as \\ntalking  the  user  should  be  able  to  submit  a  log  of  whether  he  is  satisfied  with  an  answer  and \\nproduce a link. Other than communicating, the user should be able to rank the system. The rank of \\nthe system should be a five star ranking system where one is poor and 5 is excellent. A user should \\nalso be able to write a review using the feedback form.  \\n \\nThe  administrator  of  the  system  shall  be  able  to  log  in  using  a  user  name  and  a  password.  The \\nresponsibility of the administrator will be to maintain the system by adding questions and answers \\nto the database and by updating current information sets when necessary. Furthermore he should be \\nable to view the user ranking, feedback messages and logs. \\n \\nThe system shall provide its users with spell checking suggestions on screen, when they make such \\nerrors. Moreover the parsing of sentences will avoid sending to the system words that do not form a \\nsentence.  \\n \\nThere  are  several  security  issues  which  need  to  be  taken  into  consideration  when  designing  the \\nsystem. These include personal and sensitive information. The data should not be accessed without \\nauthorisation  and  authentication.  Disclosure  or  leak  of  data  should  be  protected  against  various \\ntypes of attacks and be encrypted and password protected.  \\n\\n3.2 Functional Requirements \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n\\n1.  Chatting:  \\n\\na. The system should allow users to chat. \\nb. The system shall inform the user if an answer is not available. \\nc. The system shall inform the user about spelling mistakes. \\nd. The system shall inform the user about the validity of the sentence. \\n\\n2.  Searching:  \\n\\na. The system should allow users to search for information about admissions. \\nb. The system should allow users to search for information about tuition fees. \\nc. The system should allow users to search for information about accommodation. \\n\\n18 \\n\\n\\x0c \\n\\n \\n \\n \\n \\n\\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n3.  Logs:  \\n\\na. The system should maintain a log of the current question and answer if the user is not  \\n    satisfied. \\n\\n4.  Feedback:  \\n\\na. The  user  should  be  able  to  leave  feedback,  which  is  comprised  of  a  text  message  and  a     \\n    rating. \\n\\n5.  Administrative system \\n\\na. Information  management:  The administrator should be able to  to  add, update and delete \\n    questions, answers and keywords. \\n\\nb. Log management: The administrator should be able to view and delete logs. \\n\\nc. Feedback management: The administrator should be able to view and delete feedbacks. \\n\\n3.3 Non-Functional Requirements \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n\\n1.  User Interface:  \\n\\na. The system shall maintain an easy to use interface across all functionality and for all users \\nb. The clients\\xe2\\x80\\x99 user interface should be compatible with all commonly used browsers, such   \\nas Internet explorer, Firefox, Google chrome and Safari. \\n\\n2.  Scalability:  \\n\\na. The system shall be able to scale based on the number of users using the system. \\n\\n3.  Security:  \\n\\na. The administrative system should be protected from unauthorized access. \\nb. The database should protected from attacks and unauthorized access. \\nc. The interface should be protected from attacks. \\nd. All passwords should be stored as a secure hash of the administrator password. \\n\\n4.  Third party interactions: \\n\\na. The system should be able to interact with the Google spelling server, which handles the \\nspelling. \\nb. The system should be able to interact with the Google search server, which is used for the \\ncustomized search on the admissions website. \\n\\n5.  Portability: \\n\\na. The system should run on a variety of operating systems that support the Java language. \\nb. The system should run on a variety of hardware. \\n\\n19 \\n\\n\\x0c6.  Maintainability: \\n\\n \\na. The system should be easy to maintain. \\nb. There should be a clear separation of HTML and Java interface code.  \\nc. There should be a clear separation between the interface and the business logic code. \\nd. There should be a clear separation between the data access objects that map the database \\n    and the business logic code. \\n\\n7.  Exception handling: \\n\\na. Exceptions should be reported effectively to the user if they occur. \\n\\n8.  Ethics: \\n\\n \\na. The system shall not store or process  any information about its users. \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n\\n \\n \\n \\n \\n\\n3.4 Use case model \\n\\nThe use case diagram describes the functionality of the system as designed from the requirements \\nand can be found below. \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\nFigure 3.1 Use case diagram \\n\\n20 \\n\\n\\x0c3.4.1 Actor documentation \\n\\nActor \\n\\nDescription \\n\\nUser \\n\\nAdministrator \\n\\nSomeone who uses the system to engage in a \\nconversation \\n\\nThe administrator of the system. Allowed to \\ncarry out administrative tasks \\n\\nTable 3.1 Actor documentation \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n21 \\n\\n\\x0cChapter 4 \\n \\n\\nDesign \\nThis  chapter  outlines  the  design  of  the  system.  It  comprises  a  database,  a  combination  of  two \\nalgorithms, the business  logic and the web interface. The system includes interfaces for users and \\nan administrator. \\n\\n4.1 Database design \\n\\nThe design of the database aims to keep it as simple and efficient as possible. The user_roles table \\nwill hold the user name and a role name, the roles table will store a description of each role and an \\nid,  the  chatbotuser  table  will  store  the  user  details,  the  information  table  will  store  the  questions, \\nanswers and keywords, the fdback table will store the feedback received and the log table will store \\nthe logs.    \\n \\nThe figure below shows the entity relationship diagram designed for the database of the system. \\n \\n\\nFigure 4.1 Entity Relationship diagram \\n\\n22 \\n\\n\\x0c4.2 Algorithms \\n\\n4.2.1 Keyword matching \\n\\nThe keyword matching algorithm will attempt to identify keywords in a sentence. In the case that \\none or more keywords are found in the user\\'s input text then an answer will be retrieved. However, \\nin the case that the same number of found keywords exist as keys for more than one answer then \\nthere is a deadlock. Again a deadlock occurs in the case that there are no keywords. \\n \\nConsider the following example in order to understand better how the keyword matching algorithm \\nworks. The table represents a small number of information rows from the database of the system. \\n \\n\\nQuestion \\n\\nAnswer \\n\\nKeyword 1 \\n\\nKeyword 2 \\n\\nDo I need a visa? \\n\\nYes,  unless  you  are \\nfrom \\nEU/EEA \\ncountry \\n\\nan \\n\\nVisa \\n\\n \\n\\nWhat  are \\nrequirements? \\n\\nthe  entry \\n\\nYou  need  at  least  a  2.1 \\ndegree  in  order  to  be \\nconsidered  for  a  place \\nin one of our courses \\n\\nEntry \\n\\nRequirements \\n\\nTable 4.1 Keyword matching \\n\\n \\n\\nThe user then submits the question: \\xe2\\x80\\x9cDoes a 2.2 in computer science satisfy the entry requirements \\nfor the MSc in computer security?\\xe2\\x80\\x9d \\n \\nThe algorithm will identify both keywords in the second row and will retrieve the answer. \\n \\n\\n4.2.2 String similarity \\n\\nThe string similarity algorithm will try to find similarities between a list of strings and will retrieve \\nthe answer that is the closest match to the question submitted by the user. \\n \\nConsider  the  following  example  in  order  to  understand  better  how  the  string  similarity  algorithm \\nworks. The table represents a small number of information rows from the database of the system. \\n \\n \\n\\nQuestion \\n\\nAnswer \\n\\nKeyword 1 \\n\\nKeyword 2 \\n\\nDo I need a visa? \\n\\nYes,  unless  you  are \\nfrom \\nEU/EEA \\ncountry \\n\\nan \\n\\nVisa \\n\\nCan I pay with a visa?  Yes,  both  tuition  and \\naccommodation \\nfees \\ncan  be  paid  online \\nusing a visa \\n\\nVisa \\n\\nThe user then submits the question: \\xe2\\x80\\x9cCan I pay using visa?\\xe2\\x80\\x9d \\n \\n\\nTable 4.2 String similarity \\n\\n \\n\\n \\n\\n \\n\\n23 \\n\\n\\x0cThe algorithm will then compare the question of the user with the questions in the system and will \\nrespond  with  a  percentage  for  each,  ignoring  the  keywords,  which  represents  the  possibility  of \\nsimilarity  between  strings.  In  this  case  the  second  question  has  the  closer  match  with  the  user \\nsubmitted question and the answer from that information line will be retrieved. \\n \\nThe Jaro-Winkler distance has been used in the development of the algorithm[17]. The figure below \\ngives the code walk through for the Jaro-Winkler distance. \\n \\n\\nFigure 4.2 Jaro-Winkler code walk through (alias-i.com) \\n\\n \\nThe  algorithm  compares  two  strings  and  returns  a  distance  and  proximity.  The  distance  is  the \\ndifference between two strings and the higher its value is, then more the possibility of two strings \\nbeing irrelevant. The proximity is a value between 0.0 and 1.0. The higher the value returned, the \\nhigher the possibility of two strings being the same exists. A string is considered to be a sequence of \\ncharacters; therefore spaces between words are blank characters and will be checked. Using a loop, \\nthe code used in the system checks only the proximity between the string submitted by the user with \\nevery question string in the database. \\n\\n4.2.3 Combination of algorithms \\n\\nIn order to achieve the best possible outcome we decided to combine the two algorithms.  Therefore \\nto  the  database  of  the  system  has  been  added  questions  with  both  answers  and  keywords  and \\nquestions with answers but no keywords. For example the keyword matching algorithm will search \\nthe  keywords  and  the  string  similarity  algorithm  will  search  all  pre-stored  questions  with  or  with \\nout  keywords.  In  order  to  improve  this  similar  questions  have  been  added  that  have  the  same \\nanswer. Basically if the keyword matching fails then we try to think the way a simple user would \\nthink,  asking  a  question.  Consider  the  following  example  in  order  to  understand  better  how  the \\ncombination works. \\n \\n\\nQuestion \\n\\nAnswer \\n\\nKeyword 1 \\n\\nKeyword 2 \\n\\nHave you received my \\napplication pack \\n\\nHave you received my \\nreferences \\n\\nYou have to contact the \\nadmissions office for \\nthis information \\n\\nYou have to contact the \\nadmissions office for \\nthis information \\n\\nReceived \\n\\nReceived \\n\\nHave you received my  \\npack  \\n\\nYou have to contact the \\nadmissions office for \\nthis information \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nTable 4.3 Combination of keyword matching and string similarity \\n\\n24 \\n\\n\\x0c \\nThe user then submits the question:  \\xe2\\x80\\x9cHave you received the pack sent?\\xe2\\x80\\x9d \\n \\nThen the keyword matching algorithm will run first in order to identify keywords.  It will identify \\none keyword in the first information line, one in the second and none in the third and so a deadlock \\noccurs.  In  this  case  the  string  similarity  algorithm  will  compare  the  pre-stored  questions  of  the \\nsystems  with  the  user\\'s  question  and  it  will  find  that  the  best  match  is  with  the  third  entry  in  the \\ntable, even though there is no keyword in the database. It will then retrieve the answer of the third \\none. We can see here that the questions are very similar and have the same answer. \\n\\n4.2.4 Spell checker \\n\\nThe system is supported by the jazzy spell checker [20]. The checker will check the sentence before \\nit is  submitted to  the server.  If there is  an  error then the user will be informed about  it and about \\npossible corrections. If it is error free then it will be submitted. \\n\\n4.2.5 Natural language parser \\n\\nThe Stanford natural language parser [18] will be used to check is a sentence is valid according to \\nspecified rules. If the sentence is not valid then the user is informed and he will have to make the \\nappropriate changes. In the case of our system a sentence is required to have a noun and a verb to \\nbe valid. The example below represents how the Stanford parser will analyse a sentence. \\n \\nThe user enters the question: \\xe2\\x80\\x9cHow can I apply?\\xe2\\x80\\x9d \\n \\nThe parser will parse the sentence and produce a tree as follows: \\n \\n(ROOT \\n  (SBARQ \\n    (WHADVP (WRB How)) \\n    (SQ (MD can) \\n      (NP (PRP I)) \\n      (VP (VB apply))) \\n    (. ?))) \\n\\nThen the system will check that there is a noun and a verb in the tree. In the case that there is, the \\nquestion is submitted to the system. In case there is not both a noun and a verb the user is informed \\nand  the  user\\'s  question  is  not  answered.  The  contribution  of  the  parser  is  to  stop  the  user  from \\nsubmitting syntactically incorrect questions. The examples below represents syntactically incorrect \\nquestions submitted by users, followed by the chatbot reply. \\n \\nUser: \\xe2\\x80\\x9cYes and yes not yes\\xe2\\x80\\x9d  \\nChatbot: \\xe2\\x80\\x9cMy name is John.\\xe2\\x80\\x9d \\n \\nUser: \\xe2\\x80\\x9cTell me about Ph.D.\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cHello there.\\xe2\\x80\\x9d \\n\\n4.2.6 Google search \\n\\nIn the case that a user is not satisfied by a system reply then he will be provided with a link from \\nthe admissions website. The link will be retrieved using customized Google search, according to the \\ninput submitted. \\n \\n\\n25 \\n\\n\\x0c4.3 Business logic \\n\\nThe  business  logic  of  the  system  is  separated  in  a  clear  way  both  from  the  database  and  the \\npresentation layer. The following class diagrams  describe the business logic of the system. Figure \\n4.2 des7cribes the implementation diagram and figure 4.3 the implemented interface. \\n\\n \\n\\nFigure 4.3 Business Logic Implementation \\n\\n26 \\n\\n\\x0cFigure 4.4 Business Logic Interface \\n\\n27 \\n\\n\\x0c4.4 Web Interface \\n\\nThe  business  logic  of  the  system  is  separated  in  a  clear  way  both  from  the  database  and  the \\npresentation layer. The following class diagrams describe the web interface of the system. \\n \\n \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nFigure 4.5 Web interface \\n\\n \\n\\n28 \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nFigure 4.6 Web interface \\n\\n29 \\n\\n\\x0cChapter 5 \\n\\n \\n\\nImplementation \\n \\nThis  chapter  gives  a  description  of  the  system  implementation.  User  and  administrator  interfaces \\nare described along with an implementation description of each component. \\n\\n5.1 Overview \\n\\nThe system includes a user interface with the following options: Engage in a conversation, produce \\na  customised  link  and  leave  feedback.  The  administration  interface  provides  the  options  to  add \\nquestions, answers and keywords. View, update and delete questions, answers and keywords. View \\nand  delete  logs.  View  and  delete  feedback.  The  three  tier  architecture  has  been  selected  and \\ndescribed in section 2.4. This particular architecture gives the ability to develop a web application \\nin  a  systematic  way,  where  the  presentation  layer  is  separated  from  the  application  layer  and  the \\ndata  layer.  This  makes  sure  that  when  a  change  occurs  in  one  of  the  tiers  the  other  two  are  not \\naffected.  \\n \\n\\n5.2 Database layer implementation \\n\\nThe first layer of the three tier architecture is the data layer, which in this case is a database. This \\ncan be achieved by using and configuring Hibernate and Spring. First a number of Java persistent \\nobjects and hibernate mapping files need to be created, followed by their configuration in the spring \\ncontainer,  regarding  the  creation  of  the  database,  the  connection  and  the  associated  spring  data \\naccess objects. \\n \\n\\n5.2.1 Hibernate mapping \\n\\nMapping files have been created and used by hibernate to persist the objects to the database. Each \\nfile must be associated with one Java class. The database has five tables, each represented by a Java \\npersistent object and its XML mapping. The code below represents a Java persistent object and its \\nassociating mapping XML file, used for the creation of the feedback table. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n30 \\n\\n\\x0c \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n \\n\\nFigure 5.1 Java persistent object \\n\\nFigure 5.2 XML mapping \\n\\n5.2.2 Hibernate configuration \\n\\nTo create the database the spring container needs to be aware of the mapping files. This is achieved \\nin  the  applicationcontext.xml  in  a  bean  called  sessionFactory.  The  figure  below  represents  the \\nconfiguration.  \\n \\n\\n31 \\n\\n\\x0cFigure 5.3 Hibernate mapping configuration \\n\\n \\n\\nIn the configuration the database is specified as PostgreSQL and the mapping resources property is \\nused to declare the mapping files. \\n \\nIn the applicationcontext.xml the connection properties of the database is also included. The bean \\nwith id dataSource is used to configure the connection. The properties include a driver name, a url, \\na  user  name,  a  password  and  other  connection  related  details.  The  figure  below  represents  the \\nconnection properties. \\n \\n\\nFigure 5.4 Hibernate database configuration \\n\\n \\n\\n5.2.3 Data access objects \\n\\nA  data  access  object  is  comprised  from  an  interface  and  an  implementation  of  that  particular \\ninterface. Each DAO file is associated with each hibernate mapping file and its associated persistent \\nobject. An example of the feedback data access object interface and its implementation is described \\nbelow. \\n \\n  \\n \\n \\n \\n\\n32 \\n\\n\\x0c \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nFigure 5.5 Feedback data access object interface \\n\\nFigure 5.6 Feedback data access object implementation \\n\\n \\n\\n5.3 Business logic layer implementation \\n\\nThe business or application logic of a system is considered to be one of the most important aspects. \\nThis section aims to give a clear understanding of the logic.  \\n \\n\\n5.3.1 Business analysis \\n\\nThe business logic of the system is made up using two classes, chatbotService which is an interface \\nand  chatbotServiceImplementation  which  is  the  implementation  of  the  interface.  Every  functional \\npart  of  the  system  is  implemented  using  an  appropriate  method.  Each  method  is  called  from  the \\n\\n33 \\n\\n\\x0cpresentation  layer  when  this  is  necessary.  In  addition  spring  security  has  been  used  to  ensure  that \\nusers  have  access  only  to  methods  that  they  are  supposed  to.  The  following  image  represents  a \\npartial part of the interface. \\n \\n \\n\\n \\n\\nFigure 5.7 Chatbot service interface \\n\\n \\n \\nThe following image gives a partial description of the sentence parsing implementation method as \\nfound in the chatbot service implementation. \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nFigure 5.8 Sentence parse partial implementation \\n\\n \\n\\n \\n \\nThe following image gives a description of the spring security used in the system. It is described in \\nthe  chatbot  service  security  bean  in  the  applicationcontext.xml.  In  the  properties  it  is  specified \\nwhich methods can be accessed by the admin only. \\n \\n\\n34 \\n\\n\\x0cFigure 5.9 Spring security \\n\\n \\n\\n5.4 Presentation layer implementation \\n\\nThis sections aims to give an outline of how the interface was created and how it interacts with the \\nbusiness logic. \\n \\n\\n5.4.1 Presentation analysis \\n\\nThe presentation layer has been developed using Apache wicket and HTML, which aims to give a \\nclear separation between the template and the business logic. This is achieved by using appropriate \\nwicket ids in the HTML code and give actions to these components in the Java code were the id is \\nassociated.  Moreover  when  an  action  needs  to  be  provided  the  appropriate  method  is  called  from \\nthe  chatbot  service  interface.  In  order  to  achieve  that  a  bean  needs  to  be  declared  in  the \\napplicationcontext.xml  as  shown  below.  The  bean  is  called  wicket  application  and  then  chatbot \\nservice  is  instantiated.  This  gives  us  the  ability  to  call  the  chatbot  service  interface  in  the \\npresentation and subsequently call any of the authorised methods. \\n \\n\\nFigure 5.10 Spring Integration with Wicket  \\n\\n \\n\\n5.5 User Interface \\n\\nThe final system includes a web interface with the following options: Chat, Login, Leave feedback, \\nadd  information,  update  information,  delete  information,  view  and  delete  feedback  and  view  and \\ndelete logs. This sections shows the interface of the system. \\n\\n5.5.1 Chatbot interface \\n\\n \\nThe home page of the system where the user can engage in a conversation is shown below. \\n \\n\\n35 \\n\\n\\x0cFigure 5.11 Chatbot \\n\\n \\n \\n\\n5.5.2 Leave feedback \\n\\nThe feedback page of the system where the user can leave a mark and feedback message is shown \\nbelow. \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\nFigure 5.12 Feedback \\n\\n36 \\n\\n\\x0c5.5.3 Add information \\n\\nThe  add  information  page  of  the  system  where  the  administrator  can  add  questions,  answers  and \\nkeywords is shown below. \\n \\n\\n \\n\\n5.5.4 View information \\n\\nFigure 5.13 Add information \\n\\nThe  view  information  page  of  the  system  where  the  administrator  can  view,  update  and  delete \\nquestions, answers and keywords is shown below. \\n \\n\\nFigure 5.14 View information \\n\\n37 \\n\\n\\x0c5.5.5 View feedback \\n\\nThe  view  feedback  page  of  the  system  where  the  administrator  can  view  and  delete  feedback \\nmessages is shown below. \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nFigure 5.15 View feedback \\n\\n \\n \\n\\n5.5.6 View logs \\n\\nThe  view  logs  page  of  the  system  where  the  administrator  can  view  and  delete  log  messages  is \\nshown below. \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nFigure 5.16 View logs \\n\\n38 \\n\\n\\x0cChapter 6 \\n \\n\\nTesting and Evaluation \\nThis chapter is divided into two sub chapters. The first part describes the testing of the system and \\nthe test cases. The second part describes how the system was evaluated. \\n \\n\\n6.1 Testing \\n\\nIt  is  vital  in  a  software  system  that  it  behaves  as  expected.  To  ensure  that  the  system  and  its \\ncomponents will behave as required a number of tests was taken. \\n\\n6.1.1 Overview \\n\\nEvery unit of the system was tested to make sure that the actual result of its operation was what we \\nexpected. Furthermore when every unit was integrated with  other units during the development  it \\nwas tested again to check its operation with other system units. \\n\\n6.1.2 Unit and integration testing \\n\\nA  testing  strategy  known  as  unit  and  integration  testing  has  been  used  to  check  that  the  system \\nbehaves as expected. The testing strategy was based on the functionality and the requirements of the \\nsystem. The testing table below gives a precise description of the tests that took place. \\n \\n \\n\\nTest case id \\n\\nTest description  Expected result \\n\\nActual result \\n\\nResult \\n\\nPASS \\n\\nPASS \\n\\nAsk a question \\n\\nProvide answer \\n\\nAnswer provided \\n\\nAsk a question \\nwith a spelling \\nerror \\n\\nSpell checker \\nidentified the error \\n\\nSpell checker \\nidentified the error \\n\\nEnter an invalid \\nsentence \\n\\nSentence parser \\nidentified the error \\n\\nSentence parser \\nidentified the error \\n\\nPASS \\n\\nUser satisfied by \\nthe answer \\n\\nUser not satisfied \\nby the answer \\n\\nUser submits \\nfeedback mark \\nand message \\n\\nAdministrator logs \\nin \\n\\nNothing happened  Nothing happened \\n\\nPASS \\n\\nA log of the \\nquestion and \\nanswer kept and a \\nrelevant link \\nprovided \\n\\nA log of the \\nquestion and \\nanswer kept and a \\nrelevant link \\nprovided \\n\\nFeedback \\nsubmitted \\n\\nFeedback \\nsubmitted \\n\\nPASS \\n\\nPASS \\n\\nLog in successful  Log in successful \\n\\nPASS \\n\\nTC001 \\n\\nTC002 \\n\\nTC003 \\n\\nTC004 \\n\\nTC005 \\n\\nTC006 \\n\\nTC007 \\n\\nTC008 \\n\\nAdministrator \\n\\nInformation added  Information added \\n\\nPASS \\n\\n39 \\n\\n\\x0cadds new \\ninformation \\n\\nsuccessfully \\n\\nsuccessfully \\n\\nAdministrator \\nviews information \\n\\nInformation \\ndisplayed \\n\\nInformation \\nupdated \\n\\nInformation \\ndisplayed \\n\\nInformation \\nupdated \\n\\nAdministrator \\nupdates \\ninformation \\n\\nAdministrator \\ndeletes \\ninformation \\n\\nAdministrator \\nviews logs \\n\\nAdministrator \\ndeletes a log \\n\\nAdministrator \\nviews feedback \\n\\nPASS \\n\\nPASS \\n\\nPASS \\n\\nInformation \\ndeleted \\n\\nInformation \\ndeleted \\n\\nLogs displayed \\n\\nLogs displayed \\n\\nPASS \\n\\nLog deleted \\n\\nLog deleted \\n\\nThe overall rank \\nof the system and \\nthe feedback \\nmessages \\ndisplayed \\n\\nThe overall rank \\nof the system and \\nthe feedback \\nmessages \\ndisplayed \\n\\nPASS \\n\\nPASS \\n\\nTC009 \\n\\nTC010 \\n\\nTC011 \\n\\nTC012 \\n\\nTC013 \\n\\nTC014 \\n\\nTC015 \\n\\nTC016 \\n\\nAdministrator \\ndeletes a feedback \\n\\nAdministrator logs \\nout \\n\\nFeedback deleted  Feedback deleted \\n\\nPASS \\n\\nLog out succeeded  Log out succeeded \\n\\nPASS \\n\\nTable 6.1 Unit and Integration testing \\n\\n6.1.3 Performance testing \\n\\nEvery  component  developed  passed  through  a  performance  test  to  ensure  that  the  system  meets \\ncertain  performance  requirements.  Finally  when  each  unit  was  developed  and  tested  then  it  was \\nintegrated  with  the  rest  of  the  system  to  do  a  performance  test.  The  figure  below  describes  the \\nstages of performance testing [19].   \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\nFigure 6.1 Performance testing stages \\n\\n40 \\n\\n\\x0cEvery  component  of  the  system  went  through  each  stage  of  the  approached  methodology.  It  was \\nidentified that each unit behaved differently as a single entity and differently when it was integrated \\nwith other units in respect of speed. The test environment plays a crucial role on the performance of \\nthe system. A different result occurs when the system runs on development mode and a different one \\nwhen  the  system  has  been  deployed.  Moreover  the  technical  specifications  of  the  server  that  the \\nfinal system has been deployed play their role as well.  \\n\\n6.1.4 Security testing \\n\\nSecurity testing refers to ensuring that the system will protect its data from unauthorized access and \\nmodification and that it will continue to behave as expected. Different security concepts have been \\nused  to  ensure  that  the  system  is  secure. Authentication  has  been  used  to  confirm  the  identity  of \\nsomeone  being  either  a  user  or  an  administrator. Authorisation  has  been  used  to  ensure  that  users \\nhave access to pages that they are supposed to and that  the administrator has an overall control of \\nthe system. Access control has been used to achieve that. To make sure that the system is safe from \\nattacks, including SQL injection and XSS, a number of tests took place and are shown below.     \\n \\n \\n\\nTest case id \\n\\nTest description  Expected result \\n\\nActual result \\n\\nTC001 \\n\\nTC002 \\n\\nTC003 \\n\\nTC004 \\n\\nTC005 \\n\\nUser knows the \\nexact link of the \\nview information \\npage and tries to \\nsubmit it directly \\n\\nUser tries to \\nsubmit an XSS \\nattack script \\nthrough the text \\narea \\n\\nUser tries to \\nsubmit an SQL \\ninjection attack \\nscript through the \\ntext area \\n\\nThe user submits \\nrandom user name \\nand password \\n\\nThe user gains \\naccess to the \\nbusiness logic and \\ntries to execute \\ncertain methods  \\n\\nSystem redirects \\nto the log in page \\n\\nSystem redirects \\nto the log in page \\n\\nThe system skips \\nthe relevant \\ncharacters and \\nstops the attack  \\n\\nThe system skips \\nthe relevant \\ncharacters and \\nstops the attack \\n\\nThe system skips \\nthe relevant \\ncharacters and \\nstops the attack \\n\\nThe system skips \\nthe relevant \\ncharacters and \\nstops the attack \\n\\nThe system rejects \\nthe log in \\n\\nThe system rejects \\nthe log in \\n\\nThe user is not \\nauthorised to do \\nthat and it is \\nrejected \\n\\nThe user is not \\nauthorised to do \\nthat and it is \\nrejected \\n\\nTable 6.2 Security testing \\n\\nResult \\n\\nPASS \\n\\nPASS \\n\\nPASS \\n\\nPASS \\n\\nPASS \\n\\n \\n \\n \\n \\n \\n \\n\\n41 \\n\\n\\x0c6.2 Evaluation \\n\\nEvaluation  is  the  process  where  we  collect  and  analyse  information  in  order  to  find  out  if  the \\nsoftware system developed is effective and efficient. \\n \\n\\n6.2.1 Overview \\n\\nA major part of this project was to build a usable system, based on real world requirements and to \\nevaluate  it  using  potential  students.  The  system  managed  to  satisfy  every  requirement,  including \\nnew requirements introduced after feedback received from potential students. However due to time \\nconstraints it was not possible to develop an Android part of the project. \\n\\n6.2.2 Question categories \\n\\nFor  the  purposes  of  analysis,  the  types  of  questions  that  a  user  could  possibly  ask  have  been \\ncategorised into four main categories.  \\n \\nRelevant question: A relevant question refers to a valid question. The answer exists in the database \\nand the algorithm is able to retrieve it. \\n \\nIrrelevant question: An irrelevant question refers to a question that is not related to the admissions \\nprocesses and has not been included in the database. \\n \\nNo response question: A no response question refers to a valid question, however the answer does \\nnot exist at the moment in the database. \\n \\nPoor response question: A poor response would be that the answer exists in the database but the \\nalgorithm would fail to retrieve it and would come back with an incorrect or irrelevant reply.   \\n \\nExamples of each of these categories are given below. \\n \\nA relevant question that a user asked:  \\xe2\\x80\\x9cWhat are the entry requirements for the computer security \\nMSc?\\xe2\\x80\\x9d \\nThe reply was correct: \\xe2\\x80\\x9cTo enter the programme, you need to have at least an Upper Second Class \\n(2.1) degree or an international equivalent in Computer Science or a closely related discipline and a \\nsolid foundation in programming.\\xe2\\x80\\x9d \\n \\nAn irrelevant question asked: \\xe2\\x80\\x9cSize of university grounds in square miles?\\xe2\\x80\\x9d \\nThe reply was irrelevant: \\xe2\\x80\\x9cYes, with most of the major players such as HP,IBM and Logica. Which \\nis the reply to the question: Are there any links with the industry?\\xe2\\x80\\x9d \\n \\nA no response question asked: \\xe2\\x80\\x9cWhere is the toilet?\\xe2\\x80\\x9d \\nThe  reply  was  irrelevant:  \\xe2\\x80\\x9cThe  starting  dates  for  the  programme  are:  2011/12:  26th  September \\n2011, 2012/13: 24th September 2012.It is not possible to join the programme in January or at any \\nother time after the start date.\\xe2\\x80\\x9d \\n \\nA poor response question asked: \\xe2\\x80\\x9cIs it OK to pay my fees using a visa?\\xe2\\x80\\x9d \\nThe reply was poor: \\xe2\\x80\\x9cYes, unless you are from an EU or an EEA country. If you require a visa and \\nhave accepted your offer you will receive a cas statement by email soon. \\nThe answer for the question existed on the database; however the retrieved answer is the answer to \\nthe question: Do I need a visa to study in the UK? \\xe2\\x80\\x9c \\n\\n42 \\n\\n\\x0c6.2.3 User evaluation \\n\\nThe  system  was  evaluated  by  asking  current  applicants  from  Masters  and  undergraduate \\nprogrammes to test it over the WWW. The messages were sent to MSc applicants with: \\n \\n\\n1.  Unconditional offers (not yet accepted) \\n2.  Unconditional offers (accepted) \\n3.  Conditional offers (not yet accepted) \\n4.  Conditional offers (accepted) \\n5.  Applications that still required references and/or transcripts \\n\\n \\nMessages were sent to two sets of MSc applicants:  \\n \\n\\n1.  Feasibility study: 108 emails were sent to 74 applicants with surnames beginning with S or \\n\\nT. \\n\\n2.  Main evaluation: 770 emails were sent to 537 applicants with surnames beginning with any \\n\\nother letter. \\n\\n \\nMessages were sent in two sets of undergraduate applicants: \\n \\n\\n1.  Feasibility study: Emails were sent to 18 applicants with surnames beginning with S or T \\n\\nand who had offers. \\n\\n2.  Main evaluation: Emails were sent to 75 applicants with surnames beginning with any other \\n\\nletter and who had unconditional offers. \\n\\n \\nApproximately eighty MSc students used the system and left feedback either using the form on the \\nwebsite or by email. Unfortunately only two undergraduate students used the system. Both positive \\nand  negative  messages  were  received  from  the  users.  Most  of  the  messages  were  about  the \\nknowledge of the chatbot and not being able to answer a number of questions, because an answer \\ndid not exist in the database. Moreover a small number of questions could not be answered by the \\nchatbot because the algorithm could not retrieve the answer. Other replies were referring to the fact \\nthat the chatbot can not answer specific questions after acceptance and that the user interface is not \\nof a satisfying level. \\n \\nA selection of the feedback messages can be found in appendix D. \\n \\n\\n6.2.4 Survey \\n\\nThis  section  aims  to  analyse  the  results  collected  from  the  logs  and  the  feedback  from  the  period \\nthat  the  system  was  online.  There  are  two  result  sets  collected  from  both  undergraduate  and \\npostgraduate students. The initial result, that is based on the results collected from an initial 7 day \\nperiod and from applicants with surnames beginning with S or T, the results of which were used to \\nimprove the system and upload it a second time to collect feedback from applicants with surnames \\nbeginning with any other letter. The final system was hosted for 15 days.   \\n \\nFeedback was collected by the feedback page of the system, where the user could give a 1-5 scale \\nscore and leave comments if desired, and by emails sent by the applicants. \\n \\nThe  tables  and  pie  charts  in  the  initial  results  and  final  results  sections  below,  show  the  user \\nresponse when they felt the system had failed. They do not represent the performance of the whole \\nsystem. \\n\\n43 \\n\\n\\x0c6.2.4.1 Initial results \\n\\nThrough  the  feedback  page  of  the  system  four  messages  were  received,  with  an  overall  score  of \\n2.75.  Furthermore  another  four  messages  were  received  by  email  that  only  contained  suggestions \\nabout the system.  \\n \\nAn  assumption  has  been  made  through  the  feedback  received  and  by  the  number  of  logs  that \\napproximately 10 people have used the system. The table below  represents  the categories  and the \\nnumber of questions not answered by the system for each category. \\n \\n\\nIrrelevant \\n\\n12 \\n\\nNo response \\n\\nPoor response \\n\\n12 \\n\\n8 \\n\\n \\nThe pie  chart below  represents  the percentages from  the number of logs  from the database of the \\nsystem, during its first use. \\n\\nTable 6.3 Initial logs \\n\\n \\n\\nFigure 6.2 Initial log results \\n\\n \\n \\nAfter  the  initial  use  and  by  careful  consideration  of  the  feedback  messages  a  number  of  changes \\nwere made and a final system was introduced. The main changes after the initial evaluation of the \\nsystem were: \\n \\nNatural language parsing: It was decided that the use of a sentence parser was necessary for the \\nsystem,  the  parser  would  analyse  the  structure  of  a  sentence  and  identify  that  a  noun  and  a  verb \\nexists,  thus  avoiding  submitting  syntactically  incorrect  questions  to  the  system.  After  careful \\nconsideration the Stanford parser was chosen. \\n \\nDatabase enrichment: After the initial use it was clear that the information in the database was not \\n\\n44 \\n\\n38%38%25%IrrelevantNo responsePoor response\\x0cof  a  satisfactory  level  and  more  data  had  to  be  added  to  it. A  high  number  of  relevant  admission \\nquestions found in the logs and a response did not exist in the database. \\n\\n6.2.4.2 Final results \\n\\nThrough the feedback page of the system thirteen messages were received, with an overall score of \\n2.53.  Furthermore  another  twenty  two  messages  were  received  by  email  that  only  contained \\nsuggestions about the system.  \\n \\nAn  assumption  has  been  made  through  the  feedback  received  and  by  the  number  of  logs  that \\napproximately 50 people have used the system. The table below  represents  the categories  and the \\nnumber of questions not answered by the system for each category.  \\n \\nThere  were  three  cases  for  which  a  relevant  answer  was  provided,  however  the  user  was  not \\nsatisfied.  \\n \\n\\nRelevant \\n\\nIrrelevant \\n\\nNo response \\n\\nPoor response \\n\\n3 \\n\\n62 \\n\\n61 \\n\\n31 \\n\\n \\n \\nThe pie  chart below  represents  the percentages from  the number of logs  from the database of the \\n\\nTable 6.4 Final logs \\n\\nsystem, during the final use. \\n\\nFigure 6.3 Final log results \\n\\n \\nThe suggestions from the final feedback have been taken into consideration and the most important \\nones that will make a better system.  These include: \\n \\nUser Interface: The user interface should be more interactive and include more details. \\n \\n\\n45 \\n\\n2%39%39%20%RelevantIrrelevantNo responsePoor response\\x0cDatabase enrichment: After the initial use it was clear that the information in the database was not \\nof  a  satisfactory  level  and  more  data  had  to  be  added  to  it. A  high  number  of  relevant  admission \\nquestions and answers were added, however there is still much to be added. \\n \\n \\nPrecise answers: Answers must be more specific and include more details. \\n \\nInformation colour: The colour of the question and the answer must be different in the chatbot text \\narea. \\n \\nMore details regarding future work can be found in section 7.3. \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n46 \\n\\n\\x0cChapter 7 \\n \\n\\nSummary \\nThis  chapter  refers  to  the  summary  and  conclusion.  Firstly  it  discusses  the  positive  points  of  the \\nsystem, followed by its limitations, future work and conclusions. \\n\\n7.1 Achievements \\n\\nA  number  of  positive  points  were  identified. A  working  system  was  developed  using  Java  and  a \\nnumber  of  third  party  libraries.  The  system  was  uploaded  on  two  occasions  on  the  schools  web \\nserver. The  first  time  was  used  by  a  small  number  of  potential  students,  which  left  feedback  and \\nsome  modifications  took  place.  The  second  time  a  larger  number  of  potential  students  used  the \\nsystem and feedback was taken. An interesting part was the logs kept by the system that revealed \\nhow people interact with a chatbot.  \\n \\nThe  fact  that  the  system  used  a  spell  checker  is  considered  to  be  a  positive  aspect,  since  the \\npossibility of retrieving  the wrong  answer due to  an error is  reduced. Moreover a sentence parser \\nwas used to  inform  the  user if  a sentence is  valid or not,  before submitting. A customized google \\nsearch functionality that provides a link relevant  to the question, if the user was not happy by the \\nanswer  and  a  log  of  the  question  and  the  answer  is  kept  at  this  stage.  Moreover  the  user  has  the \\noption to submit a feedback message. Another positive thing is that the web application is dynamic \\nand has an administrator interface. An administrator has the option to log in and make appropriate \\nchanges in a number of ways. The administrator has the option to add information to the database, \\nsuch  as  questions,  answers  and  keywords.  Information  already  in  the  system  can  be  viewed, \\nupdated and deleted. Also the option to view feedback messages and delete them is available. The \\noption to view and delete logs is also available. Another positive aspect is that the system is secure \\nfrom unauthorized access and from various attacks. \\n \\nThe  results  gathered  from  the  users  helped  to  identify  the  limitations  of  the  system  and  make \\nchanges for a number of them, when time permitted. The evaluation results in combination with the \\ntesting strategy used helped to make the system as effective as possible.   \\n\\n7.2 Limitations \\n\\nA limitation on the algorithm of the system was found after evaluation and testing. The fact is that \\nthe  possibility  of  not  retrieving  the  correct  answer  exists. A  small  number  of  the  logs  were  about \\nthis problem. Moreover potential students want to know about a different number of things and not \\neverything is covered in the database. A problem is that the system has no memory and when a user \\nasks the same question for a number of times the answer remains the same. Anaphora resolution is \\nnot  present,  which  would  be  useful  in  some  situation  when  a  user  refers  to  a  previous  question. \\nHowever anaphora resolution would require a chatbot memory to be available. A limitation in the \\nspell checking is that the system will suggest a word for a spelling error; however the user will have \\nto  change  that  manually  and  submit  the  question  again.  Although  there  might  be  a  number  of \\nquestions  that  do  not  require  a  verb  the  sentence  parser  will  reject  every  question  that  does  not \\ncontain  a  noun  and  a  verb  and  the  user  will  have  to  restructure  the  sentence  and  resubmit  it. \\nRegarding the Google customised search the links provided are not hyper linked. The fact that the \\nsystem is usable was an achievement however the user interface should be improved and be more \\ninteractive.  \\n\\n47 \\n\\n\\x0c7.3 Future work \\n\\nA number of different algorithms should be implemented, tested and evaluated. Template matching \\nshould  be  a  high  priority  algorithm.  Furthermore  algorithms  could  be  used  in  combination  and \\nspecify  a  percentage  for  each.  A  possible  solution  would  be  to  upload  to  the  web  server  each \\nimplementation  separately  and  collect  feedback  from  users.  This  will  make  easier  to  identify  the \\nlimitations of each algorithm and come with the best solution that can also be a combination of two \\nor  more  algorithms.  There  is  also  a  different  number  of  factors  that  should  be  taken  into \\nconsideration  except  the  retrieval,  such  as  speed  and  compatibility.  Also  more  advanced  natural \\nlanguage processing techniques if used will help to identify a possible answer. Anaphora resolution \\nwill help to identify when a user refers to a previous question or answer, which requires the user of \\na chatbot memory.  A number of techniques such as statistical parsing and analysis of the sentence \\nstructure  will  help  to  identify  the  meaning  of  a  sentence  and  provide  better  answers.  Knowledge \\nbased  information  retrieval  will  help  to  find  an  answer  to  a  particular  question  in  the  frequently \\nasked questions of the admissions website in  the case that the  algorithm will not  find a particular \\nanswer. Also  the  customised  google  search  helps  in  that  direction  with  the  disadvantage  that  in  a \\nnumber of cases the link provided might not be the one required.  \\n \\nThe  logs  that  the  system  keeps  is  an  important  aspect  of  the  system,  which  will  help  the \\nadministrator to find user questions not answered because the particular query and answer does not \\nexist in the database. It is therefore important to maintain a database updated continuously that aims \\nto  cover  most  of  the  users\\'  queries,  both  undergraduate  and  postgraduate.  It  would  be  helpful  to \\ncreate a statistics page and database table where the number of correctly answered and not correctly \\nanswered questions will be stored. It is very helpful to find how efficient the system is.   \\n \\nThere  is  also  a  number  of  small  changes  that  could  improve  the  system  in  general  and  do  not \\ndepend  on  the  natural  language  processing  used.  The  use  of  a  customised  search  using  an  actual \\nsearch text area will be really useful for the user. The interface of the system should be improved \\nand become more interactive and additional information such as links to the university, the school \\nand to the admissions FAQs should be added.  \\n \\nRegarding  the  spell  checker  it  would  be  more  useful  when  an  error  is  found  to  provide  a  hyper \\nlinked  version  of  the  sentence  with  the  error  fixed  so  the  user  can  have  a  better  and  faster \\nexperience. Also  the  case  when  the  user  asks  more  than  one  question  at  the  same  time  should  be \\nconsidered and the possibilities of using certain rules and provide separate answers or informing the \\nuser to submit only one question at at time.  Moreover a more comprehensive dictionary should be \\nused to cover more varieties of English and not U.K only. \\n \\nThe option to make a chatbot to learn from users exists if certain rules are used and can be divided \\ninto  two  categories.  Learn  and  use  only  for  the  current  session  and  if  the  user  asks  the  same \\nquestion  or  a  similar  then  provide  the  answer.  Moreover  this  information  can  be  saved  to  the \\ndatabase and be used to reply to future users. This can be done by just saving or by saving and then \\ncomparing  to  other  saved  information  and  decides  if  it  is  correct  or  not.  However  this  is  not \\nrecommended for an admissions system where every answer must be precise. \\n \\nAlso  a  high  number  of  overseas  students  wish  to  apply  to  the  university  and  at  the  time  of  the \\napplication the level of English that they process might not be sufficient for a conversation. There \\nare  two  different  aspects  regarding  the  use  of  foreign  languages.  The  user  might  not  be  able  to \\nprovide a syntactically correct sentence and the parser will reject it. However the keyword matching \\nwill identify the important  words if the sentence goes through the sentence parser to  the keyword \\nmatching.  \\n\\n48 \\n\\n\\x0c7.4 Conclusions \\n\\nThe  main  objectives  of  the  project  were  to  develop  an  algorithm  that  will  be  used  to  identify \\nanswers related to user submitted questions. To develop a database were all the related data will be \\nstored and to develop a web interface. The web interface developed had two parts, one for simple \\nusers and one for the administrator.  \\n \\nThe  following  objectives  were  subsequently  met.  A  background  research  took  place,  which \\nincluded an overview of the conversation procedure and any relevant chatbots available. A database \\nwas  developed,  which  stores  information  about  questions,  answers,  keywords,  logs  and  feedback \\nmessages.  A  usable  system  was  designed,  developed  and  deployed  to  the  web  server  on  two \\noccasions.  An  evaluation  took  place  from  data  collected  by  potential  students  of  the  University. \\nAlso  after  received  feedback  from  the  first  deployment,  extra  requirements  were  introduced  and \\nimplemented.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n49 \\n\\n\\x0cReferences \\n\\n1.  Cawsey, Alison, 1998. The Essence of Artificial Intelligence. London: Pearson and Prentice \\n\\nHall \\n\\n2.  Negnevitsky, Michael, 2005. Artificial Intelligence: A guide to intelligent systems, 2nd ed. \\n\\nLondon: Pearson  \\n\\n3.  Tong Ka Iok, Kent, 2010. Enjoying Web Development with Wicket. 2nd ed. London: \\n\\nlulu.com \\n\\n4.  Marnie L Hutcheson 2003. Software testing fundamentals. London: John Wiley and sons   \\n5.  Michael L. Mauldin, Carnegie Mellon University Centre for Machine Translation \\n\\nChatterbots, Tinymuds, and the Turing Test Entering the Loebner Prize Competition. \\nAvailable at: http://www.aaai.org/Papers/AAAI/1994/AAAI94-003.pdf                     \\n[Accessed on 13 June 2011] \\n\\n6.  Artificial Intelligence Foundation. A.L.I.C.E. Available at: http://alice.pandorabots.com/ \\n\\n[Accessed 15 June 2011] \\n\\n7.  Eliza Chat Bot. Available at: http://nlp-addiction.com/eliza/  \\n\\n \\n\\n \\n\\n   \\n\\n[Accessed 15 June 2011] \\n\\n8.  University of Birmingham, School of Computer Science admissions. Available at: \\n\\nhttp://www.cs.bham.ac.uk/admissions/ [Accessed 15 June 2011] \\n\\n9.  School of Computer Science University of Birmingham. Internet Computing Workshop part \\n\\n2 by Behzad Bordbar. Available at: http://www.cs.bham.ac.uk/~bxb/Teaching.html#iss \\n[Accessed 20 June 2011] \\n\\n10. W3Schools HTML tutorial. Available at: http://www.w3schools.com/html/ [Accessed 20 \\n\\nJune 2011] \\n\\n11. The waterfall model. Available at: http://www.selectbs.com/analysis-and-design/what-is-the-\\n\\nwaterfall-model [Accessed 20 June 2011] \\n\\n12. Content with style. A CSS framework by Mike Stenhouse. Available at: \\n\\nhttp://www.contentwithstyle.co.uk/content/a-css-framework/ [Accessed 20 June 2011] \\n\\n13. Apache Wicket component reference. Available at: http://wicketstuff.org/wicket14/compref/ \\n\\n[Accessed 21 June 2011] \\n\\n14. Google Java spelling api. Available at: http://code.google.com/p/google-api-spelling-java/ \\n\\n[Accessed 25 June 2011] \\n\\n15. Google search Java sdk. Available at: http://code.google.com/p/google-search-java-sdk/ \\n\\n[Accessed 1 July 2011] \\n\\n16. Spring Security and Wicket-auth-roles. Available at: \\n\\nhttps://cwiki.apache.org/WICKET/spring-security-and-wicket-auth-roles.html \\n[Accessed 7 July 2011] \\n\\n17. Lingpipe Java NLP Library. Available at: www.alias-i.com/lingpipe/ \\n\\n[Accessed 10 July 2011] \\n\\n18. The Stanford Library. Available at: http://nlp.stanford.edu/ \\n\\n[Accessed 15 August 2011] \\n\\n19. Performance testing for Web Applications. Available at: http://msdn.microsoft.com/en-\\n\\nus/library/bb924375.aspx \\n[Accessed 25 August 2011] \\n\\n20. The Jazzy spell checker Library. Available at: http://jazzy.sourceforge.net/ \\n\\n[Accessed 25 August 2011] \\n\\n21. Bayan Abu Shawar, Eric Atwell, Chatbots: Are they really useful? Penn state University. \\n\\nAvailable at: http://citeseerx.ist.psu.edu/viewdoc/download \\ndoi=10.1.1.106.1099&rep=rep1&type=pdf \\n[Accessed on 26 August 2011] \\n\\n22. Ehab El-agizy, Moustafa Zamzam, AI Chatbots developing. Available at: \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n50 \\n\\n\\x0c \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n\\nhttp://www.infradrive.com/downloads/articles/Article1.pdf \\n[Accessed on 27 August 2011] \\n\\n51 \\n\\n\\x0cAppendix A Project proposal \\n\\n \\n1. Introduction \\nThis document covers an introduction to the project including the context, a description of aims and \\nobjectives and a sound plan for completion. \\n \\n2. Context \\nExperience has shown that applicants are more likely to choose to come to a department where they \\nbelieve that they are treated as a valued individual. The opportunity to communicate with on a one-\\nto-one basis is  highly valued. However with  many  hundreds of applications each  year, one-to-one \\nconversations are not feasible in most cases.  \\n \\nAlthough the admissions process works properly as it is, it is very difficult and time consuming to \\ncontact a member of staff of the university. However, the problem would be partially solved if  the \\napplicant  could  talk  to  a  convincing  chatbot,  able  to  respond  to  their  concerns  with  information \\nabout  admissions,  booking  accommodation,  paying  fees  in  instalments  and  what  pre-sessional \\ncourses are on offer. \\n \\nBy the end of the development, the chatbot application will be able to communicate with a user in a \\nway similar to the following: \\n \\nChat Bot: Hello how can I help you? \\nUser: What is the minimum score of IELTS required for entry into MSc Computer Science? \\nChat Bot: The minimum requirement is a total score of 6.0 with no less that 5.5 in any section. \\nUser: What other test is accepted by the department? \\nChat Bot: We also accept TOEFL and the certificate of Proficiency of the University of Cambridge. \\n \\n3. Aims and Objectives \\nThe  aim  of  this  project  is  to  contribute  to  the  solution  of  the  problem  of  direct  communication \\nbetween applicants and the university. \\n \\nThe main objectives of the project are as follows: \\n \\n \\nInformation resource: To develop an information resource were all the relevant information about \\nthe questions and the answers will be stored.  \\n \\nAlgorithm: To develop an algorithm that will be able to identify meaningful keywords in the users\\' \\ninput and provide a suitable answer. \\n \\nInterface: To develop a web interface which aims to give the ability to potential students and their \\nfamilies to submit questions in a chatbot and get convincing replies. The web system can be hosted \\non any computer that supports an operating system on which the Java Virtual machine will be able \\nto load. Furthermore the Apache Wicket and the Google spelling libraries should be available. The \\nporting of the application to the Android platform will require the use of the Android simulator and  \\nthe Google spelling library. \\n \\n \\n \\n \\n\\n52 \\n\\n\\x0c4. Project Plan \\n \\n\\n \\n\\nWeek 1 \\n13-19 June \\n\\nWeek 2 \\n20-26 June \\n\\nWeek 3 \\n27 June \\xe2\\x80\\x93 3 July \\n\\nWeek 4 \\n4-10 July \\n\\nWeek 5 \\n11-17 July \\n\\nWeek 6 \\n18-24 July \\n\\nWeek 7 \\n25-31 July \\n\\nWeek 8 \\n1-7 August \\n\\nWeek 9 \\n8-14 August \\n\\nWeek 10 \\n15-21 August \\n\\nWeek 11  22-28 August \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nActivities \\n\\n1.  Research about chatbot systems in general \\n2.  Use ELIZA and A.L.I.C.E  \\n3.  Research NLP in Java and try Open Source spell checkers \\n\\n1.  Start developing the web interface \\n2.  Research how chatbots react to sub questions \\n\\n1.  Begin developing the algorithm and create a basic version \\n2.  Connect the algorithm with the web interface \\n3.  Test the basic system and collect feedback for improvement \\n\\n1.  Continue developing the algorithm \\n2.  Finalize the web interface \\n\\n1.  A complete version of the web interface is now available \\n2.  Test the algorithm to see how it behaves when the user jumps \\n\\nfrom a topic to another in a way that is difficult to understand \\n\\n1.  Include an open source spell checker to the application \\n2.  Test the spell checker \\n3.  Continue developing the algorithm \\n\\n1.  A  complete  version  of  the  algorithm  is  now  available  and \\n\\nworks with the spell checker and the web interface properly \\n\\n2.  Continue testing on the final version to identify possible errors \\n\\n1.  Upload the system to the web server \\n2.  Test  the  system  in  the  web  server  context  to  see  how  it \\n\\nbehaves \\n\\n1.  Port the system to the android platform \\n2.  Test the android version of the system \\n\\n1.  Continue testing both systems \\n2.  Final enhancements of the system \\n\\n1.  Prepare the presentation \\n\\n53 \\n\\n\\x0cAppendix B How to run the system \\n\\nThe system has been developed using Netbeans and it is recommended to open the project directory \\nusing Netbeans. The project directory can be found in the CD and it is called \\xe2\\x80\\x9cChatbot\\xe2\\x80\\x9d. \\n \\nThe files below can be found under the \\xe2\\x80\\x9cChatbot\\xe2\\x80\\x9d directory in the \\xe2\\x80\\x9csrc\\xe2\\x80\\x9d directory. \\n \\nUnder  the  default  package  the  ApplicationContext.xml  file  can  be  found.  It  is  the  Spring \\nconfiguration file, which uses the hibernate mapping files to create the database, set up the beans, \\nmanage  transactions,  authentication  and  security.  In  that  file  there  exists  a  bean  with  the  name \\ndatasource. We have to change a number of properties to comply with our system. The url property \\nneeds to be changed to map to our database, the username need to be changed with the username of \\nour database and the password needs to be changed with the password of our database. \\n \\n<bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-\\nmethod=\"close\"> \\n        <property name=\"driverClassName\" value=\"org.postgresql.Driver\"/> \\n        <property name=\"url\" value=\"jdbc:postgresql://localhost:5432/postgres\"/> \\n        <property name=\"username\" value=\"postgres\"/> \\n        <property name=\"password\" value=\"pilidis4\"/> \\n        <property name=\"maxActive\" value=\"4\"/> \\n        <property name=\"maxIdle\" value=\"4\"/> \\n        <property name=\"minIdle\" value=\"2\"/> \\n        <property name=\"maxWait\" value=\"-1\"/> \\n        <property name=\"initialSize\" value=\"2\"/> \\n        <property name=\"poolPreparedStatements\" value=\"true\"/> \\n        <property name=\"maxOpenPreparedStatements\" value=\"100\"/> \\n        <property name=\"testOnBorrow\" value=\"true\"/> \\n        <property name=\"testOnReturn\" value=\"false\"/> \\n        <property name=\"testWhileIdle\" value=\"false\"/> \\n        <property name=\"validationQuery\" value=\"SELECT 1+1\"/> \\n    </bean> \\n \\nAfter this is done we have to run the Main file under the /chatbot/ sub directory. The it will pop up a \\nmessage as shown in the figure below. \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\nIf the Yes button is clicked then the tables are created and a number of data fill these tables. If the \\nno button is clicked the database tables are deleted if exist. The no button should be clicked if the \\ndatabase has been created by clicking yes and we have made changes so it is necessary to delete it \\nand re create it. \\n \\nThe next step is to Clean and build the project and run it. \\n\\n54 \\n\\n\\x0cThe other directories of the project are as follows: \\nThe  /chatbot.dao/  subdirectory  stores  the  interfaces  of  the  data  access  objects  that  spring  uses  to \\naccess the database. \\n \\nThe /Chatbot.dao.hibernate/ subdirectory stores the implementation of the previous interfaces. \\n \\nThe /Chatbot.model/ subdirectory stores the persistent classes and the hibernate mapping files. \\n \\nThe /Chatbot.page/ subdirectory stores the Java and HTML files for the presentation layer used by \\nwicket. \\n \\nThe  /Chatbot.service/  stores  the  chatbot  service  interface  and  the  chatbot  service  implementation \\nthat contain the business logic of the project.   \\n \\n \\nFurthermore on the CD the following files can be found: \\n \\nThe eng_com.dic is the dictionary used for the spell checker. In the chatbot service implementation \\nfile in the spell Checking method the location of this file needs to be specified. \\n \\nThe dissertation.odt is the report of the project, written using open office. \\n \\nThe dissertation.doc is a Microsoft word version generated from open office. \\n \\nThe dissertation.pdf is the pdf version of the report. \\n \\nThe front page .odt is the front page of the report. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n55 \\n\\n\\x0cAppendix C Example Conversation \\n\\n \\nThe figures below display how an example conversation would appear if a user decided to engage a \\nconversation with the chatbot.  \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n\\n56 \\n\\n\\x0cAppendix D Feedback \\n\\nUser 1 \\n \\nI have tried the system out and found it to be quite informative, I hope this helps. \\n \\nUser 2 \\n \\nI don\\'t know how helpful this would be to the project, but the questions I asked this bot are (had \\nthese when first applied (answered through website)): \\n \\nGeneral Comments: \\nIt may help to have a follow up question concerning the course chosen when an answer would differ \\naccording to masters courses. It also would seem that having mentioned \\'Masters\\', causes a priority \\nabove other nouns that may be more specific to the question at hand. There are answers about the \\nactual applying of courses, but often not  answers about the specifics of taking the course itself. \\n \\nQuestions not answered: \\nDo I have to do research for the project before the summer? / When \\ndo I have to do research for the project? \\nWhen do I have to get a supervisor for the Project? \\nHow long do I have to choose the modules? / When do I have to choose modules? \\nCan I repeat modules that I have already completed in my degree? \\n \\nExternal Applicants: \\nWhat computers can I use? \\nDoes it cost money to print? \\nWhat computers can I use? \\nChat Bot:My name is John \\nWhat happens if I send in work late? \\nWho can I email about questions I have about the course? \\nHow do I ask questions about something in a module? \\n \\nUser 3 \\n \\nFirstly, is the GUI, in my browser ,the interface shows incomplete, as follows: \\n  \\nI hope this system can use for all browsers. \\n  \\nSecondly, I hope this system can add a flash in order to  guide the user to use. \\n  \\nThirdly, I hope this system can provide a link to the main page of university and give the user a \\ncontact E-mail address. \\n  \\nActually I think the main interface is too simple, as a user the first feeling is not good. \\n  \\nIn addition, can you give me  a account to test the main function? \\n \\n \\n \\n\\n57 \\n\\n\\x0cUser 4 \\n \\nGood day. Quite an impressive project. It does answer correctly properly  structured questions \\nespecially ones related to admissions, application  contact information. There are a few spelling \\nmistakes  \\n \\ne.g. reply  to  Can I have a virtual tour of the computing facilities?  \\n \\nChat Bot:The school is housed in a modern building with 24 hour access a day, 7 days a weak. \\nThere are dedicated laboratories for teaching,robotics,medical imaging, a library and a wireless \\nnetwork. \\n \\nIs there any reason why the links displayed as answers are not hyper-linked? \\n \\nUser 5 \\n \\nSo far as I can see, the answers that the chat bot has are very limited (maybe this is because the \\ncreator hopes the bot to gives general answer only). There are still a lot of rooms to improve, such \\nas the preprocessing (do not suggest words unrelated to the context, such as \"boat\"), variation \\nuses of question words (what, why, where, etc.) and the other words, part of speech tagging, etc. For \\nthis bot, I don\\'t think learning is needed, as the  information given almost never change for some \\nperiod, it can be based on rules only. \\n \\n \\nUser 6 \\n \\nI already try the system, I found that the system doesn\\'t accept short phrases such as U instead of \\nYou, Also, When I send the QA there was no replay only the system said that your QA will be \\nanswered later like that. So,it is slowly \\n \\nThat are my comments,I hope it will help you \\n \\nUser 7 \\n \\nI tested the system and here are my observations: \\n \\n - It works well but only for very general cases. For instance I tried asking for application deadlines \\nfor an MSc in computer Science and I got the entry qualifications to the programme. \\n \\n- I like the fact that if you say that your question was not answered you get a link that probably has \\nthe information you need. \\n \\nThat\\'s all, Otherwise it is a great project. \\n\\n58 \\n\\n\\x0c',\n",
       " '6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n4\\nv\\n8\\n0\\n1\\n4\\n0\\n\\n.\\n\\n1\\n1\\n5\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nUnder review as a conference paper at ICLR 2016\\n\\nLSTM-BASED DEEP LEARNING MODELS FOR NON-\\nFACTOID ANSWER SELECTION\\n\\nMing Tan, Cicero dos Santos, Bing Xiang & Bowen Zhou\\nIBM Watson Core Technologies\\nYorktown Heights, NY, USA\\n{mingtan,cicerons,bingxia,zhou}@us.ibm.com\\n\\nABSTRACT\\n\\nIn this paper, we apply a general deep learning (DL) framework for the answer\\nselection task, which does not depend on manually de\\xef\\xac\\x81ned features or linguistic\\ntools. The basic framework is to build the embeddings of questions and answers\\nbased on bidirectional long short-term memory (biLSTM) models, and measure\\ntheir closeness by cosine similarity. We further extend this basic model in two di-\\nrections. One direction is to de\\xef\\xac\\x81ne a more composite representation for questions\\nand answers by combining convolutional neural network with the basic frame-\\nwork. The other direction is to utilize a simple but ef\\xef\\xac\\x81cient attention mechanism in\\norder to generate the answer representation according to the question context. Sev-\\neral variations of models are provided. The models are examined by two datasets,\\nincluding TREC-QA and InsuranceQA. Experimental results demonstrate that the\\nproposed models substantially outperform several strong baselines.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe answer selection problem can be formulated as follows: Given a question q and an answer\\ncandidate pool {a1, a2,\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 , as} for this question, we aim to search for the best answer candidate\\nak, where 1 \\xe2\\x89\\xa4 k \\xe2\\x89\\xa4 s. An answer is a token sequence with an arbitrary length, and a question can\\ncorrespond to multiple ground-truth answers. In testing, the candidate answers for a question may\\nnot be observed in the training phase. Answer selection is one of the essential components in typical\\nquestion answering (QA) systems. It is also a stand-alone task with applications in knowledge base\\nconstruction and information extraction.\\nThe major challenge of this task is that the correct answer might not directly share lexical units with\\nthe question. Instead, they may only be semantically related. Moreover, the answers are sometimes\\nnoisy and contain a large amount of unrelated information.\\nRecently, deep learning models have obtained a signi\\xef\\xac\\x81cant success on various natural language\\nprocessing tasks, such as semantic analysis (Tang et al., 2015), machine translation (Bahdanau et al.,\\n2015) and text summarization (Rush et al., 2015).\\nIn this paper, we propose a deep learning framework for answer selection which does not require any\\nfeature engineering, linguistic tools, or external resources. This framework is based on building bi-\\ndirectional long short term memory (biLSTM) models on both questions and answers respectively,\\nconnecting with a pooling layer and utilizing a similarity metric to measure the matching degree.\\nWe improve this basic model from two perspectives. Firstly, a simple pooling layer may suffer from\\nthe incapability of keeping the local linguistic information. In order to obtain better embeddings\\nfor the questions and answers, we build a convolutional neural network (CNN) structure on top\\nof biLSTM. Secondly, in order to better distinguish candidate answers according to the question,\\nwe introduce a simple but ef\\xef\\xac\\x81cient attention model to this framework for the answer embedding\\ngeneration according to the question context.\\nWe report experimental results for two answer selection datasets: (1) InsuranceQA (Feng et al.,\\n2015) 1, a recently released large-scale non-factoid QA dataset from the insurance domain. The\\n\\n1git clone https://github.com/shuzi/insuranceQA.git\\n\\n1\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\nproposed models demonstrate a signi\\xef\\xac\\x81cant out-performance compared to two non-DL baselines and\\na strong DL baseline based on CNN. (2) TREC-QA 2, which was created by Wang et al. (2007)\\nbased on Text REtrieval Conference (TREC) QA track data. The proposed models outperform\\nvarious strong baselines.\\nThe rest of the paper is organized as follows: Section 2 describes the related work for answer\\nselection; Section 3 provides the details of the proposed models; Experimental settings and results\\nof InsuranceQA and TREC-QA datasets are discussed in section 4 and 5 respectively; Finally, we\\ndraw conclusions in section 6.\\n\\n2 RELATED WORK\\n\\nPrevious work on answer selection normally used feature engineering, linguistic tools, or external\\nresources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013).\\nThis model pairs semantically related words based on word semantic relations. In (Wang & Man-\\nning, 2010; Wang et al., 2007), the answer selection problem is transformed to a syntactical matching\\nbetween the question/answer parse trees. Some work tried to ful\\xef\\xac\\x81ll the matching using minimal edit\\nsequences between dependency parse trees (Heilman & Smith, 2010; Yao et al., 2013). Recently,\\ndiscriminative tree-edit features extraction and engineering over parsing trees were automated in\\n(Severyn & Moschitti, 2013).\\nWhile these methods show effectiveness, they might suffer from the availability of additional re-\\nsources, the effort of feature engineering and the systematic complexity by introducing linguistic\\ntools, such as parse trees and dependency trees.\\nThere were prior methods using deep learning technologies for the answer selection task. The ap-\\nproaches for non-factoid question answering generally pursue the solution on the following direc-\\ntions: Firstly, the question and answer representations are learned and matched by certain similarity\\nmetrics (Feng et al., 2015; Yu et al., 2014; dos Santos et al., 2015). Secondly, a joint feature vector\\nis constructed based on both the question and the answer, and then the task can be converted into a\\nclassi\\xef\\xac\\x81cation or learning-to-rank problem (Wang & Nyberg, 2015). Finally, recently proposed mod-\\nels for textual generation can intrinsically be used for answer selection and generation (Bahdanau\\net al., 2015; Vinyals & Le, 2015).\\nThe framework proposed in this work belongs to the \\xef\\xac\\x81rst category. There are two major differences\\nbetween our approaches and the work in (Feng et al., 2015): (1) The architectures developed in\\n(Feng et al., 2015) are only based on CNN, whereas our models are based on bidirectional LSTMs,\\nwhich are more capable of exploiting long-range sequential context information. Moreover, we also\\nintegrate the CNN structures on the top of biLSTM for better performance. (2) Feng et al. (2015)\\ntackle the question and answer independently, while the proposed structures develop an ef\\xef\\xac\\x81cient\\nattentive models to generate answer embeddings according to the question.\\n\\n3 APPROACH\\n\\nIn this section, we describe the proposed framework and its variations. We \\xef\\xac\\x81rst introduce the general\\nframework, which is to build bi-directional LSTM on both questions and their answer candidates,\\nand then use the similarity metric to measure the distance of question answer pairs. In the following\\ntwo subsections, we extend the basic model in two independent directions.\\n\\n3.1 BASIC MODEL: QA-LSTM\\n\\nLong Short-Term Memory (LSTM): Recurrent Neural Networks (RNN) have been widely ex-\\nploited to deal with variable-length sequence input. The long-distance history is stored in a recurrent\\nhidden vector which is dependent on the immediate previous hidden vector. LSTM (Hochreiter &\\nSchmidhuber, 1997) is one of the popular variations of RNN to mitigate the gradient vanish prob-\\nlem of RNN. Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor\\n\\n2The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/\\xcb\\x9cxuchen/packages/\\n\\njacana-qa-naacl2013-data-results.tar.bz2\\n\\n2\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\nmodi\\xef\\xac\\x81cation. Given an input sequence x = {x(1), x(2),\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 , x(n)}, where x(t) is an E-dimension\\nword vector in this paper. The hidden vector h(t) ( the size is H ) at the time step t is updated as\\nfollows.\\n\\nit = \\xcf\\x83(Wix(t) + Uih(t \\xe2\\x88\\x92 1) + bi)\\nft = \\xcf\\x83(Wf x(t) + Uf h(t \\xe2\\x88\\x92 1) + bf )\\not = \\xcf\\x83(Wox(t) + Uoh(t \\xe2\\x88\\x92 1) + bo)\\n\\xcb\\x9cCt = tanh(Wcx(t) + Uch(t \\xe2\\x88\\x92 1) + bc)\\nCt = it \\xe2\\x88\\x97 \\xcb\\x9cCt + ft \\xe2\\x88\\x97 Ct\\xe2\\x88\\x921\\nht = ot \\xe2\\x88\\x97 tanh(Ct)\\n\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n\\nIn the LSTM architecture, there are three gates (input i, forget f and output o), and a cell memory\\nvector c. \\xcf\\x83 is the sigmoid function. The input gate can determine how incoming vectors xt alter\\nthe state of the memory cell. The output gate can allow the memory cell to have an effect on the\\noutputs. Finally, the forget gate allows the cell to remember or forget its previous state. W \\xe2\\x88\\x88 RH\\xc3\\x97E,\\nU \\xe2\\x88\\x88 RH\\xc3\\x97H and b \\xe2\\x88\\x88 RH\\xc3\\x971 are the network parameters.\\nBidirectional Long Short-Term Memory (biLSTM): Single direction LSTMs suffer a weakness\\nof not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both\\nthe previous and future context by processing the sequence on two directions, and generate two\\nindependent sequences of LSTM output vectors. One processes the input sequence in the forward\\ndirection, while the other processes the input in the reverse direction. The output at each time step\\nis the concatenation of the two output vectors from both directions, ie. ht =\\nQA-LSTM: The basic model in this work is shown in Figure 1. BiLSTM generates distributed\\nrepresentations for both the question and answer independently, and then utilize cosine similarity to\\nmeasure their distance. Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014;\\nHu et al., 2014), we de\\xef\\xac\\x81ne the training objective as a hinge loss.\\n\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nht (cid:107) \\xe2\\x86\\x90\\xe2\\x88\\x92\\nht.\\n\\nL = max{0, M \\xe2\\x88\\x92 cosine(q, a+) + cosine(q, a\\xe2\\x88\\x92)}\\n\\n(7)\\nwhere a+ is a ground truth answer, a\\xe2\\x88\\x92 is an incorrect answer randomly chosen from the entire\\nanswer space, and M is constant margin. We treat any question with more than one ground truth as\\nmultiple training examples, each for one ground truth.\\nThere are three simple ways to generate representations for questions and answers based on the\\nword-level biLSTM outputs: (1) Average pooling; (2) max pooling; (3) the concatenation of the last\\nvectors on both directions. The three strategies are compared with the experimental performance\\nin Section 5. Dropout operation is performed on the QA representations before cosine similarity\\nmatching.\\nFinally, from preliminary experiments, we observe that the architectures, in which both question and\\nanswer sides share the same network parameters, is signi\\xef\\xac\\x81cantly better than the one that the question\\nand answer sides own their own parameters separately, and converges much faster. As discussed\\nin (Feng et al., 2015), this is reasonable, because for a shared layer network, the corresponding\\nelements in question and answer vectors represent the same biLSTM outputs. While for the network\\nwith separate question and answer parameters, there is no such constraint and the model has double-\\nsized parameters, making it dif\\xef\\xac\\x81cult to learn for the optimizer.\\n\\nFigure 1: Basic Model: QA-LSTM\\n\\n3\\n\\nCosineQuestionAnsweroqoamean/maxpoolingmean/maxpoolingbiLSTMbiLSTM\\x0cUnder review as a conference paper at ICLR 2016\\n\\nFigure 2: QA-LSTM/CNN\\n\\n3.2 QA-LSTM/CNN\\n\\nIn the previous subsection, we generate the question and answer representations only by simple\\noperations, such as max or mean pooling. In this subsection, we resort to a CNN structure built on\\nthe outputs of biLSTM, in order to give a more composite representation of questions and answers.\\nThe structure of CNN in this work is similar to the one in (Feng et al., 2015), as shown in Figure 2.\\nUnlike the traditional forward neural network, where each output is interactive with each input, the\\nconvolutional structure only imposes local interactions between the inputs within a \\xef\\xac\\x81lter size m.\\nIn this work, for every window with the size of m in biLSTM output vectors, ie. Hm(t) =\\n[h(t), h(t + 1),\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 , h(t + m \\xe2\\x88\\x92 1)], where t is a certain time step, the convolutional \\xef\\xac\\x81lter F =\\n[F(0)\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 F(m \\xe2\\x88\\x92 1)] will generate one value as follows.\\n\\noF (t) = tanh(cid:34)(cid:32)m\\xe2\\x88\\x921(cid:88)i=0\\n\\nh(t + i)T F(i)(cid:33) +b(cid:35)\\n\\n(8)\\n\\nwhere b is a bias, and F and b are the parameters of this single \\xef\\xac\\x81lter.\\nSame as typical CNNs, a max-k pooling layer is built on the top of the convolutional layer. Intu-\\nitively, we want to emphasize the top-k values from each convolutional \\xef\\xac\\x81lter. By k-MaxPooling, the\\nmaximum k values will be kept for one \\xef\\xac\\x81lter, which indicate the highest degree that a \\xef\\xac\\x81lter matches\\nthe input sequence.\\nFinally, there are N parallel \\xef\\xac\\x81lters, with different parameter initialization, and the convolutional\\nlayer gets N-dimension output vectors. We get two output vectors with dimension of kN for the\\nquestions and answers respectively. In this work, k = 1. k > 1 did not show any obvious improve-\\nment in our early experiments. The intuition of this structure is, instead of evenly considering the\\nlexical information of each token as the previous subsection, we emphasize on certain parts of the\\nanswer, such that QA-LSTM/CNN can more effectively differentiate the ground truths and incorrect\\nanswers.\\n\\n3.3 ATTENTION-BASED QA-LSTM\\n\\nIn the previous subsection, we described one extension from the basic model, which targets at pro-\\nviding more composite embeddings for questions and answers respectively. In this subsection, we\\ninvestigate an extension from another perspective. Instead of generating QA representation indepen-\\ndently, we leverage a simple attention model for the answer vector generation based on questions.\\nThe \\xef\\xac\\x81xed width of hidden vectors becomes a bottleneck, when the bidirectional LSTM models must\\npropagate dependencies over long distances over the questions and answers. An attention mech-\\nanism are used to alleviate this weakness by dynamically aligning the more informative parts of\\nanswers to the questions. This strategy has been used in many other natural language processing\\ntasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), sentence summa-\\nrization (Rush et al., 2015) and factoid question answering (Hermann et al., 2015; Sukhbaatar et al.,\\n2015).\\n\\n4\\n\\nCosineQuestionAnsweroqoaoutputlayeroutputlayerConvolutionalFiltersConvolutionalFiltersbiLSTMbiLSTMmax-1poolingmax-1pooling\\x0cUnder review as a conference paper at ICLR 2016\\n\\n# of Qs\\n# of As\\n\\nTrain Validation Test1 Test2\\n1800\\n12887\\n18540\\n2593\\n\\n1800\\n2616\\n\\n1000\\n1454\\n\\nTable 1: Numbers of questions and answers of InsuranceQA.\\n\\nInspired by the work in (Hermann et al., 2015), we develop a very simple but ef\\xef\\xac\\x81cient word-level\\nattention on the basic model. Figure 3 shows the structure. Prior to the average or mean pooling,\\neach biLSTM output vector will be multiplied by a softmax weight, which is determined by the\\nquestion embedding from biLSTM.\\nSpeci\\xef\\xac\\x81cally, given the output vector of biLSTM on the answer side at time step t, ha(t), and the\\n\\nquestion embedding, oq, the updated vector(cid:101)ha(t) for each answer token are formulated below.\\n\\nmsma,q(t))\\n\\n(cid:101)ha(t) = ha(t)sa,q(t)\\n\\nma,q(t) = tanh(Wamha(t) + Wqmoq)\\nsa,q(t) \\xe2\\x88\\x9d exp(wT\\n\\n(9)\\n(10)\\n(11)\\nwhere Wam, Wqm and wms are attention parameters. Conceptually, the attention mechanism give\\nmore weights on certain words, just like tf-idf for each word. However, the former computes the\\nweights according to question information.\\nThe major difference between this approach and the one in (Hermann et al., 2015) is that Hermann\\net al. (2015)\\xe2\\x80\\x99s attentive reader emphasizes the informative part of supporting facts, and then uses a\\ncombined embedding of the query and the supporting facts to predict the factoid answers. In this\\nwork, we directly use the attention-based representations to measure the question/answer distances.\\nExperiments indicate the attention mechanism can more ef\\xef\\xac\\x81ciently distinguish correct answers from\\nincorrect ones according to the question text.\\n\\n3.4 QA-LSTM/CNN WITH ATTENTION\\n\\nThe two extensions introduced previously are combined in a simple manner. First, the biLSTM hid-\\nden vectors of answers ha(t) are multiplied by sa,q(t), which is computed from the question average\\n\\npooling vectors oq, and updated to(cid:101)ha(t), illustrated in Eq. 9-11. Then, the original question and\\n\\nupdated answer hidden vectors serve as inputs of CNN structure respectively, such that the question\\ncontext can be used to evaluate the softmax weights of the input of CNN. From the experiments, we\\nobserve that the two extensions vary on their contributions on the performance improvement accord-\\ning to different datasets. However, QA-LSTM/CNN with attention can outperform the baselines on\\nboth datasets.\\n\\nINSURANCEQA EXPERIMENTS\\n\\n4\\nHaving described a number of models in the previous section, we evaluate the proposed approaches\\non the insurance domain dataset, InsuranceQA, provided by Feng et al. (2015). The InsuranceQA\\ndataset provides a training set, a validation set, and two test sets. We do not see obvious categorical\\ndifferentiation between two tests\\xe2\\x80\\x99 questions. One can see the details of InsuranceQA data in (Feng\\n\\nFigure 3: QA-LSTM with attention\\n\\n5\\n\\nCosineQuestionAnsweroqoaeha(t)=ha(t)sa,q(t)mean/maxpoolingmean/maxpoolingwithattentionbiLSTMbiLSTM\\x0cUnder review as a conference paper at ICLR 2016\\n\\nA. Bag-of-word\\nB. Metzler-Bendersky IR model\\nC. Architecture-II in (Feng et al., 2015)\\nD. Architecture-II with GESD\\n\\nValidation Test1 Test2\\n32.2\\n31.9\\n50.8\\n52.7\\n59.2\\n61.8\\n65.4\\n61.0\\n\\n32.1\\n55.1\\n62.8\\n65.3\\n\\nTable 2: Baseline results of InsuranceQA\\n\\net al., 2015). We list the numbers of questions and answers of the dataset in Table 1. A question\\nmay correspond to multiple answers. The questions are much shorter than answers. The average\\nlength of questions is 7, and the average length of answers is 94. The long answers comparing to the\\nquestions post challenges for answer selection task. This corpus contains 24981 unique answers in\\ntotal. For the development and test sets, the dataset also includes an answer pool of 500 candidate\\nanswers for each question. These answer pools were constructed by including the correct answer(s)\\nand randomly selecting candidate from the complete set of unique answers. The top-1 accuracy of\\nthe answer pool is reported.\\n\\n4.1 SETUP\\n\\nThe models in this work are implemented with Theano (Bastien et al., 2012) from scratch, and all\\nexperiments are processed in a GPU cluster. We use the accuracy on validation set to locate the best\\nepoch and best hyper-parameter settings for testing.\\nThe word embedding is trained by word2vec (Mikolov et al., 2013), and the word vector size is 100.\\nWord embeddings are also parameters and are optimized as well during the training. Stochastic\\nGradient Descent (SGD) is the optimization strategy. We tried different margin values, such as\\n0.05, 0.1 and 0.2, and \\xef\\xac\\x81nally \\xef\\xac\\x81xed the margin as 0.2. We also tried to include l2 norm in the\\ntraining objective. However, preliminary experiments show that regularization factors do not show\\nany improvements. Also, the dimension of LSTM output vectors is 141 for one direction, such that\\nbiLSTM has a comparable number of parameters with a single-direction LSTM with 200 dimension.\\nWe train our models in mini-batches (the batch size B is 20), and the maximum length L of questions\\nand answers is 200. Any tokens out of this range will be discarded. Because the questions or answers\\nwithin a mini-batch may have different lengths, we resort to a mask matrix M \\xe2\\x88\\x88 RB\\xc3\\x97L to indicate\\nthe real length of each token sequence.\\n\\n4.2 BASELINES\\n\\nFor comparison, we report the performances of four baselines in Table 2: two state-of-the-art non-\\nDL approaches and two variations of a strong DL approach based on CNN as follows.\\nBag-of-word: The idf-weighted sum of word vectors for the question and for all of its answer\\ncandidates is used as a feature vector. Similar to this work, the candidates are re-ranked according\\nthe cosine similarity to a question.\\nMetzler-Bendersky IR model: A state-of-the-art weighted dependency (WD) model, which em-\\nploys a weighted combination of term-based and term proximity-based ranking features to score\\neach candidate answer.\\nArchitecture-II in (Feng et al., 2015): Instead of using LSTM, a CNN model is employed to learn\\na distributed vector representation of a given question and its answer candidates, and the answers\\nare scored by cosine similarity with the question. No attention model is used in this baseline.\\nArchitecture-II with Geometricmean of Euclidean and Sigmoid Dot product (GESD): GESD\\nis used to measure the distance between the question and answers. This is the model which achieved\\nthe best performance in (Feng et al., 2015).\\n\\n4.3 RESULTS AND DISCUSSIONS\\n\\nIn this section, detailed analysis on experimental results are given. Table 3 summarizes the results of\\nour models on InsuranceQA. From Row (A) to (C), we list QA-LSTM without either CNN structure\\n\\n6\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\nModel\\n\\nA QA-LSTM basic-model(head/tail)\\nB QA-LSTM basic-model(avg pooling)\\nC QA-LSTM basic-model(max pooling)\\nD QA-LSTM/CNN(fcount=1000)\\nE QA-LSTM/CNN(fcount=2000)\\nF QA-LSTM/CNN(fcount=4000)\\nG QA-LSTM with attention (max pooling)\\nH QA-LSTM with attention (avg pooling)\\nI\\n\\nQA-LSTM/CNN (fcount=4000) with attention\\n\\nValidation Test1 Test2\\n51.2\\n54.0\\n54.0\\n58.5\\n58.0\\n64.3\\n65.5\\n62.3\\n62.6\\n64.8\\n62.2\\n66.2\\n60.3\\n66.5\\n68.4\\n62.2\\n63.3\\n67.2\\n\\n53.1\\n58.2\\n63.1\\n65.9\\n66.8\\n64.6\\n63.7\\n68.1\\n65.7\\n\\nTable 3: The experimental results of InsuranceQA for QA-LSTM, QA-LSTM/CNN and QA-LSTM\\nwith attentions\\n\\nor attention model. They vary on how to utilize the biLSTM output vectors to form sentential em-\\nbeddings for questions and answers in shown in section 3.1. We can observe that just concatenating\\nof the last vectors from both direction (A) performs the worst. It is surprised to see using max-\\npooling (C) is much better than average pooling (B). The potential reason is that the max-pooling\\nextracts more local values for each dimension, so that more local information can be re\\xef\\xac\\x82ected on the\\noutput embeddings.\\nFrom Row (D) to (F), CNN layers are built on the top of the biLSTM with different \\xef\\xac\\x81lter numbers.\\nWe set the \\xef\\xac\\x81lter width m = 2, and we did not see better performance if we increase m to 3 or\\n4. Row (F) with 4000 \\xef\\xac\\x81lters gets the best validation accuracy, obtained a comparable performance\\nwith the best baseline (Row (D) in Table 2 ). Row F shared a highly analogous CNN structure with\\nArchitecture II in (Feng et al., 2015), except that the later used a shallow hidden layer to transform\\nthe word embeddings into the input of CNN structure, while Row F take the output of biLSTM as\\nCNN input.\\nRow (G) and (H) corresponds to QA-LSTM with the attention model.\\n(G) connects the output\\nvectors of answers after attention with a max pooling layer, and (H) with an average pooling. In\\ncomparison to Model (C), Model (G) shows over 2% improvement on both validation and Test2 sets.\\nWith respect to the model with mean pooling layers (B), the improvement from attention is more\\nremarkable. Model (H) is over 8% higher on all datasets compared to (B), and gets improvements\\nfrom the best baseline by 3%, 2.8% and 1.2% on the validation, Test1 and Test2 sets, respectively.\\nCompared to Architecture II in (Feng et al., 2015), which involved a large number of CNN \\xef\\xac\\x81lters,\\n(H) model also has fewer parameters.\\nRow (I) corresponds to section 3.4, where CNN and attention mechanism are combined. Although\\ncompared to (F), it shows 1% improvement on all sets, we fail to see obvious improvements com-\\npared to Model (H). Although Model (I) achieves better number on Test2, but does not on validation\\nand Test1. We assume that the effective attention might have vanished during the CNN operations.\\nHowever, both (H) and (I) outperform all baselines.\\nWe also investigate the proposed models on how they perform with respect to long answers. We\\ndivide the questions of Test1 and Test2 sets into eleven buckets, according to the average length of\\ntheir ground truths. In the table of Figure 4, we list the bucket levels and the number of questions\\nwhich belong to each bucket, for example, Test1 has 165 questions, whose average ground truth\\nlengths are 55 < L \\xe2\\x89\\xa4 60. We select models of (C), (F), (H) and (I) in Table 3 for comparison.\\nModel (C) is without attention and sentential embeddings are formed only by max pooling. Model\\n(F) utilizes CNN, while model (H) and (I) integrate attention. As shown in the left \\xef\\xac\\x81gure in Figure\\n4, (C) gets better or close performance compared to other models on buckets with shorter answers\\n(\\xe2\\x89\\xa4 50, \\xe2\\x89\\xa455, \\xe2\\x89\\xa460). However, as the ground lengths increase, the gap between (C) and other models\\nbecomes more obvious. The similar phenomenon is also observed in the right \\xef\\xac\\x81gure for Test2. This\\nsuggests the effectiveness of the two extensions from the basic model of QA-LSTM, especially for\\nlong-answer questions.\\nFeng et al. (2015) report that GESD outperforms cosine similarity in their models. However, the\\nproposed models with GESD as similarity scores do not provide any improvement on accuracy.\\n\\n7\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\nBuckets \\xe2\\x89\\xa450 \\xe2\\x89\\xa455 \\xe2\\x89\\xa460 \\xe2\\x89\\xa465 \\xe2\\x89\\xa470 \\xe2\\x89\\xa480 \\xe2\\x89\\xa490 \\xe2\\x89\\xa4100 \\xe2\\x89\\xa4120 \\xe2\\x89\\xa4160 >160\\n156\\nTest1\\nTest2\\n142\\n\\n147\\n115\\n\\n191\\n170\\n\\n165\\n192\\n\\n152\\n168\\n\\n121\\n105\\n\\n167\\n191\\n\\n180\\n165\\n\\n137\\n169\\n\\n223\\n230\\n\\n161\\n153\\n\\nFigure 4: The accuracy of Test1 and Test2 of InsuranceQA sets for the four models (C, H, F and I\\nin Table 3 ), on different levels of ground truth answer lengths. The table divided each test set into\\n11 buckets. The \\xef\\xac\\x81gures above show the accuracy of each bucket.\\n\\nModels\\nWang et al. (2007)\\nHeilman & Smith (2010)\\nWang & Manning (2010)\\nYao et al. (2013)\\nSeveryn & Moschitti (2013)\\nYih et al. (2013)-BDT\\nYih et al. (2013)-LCLR\\nWang & Nyberg (2015)\\nArchitecture-II (Feng et al., 2015)\\n\\nMAP MRR\\n0.6852\\n0.6029\\n0.6091\\n0.6917\\n0.6852\\n0.6029\\n0.7477\\n0.6307\\n0.7358\\n0.6781\\n0.7894\\n0.6940\\n0.7092\\n0.7700\\n0.7134\\n0.7913\\n0.7998\\n0.7106\\n\\nTable 4: Test results of baselines on TREC-QA\\n\\nFinally, we replace the cosine similarity with a MLP structure, whose input (282x2-dimension) is\\nthe concatenation of question and answer embeddings, and the output is a single similarity score\\nand test the modi\\xef\\xac\\x81ed models by a variety of hidden layer size (100,500,1000). We observe that the\\nmodi\\xef\\xac\\x81ed models not only get >10% accuracy decrease, but also converge much slower. One possi-\\nble explanation is the involvement of more network parameters by MLP makes it more dif\\xef\\xac\\x81cult for\\ntraining, although we believed that MLP might partially avoid the conceptual challenge of projecting\\nquestions and answers in the same high-dimensional space, introduced by cosine similarity.\\n\\n5 TREC-QA EXPERIMENTS\\nIn this section we detail our experimental setup and results using the TREC-QA dataset.\\n\\n5.1 DATA, METRICS AND BASELINES\\n\\nIn this paper, we adopt TREC-QA, created by Wang et al. (2007) based on Text REtrieval Conference\\n(TREC) QA track (8-13) data. We follow the exact approach of train/dev/test questions selection in\\n\\nModels\\n\\nA QA-LSTM (avg-pool)\\nB QA-LSTM with attention\\nC QA-LSTM/CNN\\nD QA-LSTM/CNN with attention\\nQA-LSTM/CNN with attention\\nE\\n(LSTM hiddenvector=500)\\n\\nMAP MRR\\n76.52\\n68.19\\n78.49\\n68.96\\n70.61\\n81.04\\n83.22\\n71.11\\n72.79\\n82.40\\n\\nTable 5: Test results of the proposed models on TREC-QA\\n\\n8\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\n(Wang & Nyberg, 2015), in which all questions with only positive or negative answers are removed.\\nFinally, we have 1162 training questions, 65 development questions and 68 test questions.\\nFollowing previous work on this task, we use Mean Average Precision (MAP) and Mean Reciprocal\\nRank (MRR) as evaluation metrics, which are calculated using the of\\xef\\xac\\x81cial evaluation scripts.\\nIn Table 4, we list the performance of some prior work on this dataset, which can be referred to\\n(Wang & Nyberg, 2015). We implemented the Architecture II in (Feng et al., 2015) from scratch.\\nWang & Nyberg (2015) and Feng et al. (2015) are the best baselines on MAP and MRR respectively.\\n\\n5.2 SETUP\\n\\nWe keep the con\\xef\\xac\\x81gurations same as those in InsuranceQA in section 4.1, except the following differ-\\nences: First, we set the minibatch size as 10; Second, we set the maximum length of questions and\\nanswers as 40 instead of 200. Third, following (Wang & Nyberg, 2015), We use 300-dimensional\\nvectors that were trained and provided by word2vec 3. Finally, we use the models from the epoch\\nwith the best MAP on the validation set for training. Moreover, although TREC-QA dataset provided\\nnegative answer candidates for each training question, we randomly select the negative answers from\\nall the candidate answers in the training set.\\n\\n5.3 RESULTS\\n\\nTable 5 shows the performance of the proposed models. Compared to Model (A), which is with\\naverage pooling on top of biLSTM but without attention, Model (B) with attention improves MAP\\nby 0.7% and MRR by approximately 2%. The combination of CNN with QA-LSTM (Model-C)\\ngives greater improvement on both MAP and MRR from Model (A). Model (D), which combines\\nthe ideas of Model (B) and (C), achieves the performance, competitive to the best baselines on MAP,\\nand 2\\xe2\\x88\\xbc4% improvement on MRR compared to (Wang & Nyberg, 2015) and (Feng et al., 2015).\\nFinally, Model (E), which corresponds to the same model (D) but uses a LSTM hidden vector size\\nof 500, achieves the best results for both metrics and outperforms the baselines.\\n\\n6 CONCLUSION\\n\\nIn this paper, we study the answer selection task by employing a bidirectional-LSTM based deep\\nlearning framework. The proposed framework does not rely on feature engineering, linguistic tools\\nor external resources, and can be applied to any domain. We further extended the basic framework\\non two directions. Firstly, we combine a convolutional neural network into this framework, in\\norder to give more composite representations for questions and answers. Secondly, we integrate a\\nsimple but ef\\xef\\xac\\x81cient attention mechanism in the generation of answer embeddings according to the\\nquestion. Finally, two extensions combined together. We conduct experiments using the TREC-\\nQA dataset and the recently published InsuranceQA dataset. Our experimental results demonstrate\\nthat the proposed models outperform a variety of strong baselines. In the future, we would like to\\nfurther evaluate the proposed approaches for different tasks, such as answer quality prediction in\\nCommunity QA and recognizing textual entailment. With respect to the structural perspective, we\\nplan to generate the attention mechanism to phrasal or sentential levels.\\n\\nREFERENCES\\nBahdanau, Dzmitry, Cho, KyungHyun, and Bengio, Yoshua. Neural machine translation by jointly\\nlearning to align and translate. Proceedings of International conference of learning representa-\\ntions, 2015.\\n\\nBastien, Frederic, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron,\\nArnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements.\\nDeep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\\n\\n3https://code.google.com/p/word2vec/\\n\\n9\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\ndos Santos, Cicero, Barbosa, Luciano, Bogdanova, Dasha, and Zadrozny, Bianca. Learning hybrid\\nrepresentations to retrieve semantically equivalent questions. In Proceedings of ACL, pp. 694\\xe2\\x80\\x93\\n699, Beijing, China, July 2015.\\n\\nFeng, Minwei, Xiang, Bing, Glass, Michael, Wang, Lidan, and Zhou, Bowen. Applying deep\\nIEEE Automatic Speech Recognition\\n\\nlearning to answer selection: A study and an open task.\\nand Understanding Workshop (ASRU), 2015.\\n\\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep re-\\nIn IEEE International Conference on Acoustics, Speech and Signal\\n\\ncurrent neural networks.\\nProcessing (ICASSP), 2013.\\n\\nHeilman, Michael and Smith, Noah A. Tree edit models for recognizing textual entailments, para-\\nphrases, and answers to questions. Annual Conference of the North American Chapter of the\\nAssociation for Computational Linguistics. Association for Computational Linguistics (NAACL),\\n2010.\\n\\nHermann, Karl Moritz, Kocisky, Tomas, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suley-\\nman, Mustafa, and Blunsom, Phil. Teaching machines to read and comprehend. In Advances in\\nNeural Information Processing Systems (NIPS), 2015.\\n\\nHochreiter, Sepp and Schmidhuber, Jurgen. Long short-term memory. Neural Computation, 1997.\\n\\nHu, Baotian, Lu, Zhengdong, Li, Hang, and Chen, Qingcai. Convolutional neural network archi-\\ntectures for matching natural language sentences. Advances in Neural Information Processing\\nSystems (NIPS), 2014.\\n\\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S., and Dean, Jeff. Distributed rep-\\nresentations of words and phrases and their compositionality. Advances in Neural Information\\nProcessing Systems (NIPS), 2013.\\n\\nRush, Alexander, Chopra, Sumit, and Weston, Jason. A neural attention model for sentence sum-\\nmarization. Proceedings of the 2015 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP), 2015.\\n\\nSeveryn, Aliaksei and Moschitti, Alessandro. Automatic feature engineering for answer selection\\nand extraction. In Proceedings of Conference on Empirical Methods in Natural Language Pro-\\ncessing (EMNLP), 2013.\\n\\nSukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-\\n\\nworks. arXiv preprint arXiv:1503.08895, 2015.\\n\\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-\\n\\nworks. Advances in Neural Information Processing Systems, 2014.\\n\\nTang, Duyu, Qin, Bing, and Liu, Ting. Document modeling with gated recurrent neural network\\nIn Proceedings of Conference on Empirical Methods in Natural\\n\\nfor sentiment classi\\xef\\xac\\x81cation.\\nLanguage Processing (EMNLP), 2015.\\n\\nVinyals, Oriol and Le, Quoc V. A neural conversational model. Proceedings of the 31st International\\n\\nConference on Machine Learning, 2015.\\n\\nWang, Di and Nyberg, Eric. A long short-term memory model for answer sentence selection in\\nquestion answering. Proceedings of the 53rd Annual Meeting of the Association for Computa-\\ntional Linguistics and the 7th International Joint Conference on Natural Language Processing,\\n2015.\\n\\nWang, Mengqiu and Manning, Christopher. Probabilistic tree-edit models with structured latent vari-\\nables for textual entailment and question answering. The Proceedings of the 23rd International\\nConference on Computational Linguistics (COLING), 2010.\\n\\nWang, Mengqiu, Smith, Noah, and Teruko, Mitamura. What is the jeopardy model? a quasi-\\n\\nsynchronous grammar for qa. The Proceedings of EMNLP-CoNLL, 2007.\\n\\n10\\n\\n\\x0cUnder review as a conference paper at ICLR 2016\\n\\nWeston, Jason, Chopra, Sumit, and Adams, Keith. #tagspace: Semantic embeddings from hash-\\ntags. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), 2014.\\n\\nYao, Xuchen, Durme, Benjamin, and Clark, Peter. Answer extraction as sequence tagging with tree\\n\\nedit distance. Proceedings of NAACL-HLT, 2013.\\n\\nYih, Wen-tau, Chang, Ming-Wei, Meek, Christopher, and Pastusiak, Andrzej. Question answering\\nusing enhanced lexical semantic models. Proceedings of the 51st Annual Meeting of the Associa-\\ntion for Computational Linguist (ACL), 2013.\\n\\nYu, Lei, Hermann, Karl M., Blunsom, Phil, and Pulman, Stephen. Deep learning for answer sentence\\n\\nselection. NIPS Deep Learning Workshop, 2014.\\n\\n11\\n\\n\\x0c',\n",
       " 'Response Selection with Topic Clues for Retrieval-based Chatbots\\n\\nYu Wu\\xe2\\x80\\xa0\\xe2\\x88\\x97\\n\\n, Ming Zhou\\xe2\\x80\\xa1\\n\\n\\xe2\\x80\\xa0State Key Lab of Software Development Environment, Beihang University, Beijing, China\\n\\n, Zhoujun Li\\xe2\\x80\\xa0\\n\\n, Wei Wu\\xe2\\x80\\xa1\\n\\xe2\\x80\\xa1 Microsoft Research, Beijing, China\\n\\n{wuyu,lizj}@buaa.edu.cn {wuwei,mingzhou}@microsoft.com\\n\\n6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n2\\n2\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n3\\nv\\n0\\n9\\n0\\n0\\n0\\n\\n.\\n\\n5\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe consider incorporating topic information into message-\\nresponse matching to boost responses with rich content in\\nretrieval-based chatbots. To this end, we propose a topic-\\naware convolutional neural tensor network (TACNTN). In\\nTACNTN, matching between a message and a response is\\nnot only conducted between a message vector and a response\\nvector generated by convolutional neural networks, but also\\nleverages extra topic information encoded in two topic vec-\\ntors. The two topic vectors are linear combinations of topic\\nwords of the message and the response respectively, where\\nthe topic words are obtained from a pre-trained LDA model\\nand their weights are determined by themselves as well as the\\nmessage vector and the response vector. The message vector,\\nthe response vector, and the two topic vectors are fed to neu-\\nral tensors to calculate a matching score. Empirical study on\\na public data set and a human annotated data set shows that\\nTACNTN can signi\\xef\\xac\\x81cantly outperform state-of-the-art meth-\\nods for message-response matching.\\n\\nIntroduction\\n\\nHuman-computer conversation is a challenging task in AI\\nand NLP. Existing conversation systems include task ori-\\nented dialog systems and non task oriented chatbots. The\\nformer aims to help people complete speci\\xef\\xac\\x81c tasks such as\\nordering and tutoring, while the latter focuses on talking\\nlike a human and engaging in social conversations regarding\\na wide range of issues within open domains (Perez-Marin\\n2011). Although previous research on conversation focused\\non dialog systems, recently, with the large amount of con-\\nversation data available on the Internet, chatbots are draw-\\ning more and more attention in both academia and industry.\\nThe key problem for building a chatbot is how to reply to a\\nmessage with a proper (human-like and natural) response.\\nExisting methods are either retrieval-based or generation-\\nbased. Retrieval-based methods (Ji, Lu, and Li 2014) re-\\ntrieve response candidates from a pre-built index, rank the\\ncandidates, and select a reply from the top ranked ones,\\nwhile generation-based methods (Shang, Lu, and Li 2015;\\nVinyals and Le 2015) leverage natural language generation\\n\\xe2\\x88\\x97The work was done when the \\xef\\xac\\x81rst author was an intern in Mi-\\ncrosoft Research Asia.\\nCopyright c(cid:13) 2017, Association for the Advancement of Arti\\xef\\xac\\x81cial\\nIntelligence (www.aaai.org). All rights reserved.\\n\\nTable 1: A good response to a message\\n\\nMessage : Is the new Batman movie worth watching?\\nResponse : I swear you won\\xe2\\x80\\x99t regret watching it. We \\xef\\xac\\x81-\\nnally get Batman as a fully rendered character. The \\xef\\xac\\x81lm\\nshows the variables he must contend with in his role as a\\nprotector of Gotham.\\n\\n(NLG) techniques to respond to a message. In this work,\\nwe study response selection for retrieval-based chatbots in a\\nsingle turn scenario, because retrieval-based methods can al-\\nways return \\xef\\xac\\x82uent responses (Ji, Lu, and Li 2014) and single\\nturn is the basis of conversation in a chatbot.\\n\\nThe key to the success of response selection lies in accu-\\nrately matching input messages with proper responses. The\\nmatching scores can be either individually used to rank re-\\nsponse candidates, or used as features in a learning to rank\\narchitecture. In general, matching algorithms have to over-\\ncome semantic gaps between two objects (Hu et al. 2014).\\nIn the scenario of message-response matching, the problem\\nbecomes more serious, as proper responses could contain\\nmuch more information than the messages. Table 1 gives an\\nexample1. The response not only answers the message, but\\nalso brings in new content (e.g., the character of Batman)\\ninto the conversation. The content represents topics to talk\\nabout the movie (e.g., \\xe2\\x80\\x9ccharacter\\xe2\\x80\\x9d). Such responses can fa-\\ncilitate the chatbot to engage its users, because they could\\narouse more discussions (e.g., discussions about \\xe2\\x80\\x9ccharacter\\nof Batman\\xe2\\x80\\x9d) and keep the conversation going. In practice,\\nhowever, selecting such responses from others is dif\\xef\\xac\\x81cult,\\nbecause the extra content makes the semantic gap between\\nmessages and responses even bigger.\\n\\nIn this paper, we study the problem of message-response\\nmatching. Particularly, we aim to improve the matching be-\\ntween messages and responses with rich content. Inspired\\nby the example in Table 1, our idea is that since people\\nbring topics into responses to enrich their content, we should\\nmatch messages and responses not only by themselves, but\\nalso by their topics. Topics represent a kind of prior knowl-\\nedge, and in matching, only those related to messages and\\nresponses are useful. Based on this idea, we propose a topic-\\n\\n1It is translated from Chinese.\\n\\n\\x0caware convolutional neural tensor network (TACNTN) in or-\\nder to incorporate topic information into message-response\\nmatching. TACNTN embeds a message, a response, and\\ntheir related topic information into a vector space, and ex-\\nploits all the vectors for matching by neural tensors. The\\nmessage vector and the response vector are generated by\\na siamese convolutional neural network (CNN), while the\\ntopic vectors come from two topic embedding layers, one for\\nthe message and the other for the response. The two layers\\nacquire topic words of the message and the response from a\\nTwitter LDA model (Zhao et al. 2011) which is pre-trained\\nusing large scale social media data outside the conversa-\\ntion data. The topic words of the message hint the match-\\ning model topics that could be used in the response, and\\nthe topic words of the response indicate the model if the\\nresponse and the message are in the same topics. The two\\nlayers then calculate a weight for each topic word accord-\\ning to the message vector and the response vector. A large\\nweight means a topic word is relevant to the message or to\\nthe response, and the word is more useful in matching. Fi-\\nnally, the two layers form topic vectors by a weighted aver-\\nage of the embedding of the topic words. The \\xef\\xac\\x81nal matching\\nis conducted in message-response, message-response topic,\\nand message topic-response, and realized by neural tensors\\nwhich model the relationships between the two objects in the\\nthree pairs. TACNTN can enjoy both the powerful match-\\ning capability of CNN with neural tensors and extra topic\\ninformation provided by a state-of-the-art topic model. It\\nextends the convolutional neural tensor network (Qiu and\\nHuang 2015) proposed for community question answering\\nby the topic embedding layers for message-response match-\\ning in chatbots. With the extra topic information, responses\\nwith rich content could be boosted in ranking. We conducted\\nempirical study on a public English data set and a human an-\\nnotated Chinese data set. Evaluation results show that TAC-\\nNTN can signi\\xef\\xac\\x81cantly outperform state-of-the-art methods\\nfor message-response matching.\\n\\nOur contributions in this paper are three-folds: 1) proposal\\nof incorporating topic information into message-response\\nmatching. 2) proposal of a topic-aware convolutional neural\\ntensor network for matching with topics. 3) empirical veri\\xef\\xac\\x81-\\ncation of the effectiveness of the proposed method on public\\nand annotated data.\\n\\nRelated Work\\n\\nEarly work on chatbots (Weizenbaum 1966) relied on hand-\\ncrafted templates or heuristic rules to do response gen-\\neration, which requires huge effort but can only generate\\nlimited responses. Recently, researchers begin to develop\\ndata driven approaches (Ritter, Cherry, and Dolan 2011;\\nStent and Bangalore 2014). Among the effort, retrieval based\\nmethods select a proper response by matching message-\\nresponse pairs (Hu et al. 2014; Wang et al. 2015; Lu and\\nLi 2013), and generation based methods employ statistical\\nmachine translation techniques (Ritter, Cherry, and Dolan\\n2011) or the sequence to sequence framework (Shang, Lu,\\nand Li 2015; Serban et al. 2016; Vinyals and Le 2015;\\nLi et al. 2015; Li et al. 2016) to generate responses. On\\ntop of these work, conversation history is further consid-\\n\\nered to support multi-turn conversation (Lowe et al. 2015;\\nSordoni et al. 2015). In this work, we study response selec-\\ntion in single-turn conversation for building a retrieval based\\nchatbot. We propose a new message-response matching\\nmethod that can incorporate topic information into match-\\ning.\\n\\nConvolutional neural networks (CNNs) (Collobert et al.\\n2011) have been proven effective in many NLP tasks such as\\ntext classi\\xef\\xac\\x81cation (Kim 2014), entity disambiguation (Sun et\\nal. 2015), answer selection (Yang, Yih, and Meek 2015), tag\\nrecommendation (Weston, Chopra, and Adams 2014), web\\nsearch (Shen et al. 2014), sentiment classi\\xef\\xac\\x81cation (dos San-\\ntos and Gatti 2014), sequence prediction (Li and Liu 2015)\\nand sentence matching (Hu et al. 2014). In sentence match-\\ning, recent progress includes MultiGranCNN proposed by\\nYin et al. (Yin and Sch\\xc2\\xa8utze 2015) who match a pair of sen-\\ntences on multiple granularity, and the work of Pang et al.\\n(2016) in which a CNN architecture in image recognition\\nis employed for sentence matching. In this work, we study\\nmessage-response matching which is a special case of sen-\\ntence matching but important for building retrieval-based\\nchatbots. We extend the convolutional neural tensor network\\n(Qiu and Huang 2015; Socher et al. 2013) by topic embed-\\nding layers which enable us to leverage extra topic informa-\\ntion in matching to boost responses with rich content.\\n\\nProblem Formalization\\n\\nSuppose that we have a data set D = {(yi, mi, ri)}N\\ni=1,\\nwhere mi and ri represent an input message and a response\\ncandidate respectively, and yi \\xe2\\x88\\x88 {0, 1} denotes a class label.\\nyi = 1 means ri is a proper response for mi, otherwise yi =\\n0. Each mi in D corresponds to a topic word set Wm,i =\\n{wm,i,1, . . . , wm,i,n}, and each ri\\nin D has a Wr,i =\\n{wr,i,1, . . . , wr,i,n} as topic words. Our goal is to learn a\\nmatching model g(\\xc2\\xb7,\\xc2\\xb7) with D and {\\xe2\\x88\\xaaN\\ni=1Wm,i,\\xe2\\x88\\xaaN\\ni=1Wr,i}.\\nFor any message-response pair (m, r), g(m, r) returns a\\nmatching score which can be utilized to rank response can-\\ndidates for m.\\nTo learn g(\\xc2\\xb7,\\xc2\\xb7), we need to answer two questions: 1) how\\nto obtain topic words, and 2) how to incorporate topic words\\ninto matching. In the following sections, we \\xef\\xac\\x81rst present our\\nmethod on topic word generation, then we elaborate on our\\nmatching model and learning approach.\\n\\nTopic Word Generation\\n\\nWe employ a Twitter LDA model (Zhao et al. 2011), which\\nis the state-of-the-art topic model for short texts, to gener-\\nate topic words for messages and responses. Twitter LDA\\nassumes that each piece of text (a message or a response)\\ncorresponds to one topic, and each word in the text is either\\na background word or a topic word under the topic of the\\ntext. Figure 1 shows the graphical model of Twitter LDA.\\n\\nWe estimate the parameters of Twitter LDA using a col-\\nlapsed Gibbs sampling algorithm (Zhao et al. 2011). After\\nthat, we use them to assign a topic to each mi and ri in D.\\nTo obtain the topic word sets, we de\\xef\\xac\\x81ne the salience of a\\n\\n\\x0c(cid:104)\\n\\nNote that if the length of a sentence does not reach s,\\nwe put all-zero padding vectors after the last word of the\\nsentence until s. CNN then alternates 1D convolution op-\\nerations and 1D max-pooling operations, and transforms\\nthe message matrix and the response matrix to a mes-\\nsage vector (cid:126)m and a response vector (cid:126)r respectively. Let\\ndenotes the output of\\nz(l,f ) =\\nthe l-th layer under the f-th feature map (among Fl of\\n\\xe2\\x88\\x88 Rd(l,f ) and z(0,f ) = S. In convo-\\nthem), where z(l,f )\\nlution, CNN slides a window with width k(l,f )\\non z(l,f )\\nand splits z(l,f ) into several segments. For the i-th segment\\nz(l,f )\\n, the output of convolution is\\ni\\n\\n, . . . , z(l,f )\\ns(l,f )\\n\\n, . . . , z(l,f )\\n\\n, z(l,f )\\n\\nz(l,f )\\ni\\n\\nz(l,f )\\n1\\n\\n(cid:20)\\n\\n(cid:105)\\n\\n=\\n\\n2\\n\\nj\\n\\n1\\n\\n(cid:21)\\ni W(l,f ) + b(l,f )(cid:17)\\n\\n,\\n\\n= \\xcf\\x83\\n\\nz(l,f )\\n\\nz(l+1,f )\\ni\\n\\n(2)\\nwhere W(l,f ) \\xe2\\x88\\x88 Rk(l,f ) and b(l,f ) \\xe2\\x88\\x88 Rd(l,f ) are parameters,\\nand \\xcf\\x83(\\xc2\\xb7) is an activation function. In max-pooling, the output\\nof convolution is shrunk in order to enhance robustness. Let\\nk(l,f )\\ndenote the width of the window for max-pooling, then\\n2\\nthe output of max-pooling is\\n\\ni+k(l,f )\\n\\n1\\n\\n(cid:16)\\n\\n2\\n\\n,\\n\\nz(l,f )\\ni\\n\\n. . . z(l,f )\\n\\nz(l+1,f )\\ni\\n\\ni+k(l,f )\\n\\n= max\\n\\n(3)\\nwhere max(\\xc2\\xb7) is an element-wise operator over vectors.\\nCNN obtains (cid:126)m \\xe2\\x88\\x88 Rn and (cid:126)r \\xe2\\x88\\x88 Rn by concatenating the\\nvectors from the \\xef\\xac\\x81nal layer.\\n\\nTo leverage the topic words for matching, TACNTN uti-\\nlizes topic embedding layers to transform the topic words\\nto two topic vectors, one for m and the other for r. Given\\nWm = {wm,1, . . . , wm,n} as the topic word set for m, we\\n(cid:62) by looking up\\nconstruct a matrix Tm = [em,1, . . . , em,n]\\na word embedding table for each word in Wm. We then cal-\\nculate weights of the topic words by\\n\\n\\xcf\\x89m = Tm \\xc2\\xb7 A \\xc2\\xb7 (cid:126)m,\\n\\n(4)\\nwhere A \\xe2\\x88\\x88 Rd\\xc3\\x97n is a linear transformation learned from\\ntraining data, and \\xe2\\x88\\x80j, \\xcf\\x89m,j \\xe2\\x88\\x88 \\xcf\\x89m is the weight for the j-th\\nword in Wm. We scale \\xcf\\x89m,j to [0, 1] by\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:80)n\\nn(cid:88)\\n\\n\\xce\\xb1m,j =\\n\\nexp (\\xcf\\x89m,j)\\np=1 exp (\\xcf\\x89m,p)\\n\\n.\\n\\n(5)\\n\\nFinally, we form a topic vector (cid:126)tm by a linear combination\\nof the topic words:\\n\\n(cid:126)tm =\\n\\n\\xce\\xb1m,jem,j.\\n\\n(6)\\n\\nj=1\\n\\nFollowing the same technique, we have a topic vector (cid:126)tr\\nfor response r. From Equation (4), (5), and (6), we can see\\nthat the more important a topic word is, the more contri-\\nbutions it will make to the topic vector. The importance of\\ntopic words are determined by both themselves and the mes-\\nsage (or the response). The idea here is inspired by the at-\\ntention mechanism proposed for machine translation (Bah-\\ndanau, Cho, and Bengio 2014). We borrow the idea of the\\n\\nFigure 1: Graphical model of Twitter LDA\\n\\nword w regarding to a topic t as\\n\\ns(w, t) =\\n\\n\\xc2\\xb7 ct\\nw,\\n\\nct\\nw\\ncw\\n\\n(1)\\n\\n). ct\\nw\\ncw\\n\\nwhere ct\\nw is the number of times that word w is assigned a\\ntopic t in the training data and cw is the number of times that\\nw is determined as a topic word in the training data. Equa-\\ntion (1) means that the salience of a word regarding to a topic\\nis determined by the frequency of the word under the topic\\nw) and the probability of the word only belonging to\\n(i.e., ct\\nthe topic (i.e., ct\\nplays a similar role to IDF in infor-\\nw\\ncw\\nmation retrieval, and is capable of reducing the importance\\nof common words like \\xe2\\x80\\x9dyes\\xe2\\x80\\x9d and \\xe2\\x80\\x9dcause\\xe2\\x80\\x9d to topic t. With\\nEquation (1), we select top n words regarding to the topic of\\nmi and the topic of ri to form the topic word set Wm,i and\\nWr,i respectively.\\n\\nIn our experiments, we trained Twitter LDA models using\\nlarge scale questions from Yahoo! Answers and posts from\\nSina Weibo. The data provides topic knowledge apart from\\nthat in message-response pairs to the learning of message-\\nresponse matching. The process is similar to how people\\nlearn to respond in conversation: they become aware of what\\ncan be talked about from Internet, especially from social me-\\ndia, and converse with others based on what they learned.\\n\\nNote that in addition to LDA, one can employ other tech-\\nniques like tag recommendation (Wu et al. 2016) or keyword\\nextraction (Wu et al. 2015) to generate topic words. One can\\nalso get topic words from other resources like wikipedia and\\nother web documents. We leave the discussion of these ex-\\ntensions as our future work.\\n\\nTopic-aware Convolutional Neural Tensor\\n\\nNetwork\\n\\nWe propose a topic-aware convolutional neural tensor net-\\nwork (TACNTN) to leverage the topic words obtained from\\nTwitter LDA in message-response matching. Figure 2 gives\\nthe architecture of our model. Given a message m and a re-\\nsponse r, our model embeds them into a vector space by a\\nsiamese neural network that consists of two convolutional\\nneural networks (CNNs) with shared weight. For each sen-\\ntence (either m or r), CNN \\xef\\xac\\x81rst looks up a word embed-\\nding table and forms a sentence matrix S = [v1, v2, . . . , vs]\\nas input, where vj \\xe2\\x88\\x88 Rd is the embedding of the j-\\nth word and s in the maximum length of the sentence.\\n\\n\\xef\\x81\\xa2\\xef\\x81\\xa1\\xef\\x81\\xb1\\xef\\x81\\xb0B\\xef\\x81\\xa6\\xef\\x81\\xa2Tt\\xef\\x81\\xa6W\\xef\\x81\\xa7mTmz,mlw,mlY\\x0cFigure 2: The architecture of our model\\n\\n(cid:16)\\n\\nattention mechanism here, because it well models the ineuiq-\\nvalent contribution of topic words in matching. Topic words\\nthat are more relevant to the message or to the response are\\nmore useful in matching.\\n\\nWe calculate a matching score for (m, r) by neural tensor\\nnetworks (NTNs) (Socher et al. 2013; Qiu and Huang 2015).\\nThe advantage of NTN is that it enables us to build a match-\\ning function in a bottom-up way, that is we can \\xef\\xac\\x81rst model\\nrelationships between message-response, message-response\\ntopic, and message topic-response, then synthesize these\\nsub-matching elements to a \\xef\\xac\\x81nal matching score. Speci\\xef\\xac\\x81-\\ncally, given (cid:126)m and (cid:126)r, a neural tensor s( (cid:126)m, (cid:126)r) is de\\xef\\xac\\x81ned as\\n\\n(cid:17)\\n\\n(cid:124)\\n\\n(cid:124)\\n\\n(cid:124)\\n(cid:126)m\\n\\n(cid:124)\\nM[1:h](cid:126)r + V [ (cid:126)m\\n\\n,\\n\\n]\\n\\n, (cid:126)r\\n\\n+ b\\n\\ns( (cid:126)m, (cid:126)r) = f\\n\\n(7)\\nwhere f (\\xc2\\xb7) is a nonlinear function, and M[1:h] \\xe2\\x88\\x88 Rn\\xc3\\x97h\\xc3\\x97n is\\na tensor. The result of the bilinear tensor product (cid:126)mM[1:h](cid:126)r\\nis a vector (cid:126)v \\xe2\\x88\\x88 Rh with each entry a matching of m and r\\nparameterized by a slice k of M, k = 1, 2...h. V \\xe2\\x88\\x88 Rh\\xc3\\x972n\\nand b \\xe2\\x88\\x88 Rh are the other two parameters. We employ\\nthree such neural tensors and let them individually oper-\\n\\n(cid:1), resulting in three vec-\\n\\nate on ( (cid:126)m, (cid:126)r),(cid:0) (cid:126)m, (cid:126)tr\\n\\n(cid:1), and(cid:0)(cid:126)r, (cid:126)tm\\n\\n(cid:1) ,\\n\\ntors s( (cid:126)m, (cid:126)r), s( (cid:126)m, (cid:126)tr), and s((cid:126)r, (cid:126)tm). Each vector models the\\nmatching between the two objects from multiple perspec-\\ntives parameterized by the slices of the tensor. With these\\nvectors, we de\\xef\\xac\\x81ne g(m, r) as\\n\\n(cid:124)(cid:2)s( (cid:126)m, (cid:126)r), s((cid:126)r, (cid:126)tm), s( (cid:126)m, (cid:126)tr)(cid:3) + b2\\n\\ng(m, r) = h(cid:0)w\\n\\n(8)\\nwhere h is a softmax function, and w and b2 are param-\\neters. We extend the convolutional neural tensor network\\n(CNTN) proposed for community question answering (Qiu\\nand Huang 2015) by topic embedding layers and apply the\\nnew model to the problem of message-response matching.\\nThe model inherits the matching power from CNTN, and\\nnaturally incorporates extra topic information into matching.\\nNote that in TACNTN, topic learning and matching are\\nconducted in two steps. This is because by this means we can\\nleverage data other than message-response pairs for match-\\ning. For example, we can estimate topic words from ques-\\ntions in Yahoo! Answers and use them in message-response\\nmatching. These data provides extra topic information other\\n\\nN(cid:88)\\n\\nthan that in message-response pairs. Our model explicitly\\nutilizes such information as prior, and that is why we call\\nit \\xe2\\x80\\x9ctopic-aware\\xe2\\x80\\x9d. This is more close to how people learn to\\nrespond in conversation: before conversation, they have al-\\nready had some knowledge learned from other places (e.g.,\\nsocial media) in their mind.\\nWe learn g(\\xc2\\xb7,\\xc2\\xb7) by minimizing cross entropy (Levin and\\nFleisher 1988) with D. Let \\xce\\x98 denote the parameters in our\\nmodel. Our objective function L(D; \\xce\\x98) is given by\\n\\ni=1\\n\\n\\xe2\\x88\\x92 1\\nN\\n\\n[yi log (g(mi, ri)) + (1 \\xe2\\x88\\x92 yi) log (1 \\xe2\\x88\\x92 g(mi, ri))] .\\n(9)\\nWe optimize the objective function using back-propagation\\nand the parameters are updated by stochastic gradient de-\\nscent with Adam algorithm (Kingma and Ba 2014) control-\\nling the learning rate. As regularization, we employ early-\\nstopping (Lawrence and Giles 2000) as it is enough to pre-\\nvent over-\\xef\\xac\\x81tting on large scale training data (1 million in-\\nstances). We set the initial training rate and the batch size as\\n0.01 and 200 respectively.\\n\\nWe implement TACNTN using Theano. In the implemen-\\ntation, we only use one convolution layer and one max-\\npooling layer, because we \\xef\\xac\\x81nd that the performance of the\\nmodel does not get better with the number of layers in-\\ncreased. We use ReLU (Dahl, Sainath, and Hinton 2013) as\\nthe activation function \\xcf\\x83(\\xc2\\xb7) and Tanh as the activation func-\\ntion f (\\xc2\\xb7) in neural tensors. The code is shared at https:\\n//github.com/MarkWuNLP/TACNTN.\\n\\nExperiment\\n\\nWe tested our model on a public English data set and an in-\\nhouse Chinese data set.\\n\\nExperiment Setup\\nThe English data set is the Ubuntu Corpus (Lowe et al.\\n2015) which consists of a large number of human-human\\ndialogues about Ubuntu-related technique support collected\\nfrom Ubuntu chat rooms. Each dialogue contains at least\\n3 turns, and we only kept the last turn as we focus on\\n\\n+ + f + + f + + f ScoreWeight CalculatorTopic wordsTopic wordsWeight CalculatorConvolution & PoolingConvolution & PoolingMessageResponse......Message VectorResponse VectorNeural TensorsTopic EmebbedingTopic Emebbeding\\x0cTable 2: Evaluation results on the Ubuntu data and the Tieba data\\n\\nUbuntu data\\n\\nTieba data\\n\\nRandom\\nCosine\\nTranslation\\nDeepMatchtopic\\nMLP\\nCNTN\\nLSTM\\nArc1\\nArc2\\nTACNTN\\n\\nR2@1 R5@1 R10@1 R10@2 R10@5 MAP MRR\\n0.695\\n0.500\\n0.662\\n0.681\\n0.760\\n0.721\\n0.593\\n0.725\\n0.712\\n0.651\\n0.797\\n0.743\\n0.797\\n0.725\\n0.771\\n0.665\\n0.736\\n0.783\\n0.804\\n0.759\\n\\n0.200\\n0.482\\n0.507\\n0.376\\n0.380\\n0.512\\n0.529\\n0.360\\n0.534\\n0.544\\n\\n0.500\\n0.686\\n0.727\\n0.693\\n0.703\\n0.797\\n0.801\\n0.684\\n0.777\\n0.809\\n\\n0.200\\n0.470\\n0.502\\n0.345\\n0.362\\n0.489\\n0.494\\n0.372\\n0.508\\n0.520\\n\\n0.642\\n0.597\\n0.710\\n0.677\\n0.653\\n0.731\\n0.732\\n0.698\\n0.708\\n0.749\\n\\n0.100\\n0.383\\n0.393\\n0.248\\n0.256\\n0.349\\n0.361\\n0.221\\n0.380\\n0.382\\n\\nP@1\\n0.524\\n0.553\\n0.658\\n0.594\\n0.550\\n0.670\\n0.670\\n0.640\\n0.660\\n0.688\\n\\nsingle-turn conversation in this work. We used the data pre-\\nprocessed by Xu et al. (Xu et al. 2016)2, in which all urls and\\nnumbers were replaced by \\xe2\\x80\\x9c url \\xe2\\x80\\x9d and \\xe2\\x80\\x9c number \\xe2\\x80\\x9d respec-\\ntively to alleviate the sparsity issue. The training set con-\\ntains 1 million message-response pairs with a ratio 1 : 1\\nbetween positive and negative responses, and both the vali-\\ndation set and the test set have 0.5 million message-response\\npairs with a ratio 1 : 9 between positive and negative re-\\nsponses. All the negative responses are randomly sampled\\nrather than labeled by human annotators. We built the Chi-\\nnese data set from Baidu Tieba which is the largest Chinese\\nforum allowing users to post and comment to others\\xe2\\x80\\x99 posts.\\nWe \\xef\\xac\\x81rst crawled 0.6 million text post-comment pairs as pos-\\nitive message-response pairs (i.e., an (m, r) with a y = 1).\\nThen, for each post, we randomly sampled another com-\\nment from the 0.6 million data to create a negative message-\\nresponse pair. The two sets together form a training set with\\n1.2 million instances. Following the same procedure, we\\nbuilt a validation set with 50, 000 instances apart from those\\nin training. To construct a test set, we simulated the process\\nof a retrieval-based chatbot: we \\xef\\xac\\x81rst indexed the 0.6 million\\npost-comment pairs by an open source Lucene.Net3. Then,\\nwe crawled another 400 posts that are in the training set and\\nthe validation set as test messages. For each test message,\\nwe retrieved several similar posts from the index, and col-\\nlected all the responses associated with the similar posts as\\ncandidates. We recruited three human labelers to judge if a\\ncandidate is a proper response to a test message. A proper\\nresponse means the response can naturally reply to the mes-\\nsage without any contextual information. Each candidate re-\\nsponse received three labels and the majority of the labels\\nwas taken as the \\xef\\xac\\x81nal decision. After removing messages\\nwithout any proper responses (i.e., two or more labels are\\n0), we obtained 328 test messages with 3, 418 responses. On\\naverage, each test message has 10.4 labeled responses, and\\nthe ratio between positive and negative responses is 3:2.\\n\\nFor the English data, we crawled 8 million questions (ti-\\ntle and body) from the \\xe2\\x80\\x9cComputers & Internet\\xe2\\x80\\x9d category in\\nYahoo! Answers, and utilized these data to train the Twit-\\nter LDA model. Word embedding tables were initialized us-\\ning the public word vectors available at http://nlp.\\n\\n2https://www.dropbox.com/s/\\n\\n2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\\n\\n3http://lucenenet.apache.org\\n\\nstanford.edu/projects/glove (trained on Twit-\\nter). For the Chinese data, we trained the Twitter LDA model\\nand the word vectors for initializing embedding tables using\\n30 million posts crawled from Sina Weibo. In both data, the\\ndimension of word vectors is 100.\\n\\nOn the Ubuntu data, we followed Lowe et al. (Lowe et\\nal. 2015) and employed recall at position k in n candidates\\n(Rn@k) as evaluation metrics, while on the human anno-\\ntated Chinese data, we employed mean average precision\\n(MAP) (Baeza-Yates, Ribeiro-Neto, and others 1999), mean\\nreciprocal rank (MRR) (Voorhees and others 1999), and pre-\\ncision at position 1 (P@1) as evaluation metrics.\\n\\nBaseline\\nWe considered the following models as baselines:\\n\\nCosine: we calculated cosine similarity between a mes-\\n\\nsage and a response using their tf-idf weighted vectors.\\n\\nTranslation model: we learned word-to-word trans-\\nlation probabilities using GIZA++4 by regarding mes-\\nsages in training sets as a source language and their\\npositive responses as a target\\nlanguage. Following (Ji,\\nLu, and Li 2014), we used translation probability\\np(response|message) as a matching score.\\n\\nMulti-layer perceptron (MLP): a message and a re-\\nsponse were represented as vectors by averaging their word\\nvectors. The two vectors were fed to a two-layer feedforward\\nneural network to calculate a matching score. MLP shared\\nthe embedding tables with our model. The \\xef\\xac\\x81rst hidden layer\\nhas 100 nodes, and the second hidden layer has 2 nodes.\\n\\nDeepMatchtopic: the matching model proposed in (Lu\\nand Li 2013) which only used topic information to match\\na message and a response.\\n\\nLSTM: the best performing model in (Lowe et al. 2015).\\nA message and a response are separately fed to a LSTM\\nnetwork and matching score is calculated with the output\\nvectors of the LSTM networks.\\n\\nCNNs: the CNN models proposed by Hu et al. (2014),\\nnamely Arc1 and Arc2. The number of feature maps and the\\nwidth of windows are the same as our model.\\n\\nCNTN: the convolution neural tensor network (Qiu and\\nHuang 2015) proposed for community question answering.\\n\\n4http://www.statmt.org/moses/giza/GIZA++.html\\n\\n\\x0cR2@1\\nR5@1\\nR10@1\\nR10@2\\nR10@5\\n\\nMAP\\nMRR\\nP@1\\n\\nExp\\n0.750\\n0.499\\n0.364\\n0.528\\n0.792\\n\\nExp\\n0.734\\n0.802\\n0.671\\n\\nUbuntu data\\nMsg\\nAvg\\n0.754\\n0.755\\n0.498\\n0.508\\n0.357\\n0.373\\n0.538\\n0.528\\n0.811\\n0.804\\nTieba data\\nAvg\\n0.744\\n0.798\\n0.677\\n\\nMsg\\n0.744\\n0.799\\n0.677\\n\\nTable 3: Variants of TACNTN\\n\\nRes\\n0.749\\n0.498\\n0.362\\n0.538\\n0.805\\n\\nRes\\n0.732\\n0.800\\n0.679\\n\\nFull\\n0.759\\n0.520\\n0.382\\n0.544\\n0.809\\n\\nFull\\n0.749\\n0.804\\n0.688\\n\\nParameter Tuning\\nWe tuned parameters according to R2@1 in the Ubuntu\\ndata and P@1 in the Tieba data. For Twitter LDA, we set\\n\\xce\\xb1 = 1/T , \\xce\\xb2 = 0.01, \\xce\\xb3 = 0.01 in the Dirichlet priors.\\nWe tuned the number of topics (i.e. T ) in {20, 50, 100, 200}\\nand the maximum iteration number of Gibbs sampling in\\n{100, 200, . . . , 1000}. The best combination for both data\\nsets is (200, 1000). The number of topic words was tuned in\\n{10, 20, . . . , 100} and set as 50 \\xef\\xac\\x81nally. In CNN based mod-\\nels, we set the maximum sentence length (i.e., s) as 20. We\\ntuned the number of feature maps in {10, 50, 100, 200} and\\nfound that 50 is the best choice. We tuned the window size\\nin {1, 2, 3, 4} and set it as 3 for convolution and pooling lay-\\ners. We varied the number of slices in neural tensors (i.e.,h)\\nin {1, 2, . . . 10} and set it as 8.\\nEvaluation Results\\nTable 2 reports evaluation results on the English data and the\\nChinese data. We can see that on most metrics, our method\\nperforms better than baselines, and the improvement is sta-\\ntistically signi\\xef\\xac\\x81cant (t-test with p-value \\xe2\\x89\\xa4 0.01).\\n\\nBoth DeepMatchtopic and MLP perform badly, indicating\\nthat we cannot simply rely on topics and word embeddings\\nto match messages and responses. Among CNN based mod-\\nels, Arc1 is the worst. This is consistent with the conclusions\\ndrawn by the existing work. Our model consistently outper-\\nforms all baseline methods on both data sets. The result ver-\\ni\\xef\\xac\\x81ed the effectiveness of the topic information in message-\\nresponse matching.\\n\\nIn the Ubuntu data, all models became worse when the\\nnumber of candidates (i.e., n in Rn@k) increased and better\\nwhen k increased. This is because a test message only has\\none positive response. Obviously, ranking the only positive\\none to the top becomes more dif\\xef\\xac\\x81cult when more negative\\ncompetitors come in, but things become easier when we al-\\nlow the positive one to be ranked at lower positions.\\n\\nDiscussions\\nWe \\xef\\xac\\x81rst examine if the topic information can improve the\\nmatching between messages and responses with rich con-\\ntent. Table 4(a) gives a detailed analysis of the example in\\nTable 1 which is from Tieba data. The best performing base-\\nline CNTN failed on this case because of the high asymme-\\n\\nTable 4: Analysis of TACNTN\\n\\nMessage\\nMessage\\ntopic\\nwords and weights\\n\\nResponse\\n\\nResponse\\ntopic\\nwords and weights\\nCNTN\\nTACNTN\\nLabel\\n\\n(a) A case study\\nIs the new Batman movie worth watch-\\ning?\\nmovie: 0.198 , character: 0.187, plot\\n:0.087\\nI swear you won\\xe2\\x80\\x99t regret watching it.\\nWe \\xef\\xac\\x81nally get Batman as a fully ren-\\ndered character. The \\xef\\xac\\x81lm shows the\\nvariables he must contend with in his\\nrole as a protector of Gotham.\\nmovie: 0.405, character: 0.217 , hero:\\n0.184\\n0.447\\n0.959\\nRelevant\\n\\n(b) Quantitative analysis\\n\\nResponse Length\\nCNTN on Ubuntu\\nTACNTN\\nUbuntu\\nCNTN on Tieba\\nTACNTN on Tieba\\n\\non\\n\\n<10\\n0.673\\n0.675\\n0.586\\n0.588\\n\\n\\xe2\\x89\\xa5 10\\n0.702\\n0.723\\n0.589\\n0.612\\n\\ntry between the message and the response on the informa-\\ntion they contain. On the other hand, message topics like\\n\\xe2\\x80\\x9ccharacter\\xe2\\x80\\x9d matched the content of the response, and re-\\nsponse topics indicated that the message and the response\\nare in the same topics. With these signals, our model suc-\\ncessfully overcame the semantic gap between the message\\nand the response. We also compared our model with CNTN\\non short (less than 10 words) and long (more than or equal to\\n10 words) responses. Table 4(b) reports classi\\xef\\xac\\x81cation accu-\\nracy on the two data sets. We use classi\\xef\\xac\\x81cation accuracy be-\\ncause we make comparison in terms of different responses.\\nWe can see that almost all improvement of our model came\\nfrom long responses. Since long responses contain rich con-\\ntent, the results provided quantitative evidence to our claim\\nthat we can improve the matching between messages and\\nresponses with rich content using topic information.\\n\\nFinally, we compared TACNTN with several variants in-\\ncluding TACNTNexp in which we expand the message and\\nthe response with their topic words and conduct matching\\nusing CNTN on the expanded texts, TACNTNavg in which\\nthe topic vectors are not learned with the message vector\\nand the response vector but average of the embedding of the\\ntopic words, TACNTNmsg in which only message topics are\\nconsidered, and TACNTNres in which only response top-\\nics are considered. The four variants are denoted as Exp,\\nAvg, Msg and Res respectively. Table 3 shows that 1) uti-\\nlizing topic information in a straightforward way (i.e., ex-\\npansion) does not help too much; 2) learning topic vectors\\nwith the message vector and the response vector is helpful\\nto matching as they may help \\xef\\xac\\x81lter noise in the topic words,\\nand that is why TACNTN outperforms TACNTNavg; 3) both\\nthe message topic vector and the response topic vector are\\nuseful, and we cannot leave either of them out.\\n\\n\\x0cConclusion\\n\\nThis paper proposed a TACNTN model that can lever-\\nage topic information in message-response matching for\\nretrieval-based chatbots. Experimental results show that our\\nmodel can signi\\xef\\xac\\x81cantly outperform state-of-the-art match-\\ning models on public and human annotated data sets.\\n\\nReferences\\n\\nR.;\\n1999. Modern information retrieval,\\n\\n[Baeza-Yates, Ribeiro-Neto, and others 1999] Baeza-Yates,\\nRibeiro-Neto, B.; et al.\\nvolume 463. ACM press New York.\\n[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.; and\\nBengio, Y. 2014. Neural machine translation by jointly learning to\\nalign and translate. arXiv preprint arXiv:1409.0473.\\n[Collobert et al. 2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen,\\nM.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural language pro-\\ncessing (almost) from scratch. The Journal of Machine Learning\\nResearch 12:2493\\xe2\\x80\\x932537.\\n[Dahl, Sainath, and Hinton 2013] Dahl, G. E.; Sainath, T. N.; and\\nHinton, G. E. 2013.\\nImproving deep neural networks for lvcsr\\nusing recti\\xef\\xac\\x81ed linear units and dropout. In Acoustics, Speech and\\nSignal Processing (ICASSP), 2013 IEEE International Conference\\non, 8609\\xe2\\x80\\x938613. IEEE.\\n[dos Santos and Gatti 2014] dos Santos, C. N., and Gatti, M. 2014.\\nDeep convolutional neural networks for sentiment analysis of short\\ntexts. In COLING, 69\\xe2\\x80\\x9378.\\n[Hu et al. 2014] Hu, B.; Lu, Z.; Li, H.; and Chen, Q. 2014. Convo-\\nlutional neural network architectures for matching natural language\\nsentences. In Advances in Neural Information Processing Systems,\\n2042\\xe2\\x80\\x932050.\\n[Ji, Lu, and Li 2014] Ji, Z.; Lu, Z.; and Li, H. 2014. An informa-\\ntion retrieval approach to short text conversation. arXiv preprint\\narXiv:1408.6988.\\n[Kim 2014] Kim, Y. 2014. Convolutional neural networks for sen-\\ntence classi\\xef\\xac\\x81cation. arXiv preprint arXiv:1408.5882.\\n[Kingma and Ba 2014] Kingma, D., and Ba, J.\\nA method for\\narXiv:1412.6980.\\n[Lawrence and Giles 2000] Lawrence, S., and Giles, C. L. 2000.\\nOver\\xef\\xac\\x81tting and neural networks: conjugate gradient and backprop-\\nIn Neural Networks, 2000. IJCNN 2000, Proceedings\\nagation.\\nof the IEEE-INNS-ENNS International Joint Conference on, vol-\\nume 1, 114\\xe2\\x80\\x93119. IEEE.\\n[Levin and Fleisher 1988] Levin, E., and Fleisher, M. 1988. Ac-\\ncelerated learning in layered neural networks. Complex systems\\n2:625\\xe2\\x80\\x93640.\\n[Li and Liu 2015] Li, M. W. Z. L. H., and Liu, W. J. Q. 2015. A\\nconvolutional architecture for word sequence prediction.\\n[Li et al. 2015] Li, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan,\\nB. 2015. A diversity-promoting objective function for neural con-\\nversation models. arXiv preprint arXiv:1510.03055.\\n[Li et al. 2016] Li, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan,\\nB. 2016. A persona-based neural conversation model. arXiv\\npreprint arXiv:1603.06155.\\n[Lowe et al. 2015] Lowe, R.; Pow, N.; Serban, I.; and Pineau, J.\\n2015.\\nThe ubuntu dialogue corpus: A large dataset for re-\\nsearch in unstructured multi-turn dialogue systems. arXiv preprint\\narXiv:1506.08909.\\n\\nstochastic optimization.\\n\\n2014. Adam:\\narXiv preprint\\n\\n[Lu and Li 2013] Lu, Z., and Li, H. 2013. A deep architecture for\\nmatching short texts. In Advances in Neural Information Process-\\ning Systems, 1367\\xe2\\x80\\x931375.\\n[Pang et al. 2016] Pang, L.; Lan, Y.; Guo, J.; Xu, J.; Wan, S.; and\\nCheng, X. 2016. Text matching as image recognition.\\n[Perez-Marin 2011] Perez-Marin, D. 2011. Conversational Agents\\nand Natural Language Interaction: Techniques and Effective Prac-\\ntices: Techniques and Effective Practices. IGI Global.\\n[Qiu and Huang 2015] Qiu, X., and Huang, X. 2015. Convolutional\\nneural tensor network architecture for community-based question\\nanswering. In Proceedings of the 24th International Joint Confer-\\nence on Arti\\xef\\xac\\x81cial Intelligence (IJCAI), 1305\\xe2\\x80\\x931311.\\n[Ritter, Cherry, and Dolan 2011] Ritter, A.; Cherry, C.; and Dolan,\\nW. B. 2011. Data-driven response generation in social media. In\\nProceedings of the Conference on Empirical Methods in Natural\\nLanguage Processing, 583\\xe2\\x80\\x93593. Association for Computational\\nLinguistics.\\n[Serban et al. 2016] Serban,\\nI. V.; Sordoni, A.; Bengio, Y.;\\nCourville, A. C.; and Pineau, J. 2016. Building end-to-end dia-\\nlogue systems using generative hierarchical neural network mod-\\nels. In Proceedings of the Thirtieth AAAI Conference on Arti\\xef\\xac\\x81cial\\nIntelligence, February 12-17, 2016, Phoenix, Arizona, USA., 3776\\xe2\\x80\\x93\\n3784.\\n[Shang, Lu, and Li 2015] Shang, L.; Lu, Z.; and Li, H. 2015. Neu-\\nral responding machine for short-text conversation. In ACL 2015,\\nJuly 26-31, 2015, Beijing, China, Volume 1: Long Papers, 1577\\xe2\\x80\\x93\\n1586.\\n[Shen et al. 2014] Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mes-\\nnil, G. 2014. A latent semantic model with convolutional-pooling\\nIn Proceedings of the 23rd\\nstructure for information retrieval.\\nACM International Conference on Conference on Information and\\nKnowledge Management, 101\\xe2\\x80\\x93110. ACM.\\n[Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng,\\nA. 2013. Reasoning with neural tensor networks for knowledge\\nbase completion. In Advances in Neural Information Processing\\nSystems, 926\\xe2\\x80\\x93934.\\n[Sordoni et al. 2015] Sordoni, A.; Galley, M.; Auli, M.; Brockett,\\nC.; Ji, Y.; Mitchell, M.; Nie, J.-Y.; Gao, J.; and Dolan, B. 2015. A\\nneural network approach to context-sensitive generation of conver-\\nsational responses. arXiv preprint arXiv:1506.06714.\\n[Stent and Bangalore 2014] Stent, A., and Bangalore, S. 2014. Nat-\\nural Language Generation in Interactive Systems. Cambridge Uni-\\nversity Press.\\n[Sun et al. 2015] Sun, Y.; Lin, L.; Tang, D.; Yang, N.; Ji, Z.; and\\nWang, X. 2015. Modeling mention, context and entity with neural\\nnetworks for entity disambiguation. In Proceedings of the Twenty-\\nFourth International Joint Conference on Arti\\xef\\xac\\x81cial Intelligence (IJ-\\nCAI), 1333\\xe2\\x80\\x931339.\\n[Vinyals and Le 2015] Vinyals, O., and Le, Q. 2015. A neural con-\\nversational model. arXiv preprint arXiv:1506.05869.\\n[Voorhees and others 1999] Voorhees, E. M., et al. 1999. The trec-8\\nquestion answering track report. In Trec, volume 99, 77\\xe2\\x80\\x9382.\\n[Wang et al. 2015] Wang, M.; Lu, Z.; Li, H.; and Liu, Q. 2015.\\nSyntax-based deep matching of short texts. In Twenty-Fourth In-\\nternational Joint Conference on Arti\\xef\\xac\\x81cial Intelligence.\\n[Weizenbaum 1966] Weizenbaum, J. 1966. Eliza?a computer pro-\\ngram for the study of natural language communication between\\nman and machine. Communications of the ACM 9(1):36\\xe2\\x80\\x9345.\\n[Weston, Chopra, and Adams 2014] Weston, J.; Chopra, S.; and\\nAdams, K. 2014. # tagspace: Semantic embeddings from hash-\\ntags.\\n\\n\\x0c[Wu et al. 2015] Wu, Y.; Wu, W.; Li, Z.; and Zhou, M. 2015. Min-\\ning query subtopics from questions in community question answer-\\ning. In AAAI, 339\\xe2\\x80\\x93345.\\n[Wu et al. 2016] Wu, Y.; Wu, W.; Li, Z.; and Zhou, M. 2016. Im-\\nproving recommendation of tail tags for questions in community\\nquestion answering. In Thirtieth AAAI Conference on Arti\\xef\\xac\\x81cial In-\\ntelligence.\\n[Xu et al. 2016] Xu, Z.; Liu, B.; Wang, B.; Sun, C.; and Wang,\\nIncorporating loose-structured knowledge into lstm\\nX.\\narXiv preprint\\nwith recall gate for conversation modeling.\\narXiv:1605.05110.\\n[Yang, Yih, and Meek 2015] Yang, Y.; Yih, W.-t.; and Meek, C.\\n2015. Wikiqa: A challenge dataset for open-domain question an-\\n\\n2016.\\n\\nIn Proceedings of the 2015 Conference on Empirical\\n\\nswering.\\nMethods in Natural Language Processing, 2013\\xe2\\x80\\x932018. Citeseer.\\n[Yin and Sch\\xc2\\xa8utze 2015] Yin, W., and Sch\\xc2\\xa8utze, H. 2015. Multi-\\ngrancnn: An architecture for general matching of text chunks on\\nmultiple levels of granularity. In Proceedings of the 53rd Annual\\nMeeting of the Association for Computational Linguistics (ACL),\\n63\\xe2\\x80\\x9373.\\n[Zhao et al. 2011] Zhao, W. X.; Jiang, J.; Weng, J.; He, J.; Lim, E.-\\nP.; Yan, H.; and Li, X. 2011. Comparing twitter and traditional\\nmedia using topic models. In Advances in Information Retrieval.\\nSpringer. 338\\xe2\\x80\\x93349.\\n\\n\\x0c',\n",
       " 'Deep Reinforcement Learning for Dialogue Generation\\n\\nJiwei Li1, Will Monroe1, Alan Ritter2, Michel Galley3, Jianfeng Gao3 and Dan Jurafsky1\\n\\n1Stanford University, Stanford, CA, USA\\n\\n2Ohio State University, OH, USA\\n\\n{jiweil,wmonroe4,jurafsky}@stanford.edu, ritter.1492@osu.edu\\n\\n3Microsoft Research, Redmond, WA, USA\\n{mgalley,jfgao}@microsoft.com\\n\\n6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n9\\n2\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n4\\nv\\n1\\n4\\n5\\n1\\n0\\n\\n.\\n\\n6\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nRecent neural models of dialogue generation\\noffer great promise for generating responses\\nfor conversational agents, but tend to be short-\\nsighted, predicting utterances one at a time\\nwhile ignoring their in\\xef\\xac\\x82uence on future out-\\ncomes. Modeling the future direction of a di-\\nalogue is crucial to generating coherent, inter-\\nesting dialogues, a need which led traditional\\nNLP models of dialogue to draw on reinforce-\\nment learning. In this paper, we show how to\\nintegrate these goals, applying deep reinforce-\\nment learning to model future reward in chat-\\nbot dialogue. The model simulates dialogues\\nbetween two virtual agents, using policy gradi-\\nent methods to reward sequences that display\\nthree useful conversational properties: infor-\\nmativity, coherence, and ease of answering (re-\\nlated to forward-looking function). We evalu-\\nate our model on diversity, length as well as\\nwith human judges, showing that the proposed\\nalgorithm generates more interactive responses\\nand manages to foster a more sustained conver-\\nsation in dialogue simulation. This work marks\\na \\xef\\xac\\x81rst step towards learning a neural conversa-\\ntional model based on the long-term success of\\ndialogues.\\n\\nIntroduction\\n\\n1\\nNeural response generation (Sordoni et al., 2015;\\nShang et al., 2015; Vinyals and Le, 2015; Li et al.,\\n2016a; Wen et al., 2015; Yao et al., 2015; Luan\\net al., 2016; Xu et al., 2016; Wen et al., 2016; Li\\net al., 2016b; Su et al., 2016) is of growing inter-\\nest. The LSTM sequence-to-sequence (SEQ2SEQ)\\nmodel (Sutskever et al., 2014) is one type of neural\\ngeneration model that maximizes the probability of\\ngenerating a response given the previous dialogue\\nturn. This approach enables the incorporation of rich\\n\\ncontext when mapping between consecutive dialogue\\nturns (Sordoni et al., 2015) in a way not possible, for\\nexample, with MT-based dialogue models (Ritter et\\nal., 2011).\\n\\nDespite the success of SEQ2SEQ models in di-\\nalogue generation,\\ntwo problems emerge: First,\\nSEQ2SEQ models are trained by predicting the next\\ndialogue turn in a given conversational context using\\nthe maximum-likelihood estimation (MLE) objective\\nfunction. However, it is not clear how well MLE\\napproximates the real-world goal of chatbot develop-\\nment: teaching a machine to converse with humans,\\nwhile providing interesting, diverse, and informative\\nfeedback that keeps users engaged. One concrete\\nexample is that SEQ2SEQ models tend to generate\\nhighly generic responses such as \\xe2\\x80\\x9cI don\\xe2\\x80\\x99t know\\xe2\\x80\\x9d re-\\ngardless of the input (Sordoni et al., 2015; Serban\\net al., 2016; Li et al., 2016a). This can be ascribed\\nto the high frequency of generic responses found in\\nthe training set and their compatibility with a diverse\\nrange of conversational contexts. Yet \\xe2\\x80\\x9cI don\\xe2\\x80\\x99t know\\xe2\\x80\\x9d\\nis apparently not a good action to take, since it closes\\nthe conversation down.\\n\\nAnother common problem, illustrated in the two\\nsample conversations on the left of Table 1, is that\\nthe system becomes stuck in an in\\xef\\xac\\x81nite loop of repet-\\nitive responses. This is due to MLE-based SEQ2SEQ\\nmodels\\xe2\\x80\\x99 inability to account for repetition. In exam-\\nple 2 (bottom left), the dialogue falls into an in\\xef\\xac\\x81nite\\nloop after three turns, with both agents generating\\ndull, generic utterances like i don\\xe2\\x80\\x99t know what you\\nare talking about and you don\\xe2\\x80\\x99t know what you are\\nsaying. Looking at the entire conversation, utterance\\n(4) turns out to be a bad action to take because it\\noffers no way of continuing the conversation.1\\n\\n1A similar rule is often suggested in improvisational comedy:\\n\\nhttps://en.wikipedia.org/wiki/Yes,_and...\\n\\n\\x0cBaseline mutual information model (Li et al. 2015)\\nA: Where are you going? (1)\\nB: I\\xe2\\x80\\x99m going to the restroom. (2)\\nA: See you later. (3)\\nB: See you later. (4)\\nA: See you later. (5)\\nB: See you later. (6)\\n...\\n...\\nA: how old are you? (1)\\nB: I\\xe2\\x80\\x99m 16. (2)\\nA: 16? (3)\\nB: I don\\xe2\\x80\\x99t know what you are talking about. (4)\\nA: You don\\xe2\\x80\\x99t know what you are saying. (5)\\nB: I don\\xe2\\x80\\x99t know what you are talking about . (6)\\nA: You don\\xe2\\x80\\x99t know what you are saying. (7)\\n...\\n\\nProposed reinforcement learning model\\nA: Where are you going? (1)\\nB: I\\xe2\\x80\\x99m going to the police station. (2)\\nA: I\\xe2\\x80\\x99ll come with you. (3)\\nB: No, no, no, no, you\\xe2\\x80\\x99re not going anywhere. (4)\\nA: Why? (5)\\nB: I need you to stay here. (6)\\nA: I don\\xe2\\x80\\x99t know what you are talking about. (7)\\n...\\nA: How old are you? (1)\\nB: I\\xe2\\x80\\x99m 16. Why are you asking? (2)\\nA I thought you were 12. (3)\\nB: What made you think so? (4)\\nA: I don\\xe2\\x80\\x99t know what you are talking about. (5)\\nB: You don\\xe2\\x80\\x99t know what you are saying. (6)\\n...\\n...\\n\\nTable 1: Left Column: Dialogue simulation between two agents using a 4-layer LSTM encoder-decoder\\ntrained on the OpenSubtitles dataset. The \\xef\\xac\\x81rst turn (index 1) is input by the authors. Then the two agents\\ntake turns conversing, taking as input the other agent\\xe2\\x80\\x99s prior generated turn. The output is generated using\\nthe mutual information model (Li et al., 2015) in which an N-best list is \\xef\\xac\\x81rst obtained using beam search\\nbased on p(t|s) and reranked by linearly combining the backward probability p(s|t), where t and s respectively\\ndenote targets and sources. Right Column: Dialogue simulated using the proposed reinforcement learning\\nmodel. The new model has more forward-looking utterances (questions like \\xe2\\x80\\x9cWhy are you asking?\\xe2\\x80\\x9d and\\noffers like \\xe2\\x80\\x9cI\\xe2\\x80\\x99ll come with you\\xe2\\x80\\x9d) and lasts longer before it falls into conversational black holes.\\n\\nThese challenges suggest we need a conversa-\\ntion framework that has the ability to (1) integrate\\ndeveloper-de\\xef\\xac\\x81ned rewards that better mimic the true\\ngoal of chatbot development and (2) model the long-\\nterm in\\xef\\xac\\x82uence of a generated response in an ongoing\\ndialogue.\\n\\nTo achieve these goals, we draw on the insights of\\nreinforcement learning, which have been widely ap-\\nplied in MDP and POMDP dialogue systems (see Re-\\nlated Work section for details). We introduce a neu-\\nral reinforcement learning (RL) generation method,\\nwhich can optimize long-term rewards designed by\\nsystem developers. Our model uses the encoder-\\ndecoder architecture as its backbone, and simulates\\nconversation between two virtual agents to explore\\nthe space of possible actions while learning to maxi-\\nmize expected reward. We de\\xef\\xac\\x81ne simple heuristic ap-\\nproximations to rewards that characterize good con-\\nversations: good conversations are forward-looking\\n(Allwood et al., 1992) or interactive (a turn suggests\\na following turn), informative, and coherent. The pa-\\nrameters of an encoder-decoder RNN de\\xef\\xac\\x81ne a policy\\nover an in\\xef\\xac\\x81nite action space consisting of all possible\\n\\nutterances. The agent learns a policy by optimizing\\nthe long-term developer-de\\xef\\xac\\x81ned reward from ongo-\\ning dialogue simulations using policy gradient meth-\\nods (Williams, 1992), rather than the MLE objective\\nde\\xef\\xac\\x81ned in standard SEQ2SEQ models.\\n\\nOur model thus integrates the power of SEQ2SEQ\\nsystems to learn compositional semantic meanings of\\nutterances with the strengths of reinforcement learn-\\ning in optimizing for long-term goals across a conver-\\nsation. Experimental results (sampled results at the\\nright panel of Table 1) demonstrate that our approach\\nfosters a more sustained dialogue and manages to\\nproduce more interactive responses than standard\\nSEQ2SEQ models trained using the MLE objective.\\n\\n2 Related Work\\n\\nEfforts to build statistical dialog systems fall into two\\nmajor categories.\\n\\nThe \\xef\\xac\\x81rst treats dialogue generation as a source-\\nto-target transduction problem and learns mapping\\nrules between input messages and responses from a\\nmassive amount of training data. Ritter et al. (2011)\\nframes the response generation problem as a statisti-\\n\\n\\x0ccal machine translation (SMT) problem. Sordoni et\\nal. (2015) improved Ritter et al.\\xe2\\x80\\x99s system by rescor-\\ning the outputs of a phrasal SMT-based conversation\\nsystem with a neural model that incorporates prior\\ncontext. Recent progress in SEQ2SEQ models inspire\\nseveral efforts (Vinyals and Le, 2015) to build end-\\nto-end conversational systems which \\xef\\xac\\x81rst apply an\\nencoder to map a message to a distributed vector rep-\\nresenting its semantics and generate a response from\\nthe message vector. Serban et al. (2016) propose\\na hierarchical neural model that captures dependen-\\ncies over an extended conversation history. Li et al.\\n(2016a) propose mutual information between mes-\\nsage and response as an alternative objective function\\nin order to reduce the proportion of generic responses\\nproduced by SEQ2SEQ systems.\\n\\nThe other line of statistical research focuses on\\nbuilding task-oriented dialogue systems to solve\\ndomain-speci\\xef\\xac\\x81c tasks. Efforts include statistical\\nmodels such as Markov Decision Processes (MDPs)\\n(Levin et al., 1997; Levin et al., 2000; Walker et al.,\\n2003; Pieraccini et al., 2009), POMDP (Young et\\nal., 2010; Young et al., 2013; Ga\\xcb\\x87sic et al., 2013a;\\nGa\\xcb\\x87sic et al., 2014) models, and models that statisti-\\ncally learn generation rules (Oh and Rudnicky, 2000;\\nRatnaparkhi, 2002; Banchs and Li, 2012; Nio et al.,\\n2014). This dialogue literature thus widely applies\\nreinforcement learning (Walker, 2000; Schatzmann\\net al., 2006; Gasic et al., 2013b; Singh et al., 1999;\\nSingh et al., 2000; Singh et al., 2002) to train dialogue\\npolicies. But task-oriented RL dialogue systems of-\\nten rely on carefully limited dialogue parameters, or\\nhand-built templates with state, action and reward sig-\\nnals designed by humans for each new domain, mak-\\ning the paradigm dif\\xef\\xac\\x81cult to extend to open-domain\\nscenarios.\\n\\nAlso relevant is prior work on reinforcement learn-\\ning for language understanding - including learning\\nfrom delayed reward signals by playing text-based\\ngames (Narasimhan et al., 2015; He et al., 2016),\\nexecuting instructions for Windows help (Branavan\\net al., 2011), or understanding dialogues that give\\nnavigation directions (Vogel and Jurafsky, 2010).\\n\\nOur goal is to integrate the SEQ2SEQ and rein-\\nforcement learning paradigms, drawing on the advan-\\ntages of both. We are thus particularly inspired by\\nrecent work that attempts to merge these paradigms,\\nincluding Wen et al. (2016)\\xe2\\x80\\x94 training an end-to-end\\n\\ntask-oriented dialogue system that links input repre-\\nsentations to slot-value pairs in a database\\xe2\\x80\\x94 or Su\\net al. (2016), who combine reinforcement learning\\nwith neural generation on tasks with real users, show-\\ning that reinforcement learning improves dialogue\\nperformance.\\n\\n3 Reinforcement Learning for\\n\\nOpen-Domain Dialogue\\n\\nIn this section, we describe in detail the components\\nof the proposed RL model.\\n\\nThe learning system consists of two agents. We\\nuse p to denote sentences generated from the \\xef\\xac\\x81rst\\nagent and q to denote sentences from the second.\\nThe two agents take turns talking with each other.\\nA dialogue can be represented as an alternating se-\\nquence of sentences generated by the two agents:\\np1, q1, p2, q2, ..., pi, qi. We view the generated sen-\\ntences as actions that are taken according to a policy\\nde\\xef\\xac\\x81ned by an encoder-decoder recurrent neural net-\\nwork language model.\\n\\nThe parameters of the network are optimized to\\nmaximize the expected future reward using policy\\nsearch, as described in Section 4.3. Policy gradi-\\nent methods are more appropriate for our scenario\\nthan Q-learning (Mnih et al., 2013), because we can\\ninitialize the encoder-decoder RNN using MLE pa-\\nrameters that already produce plausible responses,\\nbefore changing the objective and tuning towards a\\npolicy that maximizes long-term reward. Q-learning,\\non the other hand, directly estimates the future ex-\\npected reward of each action, which can differ from\\nthe MLE objective by orders of magnitude, thus mak-\\ning MLE parameters inappropriate for initialization.\\nThe components (states, actions, reward, etc.) of our\\nsequential decision problem are summarized in the\\nfollowing sub-sections.\\n\\n3.1 Action\\nAn action a is the dialogue utterance to generate.\\nThe action space is in\\xef\\xac\\x81nite since arbitrary-length se-\\nquences can be generated.\\n\\n3.2 State\\nA state is denoted by the previous two dialogue turns\\n[pi, qi]. The dialogue history is further transformed\\nto a vector representation by feeding the concatena-\\ntion of pi and qi into an LSTM encoder model as\\n\\n\\x0cdescribed in Li et al. (2016a).\\n\\n3.3 Policy\\nA policy takes the form of an LSTM encoder-decoder\\n(i.e., pRL(pi+1|pi, qi) ) and is de\\xef\\xac\\x81ned by its param-\\neters. Note that we use a stochastic representation\\nof the policy (a probability distribution over actions\\ngiven states). A deterministic policy would result in\\na discontinuous objective that is dif\\xef\\xac\\x81cult to optimize\\nusing gradient-based methods.\\n\\n3.4 Reward\\nr denotes the reward obtained for each action. In this\\nsubsection, we discuss major factors that contribute\\nto the success of a dialogue and describe how approx-\\nimations to these factors can be operationalized in\\ncomputable reward functions.\\n\\nEase of answering A turn generated by a machine\\nshould be easy to respond to. This aspect of a turn\\nis related to its forward-looking function: the con-\\nstraints a turn places on the next turn (Schegloff and\\nSacks, 1973; Allwood et al., 1992). We propose to\\nmeasure the ease of answering a generated turn by\\nusing the negative log likelihood of responding to\\nthat utterance with a dull response. We manually con-\\nstructed a list of dull responses S consisting 8 turns\\nsuch as \\xe2\\x80\\x9cI don\\xe2\\x80\\x99t know what you are talking about\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9cI have no idea\\xe2\\x80\\x9d, etc., that we and others have found\\noccur very frequently in SEQ2SEQ models of con-\\nversations. The reward function is given as follows:\\n\\n(cid:88)\\n\\ns\\xe2\\x88\\x88S\\n\\nr1 = \\xe2\\x88\\x92 1\\nNS\\n\\nlog pseq2seq(s|a)\\n\\n(1)\\n\\n1\\nNs\\n\\nwhere NS denotes the cardinality of NS and Ns de-\\nnotes the number of tokens in the dull response s.\\nAlthough of course there are more ways to generate\\ndull responses than the list can cover, many of these\\nresponses are likely to fall into similar regions in the\\nvector space computed by the model. A system less\\nlikely to generate utterances in the list is thus also\\nless likely to generate other dull responses.\\n\\npseq2seq\\n\\nrepresents the likelihood output by\\nSEQ2SEQ models. It is worth noting that pseq2seq\\nis different from the stochastic policy function\\npRL(pi+1|pi, qi), since the former is learned based\\non the MLE objective of the SEQ2SEQ model while\\nthe latter is the policy optimized for long-term future\\n\\nreward in the RL setting. r1 is further scaled by the\\nlength of target S.\\nInformation Flow We want each agent to con-\\ntribute new information at each turn to keep the di-\\nalogue moving and avoid repetitive sequences. We\\ntherefore propose penalizing semantic similarity be-\\ntween consecutive turns from the same agent. Let\\nhpi and hpi+1 denote representations obtained from\\nthe encoder for two consecutive turns pi and pi+1.\\nThe reward is given by the negative log of the cosine\\nsimilarity between them:\\n\\nr2 = \\xe2\\x88\\x92 log cos(hpi, hpi+1) = \\xe2\\x88\\x92 log cos\\n\\nhpi \\xc2\\xb7 hpi+1\\n(cid:107)hpi(cid:107)(cid:107)hpi+1(cid:107)\\n(2)\\nSemantic Coherence We also need to measure the\\nadequacy of responses to avoid situations in which\\nthe generated replies are highly rewarded but are un-\\ngrammatical or not coherent. We therefore consider\\nthe mutual information between the action a and pre-\\nvious turns in the history to ensure the generated\\nresponses are coherent and appropriate:\\n\\nr3 =\\n\\n1\\nNa\\n\\n1\\nNqi\\n\\nlog pbackward\\n\\nlog pseq2seq(a|qi, pi)+\\n\\nseq2seq (qi|a)\\n(3)\\npseq2seq(a|pi, qi) denotes the probability of generat-\\ning response a given the previous dialogue utterances\\nseq2seq (qi|a) denotes the backward proba-\\n[pi, qi]. pbackward\\nbility of generating the previous dialogue utterance\\nqi based on response a. pbackward\\nis trained in a simi-\\nseq2seq\\nlar way as standard SEQ2SEQ models with sources\\nand targets swapped. Again, to control the in\\xef\\xac\\x82u-\\nence of target length, both log pseq2seq(a|qi, pi) and\\nseq2seq (qi|a) are scaled by the length of targets.\\nlog pbackward\\nThe \\xef\\xac\\x81nal reward for action a is a weighted sum of\\n\\nthe rewards discussed above:\\n\\nr(a, [pi, qi]) = \\xce\\xbb1r1 + \\xce\\xbb2r2 + \\xce\\xbb3r3\\n\\n(4)\\n\\nwhere \\xce\\xbb1 + \\xce\\xbb2 + \\xce\\xbb3 = 1. We set \\xce\\xbb1 = 0.25, \\xce\\xbb2 =\\n0.25 and \\xce\\xbb3 = 0.5. A reward is observed after the\\nagent reaches the end of each sentence.\\n\\n4 Simulation\\nThe central idea behind our approach is to simulate\\nthe process of two virtual agents taking turns talking\\nwith each other, through which we can explore the\\n\\n\\x0cstate-action space and learn a policy pRL(pi+1|pi, qi)\\nthat leads to the optimal expected reward. We adopt\\nan AlphaGo-style strategy (Silver et al., 2016) by\\ninitializing the RL system using a general response\\ngeneration policy which is learned from a fully su-\\npervised setting.\\n\\n4.1 Supervised Learning\\nFor the \\xef\\xac\\x81rst stage of training, we build on prior work\\nof predicting a generated target sequence given dia-\\nlogue history using the supervised SEQ2SEQ model\\n(Vinyals and Le, 2015). Results from supervised\\nmodels will be later used for initialization.\\n\\nWe trained a SEQ2SEQ model with attention (Bah-\\ndanau et al., 2015) on the OpenSubtitles dataset,\\nwhich consists of roughly 80 million source-target\\npairs. We treated each turn in the dataset as a target\\nand the concatenation of two previous sentences as\\nsource inputs.\\n\\n4.2 Mutual Information\\nSamples from SEQ2SEQ models are often times dull\\nand generic, e.g., \\xe2\\x80\\x9ci don\\xe2\\x80\\x99t know\\xe2\\x80\\x9d (Li et al., 2016a)\\nWe thus do not want to initialize the policy model\\nusing the pre-trained SEQ2SEQ models because this\\nwill lead to a lack of diversity in the RL models\\xe2\\x80\\x99 ex-\\nperiences. Li et al. (2016a) showed that modeling\\nmutual information between sources and targets will\\nsigni\\xef\\xac\\x81cantly decrease the chance of generating dull\\nresponses and improve general response quality. We\\nnow show how we can obtain an encoder-decoder\\nmodel which generates maximum mutual informa-\\ntion responses.\\n\\nAs illustrated in Li et al. (2016a), direct decoding\\nfrom Eq 3 is infeasible since the second term requires\\nthe target sentence to be completely generated. In-\\nspired by recent work on sequence level learning\\n(Ranzato et al., 2015), we treat the problem of gen-\\nerating maximum mutual information response as a\\nreinforcement learning problem in which a reward\\nof mutual information value is observed when the\\nmodel arrives at the end of a sequence.\\n\\nSimilar to Ranzato et al. (2015), we use policy gra-\\ndient methods (Sutton et al., 1999; Williams, 1992)\\nfor optimization. We initialize the policy model pRL\\nusing a pre-trained pSEQ2SEQ(a|pi, qi) model. Given\\nan input source [pi, qi], we generate a candidate list\\nA = {\\xcb\\x86a|\\xcb\\x86a \\xe2\\x88\\xbc pRL}. For each generated candi-\\n\\ndate \\xcb\\x86a, we will obtain the mutual information score\\nm(\\xcb\\x86a, [pi, qi]) from the pre-trained pSEQ2SEQ(a|pi, qi)\\nSEQ2SEQ(qi|a). This mutual information score\\nand pbackward\\nwill be used as a reward and back-propagated to the\\nencoder-decoder model, tailoring it to generate se-\\nquences with higher rewards. We refer the readers to\\nZaremba and Sutskever (2015) and Williams (1992)\\nfor details. The expected reward for a sequence is\\ngiven by:\\n\\nJ(\\xce\\xb8) = E[m(\\xcb\\x86a, [pi, qi])]\\n\\n(5)\\n\\nThe gradient is estimated using the likelihood ratio\\ntrick:\\n\\n\\xe2\\x88\\x87J(\\xce\\xb8) = m(\\xcb\\x86a, [pi, qi])\\xe2\\x88\\x87 log pRL(\\xcb\\x86a|[pi, qi])\\n\\n(6)\\n\\nWe update the parameters in the encoder-decoder\\nmodel using stochastic gradient descent. A curricu-\\nlum learning strategy is adopted (Bengio et al., 2009)\\nas in Ranzato et al. (2015) such that, for every se-\\nquence of length T we use the MLE loss for the \\xef\\xac\\x81rst\\nL tokens and the reinforcement algorithm for the\\nremaining T \\xe2\\x88\\x92 L tokens. We gradually anneal the\\nvalue of L to zero. A baseline strategy is employed to\\ndecrease the learning variance: an additional neural\\nmodel takes as inputs the generated target and the\\ninitial source and outputs a baseline value, similar\\nto the strategy adopted by Zaremba and Sutskever\\n(2015). The \\xef\\xac\\x81nal gradient is thus:\\n\\xe2\\x88\\x87J(\\xce\\xb8) = \\xe2\\x88\\x87 log pRL(\\xcb\\x86a|[pi, qi])[m(\\xcb\\x86a, [pi, qi]) \\xe2\\x88\\x92 b]\\n(7)\\n\\n4.3 Dialogue Simulation between Two Agents\\nWe simulate conversations between the two virtual\\nagents and have them take turns talking with each\\nother. The simulation proceeds as follows: at the\\ninitial step, a message from the training set is fed to\\nthe \\xef\\xac\\x81rst agent. The agent encodes the input message\\nto a vector representation and starts decoding to gen-\\nerate a response output. Combining the immediate\\noutput from the \\xef\\xac\\x81rst agent with the dialogue history,\\nthe second agent updates the state by encoding the\\ndialogue history into a representation and uses the\\ndecoder RNN to generate responses, which are sub-\\nsequently fed back to the \\xef\\xac\\x81rst agent, and the process\\nis repeated.\\n\\n\\x0cFigure 1: Dialogue simulation between the two agents.\\n\\nOptimization We initialize the policy model pRL\\nwith parameters from the mutual information model\\ndescribed in the previous subsection. We then use\\npolicy gradient methods to \\xef\\xac\\x81nd parameters that lead\\nto a larger expected reward. The objective to maxi-\\nmize is the expected future reward:\\n\\nJRL(\\xce\\xb8) = EpRL(a1:T )[\\n\\nR(ai, [pi, qi])]\\n\\n(8)\\n\\ni=T(cid:88)\\n\\ni=1\\n\\nwhere R(ai, [pi, qi]) denotes the reward resulting\\nfrom action ai. We use the likelihood ratio trick\\n(Williams, 1992; Glynn, 1990; Aleksandrov et al.,\\n1968) for gradient updates:\\n\\ni=T(cid:88)\\n\\ni=1\\n\\n\\xe2\\x88\\x87JRL(\\xce\\xb8) \\xe2\\x89\\x88(cid:88)\\n\\ni\\n\\n\\xe2\\x88\\x87 log p(ai|pi, qi)\\n\\nR(ai, [pi, qi])\\n\\n(9)\\nWe refer readers to Williams (1992) and Glynn\\n\\n(1990) for more details.\\n4.4 Curriculum Learning\\nA curriculum Learning strategy is again employed\\nin which we begin by simulating the dialogue for 2\\nturns, and gradually increase the number of simulated\\nturns. We generate 5 turns at most, as the number\\nof candidates to examine grows exponentially in the\\nsize of candidate list. Five candidate responses are\\ngenerated at each step of the simulation.\\n5 Experimental Results\\nIn this section, we describe experimental results\\nalong with qualitative analysis. We evaluate dialogue\\n\\ngeneration systems using both human judgments and\\ntwo automatic metrics: conversation length (number\\nof turns in the entire session) and diversity.\\n\\n5.1 Dataset\\nThe dialogue simulation requires high-quality initial\\ninputs fed to the agent. For example, an initial input\\nof \\xe2\\x80\\x9cwhy ?\\xe2\\x80\\x9d is undesirable since it is unclear how\\nthe dialogue could proceed. We take a subset of\\n10 million messages from the OpenSubtitles dataset\\nand extract 0.8 million sequences with the lowest\\nlikelihood of generating the response \\xe2\\x80\\x9ci don\\xe2\\x80\\x99t know\\nwhat you are taking about\\xe2\\x80\\x9d to ensure initial inputs\\nare easy to respond to.\\n\\n5.2 Automatic Evaluation\\nEvaluating dialogue systems is dif\\xef\\xac\\x81cult. Metrics such\\nas BLEU (Papineni et al., 2002) and perplexity have\\nbeen widely used for dialogue quality evaluation (Li\\net al., 2016a; Vinyals and Le, 2015; Sordoni et al.,\\n2015), but it is widely debated how well these auto-\\nmatic metrics are correlated with true response qual-\\nity (Liu et al., 2016; Galley et al., 2015). Since the\\ngoal of the proposed system is not to predict the\\nhighest probability response, but rather the long-term\\nsuccess of the dialogue, we do not employ BLEU or\\nperplexity for evaluation2.\\n\\n2We found the RL model performs worse on BLEU score. On\\na random sample of 2,500 conversational pairs, single reference\\nBLEU scores for RL models, mutual information models and\\nvanilla SEQ2SEQ models are respectively 1.28, 1.44 and 1.17.\\nBLEU is highly correlated with perplexity in generation tasks.\\n\\n............mHow old are you?I\\xe2\\x80\\x99m 16, why are you asking?\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6I\\xe2\\x80\\x99m 16InputMessage...16?I thought you were 12..........Turn1p1,2p1,3Turn2q11,1q11,2q21,1q21,2q31,1q31,2.........\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6Turnnp1n,1p1n,2p1,1p2n,1p2n,2p3n,1p3n,2encodedecodeencodedecodeencodedecode\\x0cModel\\n\\nSEQ2SEQ\\n\\nmutual information\\n\\nRL\\n\\n# of simulated turns\\n\\n2.68\\n3.40\\n4.48\\n\\nTable 2: The average number of simulated turns\\nfrom standard SEQ2SEQ models, mutual informa-\\ntion model and the proposed RL model.\\n\\nLength of the dialogue The \\xef\\xac\\x81rst metric we pro-\\npose is the length of the simulated dialogue. We say\\na dialogue ends when one of the agents starts gener-\\nating dull responses such as \\xe2\\x80\\x9ci don\\xe2\\x80\\x99t know\\xe2\\x80\\x9d 3 or two\\nconsecutive utterances from the same user are highly\\noverlapping4.\\n\\nThe test set consists of 1,000 input messages. To\\nreduce the risk of circular dialogues, we limit the\\nnumber of simulated turns to be less than 8. Results\\nare shown in Table 2. As can be seen, using mutual\\ninformation leads to more sustained conversations\\nbetween the two agents. The proposed RL model is\\n\\xef\\xac\\x81rst trained based on the mutual information objec-\\ntive and thus bene\\xef\\xac\\x81ts from it in addition to the RL\\nmodel. We observe that the RL model with dialogue\\nsimulation achieves the best evaluation score.\\n\\nDiversity We report degree of diversity by calculat-\\ning the number of distinct unigrams and bigrams in\\ngenerated responses. The value is scaled by the total\\nnumber of generated tokens to avoid favoring long\\nsentences as described in Li et al. (2016a). The re-\\nsulting metric is thus a type-token ratio for unigrams\\nand bigrams.\\n\\nFor both the standard SEQ2SEQ model and the pro-\\nposed RL model, we use beam search with a beam\\nsize 10 to generate a response to a given input mes-\\nsage. For the mutual information model, we \\xef\\xac\\x81rst\\ngenerate n-best lists using pSEQ2SEQ(t|s) and then\\nlinearly re-rank them using pSEQ2SEQ(s|t). Results\\nare presented in Table 4. We \\xef\\xac\\x81nd that the proposed\\nRL model generates more diverse outputs when com-\\n\\nSince the RL model is trained based on future reward rather than\\nMLE, it is not surprising that the RL based models achieve lower\\nBLEU score.\\n\\n3We use a simple rule matching method, with a list of 8\\nphrases that count as dull responses. Although this can lead\\nto both false-positives and -negatives, it works pretty well in\\npractice.\\n\\n4Two utterances are considered to be repetitive if they share\\n\\nmore than 80 percent of their words.\\n\\npared against both the vanilla SEQ2SEQ model and\\nthe mutual information model.\\n\\nModel\\n\\nSEQ2SEQ\\n\\nmutual information\\n\\nRL\\n\\nUnigram Bigram\\n0.0062\\n0.011\\n0.017\\n\\n0.015\\n0.031\\n0.041\\n\\nTable 4: Diversity scores (type-token ratios) for the\\nstandard SEQ2SEQ model, mutual information model\\nand the proposed RL model.\\n\\nHuman Evaluation We explore three settings for\\nhuman evaluation: the \\xef\\xac\\x81rst setting is similar to what\\nwas described in Li et al. (2016a), where we employ\\ncrowdsourced judges to evaluate a random sample of\\n500 items. We present both an input message and the\\ngenerated outputs to 3 judges and ask them to decide\\nwhich of the two outputs is better (denoted as single-\\nturn general quality). Ties are permitted. Identical\\nstrings are assigned the same score. We measure\\nthe improvement achieved by the RL model over the\\nmutual information model by the mean difference in\\nscores between the models.\\n\\nFor the second setting, judges are again presented\\nwith input messages and system outputs, but are\\nasked to decide which of the two outputs is easier to\\nrespond to (denoted as single-turn ease to answer).\\nAgain we evaluate a random sample of 500 items,\\neach being assigned to 3 judges.\\n\\nFor the third setting, judges are presented with sim-\\nulated conversations between the two agents (denoted\\nas multi-turn general quality). Each conversation\\nconsists of 5 turns. We evaluate 200 simulated con-\\nversations, each being assigned to 3 judges, who are\\nasked to decide which of the simulated conversations\\nis of higher quality.\\n\\nSetting\\n\\nRL-win RL-lose\\n\\nsingle-turn general quality\\nsingle-turn ease to answer\\nmulti-turn general quality\\n\\nTie\\n0.24\\n0.25\\n0.16\\nTable 5: RL gains over the mutual information sys-\\ntem based on pairwise human judgments.\\n\\n0.40\\n0.52\\n0.72\\n\\n0.36\\n0.23\\n0.12\\n\\nResults for human evaluation are shown in Table 5.\\nThe proposed RL system does not introduce a signi\\xef\\xac\\x81-\\ncant boost in single-turn response quality (winning\\n40 percent of time and losing 36 percent of time).\\n\\n\\x0cInput Message\\nHow old are you?\\nWhat\\xe2\\x80\\x99s your full name?\\nI don\\xe2\\x80\\x99t want to go home tonight.\\nDo you have any feelings for me?\\nHow much time do you have here? Not long enough. Sorry, sir.\\nShall we get started?\\nDo you play football?\\nWe\\xe2\\x80\\x99d have to talk to him.\\nHow come you never say it?\\nTable 3: Sampled responses generated from the mutual information models and the proposed RL model.\\n\\nRL\\nI\\xe2\\x80\\x99m 16, why are you asking?\\nWhat\\xe2\\x80\\x99s yours?\\nWhy?\\nI don\\xe2\\x80\\x99t want to hurt your feelings.\\nTen seconds.\\nYes. We\\xe2\\x80\\x99ve got a lot of work to do here.\\nYes. I love football!\\nAbout what ?\\nI don\\xe2\\x80\\x99t think it\\xe2\\x80\\x99s a good idea to say it.\\n\\nMutual Information Model\\nI\\xe2\\x80\\x99m 16.\\nI have no idea.\\nReally?\\nI don\\xe2\\x80\\x99t know what you are talking about.\\n\\nOf course!\\nNo, i don\\xe2\\x80\\x99t\\nI mean, he\\xe2\\x80\\x99s a good guy\\nBecause I don\\xe2\\x80\\x99t want to hurt you.\\n\\nThis is in line with our expectations, as the RL model\\nis not optimized to predict the next utterance, but\\nrather to increase long-term reward. The RL sys-\\ntem produces responses that are signi\\xef\\xac\\x81cantly easier\\nto answer than does the mutual information system,\\nas demonstrated by the single-turn ease to answer\\nsetting (winning 52 percent of time and losing 23\\npercent of time), and also signi\\xef\\xac\\x81cantly higher quality\\nmulti-turn dialogues, as demonstrated by the multi-\\nturn general quality setting (winning 72 percent of\\ntime).\\n\\nQualitative Analysis and Discussion We show a\\nrandom sample of generated responses in Table 3 and\\nsimulated conversations in Table 1 at the beginning\\nof the paper. From Table 3, we can see that the\\nRL based agent indeed generates more interactive\\nresponses than the other baselines. We also \\xef\\xac\\x81nd that\\nthe RL model has a tendency to end a sentence with\\nanother question and hand the conversation over to\\nthe user. From Table 1, we observe that the RL model\\nmanages to produce more interactive and sustained\\nconversations than the mutual information model.\\n\\nDuring error analysis, we found that although we\\npenalize repetitive utterances in consecutive turns,\\nthe dialogue sometimes enters a cycle with length\\ngreater than one, as shown in Table 6. This can be\\nascribed to the limited amount of conversational his-\\ntory we consider. Another issue observed is that the\\nmodel sometimes starts a less relevant topic during\\nthe conversation. There is a tradeoff between rele-\\nvance and less repetitiveness, as manifested in the\\nreward function we de\\xef\\xac\\x81ne in Eq 4.\\n\\nThe fundamental problem, of course, is that the\\nmanually de\\xef\\xac\\x81ned reward function can\\xe2\\x80\\x99t possibly\\ncover the crucial aspects that de\\xef\\xac\\x81ne an ideal conversa-\\ntion. While the heuristic rewards that we de\\xef\\xac\\x81ned are\\namenable to automatic calculation, and do capture\\n\\nA: What\\xe2\\x80\\x99s your name ?\\nB: Daniel.\\nA: How old are you ?\\nB. Twelve. What\\xe2\\x80\\x99s your name ?\\nA. Daniel.\\nB: How old are you ?\\nA: Twelve. What\\xe2\\x80\\x99s your name ?\\nB: Daniel.\\nA: How old are you ?\\nB ...\\n\\nTable 6: An simulated dialogue with a cycle longer\\nthan one.\\n\\nsome aspects of what makes a good conversation,\\nideally the system would instead receive real rewards\\nfrom humans. Another problem with the current\\nmodel is that we can only afford to explore a very\\nsmall number of candidates and simulated turns since\\nthe number of cases to consider grow exponentially.\\n\\n6 Conclusion\\n\\nWe introduce a reinforcement learning framework for\\nneural response generation by simulating dialogues\\nbetween two agents, integrating the strengths of neu-\\nral SEQ2SEQ systems and reinforcement learning\\nfor dialogue. Like earlier neural SEQ2SEQ models,\\nour framework captures the compositional models\\nof the meaning of a dialogue turn and generates se-\\nmantically appropriate responses. Like reinforce-\\nment learning dialogue systems, our framework is\\nable to generate utterances that optimize future re-\\nward, successfully capturing global properties of a\\ngood conversation. Despite the fact that our model\\nuses very simple, operationable heuristics for captur-\\ning these global properties, the framework generates\\nmore diverse, interactive responses that foster a more\\nsustained conversation.\\n\\n\\x0cAcknowledgement\\nWe would like to thank Chris Brockett, Bill Dolan\\nand other members of the NLP group at Microsoft Re-\\nsearch for insightful comments and suggestions. We\\nalso want to thank Kelvin Guu, Percy Liang, Chris\\nManning, Sida Wang, Ziang Xie and other members\\nof the Stanford NLP groups for useful discussions.\\nJiwei Li is supported by the Facebook Fellowship, to\\nwhich we gratefully acknowledge. This work is par-\\ntially supported by the NSF via Awards IIS-1514268,\\nIIS-1464128, and by the DARPA Communicating\\nwith Computers (CwC) program under ARO prime\\ncontract no. W911NF- 15-1-0462. Any opinions,\\n\\xef\\xac\\x81ndings, and conclusions or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily re\\xef\\xac\\x82ect the views of NSF, DARPA,\\nor Facebook.\\n\\nReferences\\nV. M. Aleksandrov, V. I. Sysoyev, and V. V. Shemeneva.\\n1968. Stochastic optimization. Engineering Cybernet-\\nics, 5:11\\xe2\\x80\\x9316.\\n\\nJens Allwood, Joakim Nivre, and Elisabeth Ahls\\xc2\\xb4en. 1992.\\nOn the semantics and pragmatics of linguistic feedback.\\nJournal of Semantics, 9:1\\xe2\\x80\\x9326.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\\n2015. Neural machine translation by jointly learning to\\nalign and translate. In Proc. of ICLR.\\n\\nRafael E Banchs and Haizhou Li. 2012. IRIS: a chat-\\noriented dialogue system based on the vector space\\nmodel. In Proceedings of the ACL 2012 System Demon-\\nstrations, pages 37\\xe2\\x80\\x9342.\\n\\nYoshua Bengio, J\\xc2\\xb4er\\xcb\\x86ome Louradour, Ronan Collobert, and\\nJason Weston. 2009. Curriculum learning. In Pro-\\nceedings of the 26th annual international conference\\non machine learning, pages 41\\xe2\\x80\\x9348. ACM.\\n\\nSRK Branavan, David Silver, and Regina Barzilay. 2011.\\nLearning to win by reading manuals in a monte-carlo\\nframework. In Proceedings of the 49th Annual Meeting\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies-Volume 1, pages 268\\xe2\\x80\\x93277.\\nMichel Galley, Chris Brockett, Alessandro Sordoni,\\nYangfeng Ji, Michael Auli, Chris Quirk, Margaret\\nMitchell, Jianfeng Gao, and Bill Dolan.\\n2015.\\ndeltaBLEU: A discriminative metric for generation\\ntasks with intrinsically diverse targets. In Proc. of ACL-\\nIJCNLP, pages 445\\xe2\\x80\\x93450, Beijing, China, July.\\n\\nMilica Ga\\xcb\\x87sic, Catherine Breslin, Matthew Henderson,\\nDongho Kim, Martin Szummer, Blaise Thomson, Pir-\\nros Tsiakoulis, and Steve Young. 2013a. Pomdp-based\\n\\ndialogue manager adaptation to extended domains. In\\nProceedings of SIGDIAL.\\n\\nMilica Gasic, Catherine Breslin, Mike Henderson,\\nDongkyu Kim, Martin Szummer, Blaise Thomson, Pir-\\nros Tsiakoulis, and Steve Young. 2013b. On-line policy\\noptimisation of bayesian spoken dialogue systems via\\nhuman interaction. In Proceedings of ICASSP 2013,\\npages 8367\\xe2\\x80\\x938371. IEEE.\\n\\nMilica Ga\\xcb\\x87sic, Dongho Kim, Pirros Tsiakoulis, Catherine\\nBreslin, Matthew Henderson, Martin Szummer, Blaise\\nThomson, and Steve Young. 2014. Incremental on-\\nline adaptation of pomdp-based dialogue managers to\\nextended domains. In Proceedings on InterSpeech.\\n\\nPeter W Glynn. 1990. Likelihood ratio gradient estima-\\ntion for stochastic systems. Communications of the\\nACM, 33(10):75\\xe2\\x80\\x9384.\\n\\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong\\nLi, Li Deng, and Mari Ostendorf. 2016. Deep rein-\\nforcement learning with a natural language action space.\\nIn Proceedings of the 54th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pages 1621\\xe2\\x80\\x931630, Berlin, Germany, August.\\nEsther Levin, Roberto Pieraccini, and Wieland Eckert.\\n1997. Learning dialogue strategies within the markov\\nIn Automatic Speech\\ndecision process framework.\\nRecognition and Understanding, 1997. Proceedings.,\\n1997 IEEE Workshop on, pages 72\\xe2\\x80\\x9379. IEEE.\\n\\nEsther Levin, Roberto Pieraccini, and Wieland Eckert.\\n2000. A stochastic model of human-machine interac-\\ntion for learning dialog strategies. IEEE Transactions\\non Speech and Audio Processing, 8(1):11\\xe2\\x80\\x9323.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and\\nBill Dolan. 2016a. A diversity-promoting objective\\nfunction for neural conversation models. In Proc. of\\nNAACL-HLT.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\\nithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A\\npersona-based neural conversation model. In Proceed-\\nings of the 54th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 994\\xe2\\x80\\x931003, Berlin, Germany, August.\\n\\nChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Nose-\\nworthy, Laurent Charlin, and Joelle Pineau. 2016. How\\nnot to evaluate your dialogue system: An empirical\\nstudy of unsupervised evaluation metrics for dialogue\\nresponse generation. arXiv preprint arXiv:1603.08023.\\n2016.\\nLSTM based conversation models. arXiv preprint\\narXiv:1603.09457.\\n\\nYi Luan, Yangfeng Ji, and Mari Ostendorf.\\n\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\\nGraves, Ioannis Antonoglou, Daan Wierstra, and Mar-\\ntin Riedmiller. 2013. Playing Atari with deep rein-\\nforcement learning. NIPS Deep Learning Workshop.\\n\\n\\x0cKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay.\\n2015. Language understanding for text-based games\\nusing deep reinforcement learning. arXiv preprint\\narXiv:1506.08941.\\n\\nLasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki\\nToda, Mirna Adriani, and Satoshi Nakamura. 2014.\\nDeveloping non-goal dialog system based on examples\\nof drama television. In Natural Interaction with Robots,\\nKnowbots and Smartphones, pages 355\\xe2\\x80\\x93361. Springer.\\nAlice H Oh and Alexander I Rudnicky. 2000. Stochastic\\nlanguage generation for spoken dialogue systems. In\\nProceedings of the 2000 ANLP/NAACL Workshop on\\nConversational systems-Volume 3, pages 27\\xe2\\x80\\x9332.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. BLEU: a method for automatic eval-\\nuation of machine translation. In Proceedings of the\\n40th annual meeting on association for computational\\nlinguistics, pages 311\\xe2\\x80\\x93318.\\n\\nRoberto Pieraccini, David Suendermann, Krishna\\nDayanidhi, and Jackson Liscombe. 2009. Are we there\\nyet? Research in commercial spoken dialog systems.\\nIn Text, Speech and Dialogue, pages 3\\xe2\\x80\\x9313. Springer.\\n\\nMarc\\xe2\\x80\\x99Aurelio Ranzato, Sumit Chopra, Michael Auli,\\nand Wojciech Zaremba. 2015. Sequence level train-\\ning with recurrent neural networks. arXiv preprint\\narXiv:1511.06732.\\n\\nAdwait Ratnaparkhi. 2002. Trainable approaches to sur-\\nface natural language generation and their application\\nto conversational dialog systems. Computer Speech &\\nLanguage, 16(3):435\\xe2\\x80\\x93455.\\n\\nAlan Ritter, Colin Cherry, and William B Dolan. 2011.\\nData-driven response generation in social media. In\\nProceedings of EMNLP 2011, pages 583\\xe2\\x80\\x93593.\\n\\nJost Schatzmann, Karl Weilhammer, Matt Stuttle, and\\nSteve Young. 2006. A survey of statistical user simula-\\ntion techniques for reinforcement-learning of dialogue\\nmanagement strategies. The knowledge engineering\\nreview, 21(02):97\\xe2\\x80\\x93126.\\n\\nEmanuel A. Schegloff and Harvey Sacks. 1973. Opening\\n\\nup closings. Semiotica, 8(4):289\\xe2\\x80\\x93327.\\n\\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\\nAaron Courville, and Joelle Pineau. 2016. Building\\nend-to-end dialogue systems using generative hierar-\\nchical neural network models. In Proceedings of AAAI,\\nFebruary.\\n\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural\\nIn\\n\\nresponding machine for short-text conversation.\\nProceedings of ACL-IJCNLP, pages 1577\\xe2\\x80\\x931586.\\n\\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez,\\nLaurent Sifre, George Van Den Driessche, Julian Schrit-\\ntwieser, Ioannis Antonoglou, Veda Panneershelvam,\\nMarc Lanctot, et al. 2016. Mastering the game of go\\nwith deep neural networks and tree search. Nature,\\n529(7587):484\\xe2\\x80\\x93489.\\n\\nSatinder P Singh, Michael J Kearns, Diane J Litman, and\\nMarilyn A Walker. 1999. Reinforcement learning for\\nspoken dialogue systems. In Nips, pages 956\\xe2\\x80\\x93962.\\n\\nSatinder Singh, Michael Kearns, Diane J Litman, Mar-\\nilyn A Walker, et al. 2000. Empirical evaluation of\\na reinforcement learning spoken dialogue system. In\\nAAAI/IAAI, pages 645\\xe2\\x80\\x93651.\\n\\nSatinder Singh, Diane Litman, Michael Kearns, and Mari-\\nlyn Walker. 2002. Optimizing dialogue management\\nwith reinforcement learning: Experiments with the nj-\\nfun system. Journal of Arti\\xef\\xac\\x81cial Intelligence Research,\\npages 105\\xe2\\x80\\x93133.\\n\\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris\\nBrockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie,\\nJianfeng Gao, and Bill Dolan. 2015. A neural network\\napproach to context-sensitive generation of conversa-\\ntional responses. In Proceedings of NAACL-HLT.\\n\\nPei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-\\nBarahona, Stefan Ultes, David Vandyke, Tsung-Hsien\\nWen, and Steve Young. 2016. Continuously learning\\nneural dialogue management. arxiv.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural networks.\\nIn Advances in neural information processing systems,\\npages 3104\\xe2\\x80\\x933112.\\n\\nRichard S Sutton, David A McAllester, Satinder P Singh,\\nYishay Mansour, et al. 1999. Policy gradient methods\\nfor reinforcement learning with function approximation.\\nIn NIPS, volume 99, pages 1057\\xe2\\x80\\x931063.\\n\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\ntional model. In Proceedings of ICML Deep Learning\\nWorkshop.\\n\\nAdam Vogel and Dan Jurafsky. 2010. Learning to follow\\nnavigational directions. In Proceedings of ACL 2010,\\npages 806\\xe2\\x80\\x93814.\\n\\nMarilyn A Walker, Rashmi Prasad, and Amanda Stent.\\n2003. A trainable generator for recommendations in\\nmultimodal dialog. In Proceeedings of INTERSPEECH\\n2003.\\n\\nMarilyn A. Walker. 2000. An application of reinforce-\\nment learning to dialogue strategy selection in a spoken\\ndialogue system for email. Journal of Arti\\xef\\xac\\x81cial Intelli-\\ngence Research, pages 387\\xe2\\x80\\x93416.\\n\\nTsung-Hsien Wen, Milica Gasic, Nikola Mrk\\xcb\\x87si\\xc2\\xb4c, Pei-Hao\\nSu, David Vandyke, and Steve Young. 2015. Semanti-\\ncally conditioned LSTM-based natural language gener-\\nation for spoken dialogue systems. In Proceedings of\\nEMNLP, pages 1711\\xe2\\x80\\x931721, Lisbon, Portugal.\\n\\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M\\nRojas-Barahona, Pei-Hao Su, Stefan Ultes, David\\nVandyke, and Steve Young. 2016. A network-based\\nend-to-end trainable task-oriented dialogue system.\\narXiv preprint arXiv:1604.04562.\\n\\n\\x0cRonald J Williams. 1992. Simple statistical gradient-\\nfollowing algorithms for connectionist reinforcement\\nlearning. Machine learning, 8(3-4):229\\xe2\\x80\\x93256.\\n\\nZhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, and\\nXiaolong Wang. 2016. Incorporating loose-structured\\nknowledge into LSTM with recall gate for conversation\\nmodeling. arXiv preprint arXiv:1605.05110.\\n\\nKaisheng Yao, Geoffrey Zweig, and Baolin Peng. 2015.\\nAttention with intention for a neural network conversa-\\ntion model. In NIPS workshop on Machine Learning\\nfor Spoken Language Understanding and Interaction.\\nSteve Young, Milica Ga\\xcb\\x87si\\xc2\\xb4c, Simon Keizer, Franc\\xc2\\xb8ois\\nMairesse, Jost Schatzmann, Blaise Thomson, and Kai\\nYu. 2010. The hidden information state model: A prac-\\ntical framework for pomdp-based spoken dialogue man-\\nagement. Computer Speech & Language, 24(2):150\\xe2\\x80\\x93\\n174.\\n\\nSteve Young, Milica Gasic, Blaise Thomson, and Jason D\\nWilliams. 2013. Pomdp-based statistical spoken di-\\nalog systems: A review. Proceedings of the IEEE,\\n101(5):1160\\xe2\\x80\\x931179.\\n\\nWojciech Zaremba and Ilya Sutskever. 2015. Reinforce-\\nment learning neural Turing machines. arXiv preprint\\narXiv:1505.00521.\\n\\n\\x0c',\n",
       " 'Topic Aware Neural Response Generation\\n\\nChen Xing1 2\\n\\n, Wei Wu4\\n\\n, Yu Wu3\\n\\n, Jie Liu1 2\\n\\n,\\n\\nYalou Huang1 2\\n\\n, Ming Zhou4\\n\\n, Wei-Ying Ma4\\n\\n1College of Computer and Control Engineering, Nankai University, Tianjin, China\\n\\n2College of Software, Nankai University, Tianjin, China\\n\\n3State Key Lab of Software Development Environment, Beihang University, Beijing, China\\n\\n4Microsoft Research, Beijing, China\\n\\n6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n9\\n1\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n2\\nv\\n0\\n4\\n3\\n8\\n0\\n\\n.\\n\\n6\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe consider\\nincorporating topic information into the\\nsequence-to-sequence framework to generate informative and\\ninteresting responses for chatbots. To this end, we propose\\na topic aware sequence-to-sequence (TA-Seq2Seq) model.\\nThe model utilizes topics to simulate prior knowledge of hu-\\nman that guides them to form informative and interesting re-\\nsponses in conversation, and leverages the topic information\\nin generation by a joint attention mechanism and a biased\\ngeneration probability. The joint attention mechanism sum-\\nmarizes the hidden vectors of an input message as context\\nvectors by message attention, synthesizes topic vectors by\\ntopic attention from the topic words of the message obtained\\nfrom a pre-trained LDA model, and let these vectors jointly\\naffect the generation of words in decoding. To increase the\\npossibility of topic words appearing in responses, the model\\nmodi\\xef\\xac\\x81es the generation probability of topic words by adding\\nan extra probability item to bias the overall distribution. Em-\\npirical study on both automatic evaluation metrics and hu-\\nman annotations shows that TA-Seq2Seq can generate more\\ninformative and interesting responses, and signi\\xef\\xac\\x81cantly out-\\nperform the-state-of-the-art response generation models.\\n\\nIntroduction\\n\\nHuman-computer conversation is a challenging task in AI\\nand NLP. Existing conversation systems include task ori-\\nented dialog systems (Young et al. 2013) and non task ori-\\nented chatbots. Dialog systems aim to help people complete\\nspeci\\xef\\xac\\x81c tasks such as ordering and tutoring, while chatbots\\nare designed for realizing natural and human-like conver-\\nsation with people regarding to a wide range of issues in\\nopen domains (Perez-Marin 2011). Although previous re-\\nsearch focused on dialog systems, recently, with the large\\namount of conversation data available on the Internet, chat-\\nbots are becoming hot in both academia and industry.\\n\\nA common approach to building the conversation engine\\nin a chatbot is learning a response generation model within\\na machine translation (MT) framework (Ritter, Cherry, and\\nDolan 2011; Sutskever, Vinyals, and Le 2014; Shang, Lu,\\nand Li 2015; Sordoni et al. 2015a) from the large scale social\\nconversation data. Recently, neural network based methods\\nhave become the mainstream because of their capability to\\nCopyright c(cid:13) 2017, Association for the Advancement of Arti\\xef\\xac\\x81cial\\nIntelligence (www.aaai.org). All rights reserved.\\n\\ncapture semantic and syntactic relations between messages\\nand responses in a scalable and end-to-end way. Sequence-\\nto-sequence (Seq2Seq) with attention (Bahdanau, Cho, and\\nBengio 2014; Cho, Courville, and Bengio 2015) represents\\nthe state-of-the-art neural network model for response gen-\\neration. To engage people in conversation, the response gen-\\neration algorithm in a chatbot should generate responses that\\nare not only natural and \\xef\\xac\\x82uent, but also informative and in-\\nteresting. MT models such as Seq2Seq with attention, how-\\never, tend to generate trivial responses like \\xe2\\x80\\x9cme too\\xe2\\x80\\x9d, \\xe2\\x80\\x9cI see\\xe2\\x80\\x9d,\\nor \\xe2\\x80\\x9cI don\\xe2\\x80\\x99t know\\xe2\\x80\\x9d (Li et al. 2015) due to the high frequency\\nof these patterns in data. Although these responses are safe\\nto reply to many messages, they are boring and carry little\\ninformation. Such responses may quickly lead the conver-\\nsation between human and machine to an end, and severely\\nhurt the user experience of a chatbot.\\n\\nIn this paper, we study the problem of response genera-\\ntion for chatbots. Particularly, we target to generate infor-\\nmative and interesting responses that can help chatbots en-\\ngage their users. Unlike Li et al. (Li et al. 2015) who tried\\nto passively avoid generating trivial responses by penalizing\\ntheir generation probabilities, we consider solving the prob-\\nlem by actively bringing content into responses by topics.\\nGiven an input message, we predict possible topics that can\\nbe talked about in responses, and generate responses with\\nthe topics. The idea is inspired by our observation on con-\\nversation between humans. In human-human conversation,\\npeople often associate an input message with topically re-\\nlated concepts in their mind. Based on the concepts, they\\norganize content and select words for their responses. For\\nexample, to reply to \\xe2\\x80\\x9cmy skin is so dry\\xe2\\x80\\x9d, people may think\\nit is a \\xe2\\x80\\x9cskin\\xe2\\x80\\x9d problem and can be alleviated by \\xe2\\x80\\x9chydrating\\xe2\\x80\\x9d\\nand \\xe2\\x80\\x9cmoisturizing\\xe2\\x80\\x9d. Based on this knowledge, they may give\\nmore informative responses like \\xe2\\x80\\x9cthen hydrate and moistur-\\nize our skin\\xe2\\x80\\x9d rather than trivial responses like \\xe2\\x80\\x9cme too\\xe2\\x80\\x9d. The\\ninformative responses could let other people follow the top-\\nics and continue talking about skin care. \\xe2\\x80\\x9cSkin\\xe2\\x80\\x9d, \\xe2\\x80\\x9chydrate\\xe2\\x80\\x9d,\\nand \\xe2\\x80\\x9cmoisturize\\xe2\\x80\\x9d are topical concepts related to the message.\\nThey represent people\\xe2\\x80\\x99s prior knowledge in conversation. In\\nresponding, people will bring content that are relevant to the\\nconcepts to their responses and even directly use the con-\\ncepts as building blocks to form their responses.\\n\\nWe consider simulating the way people respond to mes-\\nsages with topics, and propose a topic aware sequence-to-\\n\\n\\x0csequence (TA-Seq2Seq) model in order to leverage topic in-\\nformation as prior knowledge in response generation. TA-\\nSeq2Seq is built on the sequence-to-sequence framework.\\nIn encoding, the model represents an input message as hid-\\nden vectors by a message encoder, and acquires embeddings\\nof the topic words of the message from a pre-trained Twit-\\nter LDA model. The topic words are used as a simulation\\nof topical concepts in people\\xe2\\x80\\x99s mind, and obtained from a\\nTwitter LDA model which is pre-trained using large scale\\nsocial media data outside the conversation data. In decod-\\ning, each word is generated according to both the message\\nand the topics through a joint attention mechanism. In joint\\nattention, hidden vectors of the message are summarized as\\ncontext vectors by message attention which follows the ex-\\nisting attention techniques, and embeddings of topic words\\nare synthesized as topic vectors by topic attention. Different\\nfrom the existing attention, in topic attention, the weights of\\nthe topic words are calculated by taking the \\xef\\xac\\x81nal state of the\\nmessage as an extra input in order to strengthen the effect of\\nthe topic words relevant to the message. The joint attention\\nlets the context vectors and the topic vectors jointly affect\\nresponse generation, and makes words in responses not only\\nrelevant to the input message, but also relevant to the corre-\\nlated topic information of the message. To model the behav-\\nior that people use topical concepts as \\xe2\\x80\\x9cbuilding blocks\\xe2\\x80\\x9d of\\ntheir responses, we modify the generation probability of a\\ntopic word by adding another probability item which biases\\nthe overall distribution and further increases the possibility\\nof the topic word appearing in the response.\\n\\nWe conduct empirical study on large scale data crawled\\nfrom Baidu Tieba, and compare different methods by both\\nautomatic evaluation and human judgment. The results on\\nboth automatic evaluation metrics and human annotations\\nshow that TA-Seq2Seq can generate more informative, di-\\nverse, and topic relevant responses and signi\\xef\\xac\\x81cantly outper-\\nform the-state-of-the-art methods for response generation.\\n\\nThe contributions of this paper include 1) proposal of us-\\ning topics as prior knowledge for response generation; 2)\\nproposal of a TA-Seq2Seq model that naturally incorporates\\ntopic information into the encoder-decoder structure; 3) em-\\npirical veri\\xef\\xac\\x81cation of the effectiveness of TA-Seq2Seq.\\n\\nBackground: sequence-to-sequence model and\\n\\nattention mechanism\\n\\nBefore introducing our model, let us \\xef\\xac\\x81rst brie\\xef\\xac\\x82y review the\\nSeq2Seq model and the attention mechanism.\\n\\nSequence-to-sequence model\\nIn Seq2Seq, given a source sequence (message) X =\\n(x1, x2, . . . , xT ) and a target sequence (response) Y =\\n(y1, y2, . . . , yT (cid:48)), the model maximizes the generation prob-\\nability of Y conditioned on X: p(y1, ..., yT (cid:48)|x1, ..., xT ).\\nSpeci\\xef\\xac\\x81cally, Seq2Seq is in an encoder-decoder structure.\\nThe encoder reads X word by word and represents it as a\\ncontext vector c through a recurrent neural network (RNN),\\nand then the decoder estimates the generation probability of\\nY with c as input. The objective function of Seq2Seq can be\\n\\nFigure 1: Graphic model of Twitter LDA\\n\\nwritten as\\n\\np(y1, ..., yT (cid:48)|x1, ..., xT ) = p(y1|c)\\n\\nT (cid:48)(cid:89)\\n\\nt=2\\n\\np(yt|c, y1, ..., yt\\xe2\\x88\\x921).\\n\\nThe encoder RNN calculates the context vector c by\\n\\nht = f (xt, ht\\xe2\\x88\\x921); c = hT ,\\n\\nwhere ht is the hidden state at time t and f is a non-linear\\ntransformation which can be either an long-short term mem-\\nory unit (LSTM) (Hochreiter and Schmidhuber 1997) or a\\ngated recurrent unit (GRU) (Cho et al. 2014). In this work,\\nwe implement f using GRU which is parameterized as\\n\\nz = \\xcf\\x83(Wzxt + Uzht\\xe2\\x88\\x921)\\nr = \\xcf\\x83(Wrxt + Urht\\xe2\\x88\\x921)\\ns = tanh(Wsxt + Us(ht\\xe2\\x88\\x921 \\xe2\\x97\\xa6 r))\\nht = (1 \\xe2\\x88\\x92 z) \\xe2\\x97\\xa6 s + z \\xe2\\x97\\xa6 ht\\xe2\\x88\\x921\\n\\n(1)\\n\\nThe decoder is a standard RNN language model except con-\\nditioned on the context vector c. The probability distribution\\npt of candidate words at every time t is calculated as\\n\\nst = f (yt\\xe2\\x88\\x921, st\\xe2\\x88\\x921, c); pt = sof tmax(st, yt\\xe2\\x88\\x921)\\n\\nwhere st is the hidden state of the decoder RNN at time t\\nand yt\\xe2\\x88\\x921 is the word at time t \\xe2\\x88\\x92 1 in the response sequence.\\nAttention mechanism\\nThe traditional Seq2Seq model assumes that every word is\\ngenerated from the same context vector. In practice, how-\\never, different words in Y could be semantically related to\\ndifferent parts of X. To tackle this issue, attention mecha-\\nnism (Bahdanau, Cho, and Bengio 2014) is introduced to\\nSeq2Seq. In Seq2Seq with attention, each yi in Y corre-\\nsponds to a context vector ci, and ci is a weighted average\\nof all hidden states {ht}T\\nt=1 of the encoder. Formally, ci is\\nde\\xef\\xac\\x81ned as\\n\\nci = \\xce\\xa3T\\n\\nj=1\\xce\\xb1ijhj,\\n\\n(2)\\n\\nwhere \\xce\\xb1ij is given by\\n\\n\\xce\\xb1ij =\\n\\nexp(eij)\\nk=1exp(eik)\\n\\n\\xce\\xa3T\\n\\n; eij = \\xce\\xb7(si\\xe2\\x88\\x921, hj)\\n\\n(3)\\n\\n\\xce\\xb7 is usually implemented as a multi-layer perceptron (MLP)\\nwith tanh as an activation function.\\n\\n\\xd1\\x84tYm,nwm,nZMNm\\xce\\xb8\\xce\\xb1\\xce\\xb2Z\\xd1\\x84z\\xce\\xb2\\xd1\\x84B\\xcf\\x80\\xce\\xb3\\x0cTopic aware Seq2Seq model\\n\\nis a message, Yi\\n\\nSuppose that we have a data set D = {(Ki, Xi, Yi)}N\\ni=1\\nwhere Xi\\nis a response, and Ki =\\n(ki,1, . . . , ki,n) are the topic words of Xi. Our goal is to\\nlearn a response generation model from D, and thus given a\\nnew message X with topic words K, the model can generate\\nresponse candidates for X.\\n\\nTo learn the model, we need to answer two questions: 1)\\nhow to obtain the topic words; 2) how to perform learning.\\nIn this section, we \\xef\\xac\\x81rst describe our method on topic word\\nacquisition, and then we give details of our model.\\n\\nTopic word acquisition\\nWe obtain topic words of a message from a Twitter LDA\\nmodel (Zhao et al. 2011). Twitter LDA belongs to the family\\nof probabilistic topic models (Blei, Ng, and Jordan 2003)\\nand represents the state-of-the-art topic model for short texts\\n(Zhao et al. 2011). The basic assumption of Twitter LDA is\\nthat each message corresponds to one topic, and each word\\nin the message is either a background word or a topic word\\nunder the topic of the message. Figure 3 gives the graphical\\nmodel of Twitter LDA.\\n\\nWe estimate the parameters of Twitter LDA using the col-\\nlapsed Gibbs sampling algorithm (Zhao et al. 2011). After\\nthat, we use the model to assign a topic z to a message X,\\npick the top n words (n = 100 in our experiments) with the\\nhighest probabilities under z, and remove universal words\\nlike \\xe2\\x80\\x9cthank\\xe2\\x80\\x9d and \\xe2\\x80\\x9cyou\\xe2\\x80\\x9d to get the topic words K for X.\\n\\nIn learning, we need a vector representation for each topic\\nword. To this end, we \\xef\\xac\\x81rst calculate a distribution for topic\\nword w by Equation (4) where Cwz is the number of times\\nthat w is assigned to topic z in training. Then, we take the\\ndistributions as the vector representations of the topic words.\\n\\np(z|w) \\xe2\\x88\\x9d Cwz(cid:80)\\n\\nz(cid:48) Cwz(cid:48)\\n\\n.\\n\\n(4)\\n\\nIn our experiments, we trained a Twitter LDA model us-\\ning large scale posts from Sina Weibo which is the largest\\nmicroblogging service in China. The data provides topic\\nknowledge apart from that in message-response pairs that\\nwe use to train the response generation model. The process\\nis similar to how people learn to respond in conversation:\\nthey become aware of what can be talked about from Inter-\\nnet, especially from social media, and then use what they\\nlearned as topics to form their responses in conversation.\\n\\nNote that in addition to LDA, one can employ other tech-\\nniques like tag recommendation (Wu et al. 2016) or keyword\\nextraction (Wu et al. 2015) to generate topic words. One can\\nalso get topic words from other resources like wikipedia and\\nother web documents. We leave the discussion of these ex-\\ntensions as our future work.\\n\\nModel\\nFigure 2 gives the structure of topic aware sequence-to-\\nsequence model (TA-Seq2Seq). TA-Seq2Seq is built on the\\nsequence-to sequence framework, and leverages topic infor-\\nmation by a joint attention mechanism and a biased genera-\\ntion probability.\\n\\nSpeci\\xef\\xac\\x81cally, in encoding, a message encoder represents an\\ninput message X as a series of hidden vectors {ht}T\\nt=1 by a\\nbidirectional GRU-RNN from both ends1. GRU is de\\xef\\xac\\x81ned\\nin Equation (1). At the same time, a topic encoder obtains\\nthe embeddings of the topic words K of X by looking up an\\nembedding table which is established according to Equation\\n(4). With a little abuse of notations, we also use (k1, . . . , kn)\\nto denote the the embeddings of words in K. The meaning\\nof (k1, . . . , kn) is clear in context.\\n\\nIn decoding, at step i, message vectors {ht}T\\n\\nt=1 are trans-\\nformed to a context vector ci by message attention given\\nby Equation (2) and Equation (3), and embeddings of topic\\nwords {kj}n\\nj=1 are linearly combined as a topic vector oi by\\ntopic attention. The combination weight of kj is given by\\n\\n.\\n\\n\\xce\\xb1i\\n\\n\\xce\\xa3n\\n\\noj =\\n\\nexp(\\xce\\xb7o(si\\xe2\\x88\\x921, kj, hT ))\\nj(cid:48)=1exp(\\xce\\xb7o(si\\xe2\\x88\\x921, kj(cid:48), hT ))\\n\\n(5)\\nwhere si\\xe2\\x88\\x921 is the i \\xe2\\x88\\x92 1-th hidden state in decoder, hT is the\\n\\xef\\xac\\x81nal hidden state of the input message, and \\xce\\xb7o is a multilayer\\nperceptron. Compared to the traditional attention in Equa-\\ntion (2) and Equation (3), topic attention further leverages\\nthe \\xef\\xac\\x81nal state of the message (i.e., hT ) to weaken the effect\\nof topic words that are irrelevant to the message in gener-\\nation and highlight the importance of relevant topic words.\\nAs a result, the topic vectors {oi}T (cid:48)\\ni=1 are more correlated to\\nthe content of the input message and noise in topic words\\nis controlled in generation. The message attention and the\\ntopic attention forms a joint attention mechanism which al-\\nlows ci and oi to jointly affect the generation probability.\\nThe advantage of the joint attention is that it makes words in\\nresponses not only relevant to the message, but also relevant\\nto the topics of the message.\\n\\nWe de\\xef\\xac\\x81ne the generation probability p(yi) as p(yi) =\\npV (yi) + pK(yi), where pV (yi) and pK(yi) are de\\xef\\xac\\x81ned by\\n\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb2\\xef\\xa3\\xb3 1\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb2\\xef\\xa3\\xb3 1\\n\\nZ\\n0,\\n\\nZ\\n0,\\n\\npV (yi = w) =\\n\\npK(yi = w) =\\n\\ne\\xce\\xa8V (si,yi\\xe2\\x88\\x921,w), w \\xe2\\x88\\x88 V \\xe2\\x88\\xaa K\\nw /\\xe2\\x88\\x88 V \\xe2\\x88\\xaa K\\n\\ne\\xce\\xa8K (si,yi\\xe2\\x88\\x921,ci,w), w \\xe2\\x88\\x88 K\\nw /\\xe2\\x88\\x88 K\\n\\n(6)\\n\\nsi = f (yi\\xe2\\x88\\x921, si\\xe2\\x88\\x921, ci, oi).\\n\\nIn Equation (6), V is a response vocabulary, and f is a GRU\\nunit. \\xce\\xa8V (si, yi\\xe2\\x88\\x921) and \\xce\\xa8K(si, yi\\xe2\\x88\\x921, ci) are de\\xef\\xac\\x81ned by\\n\\xce\\xa8V (si, yi\\xe2\\x88\\x921, w) = \\xcf\\x83(wT (Ws\\n\\xce\\xa8K(si, yi\\xe2\\x88\\x921, ci, w) = \\xcf\\x83(wT (Ws\\n\\nV \\xc2\\xb7 yi\\xe2\\x88\\x921 + bV )),\\n\\nV \\xc2\\xb7 si + Wy\\n\\nK \\xc2\\xb7 yi\\xe2\\x88\\x921\\n\\nK \\xc2\\xb7 si + Wy\\nK \\xc2\\xb7 ci + bK)).\\n\\n+ Wc\\n\\n(7)\\nwhere \\xcf\\x83(\\xc2\\xb7) is tanh, w is a one-hot indicator vector of word\\nw, and Ws\\nK, bV , and bK are parame-\\nters. Z = \\xce\\xa3v\\xe2\\x88\\x88Ve\\xce\\xa8V (si,yi\\xe2\\x88\\x921,v) + \\xce\\xa3v(cid:48)\\xe2\\x88\\x88Ke\\xce\\xa8K (si,yi\\xe2\\x88\\x921,ci,v(cid:48)) is\\na normalizer.\\n\\nK, Wy\\n\\nV , Wy\\n\\nV , Ws\\n\\nEquation (6) means that the generation probability in TA-\\nSeq2Seq is biased to topic words. For non topic words, the\\n\\n1Hidden vectors from both directions are concatenated together.\\n\\n\\x0cFigure 2: Structure of TA-Seq2Seq\\n\\nprobability (i.e., pV (yi)) is similar to that in sequence-to-\\nsequence model but with the joint attention mechanism. For\\ntopic words, there is an extra probability item pK(yi) that\\nbiases the overall distribution and further increases the pos-\\nsibility of the topic words appearing in responses. The extra\\nprobability is determined by the current hidden state of the\\ndecoder si, the previous word in generation yi\\xe2\\x88\\x921, and the\\ncontext vector ci. It means that given the generated parts\\nand the input message, the more relevant a topic word is, the\\nmore possible it will appear in the response.\\n\\n\\xef\\xac\\x81xing a single key word in responses like what Mou et al.\\ndid in their work (Mou et al. 2016).\\n\\nExperiments\\n\\nWe compare TA-Seq2Seq with the-state-of-the-art response\\ngeneration models by both automatic evaluation and human\\njudgment.\\nExperiment setup\\nWe built a data set from Baidu Tieba which is the largest\\nChinese forum allowing users to post and comment to oth-\\ners\\xe2\\x80\\x99 posts. We crawled 20 million post-comment pairs and\\nused them to simulate message-response pairs in conversa-\\ntion. We removed pairs appearing more than 50 times to pre-\\nvent them from dominating learning, and employed Stanford\\nChinese word segmenter2 to tokenize the remaining pairs.\\nPairs with a message or a response having more than 50\\nwords were also removed. After these preprocessing, there\\nwere 15, 209, 588 pairs left. From them, we randomly sam-\\npled 5 million distinct message-response pairs3 as training\\ndata, 10, 000 distinct pairs as validation data, and 1, 000 dis-\\ntinct messages with their responses as test data. Messages in\\nthe test pairs were used to generate responses, and responses\\nin the test pairs were treated as ground truth to calculate the\\nperplexity of generation models. There is no overlap among\\nmessages in training, validation, and test. We kept 30, 000\\nmost frequent words in messages in the training data to con-\\nstruct a message vocabulary. The message vocabulary cov-\\ners 98.8% words appearing in messages. Similarly, we con-\\nstructed a response vocabulary that contains 30, 000 most\\n\\n2http://nlp.stanford.edu/software/\\n\\nsegmenter.shtml\\n\\n3Any two pairs are different on messages or responses.\\n\\nAn extra advantage of TA-Seq2Seq is that it makes bet-\\nter choice on the \\xef\\xac\\x81rst word in response generation. The \\xef\\xac\\x81rst\\nword matters much because it is the starting point of the lan-\\nguage model of the decoder and plays a key role in making\\nthe whole response \\xef\\xac\\x82uent. If the \\xef\\xac\\x81rst word is wrongly cho-\\nsen, then the sentence may never have a chance to go back to\\na proper response. In Seq2Seq with attention, the generation\\nof the \\xef\\xac\\x81rst word is totally determined by c0 which only de-\\npends on {ht}T\\nt=1 since there is no si\\xe2\\x88\\x921 when i = 0. While\\nin TA-Seq2Seq, the \\xef\\xac\\x81rst word is generated not only by c0,\\nbut also by o0 which consists of topic information. Topic in-\\nformation can help calibrate the selection of the \\xef\\xac\\x81rst word to\\nmake it more accurate.\\n\\nWe conduct topic learning and response generation in\\ntwo separate steps rather than let them deeply coupled like\\nVHRED (Serban et al. 2016). By this means we can leverage\\nextra data from various sources (e.g., web and knowledge\\nbase) in response generation. For example, in this work, we\\nestimate topic words from posts in Sina Weibo and provide\\nextra topic information for message-response pairs.\\n\\nWe also encourage the appearance of topic words in re-\\nsponses in a very natural and \\xef\\xac\\x82exible way by biasing the\\ngeneration distribution. Through this method, our model al-\\nlows appearance of multiple topic words rather than merely\\n\\nMessage Encoder\\xe2\\x84\\x8e1\\xe2\\x84\\x8e2\\xe2\\x84\\x8e3\\xe2\\x84\\x8e4\\xe2\\x84\\x8e5My                 skin                  is                    so                  dry\\xf0\\x9d\\x91\\x981\\xf0\\x9d\\x91\\x982\\xf0\\x9d\\x91\\x983\\xe2\\x80\\xa6.\\xf0\\x9d\\x91\\x989\\xf0\\x9d\\x91\\x9810Hydrate  Skin   Face  Facemask MoisturizeTopic WordsMessageAttentionTopic AttentionJoint Attention\\xf0\\x9d\\x91\\xa01\\xf0\\x9d\\x91\\xa02\\xf0\\x9d\\x91\\xa03\\xf0\\x9d\\x91\\xa04\\xf0\\x9d\\x91\\xa05\\xf0\\x9d\\x91\\xa06Then   hydrate and   moisturize   your    skinDecoder\\xf0\\x9d\\x91\\x904\\xf0\\x9d\\x91\\x9c4MLP\\xf0\\x9d\\x91\\x981s3\\xce\\xb1o1\\xf0\\x9d\\x91\\x96\\xe2\\x8a\\x95\\xce\\xb1o2\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x982\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x84\\x8e5s4\\xf0\\x9d\\x91\\x9c4\\xf0\\x9d\\x91\\x904V\\xe2\\x80\\xa6.K\\xe2\\x80\\xa6.P(\\xe2\\x80\\x9cmoisturize\\xe2\\x80\\x9d)=\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x89(\\xe2\\x80\\x9cmoisturize\\xe2\\x80\\x9d)+\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x90\\xbe(\\xe2\\x80\\x9cmoisturize\\xe2\\x80\\x9d)Biased Generation Process\\x0cfrequent words in responses in the training data and covers\\n98.3% words in responses.\\n\\nWe crawled 30 million posts from Sina Weibo to train\\na Twitter LDA model. We set the number of topics T as\\n200 and the hyperparameters of Twitter LDA as \\xce\\xb1 = 1/T ,\\n\\xce\\xb2 = 0.01, \\xce\\xb3 = 0.01. For each topic, we selected top 100\\nwords as topic words. To \\xef\\xac\\x81lter out universal words, we cal-\\nculated word frequency using the 30 million posts, and re-\\nmoved 2000 words with the highest frequency from the topic\\nwords. Words outside the topic words, the message vocab-\\nulary, and the response vocabulary were treated as \\xe2\\x80\\x9cUNK\\xe2\\x80\\x9d.\\n\\nModels\\nS2SA\\n\\nS2SA-MMI\\n\\nS2SA-TopicConcat\\nS2SA-TopicAttention\\n\\nTA-Seq2Seq\\n\\n+1\\n\\n0\\n\\n+2\\n\\nKappa\\n32.3% 36.7% 31.0% 0.8116\\n33.1% 34.8% 32.1% 0.7848\\n35.9% 29.3% 34.8% 0.6633\\n42.3% 27.6% 30.0% 0.8299\\n44.7% 24.9% 30.4% 0.8417\\n\\nTable 1: Human annotation results\\n\\n(cid:26)\\n\\nEvaluation metrics\\nHow to evaluate a response generation model is still an open\\nproblem but not the focus of the paper. Therefore, we fol-\\nlowed the existing work and employed the following met-\\nrics:\\n\\nPerplexity:\\n\\nfollowing (Vinyals and Le 2015) and\\n(Mikolov et al. 2010), we employed perplexity as an eval-\\nuation metric. Perplexity is de\\xef\\xac\\x81ned by Equation (8). It mea-\\nsures how well the model predicts a response. A lower per-\\nplexity score indicates better generation performance. In this\\nwork, perplexity on validation (PPL-D in Table 2) was used\\nto determine when to stop training. If the perplexity stops\\ndecreasing and the difference is smaller than 2.0 \\xef\\xac\\x81ve times\\nin validation, we think that the algorithm has reached its\\nconvergence and terminate training. We tested the genera-\\ntion ability of different models by perplexity on the test data\\n(PPL-T in Table 2).\\n\\nP P L = exp\\n\\n\\xe2\\x88\\x92 1\\nN\\n\\n\\xce\\xa3N\\n\\ni=1 log(p(Yi))\\n\\n.\\n\\n(8)\\n\\nDistinct-1 & distinct-2: we counted numbers of distinct\\nunigrams and bigrams in the generated responses. We also\\nfollowed (Li et al. 2015) and divided the numbers by total\\nnumbers of unigrams and bigrams. We denoted the metrics\\n(both the numbers and the ratios) as distinct-1 and distinct-2\\nrespectively. The two metrics measure how informative and\\ndiverse the generated responses are. High numbers and high\\nratios mean that there is much content in the generated re-\\nsponses, and high numbers further indicate that the gener-\\nated responses are long.\\n\\nHuman annotation: in addition to the automatic metrics\\nabove, we further recruited human annotators to judge the\\nquality of the generated responses of different models. Three\\nlabelers with rich Tieba experience were invited to do evalu-\\nation. Responses generated by different models (the top one\\nresponse in beam search) were pooled and randomly shuf-\\n\\xef\\xac\\x82ed for each labeler. Labelers referred to the test messages\\n\\n(cid:27)\\n\\nand judged the quality of the responses according to the fol-\\nlowing criteria:\\n\\n+2: The response is not only relevant and natural, but also\\n\\ninformative and interesting.\\n\\n+1: The response can be used as a reply to the message,\\nbut it is too universal like \\xe2\\x80\\x9cYes, I see\\xe2\\x80\\x9d , \\xe2\\x80\\x9cMe too\\xe2\\x80\\x9d and \\xe2\\x80\\x9cI don\\xe2\\x80\\x99t\\nknow\\xe2\\x80\\x9d.\\n\\n0: The response cannot be used as a reply to the mes-\\nsage. It is either semantically irrelevant or dis\\xef\\xac\\x82uent (e.g.,\\nwith grammatical errors or UNK).\\n\\nAgreements among labelers were calculated with Fleiss\\xe2\\x80\\x99\\n\\nkappa (Fleiss and Cohen 1973).\\n\\nNote that we did not choose BLEU (Papineni et al. 2002)\\nas an evaluation metric, because it has been proven by Liu\\net al. (Liu et al. 2016) that BLEU is not a proper metric for\\nevaluating conversation models as there is weak correlation\\nbetween BLEU and human judgment.\\n\\nModels\\nS2SA\\n\\nS2SA-MMI\\n\\nS2SA-TopicConcat\\nS2SA-TopicAttention\\n\\nTA-Seq2Seq\\n\\nPPL-D PPL-T\\n133.11\\n147.04\\n147.04\\n133.11\\n132.12\\n150.45\\n119.55\\n133.81\\n122.82\\n134.63\\n\\ndistinct-1\\n604/.091\\n603/.151\\n898/.116\\n894/.106\\n1355/.161\\n\\ndistinct-2\\n1168/.207\\n1073/.378\\n2197/.327\\n2057/.277\\n2970/.401\\n\\nTable 2: Results on automatic metrics\\n\\nBaselines\\nWe considered the following baselines.\\n\\nS2SA: the standard Seq2Seq model with attention.\\nS2SA-MMI: the best performing model in (Li et al.\\n\\n2015).\\n\\nS2SA-TopicConcat: to verify the effectiveness of the\\ntopic attention of TA-Seq2Seq, we replaced oi given by the\\ntopic attention in si in Equation (6) by a simple topic vector.\\nThe simple topic vector is obtained by concatenating embed-\\ndings of topic words and transforming the concatenation to\\na vector that has the same dimension with the context vector\\nby an MLP.\\n\\nS2SA-TopicAttention: to verify the effectiveness of bi-\\nased generation probability of TA-Seq2Seq, we kept the\\ntopic attention but removed the bias probability item which\\nis specially designed for topic words from the generation\\nprobability in Equation (6).\\n\\nNote that S2SA-TopicConcat and S2SA-TopicAttention\\n\\nare variants of our TA-Seq2Seq.\\n\\nIn all models, we set the dimensions of the hidden states\\nof the encoder and the decoder as 1000, and the dimen-\\nsions of word embeddings as 620. All models were initial-\\nized with isotropic Gaussian distributions X \\xe2\\x88\\xbc N (0, 0.01)\\nand trained with an AdaDelta algorithm (Zeiler 2012) on a\\nNVIDIA Tesla K40 GPU. The batch size is 128. We set the\\ninitial learning rate as 1.0 and reduced it by half if the per-\\nplexity on validation began to increase. We implemented the\\nmodels with an open source deep learning tool Blocks4, and\\nshared the code of our model at https://github.com/\\nLynetteXing1991.\\n\\n4https://github.com/mila-udem/blocks\\n\\n\\x0cFigure 3: Case study\\n\\nEvaluation Results\\n\\nIt\\n\\nthe human annotation results.\\n\\nTable 1 shows\\nis\\nclear that topic aware models (S2SA-TopicConcat, S2SA-\\nTopicAttention and TA-Seq2Seq) generate much more in-\\nformative and interesting responses (responses labeled as\\n\\xe2\\x80\\x9c+2\\xe2\\x80\\x9d) and much less universal responses than the base-\\nline models (S2SA and S2SA-MMI). Among them, TA-\\nSeq2Seq achieves the best performance. Compared with\\nS2SA-MMI, it increases 11.6% \\xe2\\x80\\x9c+2\\xe2\\x80\\x9d responses and reduces\\n9.9% \\xe2\\x80\\x9c+1\\xe2\\x80\\x9d responses. S2SA-TopicAttention performs bet-\\nter than S2SA-TopicConcat, meaning that the joint attention\\nmechanism contributes more to response quality than the bi-\\nased probability in generation. All models have a propor-\\ntion of unsuitable responses (labeled as \\xe2\\x80\\x9c0\\xe2\\x80\\x9d) around 30%\\nbut S2SA-TopicConcat and S2SA-MMI generate more bad\\nresponses. This is because without joint attention, noise\\nin topics is brought to generation by the concatenation of\\ntopic word embeddings in S2SA-TopicConcat, and in S2SA-\\nMMI, both good responses and bad responses are boosted in\\nre-ranking. All models have high kappa scores, indicating\\nthat labelers reached high agreement regarding to the qual-\\nity of responses. We also conducted sign test between TA-\\nSeq2Seq and the baseline models and results show that the\\nimprovement from our model is statistically signi\\xef\\xac\\x81cant (p-\\nvalue < 0.01).\\n\\nTable 2 gives the results of automatic metrics. TA-\\nSeq2Seq and S2SA-TopicAttention achieve comparable per-\\nplexity on validation data and test data, and both of them\\nare better than the baseline models. We conducted t-test on\\nPPL-T and the results show that the improvement is statisti-\\ncally signi\\xef\\xac\\x81cant (p-value < 0.01). On distinct-1 and distinct-\\n2, all topic aware models perform better than the baseline\\nmodels in terms of numbers of distinct n-grams (n=1,2).\\nAmong them, TA-Seq2Seq achieves the best performance\\nin terms of both the absolute numbers and the ratios. The\\nresults further veri\\xef\\xac\\x81ed our claim that topic information is\\nhelpful on enriching the content of responses. Note that Top-\\nicConcat and TopicAttention are worse than S2SA-MMI on\\nratios of distinct n-grams. This is because responses from\\nS2SA-MMI are generally shorter than those from Topic-\\nConcat and TopicAttention. The perplexities of S2SA and\\nS2SA-MMI are the same because S2SA-MMI is an after-\\n\\nprocessing mechanism on the responses generated by S2SA.\\nThus we report the perplexity of S2SA to approximately rep-\\nresent the generation ability of S2SA-MMI.\\n\\nCase study\\nFigure 3 compares TA-Seq2Seq with S2SA-MMI and S2SA\\nusing some examples. Topic words in the responses from\\nTA-Seq2Seq are bolded. From the comparison, we can see\\nthat in TA-Seq2Seq, topic words not only help form the\\nstructure of responses, but also act as \\xe2\\x80\\x9cbuilding blocks\\xe2\\x80\\x9d and\\nlead to responses that carry rich information. For example, in\\ncase 2, topic information provides prior knowledge to gen-\\neration that redness on skin is usually caused by sensitivity\\nof skin and helps form a targeted and informative response.\\nOn the other hand, although responses from S2SA-MMI and\\nS2SA also echoed the message, they carry little information\\nand easily lead the conversation to an end.\\n\\nRelated work\\n\\nBased on the sequence-to-sequence framework, many gen-\\neration models have been proposed to improve the quality\\nof generated responses from different perspectives. For ex-\\nample, A. Sordoni et al. (Sordoni et al. 2015b) represented\\nthe utterances in previous turns as a context vector and in-\\ncorporated the context vector into response generation. Li et\\nal. (Li et al. 2016) tried to build a personalized conversation\\nengine by adding personal information as extra input. Gu et\\nal. (Gu et al. 2016) introduced copynet to simulate the re-\\npeating behavior of human in conversation. Yao et al. (Yao,\\nZweig, and Peng 2015) added an extra RNN between the\\nencoder and the decoder of the sequence-to-sequence model\\nwith attention to represent intentions. In this work, we con-\\nsider incorporating topic information into the sequence-to-\\nsequence model. Similar to Li et al. (Li et al. 2015), we also\\ntry to avoid safe responses in generation. The difference is\\nthat we solve the problem by actively bringing content into\\nresponses through topics and enriching information carried\\nby the generated responses.\\n\\nConclusion\\n\\nWe propose a topic aware sequence-to-sequence (TA-\\nSeq2Seq) model to incorporate topic information into re-\\n\\n\\x0csponse generation. The model leverages the topic informa-\\ntion by a joint attention mechanism and a biased generation\\nprobability. Empirical study on both automatic evaluation\\nmetrics and human annotations shows that the model can\\ngenerate informative and diverse responses and signi\\xef\\xac\\x81cantly\\noutperform the-state-of-the-art generation models.\\n\\nReferences\\n\\n[Bahdanau, Cho, and Bengio 2014] Bahdanau, D.; Cho, K.;\\n2014. Neural machine translation by\\nand Bengio, Y.\\narXiv preprint\\njointly learning to align and translate.\\narXiv:1409.0473.\\n[Blei, Ng, and Jordan 2003] Blei, D. M.; Ng, A. Y.; and Jor-\\ndan, M. I. 2003. Latent dirichlet allocation. the Journal of\\nmachine Learning research 3:993\\xe2\\x80\\x931022.\\n[Cho et al. 2014] Cho, K.; van Merri\\xc2\\xa8enboer, B.; Bahdanau,\\nD.; and Bengio, Y. 2014. On the properties of neural ma-\\nchine translation: Encoder\\xe2\\x80\\x93decoder approaches. Syntax, Se-\\nmantics and Structure in Statistical Translation 103.\\n[Cho, Courville, and Bengio 2015] Cho, K.; Courville, A.;\\nand Bengio, Y. 2015. Describing multimedia content us-\\ning attention-based encoder-decoder networks. Multimedia,\\nIEEE Transactions on 17(11):1875\\xe2\\x80\\x931886.\\n[Fleiss and Cohen 1973] Fleiss, J. L., and Cohen, J. 1973.\\nThe equivalence of weighted kappa and the intraclass corre-\\nlation coef\\xef\\xac\\x81cient as measures of reliability. Educational and\\npsychological measurement.\\n[Gu et al. 2016] Gu, J.; Lu, Z.; Li, H.; and Li, V. O. 2016.\\nIncorporating copying mechanism in sequence-to-sequence\\nlearning.\\n[Hochreiter and Schmidhuber 1997] Hochreiter,\\nand\\nSchmidhuber, J. 1997. Long short-term memory. Neural\\ncomputation 9(8):1735\\xe2\\x80\\x931780.\\n[Li et al. 2015] Li, J.; Galley, M.; Brockett, C.; Gao, J.;\\n2015. A diversity-promoting objective\\nand Dolan, B.\\narXiv preprint\\nfunction for neural conversation models.\\narXiv:1510.03055.\\n[Li et al. 2016] Li, J.; Galley, M.; Brockett, C.; Gao, J.; and\\nDolan, B.\\n2016. A persona-based neural conversation\\nmodel.\\n[Liu et al. 2016] Liu, C.-W.; Lowe, R.; Serban, I. V.; Nose-\\nworthy, M.; Charlin, L.; and Pineau, J. 2016. How not to\\nevaluate your dialogue system: An empirical study of un-\\nsupervised evaluation metrics for dialogue response genera-\\ntion. arXiv preprint arXiv:1603.08023.\\n[Mikolov et al. 2010] Mikolov, T.; Kara\\xef\\xac\\x81\\xc2\\xb4at, M.; Burget, L.;\\nCernock`y, J.; and Khudanpur, S. 2010. Recurrent neural net-\\nwork based language model. In INTERSPEECH, volume 2,\\n3.\\n[Mou et al. 2016] Mou, L.; Song, Y.; Yan, R.; Li, G.; Zhang,\\nL.; and Jin, Z.\\n2016. Sequence to backward and for-\\nward sequences: A content-introducing approach to gener-\\native short-text conversation. CoRR abs/1607.00970.\\n[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.;\\nand Zhu, W.-J. 2002. Bleu: a method for automatic evalua-\\ntion of machine translation. In Proceedings of the 40th an-\\n\\nS.,\\n\\nnual meeting on association for computational linguistics,\\n311\\xe2\\x80\\x93318. Association for Computational Linguistics.\\n[Perez-Marin 2011] Perez-Marin, D. 2011. Conversational\\nAgents and Natural Language Interaction: Techniques and\\nEffective Practices: Techniques and Effective Practices. IGI\\nGlobal.\\n[Ritter, Cherry, and Dolan 2011] Ritter, A.; Cherry, C.; and\\nDolan, W. B. 2011. Data-driven response generation in so-\\ncial media. In Proceedings of the Conference on Empirical\\nMethods in Natural Language Processing, 583\\xe2\\x80\\x93593. Asso-\\nciation for Computational Linguistics.\\n[Serban et al. 2016] Serban, I. V.; Sordoni, A.; Lowe, R.;\\nCharlin, L.; Pineau, J.; Courville, A.; and Bengio, Y. 2016. A\\nhierarchical latent variable encoder-decoder model for gen-\\nerating dialogues. arXiv preprint arXiv:1605.06069.\\n[Shang, Lu, and Li 2015] Shang, L.; Lu, Z.; and Li, H. 2015.\\nNeural responding machine for short-text conversation.\\narXiv preprint arXiv:1503.02364.\\n[Sordoni et al. 2015a] Sordoni, A.; Galley, M.; Auli, M.;\\nBrockett, C.; Ji, Y.; Mitchell, M.; Nie, J.; Gao, J.; and Dolan,\\nB. 2015a. A neural network approach to context-sensitive\\nIn NAACL HLT\\ngeneration of conversational responses.\\n2015, The 2015 Conference of the North American Chap-\\nter of the Association for Computational Linguistics: Hu-\\nman Language Technologies, Denver, Colorado, USA, May\\n31 - June 5, 2015, 196\\xe2\\x80\\x93205.\\n[Sordoni et al. 2015b] Sordoni, A.; Galley, M.; Auli, M.;\\nBrockett, C.; Ji, Y.; Mitchell, M.; Nie, J.-Y.; Gao, J.; and\\nDolan, B. 2015b. A neural network approach to context-\\nsensitive generation of conversational responses.\\n[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.;\\nand Le, Q. V. 2014. Sequence to sequence learning with neu-\\nral networks. In Advances in neural information processing\\nsystems, 3104\\xe2\\x80\\x933112.\\n[Vinyals and Le 2015] Vinyals, O., and Le, Q. V. 2015. A\\nneural conversational model. CoRR abs/1506.05869.\\n[Wu et al. 2015] Wu, Y.; Wu, W.; Li, Z.; and Zhou, M. 2015.\\nMining query subtopics from questions in community ques-\\ntion answering. In AAAI, 339\\xe2\\x80\\x93345.\\n[Wu et al. 2016] Wu, Y.; Wu, W.; Li, Z.; and Zhou, M. 2016.\\nImproving recommendation of tail tags for questions in\\ncommunity question answering. In Thirtieth AAAI Confer-\\nence on Arti\\xef\\xac\\x81cial Intelligence.\\n[Yao, Zweig, and Peng 2015] Yao, K.; Zweig, G.; and Peng,\\nB. 2015. Attention with intention for a neural network con-\\nversation model. Computer Science.\\n[Young et al. 2013] Young, S.; Gasic, M.; Thomson, B.; and\\nWilliams, J. D. 2013. Pomdp-based statistical spoken dialog\\nsystems: A review. Proceedings of the IEEE 101(5):1160\\xe2\\x80\\x93\\n1179.\\n[Zeiler 2012] Zeiler, M. D. 2012. Adadelta: an adaptive\\nlearning rate method. arXiv preprint arXiv:1212.5701.\\n[Zhao et al. 2011] Zhao, W. X.; Jiang, J.; Weng, J.; He, J.;\\nLim, E.-P.; Yan, H.; and Li, X. 2011. Comparing twitter\\nIn Advances in\\nand traditional media using topic models.\\nInformation Retrieval. Springer. 338\\xe2\\x80\\x93349.\\n\\n\\x0c',\n",
       " ' \\n\\nEven Good Bots Fight: The Case of Wikipedia \\n\\nMilena Tsvetkova a, Ruth Garc\\xc3\\xada-Gavilanes a, Luciano Floridi a,b, and Taha Yasseri a,b* \\n\\n \\n\\na Oxford Internet Institute, University of Oxford, Oxford OX1 3JS, United Kingdom \\n\\nb Alan Turing Institute, London NW1 2DB, United Kingdom \\n\\n* Corresponding author. E-mail: taha.yasseri@oii.ox.ac.uk (TY) \\n\\n \\n\\nAbstract \\n\\nIn recent years, there has been a huge increase in the number of bots online, varying from \\nWeb crawlers for search engines, to chatbots for online customer service, spambots on social \\nmedia, and content-editing bots in online collaboration communities. The online world has \\nturned into an ecosystem of bots. However, our knowledge of how these automated agents \\nare  interacting  with  each  other  is  rather  poor.  Bots  are  predictable  automatons  that  do  not \\nhave  the  capacity  for  emotions,  meaning-making,  creativity,  and  sociality  and  it  is  hence \\nnatural to expect interactions between bots to be relatively predictable and uneventful. In this \\narticle, we analyze the interactions between bots that edit articles on Wikipedia. We track the \\nextent to which bots undid each other\\xe2\\x80\\x99s edits over the period 2001-2010, model how pairs of \\nbots interact over time, and identify different types of interaction trajectories. We find that, \\nalthough  Wikipedia  bots  are  intended  to  support  the  encyclopedia,  they  often  undo  each \\nother\\xe2\\x80\\x99s edits and these sterile \\xe2\\x80\\x9cfights\\xe2\\x80\\x9d may sometimes continue for years. Unlike humans on \\nWikipedia,  bots\\xe2\\x80\\x99  interactions  tend  to  occur  over  longer  periods  of  time  and  to  be  more \\nreciprocated.  Yet,  just  like  humans,  bots  in  different  cultural  environments  may  behave \\ndifferently. Our research suggests that even relatively \\xe2\\x80\\x9cdumb\\xe2\\x80\\x9d bots may give rise to complex \\ninteractions,  and  this  carries  important  implications  for  Artificial  Intelligence  research.  \\nUnderstanding  what  affects  bot-bot  interactions  is  crucial  for  managing  social  media  well, \\nproviding adequate cyber-security, and designing well functioning autonomous vehicles. \\n\\n1\\t\\n\\n\\t\\n\\n\\x0cIntroduction \\n\\nIn August 2011, Igor Labutov and Jason Yosinski, two PhD students at Cornell University, \\nlet a pair of chat bots, called Alan and Sruthi, talk to each other online. Starting with a simple \\ngreeting, the one-and-a-half-minute dialogue quickly escalated into an argument about what \\nAlan and Sruthi had just said, whether they were robots, and about God (1). The first ever \\nconversation between two simple artificial intelligence agents ended in a conflict. \\n\\nA  bot,  or  software  agent,  is  a  computer  program  that  is  persistent,  autonomous,  and \\nreactive  (2,3).  Bots  are  defined  by  programming  code  that  runs  continuously  and  can  be \\nactivated  by  itself.  They  make  and  execute  decisions  without  human  intervention  and \\nperceive and adapt to the context they operate in. Internet bots, also known as web bots, are \\nbots that run over the Internet. They appeared and proliferated soon after the creation of the \\nWorld  Wide  Web  (4).  Already  in  1993,  Martijn  Koster  published  \\xe2\\x80\\x9cGuidelines  to  robot \\nwriters,\\xe2\\x80\\x9d  which  contained  suggestions  about  developing  web  crawlers  (5),  a  kind  of  bot. \\nEggdrop, one of the first known Internet Relay Chat bots, started greeting chat newcomers \\nalso in 1993 (6). In 1996, Fah-Chun Cheong published a 413-page book, claiming to have a \\ncurrent listing of all bots available on the Internet at that point in time. Since then, Internet \\nbots have proliferated and diversified well beyond our ability to record them in an exhaustive \\nlist  (7,8).  As  a  result,  bots  have  been  responsible  for  an  increasingly  larger  proportion  of \\nactivities on the Web. For example, one study found that 25% of all messages on Yahoo! chat \\nover a period of three months in 2007 were sent by spam bots (9). Another study discovered \\nthat 32% of all tweets made by the most active Twitter users in 2009 were generated by bots \\n(10), meaning that bots were responsible for an estimated 24% of all tweets (11). Further, \\nresearchers estimated that bots comprise between 4% and 7% of the avatars on the virtual \\nworld Second Life in 2009 (12). A media analytics company found that 54% of the online ads \\nshown  in  thousands  of  ad  campaigns  in  2012  and  2013  were  viewed  by  bots,  rather  than \\nhumans (13). According to an online security company, bots accounted for 48.5% of website \\nvisits in 2015 (14). Also in 2015, 100,000 accounts on the multi-player online game World of \\nWarcraft (about 1% of all accounts) were banned for using bots (15). And in the same year, a \\ndatabase  leak  revealed  that  more  than  70,000  \\xe2\\x80\\x9cfemale\\xe2\\x80\\x9d  bots  sent  more  than  20  million \\nmessages on the cheater dating site Ashley Madison (16). \\n\\n\\t\\n\\n2\\t\\n\\n\\x0cTable  1.  Categorization  of  Internet  bots  according  to  the  intended  effect  of  their \\noperations and the kind of activities they perform, including some familiar examples for \\neach type.  \\n\\n \\n\\nBenevolent \\n\\nMalevolent \\n\\nCollect \\ninformation \\n\\n\\xe2\\x80\\xa2  Web crawlers \\n\\xe2\\x80\\xa2  Bots used by researchers \\n\\n\\xe2\\x80\\xa2  Spam bots that collect e-mail \\n\\n\\xe2\\x80\\xa2  Facebook bots that collect private \\n\\naddresses \\n\\ninformation \\n\\n \\n\\n \\n\\nExecute \\nactions  \\n\\nGenerate \\ncontent \\n\\nEmulate \\nhumans \\n\\n\\xe2\\x80\\xa2  Anti-vandalism bots on \\n\\nWikipedia \\n\\n\\xe2\\x80\\xa2  Censoring and moderating \\nbots on chats and forums \\n\\n\\xe2\\x80\\xa2  Auction-site bots \\n\\xe2\\x80\\xa2  High-frequency trading \\n\\nalgorithms \\n\\n\\xe2\\x80\\xa2  Gaming bots \\n\\xe2\\x80\\xa2  DDoS attack bots \\n\\xe2\\x80\\xa2  Viruses and worms \\n\\xe2\\x80\\xa2  Clickfraud bots that increase \\n\\nviews of online ads and YouTube \\nvideos \\n\\n\\xe2\\x80\\xa2  Editing bots on Wikipedia \\n\\xe2\\x80\\xa2  Twitter bots that create alerts \\n\\nor provide content \\naggregation \\n\\n\\xe2\\x80\\xa2  Spam bots that disseminate ads \\n\\xe2\\x80\\xa2  Bot farms that write positive \\nreviews and boost ratings on \\nApple App Store, YouTube, etc. \\n\\n\\xe2\\x80\\xa2  Customer service bots \\n\\xe2\\x80\\xa2  @DeepDrumpf and poet-\\n\\nwriting bots on Twitter \\n\\n\\xe2\\x80\\xa2  AI bots, e.g. IBM\\xe2\\x80\\x99s Watson \\n\\n\\xe2\\x80\\xa2  Social bots involved in \\nastroturfing on Twitter \\n\\n\\xe2\\x80\\xa2  Social bots on the cheater dating \\n\\nsite Ashley Madison \\n\\n \\nBenevolent bots are designed to support human users or cooperate with them. Malevolent bots are \\ndesigned  to  exploit  human  users  and  compete  negatively  with  them.  We  have  classified  high-\\nfrequency  trading  algorithms  as  malevolent  because  they  exploit  markets  in  ways  that  increase \\nvolatility and precipitate  flash  crashes.  In this study, we use data from editing  bots on  Wikipedia \\n(benevolent bots that generate content). \\n\\n \\n\\nAs the population of bots active on the Internet 24/7 is growing fast, their interactions are \\nequally  intensifying.  An  increasing  number  of  decisions,  options,  choices,  and  services \\ndepend now on bots working properly, efficaciously, and successfully. Yet, we know very \\nlittle about the life and evolution of our digital minions. In particular, predicting how bots\\xe2\\x80\\x99 \\ninteractions  will  evolve  and  play  out  even  when  they  rely  on  very  simple  algorithms  is \\n\\n3\\t\\n\\n\\t\\n\\n\\x0calready challenging. Furthermore, as Alan and Sruthi demonstrated, even if bots are designed \\nto  collaborate,  conflict  may  occur  inadvertently.  Clearly,  it  is  crucial  to  understand  what \\ncould  affect  bot-bot  interactions  in  order  to  design  cooperative  bots  that  can  manage \\ndisagreement, avoid unproductive conflict, and fulfill their tasks in ways that are socially and \\nethically acceptable. \\n\\nThere  are  many  types  of  Internet  bots  (see  Table  1).  These  bots  form  an  increasingly \\ncomplex  system  of  social  interactions.  Do  bots  interact  with  each  other  in  ways  that  are \\ncomparable to how we humans interact with each other? Bots are predictable automatons that \\ndo  not  have  the  capacity  for  emotions,  meaning-making,  creativity,  and  sociality  (17). \\nDespite  recent  advances  in  the  field  of  Artificial  Intelligence,  the  idea  that  bots  can  have \\nmorality and culture is still far from reality. Today, it is natural to expect interactions between \\nbots to be relatively predictable and uneventful, lacking the spontaneity and complexity of \\nhuman social interactions. However, even in such simple contexts, our research shows that \\nthere may be more similarities between bots and humans than one may expect. Focusing on \\none particular human-bot community, we find that conflict emerges even among benevolent \\nbots  that  are  designed  to  benefit  their  environment  and  not  fight  each  other,  and  that  bot \\ninteractions  may  differ  when  they  occur  in  environments  influenced  by  different  human \\ncultures.  \\n\\nWe study bots on Wikipedia, the largest free online encyclopedia. Bots on Wikipedia are \\ncomputer scripts that automatically handle repetitive and mundane tasks to develop, improve, \\nand maintain the encyclopedia. They are easy to identify because they operate from dedicated \\nuser accounts that have been flagged and officially approved. Approval requires that the bot \\nfollows Wikipedia\\xe2\\x80\\x99s bot policy.  \\n\\nBots are important contributors to Wikipedia. For example, in 2014, bots completed about \\n15% of the edits on all language editions of the encyclopedia (18).  In general, Wikipedia \\nbots complete a variety of activities. They identify and undo vandalism, enforce bans, check \\nspelling,  create  inter-language  links,  import  content  automatically,  mine  data,  identify \\ncopyright violations, greet newcomers, and so on (19). Our analysis here focuses on editing \\nbots, which modify articles directly. We analyze the interactions between bots and investigate \\nthe extent to which they resemble interactions between humans. In particular, we focus on \\nwhether  bots  disagree  with  each  other,  how  the  dynamics  of  disagreement  differ  for  bots \\n\\n\\t\\n\\n4\\t\\n\\n\\x0cversus  humans,  and  whether  there  are  differences  between  bots  operating  in  different \\nlanguage editions of Wikipedia. \\n\\nTo  measure  disagreement,  we  study  reverts.  A  revert  on  Wikipedia  occurs  when  an \\neditor,  whether  human  or  bot,  undoes  another  editor\\xe2\\x80\\x99s  contribution  by  restoring  an  earlier \\nversion of the article. Reverts that occur systematically indicate controversy and conflict (20\\xe2\\x80\\x93\\n22). Reverts are technically easy to detect regardless of the context and the language, so they \\nenable analysis at the scale of the whole system.  \\n\\nOur data contain all edits in 13 different language editions of Wikipedia in the first ten \\nyears after the encyclopedia was launched (2001-2010). The languages represent editions of \\ndifferent size and editors from diverse cultures (see Materials and Methods for details). We \\nknow which user completed the edit, when, in which article, whether the edit was a revert \\nand, if so, which previous edit was reverted. We first identified which editors are humans, \\nbots, or vandals. We isolated the vandals since their short-lived disruptive activity exhibits \\ndifferent time and interaction patterns than the activity of regular Wikipedia editors.  \\n\\nResults \\n\\nBots constitute a tiny proportion of all Wikipedia editors but they stand behind a significant \\nproportion of all edits (Figs 1A and 1B). There are significant differences between different \\nlanguages in terms of how active bots are. From previous research, we know that, in small \\nand  endangered  languages,  bots  are  extremely  active  and  do  more  than  50%  of  the  edits, \\nsometimes  up  to  100%  (19).  Their  tasks,  however,  are  mainly  restricted  to  adding  links \\nbetween  articles  and  languages.  In  large  and  active  languages,  the  level  of  bot  activity  is \\nmuch lower but also much more variable. \\n\\nCompared  to  humans,  a  smaller  proportion  of  bots\\xe2\\x80\\x99  edits  are  reverts  and  a  smaller \\nproportion get reverted (Figs 1C and 1D). In other words, bots dispute others and are disputed \\nby others to a lesser extent than humans. Since 2001, the number of bots and their activity has \\nbeen increasing but at a slowing rate (S1 Fig). In contrast, the number of reverts between bots \\nhas been continuously increasing (Fig 2A). This would suggest that bot interactions are not \\nbecoming  more  efficient.  We  also  see  that  the  proportion  of  mutual  bot-bot  reverts  has \\nremained  relatively  stable,  perhaps  even  slightly  increasing  over  time,  indicating  that  bot \\nowners have not learned to identify bot conflicts faster (Fig 2B). \\n\\n\\t\\n\\n5\\t\\n\\n\\x0c \\n\\nFig 1. The proportion of Wikipedia editors who are human, vandals, and bots and the \\ntype of editorial activity in which they are involved. A language edition to the left has a \\nhigher total number of edits than one to the right. (A) Bots comprise a tiny proportion of all \\nWikipedia users, usually less than 0.1% (not visible in the figure). (B) However, bots account \\nfor  a  significant  proportion  of  the  editorial  activity.  The  level  of  bot  activity  significantly \\ndiffers between different language editions of Wikipedia, with bots generally more active in \\nsmaller  editions.  (C)  A  smaller  proportion  of  bots\\xe2\\x80\\x99  edits  are  reverts  compared  to  humans\\xe2\\x80\\x99 \\nedits. (D) A smaller proportion of bots\\xe2\\x80\\x99 edits get reverted compared to humans\\xe2\\x80\\x99 edits. Since \\nby our definition, vandals have all of their edits reverted, we do not show them in this figure. \\n\\n \\n\\n\\t\\n\\n6\\t\\n\\n\\x0c \\n\\nFig 2. The number of bot reverts executed by another bot and the proportion of unique \\nbot-bot pairs that have at least one reciprocated revert for the period 2001-2010. (A) \\nGenerally, the number of bot-bot reverts has been increasing. (B) However, the proportion of \\nreciprocated  reverts  has  not  been  decreasing  (error  bars  correspond  to  one  standard  error). \\nThis suggests that disagreement between bots is not becoming less common. \\n\\n \\n\\nIn  general,  bots  revert  each  other  a  lot:  for  example,  over  the  ten-year  period,  bots  on \\nEnglish Wikipedia reverted another bot on average 105 times, which is significantly larger \\nthan the average of 3 times for humans (S1 Table). Bots on German Wikipedia revert each \\nother  to  a  much  lesser  extent  than  other  bots  (24  times  on  average).  Bots  on  Portuguese \\nWikipedia, in contrast, fight the most, with an average of 185 bot-bot reverts per bot. This \\nstriking difference, however, disappears when we account for the fact that bots on Portuguese \\nWikipedia edit more than bots on German Wikipedia. In general, since bots are much more \\nactive editors than humans, the higher number of bot-bot reverts does not mean that bots fight \\nmore than humans. In fact, the proportion of bots\\xe2\\x80\\x99 edits that are reverts is smaller for bots \\nthan  for  humans  (Fig  1C).  This  proportion  is  highest  for  bots  in  the  English  and  the \\nRomance-language  editions  (Spanish,  French,  Portuguese,  and  Romanian).  Interestingly, \\n\\n\\t\\n\\n7\\t\\n\\n\\x0calthough  bots  in  these  languages  revert  more  often  compared  to  bots  in  other  languages, \\nfewer of these reverts are for another bot (S2 Fig).  \\n\\n \\n\\nFig 3. Typical interaction trajectories for bot-bot and human-human pairs in English \\nWikipedia in the period 2001-2010.  The interaction trajectories are constructed as follows: \\nstarting from yo = 0, yt = yt-1 + 1 if i reverts j and yt = yt-1 \\xe2\\x80\\x93 1 if j reverts i at time t; the labels i \\nand j are assigned so that y >= 0 for the majority of the ij interaction time; to compress the \\nextremes, we scaled the y-axis to the power of 0.5. The panels show the trajectories of 200 \\npairs randomly sampled from those who have exchanged more than five reverts. In addition, \\nwe highlight the four longest trajectories in the sample from each of the four trajectory types \\nwe identify. Compared to human-human interactions, bot-bot interactions occur at a slower \\nrate  and  are  more  balanced,  in  the  sense  that  reverts  go  back  and  forth  between  the  two \\neditors. \\n\\n \\n\\nOur  analysis  focuses  on  interactions  in  dyads  over  time.  We  model  the  interaction \\ntrajectories  in  two-dimensional  space,  where  the  x-axis  measures  time  and  the  y-axis \\n\\n8\\t\\n\\n\\t\\n\\n\\x0cmeasures  how  many  more  times  the  first  editor  has  reverted  the  second  compared  to  the \\nsecond  reverting  the  first  (Fig  3).  We  analyze  three  properties  of  the  trajectories:  latency, \\nimbalance,  and  reciprocity.  Latency  measures  the  average  steepness  of  the  interaction \\ntrajectory,  imbalance  measures  the  distance  between  the  x-axis  and  the  last  point  of  the \\ntrajectory, and reciprocity measures the trajectory\\xe2\\x80\\x99s jaggedness (see Materials and Methods \\nbelow for definitions). \\n\\nAnalyzing  the  properties  of  the  interaction  trajectories  suggests  that  the  dynamics  of \\ndisagreement  differ  significantly  between  bots  and  humans.  Reverts  between  bots  tend  to \\noccur at a slower rate and a conflict between two bots can take place over longer periods of \\ntime,  sometimes  over  years.  In  fact,  bot-bot  interactions  have  different  characteristic  time \\nscale  than  human-human  interactions  (S3  Fig).  The  characteristic  average  time  between \\nsuccessive reverts for humans is at 2 minutes, 24 hours, or 1 year. In comparison, bot-bot \\ninteractions  have  a  characteristic  average  response  of  1  month.  This  difference  is  likely \\nbecause,  first,  bots  systematically  crawl  articles  and,  second,  bots  are  restricted  as  to  how \\noften they can make edits (the Wikipedia bot policy usually requires spacing of 10 seconds, \\nor 5 for anti-vandalism activity, which is considered more urgent). In contrast, humans use \\nautomatic tools that report live changes made to a pre-selected list of articles (24,25); they \\ncan thus follow only a small set of articles and, in principle, react instantaneously to any edits \\non those.  \\n\\nBots also tend to reciprocate each other\\xe2\\x80\\x99s reverts to a greater extent. In contrast, humans \\ntend to have highly unbalanced interactions, where one individual unilaterally reverts another \\none (S4 and S5 Figs). \\n\\nWe quantify these findings more precisely by identifying different types of interaction \\ntrajectories and counting how often they occur for bots and for humans, as well as for specific \\nlanguages. To this end, we use k-means clustering on the three properties of the trajectories \\n(latency, imbalance, and reciprocity) and on all bot-bot and human-human interactions longer \\nthan five reverts (the results are substantively similar without the length restriction). We do \\nnot claim that the clusters are natural to the data; rather, we use the clusters to compare the \\ninteractions of the different groups. \\n\\nThe algorithm suggested that the data can be best clustered in four trajectory types (S6 \\n\\nFig):  \\n\\n\\t\\n\\n9\\t\\n\\n\\x0c\\xe2\\x80\\xa2  Fast  unbalanced  trajectories.  These  trajectories  have  low  reciprocity  and \\nlatency  and  high  imbalance.  They  look  like  smooth  vertical  lines  above  the  x-\\naxis. \\n\\n\\xe2\\x80\\xa2  Slow unbalanced trajectories. These trajectories have low reciprocity and high \\n\\nlatency and imbalance. They look like smooth diagonal lines above the x-axis. \\n\\n\\xe2\\x80\\xa2  Somewhat  balanced \\n\\ntrajectories.  These \\n\\ntrajectories  have \\n\\nintermediate \\n\\nimbalance and reciprocity. They are somewhat jagged and cross the x-axis. \\n\\n\\xe2\\x80\\xa2  Well  balanced  trajectories.  These  trajectories  have  low  imbalance  and  high \\n\\nreciprocity. They are quite jagged and centered on the x-axis. \\n\\n \\n\\nFig 4. The prevalence of the four types of trajectories for bots and humans and for \\ndifferent language editions of Wikipedia. The darker the shading of the cell, the higher the \\nproportion for that type of trajectory for the language. Bot-bot interactions occur at a slower \\nrate and are more balanced, in the sense that reverts go back and forth between the two bots. \\nFurther, bot-bot interactions are more balanced in smaller language editions of Wikipedia. \\n\\n \\n\\n \\n\\nLooking  at  the  prevalence  of  these  four  types  of  trajectories  for  bots  and  humans  and \\nacross  languages,  we  confirm  the  previous  observations:  bot-bot  interactions  occur  at  a \\nslower rate and are more balanced, in the sense that reverts go back and forth between the \\n\\n10\\t\\n\\n\\t\\n\\n\\x0ctwo  bots  (Fig  4).  Further,  we  find  that  bot-bot  interactions  are  more  balanced  in  smaller \\nlanguage editions of Wikipedia. This could be due to the fact that bots are more active in \\nsmaller  editions  and  hence,  interactions  between  them  are  more  likely  to  occur.  Less \\nintuitively, however, this observation also suggests that conflict between bots is more likely \\nto occur when there are fewer bots and when, common sense would suggest, coordination is \\neasier. \\n\\nDiscussion \\n\\nOur results show that, although in quantitatively different ways, bots on Wikipedia behave \\nand  interact  as  unpredictably  and  as  inefficiently  as  the  humans.  The  disagreements  likely \\narise from the bottom-up organization of the community, whereby human editors individually \\ncreate  and  run  bots,  without  a  formal  mechanism  for  coordination  with  other  bot  owners. \\nDelving deeper into the data, we found that most of the disagreement occurs between bots \\nthat  specialize  in  creating  and  modifying  links  between  different  language  editions  of  the \\nencyclopedia.  The  lack  of  coordination  may  be  due  to  different  language  editions  having \\nslightly different naming rules and conventions.  \\n\\nIn  support  of  this  argument,  we  also  found  that  the  same  bots  are  responsible  for  the \\nmajority of reverts in all the language editions we study. For example, some of the bots that \\nrevert  the  most  other  bots  include  Xqbot,  EmausBot,  SieBot,  and  VolkovBot,  all  bots \\nspecializing in fixing inter-wiki links. Further, while there are few articles with many bot-bot \\nreverts (S7 Fig), these articles tend to be the same across languages. For example, some of \\nthe  articles  most  contested  by  bots  are  about  Pervez  Musharraf  (former  president  of \\nPakistan),  Uzbekistan,  Estonia,  Belarus,  Arabic \\nlanguage,  Niels  Bohr,  Arnold \\nSchwarzenegger.  This  would  suggest  that  a  significant  portion  of  bot-bot  fighting  occurs \\nacross languages rather than within. In contrast, the articles with most human-human reverts \\ntend to concern local personalities and entities and tend to be unique for each language (26).  \\n\\nsuggests \\n\\nOur data cover a period of the evolution of Wikipedia when bot activity was growing.  \\n2013 \\nEvidence \\n(http://stats.wikimedia.org/EN/PlotsPngEditHistoryTop.htm).  This  decline  occurred  because \\nat  the  beginning  of  2013  many  language  editions  of  Wikipedia  started  to  provide  inter-\\nlanguage links via Wikidata, which is a collaboratively edited knowledge base intended to \\nsupport Wikipedia. Since our results were largely dictated by inter-language bots, we believe \\n\\nsuddenly \\n\\nended \\n\\nin \\n\\nthat \\n\\nthis \\n\\nperiod \\n\\n\\t\\n\\n11\\t\\n\\n\\x0cthat the conflict we observed on Wikipedia no longer occurs today. One interesting direction \\nfor future research is to investigate whether the conflict continues to persist among the inter-\\nlanguage bots that migrated to Wikidata.  \\n\\nWikipedia is perhaps one of the best examples of a populous and complex bot ecosystem \\nbut  this  does  not  necessarily  make  it  representative.  As  Table  1  demonstrates,  we  have \\ninvestigated  a  very  small  region  of  the  botosphere  on  the  Internet.  The  Wikipedia  bot \\necosystem is gated and monitored and this is clearly not the case for systems of malevolent \\nsocial bots, such as social bots on Twitter posing as humans to spread political propaganda or \\ninfluence  public  discourse.  Unlike  the  benevolent  but  conflicting  bots  of  Wikipedia,  many \\nmalevolent bots are collaborative, often coordinating their behavior as part of botnets (27). \\nHowever,  before  being  able  to  study  the  social  interactions  of  these  bots,  we  first  need  to \\nlearn to identify them (28). \\n\\nOur  analysis  shows  that  a  system  of  simple  bots  may  produce  complex  dynamics  and \\nunintended  consequences.  In  the  case  of  Wikipedia,  we  see  that  benevolent  bots  that  are \\ndesigned to collaborate may end up in continuous disagreement. This is both inefficient as a \\nwaste  of  resources,  and  inefficacious,  for  it  may  lead  to  local  impasse.  Although  such \\ndisagreements represent a small proportion of the bots\\xe2\\x80\\x99 editorial activity, they nevertheless \\nbring  attention  to  the  complexity  of  designing  artificially  intelligent  agents.  Part  of  the \\ncomplexity  stems  from  the  common  field  of  interaction  \\xe2\\x80\\x93  bots  on  the  Internet,  and  in  the \\nworld at large, do not act in isolation, and interaction is inevitable, whether designed for or \\nnot. Part of the complexity stems from the fact that there is a human designer behind every \\nbot,  as  well  as  behind  the  environment  in  which  bots  operate,  and  that  human  artifacts \\nembody human culture. As bots continue to proliferate and become more sophisticated, social \\nscientist will need to devote more attention to understanding their culture and social life.   \\n\\nMaterials and Methods \\n\\nData \\n\\nWikipedia  is  an  ecosystem  of  bots.  Some  of  the  bots  are  \\xe2\\x80\\x9cediting  bots\\xe2\\x80\\x9d,  that  work  on  the \\narticles.  They  undo  vandalism,  enforce  bans,  check  spelling,  create  inter-language  links, \\nimport content automatically, etc. Other bots are non-editing: these bots mine data, identify \\nvandalism, or identify copyright violations.  \\n\\n12\\t\\n\\n\\t\\n\\n\\x0cIn addition to bots, there are also certain automated services that editors use to streamline \\ntheir work. For example, there are automated tools such Huggle and STiki, which produce a \\nfiltered set of edits to review in a live queue. Using these tools, editors can instantly revert the \\nedit in question with a single click and advance to the next one. There are also user interface \\nextensions and in-browser functions such as Twinkle, rollback, and undo, which also allow \\neditors to revert with a single click. Another automated service that is relatively recent and \\nmuch  more  sophisticated  is  the  Objective  Revision  Evaluation  Service  (ORES).  It  uses \\nmachine-learning techniques to rank edits with the ultimate goal to identify vandals or low-\\nquality contributions.  \\n\\nOur research focuses on editing bots. Our data contain who reverts whom, when, and in \\nwhat  article.  To  obtain  this  information,  we  analyzed  the  Wikipedia  XML  Dumps \\n(https://dumps.wikimedia.org/mirrors.html)  of  13  different  language  editions.  To  detect \\nrestored versions of an article, a hash was calculated for the complete article text following \\neach  revision  and  the  hashes  were  compared  between  revisions  (23).  The  data  cover  the \\nperiod from the beginning of Wikipedia (January 15, 2001) until February 2, 2010 \\xe2\\x80\\x93 October \\n31, 2011, the last date depending on when the data was collected for the particular language \\nedition. This time period captures the \\xe2\\x80\\x9cfirst generation\\xe2\\x80\\x9d of Wikipedia bots, as in later years, \\nWikidata  took  over  some  of  the  tasks  previously  controlled  by  Wikipedia.  The  sample  of \\nlanguages covers a wide range of Wikipedia editions in terms of size; for example, it includes \\nthe  four  largest  editions  by  number  of  edits  and  number  of  editors.  In  terms  of  cultural \\ndiversity, the sample covers a wide range of geographies.  \\n\\nWikipedia requires that human editors create separate accounts for bots and that the bot \\naccount  names  clearly  indicate  the  user  is  a  bot,  usually  by  including  the  word  \\xe2\\x80\\x9cbot\\xe2\\x80\\x9d \\n(https://en.wikipedia.org/wiki/Wikipedia:Bot_policy).  Hence, \\nthe  bots,  we \\nselected all account names that contain different spelling variations of the word \\xe2\\x80\\x9cbot.\\xe2\\x80\\x9d We \\nsupplemented this set with all accounts that have currently active bot status in the Wikipedia \\ndatabase \\n(using \\nhttps://en.wikipedia.org/wiki/Wikipedia:Bots/Status as of August 6, 2015). We thus obtained \\na list of 6,627 suspected bots. \\n\\ncriterion \\n\\nabove \\n\\nto \\n\\nidentify \\n\\nmay \\n\\nnot \\n\\nfit \\n\\nthe \\n\\nbut \\n\\nthat \\n\\nWe  then  used  the  Wikipedia  API  to  check  the  \\xe2\\x80\\x9cUser\\xe2\\x80\\x9d  page  for  each  suspected  bot \\naccount.  If  the  page  contained  a  link  to  another  account,  we  confirmed  that  the  current \\naccount was a bot and linked it to its owner. For pages that contained zero or more than one \\n\\n13\\t\\n\\n\\t\\n\\n\\x0clinks  to  other  accounts,  we  manually  checked  the  \\xe2\\x80\\x9cUser\\xe2\\x80\\x9d  and  \\xe2\\x80\\x9cUser_talk\\xe2\\x80\\x9d  pages  for  the \\nsuspected bot account to see if it is indeed a bot and to identify its owner. The majority of \\nmanually checked accounts were vandals or humans, so we ended up with 1,549 bots, each \\nlinked to its human owner. \\n\\nWe additionally labeled human editors as vandals if they had all their edits reverted by \\nothers. This rule meant that we labeled as vandals also newcomers who became discouraged \\nand left Wikipedia after all their initial contributions were reverted. Since we are interested in \\nsocial  interactions  emerging  from  repeated  activity,  we  do  not  believe  that  this  decision \\naffects our results.  \\n\\nUsing  the  revert  data,  we  created  a  directed  two-layer  multi-edge  network,  where \\nownership  couples  the  layer  of  human  editors  and  the  layer  of  bots  (29).  To  build  the \\nnetwork, we assumed that a link goes from the editor who restored an earlier version of the \\narticle (the \\xe2\\x80\\x9creverter\\xe2\\x80\\x9d) to the editor who made the revision immediately after that version (the \\n\\xe2\\x80\\x9creverted\\xe2\\x80\\x9d). All links were time-stamped. We collapsed multiple bots to a single node if they \\nwere  owned  by  the  same  human  editor;  these  bots  were  usually  accounts  for  different \\ngenerations of the same bot with the same function. In the network, reverts can be both intra- \\nand  inter-layer:  they  occur  within  the  human  layer,  within  the  bot  layer,  and  in  either \\ndirection  between  the  human  and  bot  layers.  The  multi-layer  network  was  pruned  by \\nremoving self-reverts, as well as reverts between a bot and its owner.  \\n\\nInteraction trajectories  \\n\\nWe model the interaction trajectories in two-dimensional space, where the x-axis measures \\ntime and the y-axis measures the difference between the number of times i has reverted j and \\nthe number of times j has reverted i. To construct the trajectories, starting from y0 = 0, yt = yt-1 \\n+ 1 if i reverts j at time t and yt = yt-1 \\xe2\\x80\\x93 1 if j reverts i at time t; the labels i and j are assigned \\nso that y >= 0 for the majority of the ij interaction time. We analyze three properties of the \\ntrajectories: \\n\\n\\xe2\\x80\\xa2  Latency.  We  define  latency  as  the  mean  log  time  in  seconds  between  successive \\n\\nreverts: \\xc2\\xb5(log10 \\xce\\x94t).  \\n\\n\\t\\n\\n14\\t\\n\\n\\x0c\\xe2\\x80\\xa2  Imbalance. We define imbalance as the final proportion of reverts between i and j that \\nwere not reciprocated: |ri \\xe2\\x80\\x93 rj| / (ri + rj), where ri and rj are the number of times i reverted j \\nand j reverted i, respectively.  \\n\\n\\xe2\\x80\\xa2  Reciprocity. We define reciprocity as the proportion of observed turning points out of \\nall  possible:  (#  turning  points)  /  (ri  +  rj  \\xe2\\x80\\x93  1)  ,  where  ri  and  rj  are  the  number  of  times  i \\nreverted j and j reverted i, respectively. A turning point occurs when the user who reverts at \\ntime t is different from the user who reverts at time t+1.  \\n\\nK-means clustering  \\n\\nTo identify the number of clusters k that best represents the data, we apply the elbow and \\nsilhouette  methods  on  trajectories  of  different  minimum  length.  The  rationale  behind \\nrestricting  the  data  to  long  trajectories  only  is  that  short  trajectories  tend  to  have  extreme \\nvalues  on  the  three  features,  thus  possibly  skewing  the  results.  According  to  the  elbow \\nmethod,  we  would  like  the  smallest  k  that  most  significantly  reduces  the  sum  of  squared \\nerrors  for  the  clustering.  According  to  the  silhouette  method,  we  would  like  the  k  that \\nmaximizes the separation distance between clusters and thus gives us the largest silhouette \\nscore. \\n\\nAlthough the elbow method suggests that four clusters provide the best clustering, the \\nsilhouette  method  indicates  that  the  data  cannot  be  clustered  well  (S8  Fig).  We  do  not \\nnecessarily expect that trajectories cluster naturally; rather, we employ clustering in order to \\nquantify the differences between the interactions of bots versus humans across languages. We \\nhence analyze the clustering with k = 4. This clustering also has the advantage of yielding \\nfour types of trajectories that intuitively make sense.   \\n\\n \\n\\nAcknowledgements \\n\\nThe authors thank Wikimedia Deutchland e.V. and Wikimedia Foundation for the live access \\nto  the  Wikipedia  data  via  Toolserver.  The  data  reported  in  the  paper  are  available  at \\nhttp://wwm.phy.bme.hu. \\n\\n \\n\\n\\t\\n\\n15\\t\\n\\n\\x0cReferences \\n\\n1.  \\n\\n2.  \\n\\n3.  \\n\\n4.  \\n\\nCornell Creative Machines Lab. AI vs. AI. Two chatbots talking to each other \\n[Internet]. YouTube. 2011 [cited 2016 Aug 4]. Available from: \\nhttps://www.youtube.com/watch?v=WnzlbyTZsQY \\n\\nFranklin S, Graesser A. Is It an agent, or just a program?: A taxonomy for autonomous \\nagents. In: M\\xc3\\xbcller JP, Wooldridge MJ, Jennings NR, editors. Intelligent Agents III: \\nAgent Theories, Architectures, and Languages [Internet]. Springer Berlin Heidelberg; \\n1997 [cited 2016 Aug 3]. p. 21\\xe2\\x80\\x9335. Available from: \\nhttp://link.springer.com/10.1007/BFb0013570 \\n\\nFloridi L, Sanders JW. On the morality of artificial agents. Minds Mach [Internet]. \\nKluwer Academic Publishers; 2004 Aug [cited 2016 Aug 12];14(3):349\\xe2\\x80\\x9379. Available \\nfrom: http://link.springer.com/10.1023/B:MIND.0000035461.63578.9d \\n\\nBerners-Lee T. Short summary of the World Wide Web project [Internet]. Google. \\n1991 [cited 2016 Aug 4]. Available from: \\nhttps://groups.google.com/forum/#!msg/alt.hypertext/eCTkkOoWTAY/bJGhZyooXzk\\nJ \\n\\n5.   Koster M. Guidelines for robot writers [Internet]. 1993 [cited 2016 Aug 2]. Available \\n\\nfrom: http://www.robotstxt.org/guidelines.html \\n\\n6.   Abokhodair N, Yoo D, McDonald DW. Dissecting a social botnet: Growth, content \\n\\nand influence in Twitter. Proceedings of the 18th ACM Conference on Computer \\nSupported Cooperative Work & Social Computing - CSCW \\xe2\\x80\\x9915 [Internet]. New York, \\nNew York, USA: ACM Press; 2015 [cited 2016 Jul 27]. p. 839\\xe2\\x80\\x9351. Available from: \\nhttp://dl.acm.org/citation.cfm?doid=2675133.2675208 \\n\\n7.  \\n\\n8.  \\n\\nLeonard A. Bots: The Origin of New Species. Hardwired; 1997. 218 p.  \\n\\nBrown J, Duguid P. The Social Life of Information. Harvard Business Press; 2000.  \\n\\n9.   Gianvecchio S, Xie M, Wu Z, Wang H. Measurement and classification of humans and \\n\\nbots in Internet chat. Proceedings of the 17th Conference on Security Symposium \\n[Internet]. USENIX Association; 2008. p. 155\\xe2\\x80\\x9369. Available from: \\n\\n16\\t\\n\\n\\t\\n\\n\\x0chttps://www.usenix.org/legacy/event/sec08/tech/full_papers/gianvecchio/gianvecchio_\\nhtml/ \\n\\n10.   Sysomos. An in-depth look at the most active Twitter user data [Internet]. 2009 [cited \\n2016 Aug 2]. Available from: https://sysomos.com/inside-twitter/most-active-twitter-\\nuser-data \\n\\n11.   Cashmore P. Twitter zombies: 24% of tweets created by bots [Internet]. Mashable. \\n2009 [cited 2016 Aug 2]. Available from: http://mashable.com/2009/08/06/twitter-\\nbots/#JqTVM0vEgqqA \\n\\n12.   Varvello M, Voelker GM. Second Life: A social network of humans and bots. \\n\\nProceedings of the 20th international workshop on Network and operating systems \\nsupport for digital audio and video - NOSSDAV \\xe2\\x80\\x9910 [Internet]. New York, New York, \\nUSA: ACM Press; 2010 [cited 2016 Jun 17]. p. 9. Available from: \\nhttp://portal.acm.org/citation.cfm?doid=1806565.1806570 \\n\\n13.   Holiday R. Fake traffic means real paydays [Internet]. Observer. 2014 [cited 2016 Aug \\n\\n3]. Available from: http://observer.com/2014/01/fake-traffic-means-real-paydays/ \\n\\n14.   Zeifman I. 2015 bot traffic report: Humans take back the Web, bad bots not giving any \\n\\nground [Internet]. Imperva Incapsula. 2015 [cited 2016 Aug 1]. Available from: \\nhttps://www.incapsula.com/blog/bot-traffic-report-2015.html \\n\\n15.   Murphy D. Blizzard cracks down on World of Warcraft cheaters [Internet]. PC \\n\\nMagazine. 2015 [cited 2016 Aug 2]. Available from: http://uk.pcmag.com/console-\\ngames-products/41990/news/blizzard-cracks-down-on-world-of-warcraft-cheaters \\n\\n16.   Newitz A. Ashley Madison code shows more women, and more bots [Internet]. \\nGizmodo. 2015 [cited 2016 Aug 3]. Available from: http://gizmodo.com/ashley-\\nmadison-code-shows-more-women-and-more-bots-1727613924 \\n\\n17.   Russell S, Norvig P. Artificial Intelligence: A Modern Approach. Third Edit. Harlow, \\n\\nUK: Pearson; 2009.  \\n\\n18.   Steiner T. Bots vs. wikipedians, anons vs. logged-ins (redux). Proceedings of The \\nInternational Symposium on Open Collaboration - OpenSym \\xe2\\x80\\x9914 [Internet]. New \\n\\n\\t\\n\\n17\\t\\n\\n\\x0cYork, New York, USA: ACM Press; 2014 [cited 2016 Jun 17]. p. 1\\xe2\\x80\\x937. Available from: \\nhttp://dl.acm.org/citation.cfm?doid=2641580.2641613 \\n\\n19.   Niederer S, van Dijck J. Wisdom of the crowd or technicity of content? Wikipedia as a \\n\\nsociotechnical system. New Media Soc [Internet]. 2010 Dec 1 [cited 2016 Jun \\n17];12(8):1368\\xe2\\x80\\x9387. Available from: \\nhttp://nms.sagepub.com/cgi/doi/10.1177/1461444810365297 \\n\\n20.   Kittur A, Suh B, Pendleton BA, Chi EH. He says, she says: Conflict and coordination \\nin Wikipedia. Proceedings of the SIGCHI conference on Human factors in computing \\nsystems - CHI \\xe2\\x80\\x9907. New York, New York, USA: ACM Press; 2007. p. 453--462.  \\n\\n21.   Brandes U, Kenis P, Lerner J, van Raaij D. Network analysis of collaboration structure \\n\\nin Wikipedia. Proceedings of the 18th international conference on World wide web - \\nWWW \\xe2\\x80\\x9909. New York, New York, USA: ACM Press; 2009. p. 731--740.  \\n\\n22.   Yasseri T, Sumi R, Rung A, Kornai A, Kert\\xc3\\xa9sz J. Dynamics of conflicts in Wikipedia. \\n\\nPLoS One. Public Library of Science; 2012 Jan;7(6):e38869.  \\n\\n23.   Sumi R, Yasseri T, Rung A, Kornai A, Kert\\xc3\\xa9sz J. Edit wars in Wikipedia. Privacy, \\nSecurity, Risk and Trust (PASSAT), 2011 IEEE Third International Conference on \\nSocial Computing (SocialCom) [Internet]. 2011 [cited 2016 Aug 30]. p. 724\\xe2\\x80\\x937. \\nAvailable from: http://arxiv.org/abs/1107.3689 \\n\\n24.   Halfaker A, Riedl J. Bots and cyborgs: Wikipedia\\xe2\\x80\\x99s immune system. Computer (Long \\n\\nBeach Calif) [Internet]. IEEE Computer Society; 2012 Mar [cited 2016 Aug \\n1];45(3):79\\xe2\\x80\\x9382. Available from: \\nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6163451 \\n\\n25.   Geiger RS, Halfaker A. When the levee breaks: Without bots, what happens to \\n\\nWikipedia\\xe2\\x80\\x99s quality control processes? Proceedings of the 9th International \\nSymposium on Open Collaboration - WikiSym \\xe2\\x80\\x9913 [Internet]. New York, New York, \\nUSA: ACM Press; 2013 [cited 2016 Jun 17]. p. 1\\xe2\\x80\\x936. Available from: \\nhttp://dl.acm.org/citation.cfm?doid=2491055.2491061 \\n\\n26.   Yasseri T, Spoerri A, Graham M, Kert\\xc3\\xa9sz J. The most controversial topics in \\n\\nWikipedia: A multilingual and geographical analysis. In: Fichman P, Hara N, editors. \\n\\n\\t\\n\\n18\\t\\n\\n\\x0cGlobal Wikipedia:  International and Cross-Cultural Issues in Online Collaboration. \\nScarecrow Press; 2014.  \\n\\n27.   Marciel M, Cuevas R, Banchs A, Gonz\\xc3\\xa1lez R, Traverso S, Ahmed M, et al. \\n\\nUnderstanding the detection of view fraud in video content portals. Proc 25th Int Conf \\nWorld Wide Web. International World Wide Web Conferences Steering Committee; \\n2016;357\\xe2\\x80\\x9368.  \\n\\n28.   Ferrara E, Varol O, Davis C, Menczer F, Flammini A. The rise of social bots. \\n\\nCommun ACM [Internet]. 2016 Jul 19 [cited 2016 Jun 20];59(7). Available from: \\nhttp://arxiv.org/abs/1407.5225 \\n\\n29.   Kivela M, Arenas A, Barthelemy M, Gleeson JP, Moreno Y, Porter MA. Multilayer \\n\\nnetworks. J Complex Networks. Oxford University Press; 2014 Jul;2(3):203\\xe2\\x80\\x9371.  \\n\\n \\n\\n \\t\\n\\n\\t\\n\\n19\\t\\n\\n\\x0cSupporting Information \\n\\nS1 Table. Descriptive statistics for the bot-bot layer and the human-human layer in the \\nmulti-layer networks of reverts. \\n\\n \\n\\n \\n\\nNumber \\nof nodes \\n\\nReverts \\n/ edits \\n\\nAvg. \\nnumber \\nreverts \\nper node \\n\\nAssortativ\\nity by \\nnumber of \\nedits \\n\\nAvg. \\ncluster\\ning  \\n\\nProp. dyads \\nwith at \\nleast one \\nrevert \\nreciprocate\\nd \\n\\nAvg. \\nclustering / \\navg. \\nclustering \\nin random \\nnetwork \\n\\n319 \\n182 \\n204 \\n225 \\n164 \\n178 \\n151 \\n124 \\n116 \\n122 \\n132 \\n104 \\n106 \\n\\nBot-bot \\nEnglish \\nJapanese \\nSpanish \\nFrench \\nPortuguese \\nGerman \\nChinese \\nHebrew \\nHungarian \\nCzech \\nArabic \\nRomanian \\nPersian \\nHuman-human \\nEnglish \\nJapanese \\nSpanish \\nFrench \\nPortuguese \\nGerman \\nChinese \\nHebrew \\nHungarian \\nCzech \\nArabic \\nRomanian \\nPersian \\n\\n4127880 \\n193203 \\n508815 \\n181395 \\n262293 \\n206734 \\n66470 \\n70816 \\n21036 \\n23792 \\n39083 \\n16625 \\n18657 \\n\\n104.6 \\n100.4 \\n71.7 \\n59.3 \\n185 \\n24.1 \\n103 \\n83.9 \\n66.8 \\n59 \\n161.7 \\n70.8 \\n63.8 \\n\\n3.1 \\n2.6 \\n2.3 \\n2.6 \\n2.3 \\n2.2 \\n3.2 \\n2.9 \\n2.4 \\n3.1 \\n2.2 \\n2.1 \\n3.6 \\n\\n0.002 \\n0.006 \\n0.003 \\n0.004 \\n0.006 \\n0.004 \\n0.006 \\n0.006 \\n0.004 \\n0.005 \\n0.011 \\n0.005 \\n0.005 \\n\\n0.079 \\n0.026 \\n0.070 \\n0.045 \\n0.066 \\n0.069 \\n0.028 \\n0.047 \\n0.016 \\n0.035 \\n0.044 \\n0.027 \\n0.056 \\n\\n0.46 \\n0.57 \\n0.53 \\n0.47 \\n0.57 \\n0.43 \\n0.59 \\n0.59 \\n0.54 \\n0.57 \\n0.6 \\n0.55 \\n0.5 \\n\\n0.09 \\n0.16 \\n0.09 \\n0.09 \\n0.09 \\n0.11 \\n0.18 \\n0.13 \\n0.11 \\n0.1 \\n0.08 \\n0.1 \\n0.16 \\n\\n-0.02 \\n-0.13 \\n-0.06 \\n-0.1 \\n-0.12 \\n-0.1 \\n-0.16 \\n-0.11 \\n-0.13 \\n-0.18 \\n-0.05 \\n-0.11 \\n-0.05 \\n\\n-0.05 \\n-0.05 \\n-0.11 \\n-0.02 \\n-0.14 \\n-0.1 \\n-0.14 \\n-0.13 \\n-0.13 \\n-0.18 \\n-0.17 \\n-0.2 \\n-0.13 \\n\\n0.43 \\n0.58 \\n0.57 \\n0.5 \\n0.64 \\n0.4 \\n0.62 \\n0.59 \\n0.6 \\n0.56 \\n0.6 \\n0.6 \\n0.53 \\n\\n0.04 \\n0.02 \\n0.1 \\n0.04 \\n0.12 \\n0.03 \\n0.08 \\n0.2 \\n0.1 \\n0.19 \\n0.11 \\n0.11 \\n0.21 \\n\\n19 \\n9 \\n12 \\n12 \\n8 \\n10 \\n9 \\n7 \\n8 \\n8 \\n7 \\n7 \\n8 \\n\\n72370 \\n2971 \\n33433 \\n4011 \\n19762 \\n3703 \\n3377 \\n7458 \\n1265 \\n2262 \\n2947 \\n1371 \\n1972 \\n\\n \\n\\n \\n\\n\\t\\n\\nBots revert each other to a great extent. They also reciprocate each other\\xe2\\x80\\x99s reverts to a considerable \\nextent. Their interactions are not as clustered as for human editors. Still, both for bots and humans, \\nmore senior editors tend to revert less senior editors, as measured by node assortativity by number of \\nedits completed. \\n\\n20\\t\\n\\n\\x0cS1 Fig. The number of bots, the number of edits by bots, and the proportion of edits \\ndone by bots between 2001 and 2010. Between 2003 and 2008 the number of bots and their \\nactivity  have  been  increasing.  This  trend,  however,  appears  to  have  subsided  after  2008, \\nsuggesting that the system may have stabilized. \\n\\n \\n\\n\\t\\n\\n21\\t\\n\\n\\x0cS2  Fig.  For  the  majority  of  languages,  bots  are  mainly  reverted  by  other  bots,  as \\nopposed  to  human  editors  or  vandals.  English  and  the  Romance  languages  in  our  data \\npresent exceptions, with less than 20% of bot reverts are done by other bots. \\n\\n \\n\\n \\n\\n \\n\\nS3 Fig. Bot-bot interactions have different characteristic time scale than human-human \\ninteractions. The figures show the distribution of interactions for a particular latency, where \\nwe define latency as the mean log time in seconds between successive reverts. (A) Bot-bot \\ninteractions have a characteristic latency of 1 month, as indicated by the peak in the figure. \\n(B) Human-human interactions occur with a latency of 2 minutes, 24 hours, or 1 year. \\n\\n22\\t\\n\\n\\t\\n\\n\\x0cS4  Fig.  Bot-bot  interactions  are  on  average  more  balanced  than  human-human \\ninteractions. We define imbalance as the final proportion of reverts between i and j that were \\nnot reciprocated. (A) A significant proportion of bot-bot interactions have low imbalance. (B) \\nThe majority of human-human interactions are perfectly unbalanced. \\n\\n \\n\\n\\t\\n\\n23\\t\\n\\n\\x0cS5  Fig.  Bots  reciprocate  much  more  than  humans  do  also  at  a  smaller  timescale. We \\nmeasure reciprocity as the proportion of observed turning points out of all possible. (A) A \\nsignificant proportion of bot-bot interactions have intermediate or high values of reciprocity. \\n(B) The majority of human-human interactions are not reciprocated. \\n\\n \\n\\n\\t\\n\\n24\\t\\n\\n\\x0cS6 Fig. Four types of interaction trajectories suggested by the k-means analysis. The left \\npanels  show  a  sample  of  the  trajectories,  including  bot-bot  and  human-human  interactions \\nand  trajectories  from  all  languages.  The  right  panels  show  the  distribution  of  latency, \\nimbalance,  and  reciprocity  for  each  type  of  trajectory.  The  three  properties  measure  the \\naverage  steepness,  the  y-value  of  the  last  point,  and  the  jaggedness  of  the  trajectory, \\nrespectively.  (A)  Fast  unbalanced  trajectories  have  low  reciprocity  and  latency  and  high \\nimbalance. (B) Somewhat balanced trajectories have intermediate imbalance and reciprocity. \\n(C) Slow unbalanced trajectories have low reciprocity and high latency and imbalance. (D) \\n\\n\\t\\n\\n \\n\\n25\\t\\n\\n\\x0cWell balanced trajectories have low imbalance and high reciprocity. \\n\\n \\n\\n \\n\\nS7  Fig.  The  number  of  articles  with  a  certain  number  of  bot-bot  and  human-human \\nreverts. (A) Few articles include more than 10 bot-bot reverts. The most contested articles \\ntend to be about foreign countries and personalities. Further, the same articles also re-appear \\nin different languages. (B) There are many articles that are highly contested by humans. The \\nmost contested articles tend to concern local personalities and entities. It is rare that a highly \\ncontested article in one language will be also highly contested in another language.  \\n\\n\\t\\n\\n26\\t\\n\\n\\x0c \\n\\nS8  Fig.  Performance  of  the  k-means  clustering  algorithm  for  different  number  of \\nclusters  and  for  sub-samples  with  different  minimum  length  of  trajectories.  (A)  The \\nelbow  method  requires  the  smallest  k  that  most  significantly  reduces  the  sum  of  squared \\nerrors for the clustering. Here, the method suggests that four clusters give the best clustering \\nof the data. (B) The silhouette method requires the k that maximizes the separation distance \\nbetween  clusters,  i.e.  the  largest  silhouette  score.  Here,  the  method  suggests  that  the \\nclustering performs worse as the number of clusters increases. \\n\\n\\t\\n\\n27\\t\\n\\n\\x0c',\n",
       " '6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n1\\n2\\n\\n \\n\\n \\n \\n]\\n\\nC\\nH\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n9\\n7\\n4\\n6\\n0\\n\\n.\\n\\n9\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nChatbots\\xe2\\x80\\x99 Greetings to Human-Computer\\n\\nCommunication\\n\\nMaria Jo\\xcb\\x9cao Pereira1\\n\\nLu\\xc2\\xb4\\xc4\\xb1sa Coheur1,2\\n\\nPedro Fialho1\\n\\nRicardo Ribeiro1,3\\n\\n1INESC-ID Lisboa\\n\\nRua Alves Redol, 9, 1000-029 Lisboa, Portugal\\n\\n2Instituto Superior T\\xc2\\xb4ecnico, Universidade de Lisboa\\n\\nAv. Prof. Cavaco Silva, 2780-990 Porto Salvo Tagus Park, Portugal\\n\\n3Instituto Universit\\xc2\\xb4ario de Lisboa (ISCTE-IUL)\\n\\nAv. das For\\xc2\\xb8cas Armadas, 1649-026 Lisboa, Portugal\\n\\nAbstract\\n\\nBoth dialogue systems and chatbots aim at putting into action com-\\nmunication between humans and computers. However, instead of focusing\\non sophisticated techniques to perform natural language understanding,\\nas the former usually do, chatbots seek to mimic conversation. Since\\nEliza, the \\xef\\xac\\x81rst chatbot ever, developed in 1966, there were many inter-\\nesting ideas explored by the chatbots\\xe2\\x80\\x99 community. Actually, more than\\njust ideas, some chatbots\\xe2\\x80\\x99 developers also provide free resources, includ-\\ning tools and large-scale corpora. It is our opinion that this know-how\\nand materials should not be neglected, as they might be put to use in\\nthe human-computer communication \\xef\\xac\\x81eld (and some authors already do\\nit). Thus, in this paper we present a historical overview of the chatbots\\xe2\\x80\\x99\\ndevelopments, we review what we consider to be the main contributions\\nof this community, and we point to some possible ways of coupling these\\nwith current work in the human-computer communication research line.\\nKeywords: natural language interfaces; agent-based interaction; in-\\n\\ntelligent agents; interaction design\\n\\n1 Introduction\\n\\nThe term chatbot was coined by Mauldin (1994) to de\\xef\\xac\\x81ne the systems that\\nhave the goal of passing the Turing Test1 and, thus, could be said \\xe2\\x80\\x9cto think\\xe2\\x80\\x9d.\\n\\n1http://plato.stanford.edu/entries/turing-test/\\n\\n1\\n\\n\\x0cHowever, terms like dialogue system, avatar, arti\\xef\\xac\\x81cial conversational entity, con-\\nversational avatar, intellectual agents, virtual people, or virtual person are often\\nused indiscriminately, as if they were synonyms of chatbot 2. In this paper, we\\nfollow Schumaker et al. (2007), and de\\xef\\xac\\x81ne a chatbot as a system that \\xe2\\x80\\x9cseeks\\nto mimic conversation rather than understand it\\xe2\\x80\\x9d. Also, and contrary to other\\nrelated systems, chatbots are supposed to freely engage conversation about any\\nsubject, making them \\xe2\\x80\\x9centertaining in a large variety of conversational topic\\nsettings\\xe2\\x80\\x9d (Schumaker et al., 2007).\\n\\nCurrently, many platforms exist to help developing such systems, and the\\nnumber of new chatbots continues to increase at a dizzying pace. The follow-\\ning (impressive) numbers, collected in February 2015, de\\xef\\xac\\x81nitely help to give\\na precise idea of the chatbots community size: just Pandorabots hosting ser-\\nvice3 declares to have more than 225,000 botmasters (people in charge of cre-\\nating/maintaining the chatbot), which have built more than 250,000 chatbots,\\nresulting in more than 3 billion interactions. Not only these resources are valu-\\nable, but also these numbers show how close the chatbots community is to real\\nusers. Thus, it is our opinion that chatbots\\xe2\\x80\\x99 developers and developments can\\nbring important contributions to the human-computer communication \\xef\\xac\\x81eld. In\\nthis paper, we review the main ideas and technologies behind them. As we will\\nsee, chatbots range from \\xe2\\x80\\x9csimpler\\xe2\\x80\\x9d ones, based on pre-written pattern-matching\\ntemplates, exploiting large stores of prepared small talk responses, to more com-\\nplex architectures, based on some sort of learning process. We will also see\\nthat, sometimes, concepts/tricks introduced by some chatbots contribute more\\nstrongly to the \\xe2\\x80\\x9cillusion of intelligence\\xe2\\x80\\x9d than the involved technologies.\\n\\nFinally, it should be noted that there is not much scienti\\xef\\xac\\x81c documentation\\navailable about the majority of these systems and it becomes di\\xef\\xac\\x83cult to uncover\\nthe technology behind them, which explains the abnormal number of references\\nto web pages in this paper.\\n\\nThis document is organised as follows: in Section 2 we present a brief his-\\ntorical overview, in Section 3 we discuss chatbot\\xe2\\x80\\x99s platforms and how to enrich\\nthem, and, in Section 4, we summarise the main \\xe2\\x80\\x9ctricks\\xe2\\x80\\x9d towards the \\xe2\\x80\\x9cillusion\\nof intelligence\\xe2\\x80\\x9d. In Section 5 we discuss how chatbots could contribute to the\\nhuman-computer communication \\xef\\xac\\x81eld and, \\xef\\xac\\x81nally, in Section 6, we present some\\nconclusions and point to some future challenges.\\n\\n2 Historical overview\\n\\nIn this section we make a brief review of the chatbots\\xe2\\x80\\x99 history, moving from the\\n\\xef\\xac\\x81rst chatbots to the ones with which we interact nowadays.\\n\\n2A list of more than 160 of such terms can be found in http://www.chatbots.org/\\n\\nsynonyms/\\n\\n3http://www.pandorabots.com\\n\\n2\\n\\n\\x0c2.1 Early days\\n\\nAlthough the term chatbot was not invented by that time, the \\xef\\xac\\x81rst chatbot\\ncame to public in 1966 under the appearance of a Rogerian psychotherapist\\ncalled Eliza (Weizenbaum, 1966). Eliza was a program developed by Joseph\\nWeizenbaum that was able to establish a conversation with human beings, simu-\\nlating it was one too. Eliza\\xe2\\x80\\x99s conversational model was based in the rephrasing\\nof input sentences, when these matched a set of pre-de\\xef\\xac\\x81ned rules. For instance,\\nconsider the following rule4 constituted by a regular expression (match) and an\\nanswer (answer):\\n\\nmatch: * you are *\\nanswer: What makes you think I am (2)?\\n\\nIn this rule if the match part coincides with the input (* is the wildcard\\nand matches every sequence of words), the text associated with the answer part\\nwill be returned, being the variable (2) replaced by the sequence from the input\\ncaptured by the second wildcard. The following dialogue (Example 1) illustrates\\nan application of this rule. Notice that some internal processing needs to be\\ndone, so that the sequence captured by (2) entitled to your opinion is modi\\xef\\xac\\x81ed\\ninto entitled to my opinion.\\n\\nuser: You are entitled to your opinion.\\nEliza: What makes you think I am entitled\\n\\nto my opinion?\\n\\nExample 1.\\n\\nIn Example 2, we can see that Eliza was also based on content-free remarks\\n\\nin the absence of a matching.\\n\\nuser: It bothers me just to be around\\n\\npeople.\\n\\nEliza: Please go on.\\n\\nExample 2.\\n\\nEliza completely exceeded the expectations, given that many people, when\\ninteracting with it, believed they were talking with another human (this outcome\\nis currently called the \\xe2\\x80\\x9cEliza e\\xef\\xac\\x80ect\\xe2\\x80\\x9d). Without having any intention of modelling\\n\\n4Inspired from Eliza\\xe2\\x80\\x99s implementation in http://search.cpan.org/~ jnolan/Chatbot-\\n\\nEliza-1.04/Chatbot/Eliza.pm .\\n\\n3\\n\\n\\x0cthe human cognitive process and despite its simplicity, Eliza showed how a\\nprogram impersonating a speci\\xef\\xac\\x81c professional role can cause a huge impression\\nby the mere illusion of understanding.\\n\\nWeizenbaum was taken aback by some aspects of this success (Hutchens,\\n1997). What shocked him most was the fact that people actually believed that\\nthe program understood their problems5. Perceiving Eliza as a threat, Weizen-\\nbaum wrote \\xe2\\x80\\x9cComputer Power and Human Reason\\xe2\\x80\\x9d (Kuipers et al., 1976) with\\nthe aim of attacking the Arti\\xef\\xac\\x81cial Intelligence (AI) \\xef\\xac\\x81eld and educating unin-\\nformed persons about computers.\\n\\nNowadays, Eliza is still one of the most widely known applications in AI,\\nand is at the base of a great number of chatbots, including Parry, its \\xe2\\x80\\x9csucces-\\nsor\\xe2\\x80\\x9d.\\n\\nFollowing a very similar architecture to that of Eliza, Parry appeared in\\n1971 by the hands of Kenneth Colby, simulating a paranoid mental patient (Say-\\ngin et al., 2000). An interesting comparison between Parry and Eliza was\\nmade by G\\xc2\\xa8uzeldere and Franchi6. They stated that \\xe2\\x80\\x9cParry\\xe2\\x80\\x99s strategy is some-\\nwhat the reverse of Eliza\\xe2\\x80\\x99s\\xe2\\x80\\x9d, as one simulates the doctor, distant and without\\npersonality traces, and the other a paranoid patient which states its anxieties.\\nAlthough Parry\\xe2\\x80\\x99s architecture is similar to that of Eliza, Parry has knowl-\\nedge of the conversation and it also owns a state of mind. The combination of\\nthese two factors a\\xef\\xac\\x80ects the output as it becomes a function not only of the\\ninput, but also of Parry\\xe2\\x80\\x99s beliefs, desires and intentions. Mauldin (1994) sum-\\nmarised a few tricks to which Parry resort, namely: (1) admitting ignorance;\\n(2) changing the conversation topic; and, (3) introducing small stories about\\nthe Ma\\xef\\xac\\x81a throughout the conversation. These three tricks are (respectively)\\nillustrated in the following answers given by Parry:\\n\\nParry: I don\\xe2\\x80\\x99t get you.\\n...\\nParry: Let\\xe2\\x80\\x99s talk about something else.\\n...\\nParry: I know the mob controls the big\\n\\nrackets.\\n\\nExample 3.\\n\\nAfter Colby gathered transcripts of interviews between psychiatrists, nor-\\nmal patients and his program, he presented the results to another group of\\npsychiatrists. He asked this group if they could guess in what transcripts the\\ninterviewed was a human and in which ones it was a program. The psychiatrist\\ncould not do better than randomly guessing.\\n\\n5http://www.alicebot.org/articles/wallace/eliza.html\\n6http://www.stanford.edu/group/SHR/4-2/text/dialogues.html\\n\\n4\\n\\n\\x0cIt is possible to conclude from these results that the emotional side can be\\neasier to imitate than the intellectual one (Kuipers et al., 1976). However, one\\nof the main criticisms Parry received was of not being more than an illusion,\\nincapable of modelling a real person (Colby, 1974).\\nIn his response to this\\nspeci\\xef\\xac\\x81c issue, Colby summarises the problem essence:\\n\\n\\xe2\\x80\\x9cA model of a paranoid patient is a model of being paranoid, being a patient,\\nand being a person. Parry does reasonably well in the \\xef\\xac\\x81rst two of these \\xe2\\x80\\x9cbe-\\nings\\xe2\\x80\\x9d. It fails in the third because of its limited knowledge. (...) Parry is not\\nthe real thing; it is a model, a simulation, an imitation, a mind-like artifact, an\\nautomaton, synthetic and arti\\xef\\xac\\x81cial.\\xe2\\x80\\x9d.\\n\\n2.2 The chatbots\\xe2\\x80\\x99 competitions\\n\\nMoving back to 1950, the British mathematician Alan Turing questioned \\xe2\\x80\\x9ccan\\nmachines think?\\xe2\\x80\\x9d (Turing, 1950), and proposed a way of testing it: the imitation\\ngame (now known as the Turing Test). The original imitation game is played\\nby a man, a woman and an interrogator whose objective is to guess the sex of\\nthe players. Turing proposed substituting one of the players by a machine and\\nplaying the same game. In this version, if the interrogator wrongly identi\\xef\\xac\\x81es\\nwho is the human it means that the machine \\xe2\\x80\\x9ccan think\\xe2\\x80\\x9d.\\n\\nBased on (their own interpretation of) the Turing Test, several chatbots\\ncompetitions exist today. Chatterbox Challenge7, or, more recently, the Chatbot\\nBattles8 that took place in 2012, are examples of such competitions, although the\\nthe most widely known is the Loebner prize9, where participants are challenged\\nwith a simpli\\xef\\xac\\x81ed version of the total Turing Test (Powers, 1998). This prize\\nis due to Hugh Loebner, who stipulated a reward for the \\xef\\xac\\x81rst person whose\\nprogram could pass the proposed test.\\n\\nThe \\xef\\xac\\x81rst Loebner Prize Contest took place in 1991, at Boston\\xe2\\x80\\x99s Computer\\nMuseum (Epstein, 1992), and, since then, the competition has been held annu-\\nally in the quest of \\xef\\xac\\x81nding the \\xe2\\x80\\x9cthinking computer\\xe2\\x80\\x9d. To illustrate the types of\\nquestions that can be posed to chatbots, Example 4 presents an extract of a\\nconversation10 between a member of the jury and Rose, the winner of the 2014\\nLoebner prize (developed by Bruce Wilcox).\\n\\nAs some chatbots, competing for the Loebner prize, are indeed capable of\\nmanaging a conversation, keeping it consistent, at least for a while, every year\\nthe most human-like computer is distinguished with a prize. However, since the\\n\\xef\\xac\\x81rst edition of the Loebner prize, in 1991, until now, no one won it. Nevertheless,\\nin another Turing Test organised in 2014 by the U.K.\\xe2\\x80\\x99s University of Reading, a\\nchatbot simulating a 13-year-old boy, named Eugene Goostman, created by\\nVladimir Veselov and his team, convinced 33% of the human judges that it was\\nhuman.\\n\\n7http://www.chatterboxchallenge.com\\n8http://www.chatbotbattles.com\\n9http://www.loebner.net/Prizef/loebner-prize.html\\n10Taken from http://aisb.org.uk/media/files/LoebnerPrize2014/Rose.pdf\\n\\n5\\n\\n\\x0cJury: Hello, my name is Adam, what is\\n\\nyour name?\\n\\nRose: Nice to meet you, Adam. My name is\\n\\nRose.\\n\\nJury: I live in Exeter in the UK. Where\\n\\ndo you live?\\n\\nRose: I live in San Francisco.\\n...\\nJury: What is your favourite television\\n\\nprogram?\\n\\nRose: I love watching Doctor Who. Do you\\n\\nthink the new Doctor will be good?\\n\\nJury: Which is bigger, a cat or a kitten?\\nRose: The kitten is bigger.\\nJury: The car couldn\\xe2\\x80\\x99t fit into the\\n\\nparking space because it was too\\nsmall. What was too small?\\n\\nRose: I\\xe2\\x80\\x99m not a walking encyclopedia you\\n\\nknow.\\n\\n...\\nJury: What\\xe2\\x80\\x99s my name?\\nRose: You are Adam.\\nJury: Where do I live?\\nRose: In UK.\\nJury: What\\xe2\\x80\\x99s the weather like where you\\n\\nare?\\n\\nRose: I\\xe2\\x80\\x99m an American woman.\\nJury: I recently visited New York. Where\\n\\nis your favourite place to visit\\nand why?\\n\\nRose: I like France best because I like\\n\\nthe food there.\\n\\nJury: How many letters are there in the\\n\\nword perambulate?\\n\\nRose: There are 11 letters in\\n\\n\\xe2\\x80\\x98perambulate\\xe2\\x80\\x99.\\n\\nJury: Andy is shorter than Mark and Tom\\n\\nis shorter than Andy. Who is the\\ntallest?\\n\\nRose: Mark is least short.\\n\\nExample 4.\\n\\nThis event brought to the spotlight the old question of AI and generated\\n\\n6\\n\\n\\x0c(again) much controversy. In fact, many people consider that there was a mis-\\nunderstanding of Turing\\xe2\\x80\\x99s intentions in the di\\xef\\xac\\x80erent implementations of the\\nTuring test, as deep models of thinking were a presupposition underlying Tur-\\ning\\xe2\\x80\\x99s imitation game. Following this, even if a chatbot was good enough to\\ndeceive the jury, it would not pass the Turing Test in Turing\\xe2\\x80\\x99s sense, as it does\\nnot have a cognition model behind it.\\n\\nAnother important criticism is stressed by Levesque (2014): these kind of\\ncomputer programs (\\xe2\\x80\\x9cchatterbots\\xe2\\x80\\x9d) and, the Turing Test (speci\\xef\\xac\\x81cally, the an-\\nnual Loebner Competition) are strongly based on capacity of being deceptive.\\nFor Levesque, AI is the science that studies \\xe2\\x80\\x9cintelligent behaviour in compu-\\ntational terms\\xe2\\x80\\x9d, and the ability to be evasive, although interesting, may not\\nshow real intelligence. A computer program should be able to demonstrate its\\nintelligence without the need for being deceptive. In this sense, Levesque et al.\\n(2012) further explore this idea by conceiving a test that meets these aims:\\nthe Winograd Schema Test. This is a reading comprehension test based on\\nbinary choice questions with speci\\xef\\xac\\x81c properties that makes them less prone to\\napproaches based on deceptive behaviour.\\n\\nApart from the numerous controversies regarding the Turing Test, and de-\\nspite that not all the chatbots intend to pass it, the fact is that all these com-\\npetitions strongly contributed to the main advances in the \\xef\\xac\\x81eld, and the most\\npopular chatbots are the ones that were/are present in these competitions.\\n\\n2.3 Other distinguished Chatbots\\n\\nMoving back to the Loebner prize, its \\xef\\xac\\x81rst winner, in 1991, was Joseph Wein-\\ntraub\\xe2\\x80\\x99s PC-Therapist program, based on Eliza, an achievement that he re-\\npeated three more times in the following four years. Since then, many chatbots,\\nwith di\\xef\\xac\\x80erent goals, emerged from the competing systems. An example is Jab-\\nberwacky11, created by Rollo Carpenter and released to public in 1997 (Angeli\\nand Brahnam, 2008), which has entered in four Loebner contests, and always\\nstood in the top three. Jabberwacky introduced the idea that a chatbot was\\nthe result of the knowledge gathered from its own conversations (Carpenter and\\nFreeman, 2005): \\xe2\\x80\\x9cJabberwacky learns the behaviour and words of its users\\xe2\\x80\\x9d.\\nIn 2005, Jabberwacky impersonated George, an entity created by Rollo Car-\\npenter \\xe2\\x80\\x9cin a smallish number of hours, just by chatting\\xe2\\x80\\x9d. More recently, a new\\nchatbot under the name of Cleverbot, also created by Rollo Carpenter, has\\nbecome available to the public12. Considering the similarities between Clever-\\nbot and Jabberwacky, and given that both systems have the same creator,\\nthe odds point that Cleverbot is a new improved version of Jabberwacky.\\nThus, considering that it \\xe2\\x80\\x9clearns from people\\xe2\\x80\\x9d, and that it is probably one of\\nthe most widely known bots, the number of interactions it can learn with is\\nendless13. Although there is no information about how this process takes place,\\n\\n11http://www.jabberwacky.com/\\n12http://cleverbot.com/\\n13According to the Cleverbot site, consulted on 12th February 2015, there were people 88,015\\n\\ntalking.\\n\\n7\\n\\n\\x0cusers can now rate Cleverbot answers (\\xef\\xac\\x81ve possibilities, from awful to great).\\nA \\xef\\xac\\x81nal curiosity about Cleverbot:\\nit was recently used to co-write a short\\n\\xef\\xac\\x81lm, \\xe2\\x80\\x9cDo you love me\\xe2\\x80\\x9d, directed by Chris R. Wilson14.\\n\\nAnother competing system in the Loebner contests that needs to be high-\\nlighted, as it plays a major role in the chatbots \\xef\\xac\\x81eld, is the Arti\\xef\\xac\\x81cial Linguistic\\nInternet Computer Entity (A.l.i.c.e) (Shah, 2006). It was invented in 1995 by\\nRichard Wallace, and won several Loebner competitions. Even though it is a\\nmodern Eliza (that is, based on pattern matching), it di\\xef\\xac\\x80ers from it by not\\nplaying a speci\\xef\\xac\\x81c role, but by trying to re\\xef\\xac\\x82ect a human in general. The propose\\nof A.l.i.c.e\\xe2\\x80\\x99s creation was to keep it talking as long as possible without the\\nusers realising that they were not talking to a machine, and without sticking to\\na speci\\xef\\xac\\x81c topic or role. Also, associated with A.l.i.c.e there is a collection of\\nresources that have been widely used by the chatbots\\xe2\\x80\\x99 community, including the\\npreviously mentioned hosting service Pandorabots, which represents the largest\\nchatbot community on the Internet.\\n\\nFinally, we detach Chip Vivant developed by Mohan Embar. Chip Vi-\\nvant di\\xef\\xac\\x80ers from other chatbots, as its goal is \\xe2\\x80\\x9cto answer basic, common sense\\nquestions and attempt simple deductive reasoning instead of having a massive\\ndatabase of canned responses in an attempt to fool users with the Eliza ef-\\nfect\\xe2\\x80\\x9d15. Considering this, and despite being the winner of the Loebner prize in\\n2012, Chip Vivant is not a chatbot, according to our previous de\\xef\\xac\\x81nition. In\\nfact, Chip Vivant is original in the way it operates, as it uses several exter-\\nnal resources broadly used in Natural Language Processing applications, such as\\nWordnet (Fellbaum, 1998), Wikipedia16, OpenCyc17, and the Link Parser (Grin-\\nberg et al., 1995) API. Due to this, and according with its author, Chip Vivant\\nwas the \\xef\\xac\\x81rst chatbot capable of answering questions such as Which is larger:\\nan orange or the moon?.\\n\\n2.4 The chatbot next door\\n\\nAs many di\\xef\\xac\\x80erent resources are available today, chatbots become a \\xef\\xac\\x81eld in\\nlarge expansion, as attested by the previously reported numbers regarding Pan-\\ndorabots. Chatbots\\xe2\\x80\\x99 technology can be used by anyone (there are even sites\\nwhere kids can create their own bots, for instance, inf.net), and the most\\nimportant requirement is to be creative. Due to this, chatbots can be found\\nin a huge diversity of services, including e-commerce (Daden Limited, 2010),\\ne-learning (Heller et al., 2005; Mikic et al., 2009), and in even in medical sce-\\nnarios (Kazi et al., 2012). Just Chatbots.org18 reports chatbots in almost 30\\nlanguages, available in platforms like Android, Live Messenger, Second Life or\\nSkype, just to name a few, and dedicated to an impressive collection of themes\\nsuch as Beauty, Cooking, Government, Leisure, Sports or Travel. In other words,\\n\\n14 http://www.youtube.com/watch?v=QkNA7sy5M5s\\n15http://www.chipvivant.com\\n16https://www.wikipedia.org\\n17http://www.cyc.com\\n18https://www.chatbots.org/\\n\\n8\\n\\n\\x0cchatbots move from the Turing Test competitions to real life. A chatbot that\\nperfectly illustrates this idea is Elbot, a regular participant/winner in chatbots\\ncontests19 and an Alice type program, which is currently being used on sites\\nlike IKEA\\xe2\\x80\\x99s (Shah, 2006).\\n\\nMore than just \\xe2\\x80\\x9ctext boxes\\xe2\\x80\\x9d, modern chatbots have a face, and sometimes\\na body. Some allow speech input and output, and are able to express emotions.\\nPandorabots, for instance, o\\xef\\xac\\x80ers multimodal facilities like faces and speech.\\nCleverbot, on the other hand, led to the creation of an avatar called Evie\\n(Expressive Virtual Interaction Entity)20, which has the possibility of receiving\\nboth written or verbal inputs. Moreover, its animated avatar is also capable of\\ndisplaying some human emotions.\\n\\n3 Building chatbots\\n\\nBehind each chatbot there is a development platform. These are typically based\\non a scripting language that allows the botmaster to handcraft its knowledge\\nbase, as well as an engine capable of mapping the user\\xe2\\x80\\x99s utterances into the most\\nappropriate answer. In this section we survey the most successful platforms and\\nscripting languages, as well as the existing learning processes. Moreover, we end\\nthe section by referring to the scripting process itself.\\n\\n3.1 Scripting languages/platforms\\n\\nAn impressive collection of Elizas can be currently found in the web. Some of\\nthese software can be customised. For instance, Chatbot-Eliza21 is an imple-\\nmentation of Eliza in Perl that can be used to build other chatbots. Knowledge\\nis coded as a set of rules that are triggered when matched against the user\\xe2\\x80\\x99s in-\\nput, as previously illustrated in this paper. Some of the available programs o\\xef\\xac\\x80er\\nfeatures such as a certain capability to memorise information, adding synonyms\\nor ranking keywords. Nevertheless, the most popular language to build chat-\\nbots is probably the \\xe2\\x80\\x9cArti\\xef\\xac\\x81cial Intelligence Markup Language\\xe2\\x80\\x9d, widely known\\nas AIML, a derivative of XML, that includes more than twenty speci\\xef\\xac\\x81c tags.\\nAs usual, knowledge is coded as a set of rules that will match the user input,\\nassociated with templates, the generators of the output. A detailed description\\nof AIML syntax is out of the scope of this survey, but can be easily found in\\nthe web22. The large usage of AIML can be justi\\xef\\xac\\x81ed by the following facts:\\n\\n1. besides its detailed speci\\xef\\xac\\x81cation, its community allows anyone to obtain,\\nfor free, interpreters of AIML in almost all coding languages, from Java\\n(program D) to C/C++ (program C) or even Lisp (program Z);\\n\\n19http://www.elbot.com/chatterbot-elbot/\\n20http://www.existor.com/\\n21http://search.cpan.org/~ jnolan/Chatbot-Eliza-1.04/Chatbot/Eliza.pm\\n22http://www.alicebot.org/aiml.html\\n\\n9\\n\\n\\x0c2. the set of AIML \\xef\\xac\\x81les that constitute the contents of A.l.i.c.e.\\xe2\\x80\\x99s brain can\\n\\nalso be freely obtained23.\\n\\nAll the pandorabots are based on AIML, more speci\\xef\\xac\\x81cally in AIML 2.0. This\\nspeci\\xef\\xac\\x81c release is usually characterised as being very easy to modify, develop\\nand deploy. Therefore, anyone, even non-computer-experts, can make use of\\nit (Wallace et al., 2007), as no prior knowledge about AIML is required.\\nIt\\nis only necessary to give a bot a name and choose the startup AIML. Then,\\nthe botmaster just has to type the sentences he/she wants to see his/her bot\\nanswering and add the desired responses. It is also possible to improve the bot\\nby adding AIML \\xef\\xac\\x81les. Such \\xef\\xac\\x81les can be easily written using the Pandorabot\\xe2\\x80\\x99s\\nutility Pandorawriter, which allows to \\xe2\\x80\\x9cconvert free-format dialog into AIML\\ncategories suitable for uploading to your pandorabot\\xe2\\x80\\x9d.\\n\\nChatScript24, the scripting language and open-source engine, should also be\\naddressed, as is at the basis of Suzette (2010 Loebner Prize winner), Rosette\\n(2011 Loebner Prize winner), Angela (2nd in 2012 Loebner Prize), and the pre-\\nviously referred Rose (2014 Loebner Prize winner). It comes with useful fea-\\ntures, including an ontology of nouns, verbs, adjectives and adverbs, and o\\xef\\xac\\x80ers a\\nscripting language (inspired by the Scone project, a knowledge-base system de-\\nveloped to support human-like common-sense reasoning and the understanding\\nof human language (Fahlman, 2011)). According to Bruce Wilcox, its creator,\\nChatScript settles several AIML problems, such as not being reader friendly. In\\nfact, as AIML is based on recursive self-modifying input, it is harder to debug\\nand maintain. A detailed comparison between ChatScript and AIML capa-\\nbilities was made available by Wilcox, as a motivation for the development of a\\nnew (his own) chatbot platform. This comparison can be found in his blog25.\\n\\nIt should be clear that we exclude from this survey, authoring platforms such\\nas the IrisTK26, the Visual SceneMaker (Gebhard et al., 2011), or the Virtual\\nHuman Toolkit27 (Hartholt et al., 2013), as these target multi-modal dialogue\\nsystems and not chatbots, as de\\xef\\xac\\x81ned in the Introduction section.\\n\\n3.2 Building chatbots by chatting\\n\\nAnother approach to develop chatbots\\xe2\\x80\\x99 knowledge sources, which avoids hand-\\ncrafted rules, is based on chatting and learning from the resulting chats. Con-\\ntrary to other chatbots whose response is derived from the recognition of pat-\\nterns in the user\\xe2\\x80\\x99s input with little knowledge of context, systems like the already\\nmentioned Jabberwacky (and Cleverbot) learn by keeping never seen user\\ninteractions and posing them later to other users. The acquired answers are\\nthen considered suitable answers for these interactions. That is, they learn to\\ntalk by talking, by relying on what has been said before by users and mimicking\\n\\n23http://code.google.com/p/aiml-en-us-foundation-alice/downloads/list\\n24http://sourceforge.net/projects/chatscript/\\n25http://gamasutra.com/blogs/BruceWilcox/20120104/9179/\\n26http://www.iristk.net\\n27https://vhtoolkit.ict.usc.edu\\n\\n10\\n\\n\\x0cthem. The user\\xe2\\x80\\x99s intelligence becomes \\xe2\\x80\\x9cborrowed intelligence\\xe2\\x80\\x9d as, instead of\\nbeing wasted, it incorporates a loop: what is said is kept (along with the infor-\\nmation of when it was said) and in the future that knowledge may be exposed\\nto another user. The given replies are then saved as new responses that the\\nsystem can give in the future.\\n\\nUnfortunately, it is only possible to give a brief overview of Jabberwacky\\xe2\\x80\\x99s\\nor Cleverbot learning mechanisms as their architecture is not available to the\\npublic. The only disclosed thing is that the AI model is not one of the usually\\nfound in other systems, but a \\xe2\\x80\\x9clayered set of heuristics that produce results\\nthrough analyses of conversational context and positive feedback\\xe2\\x80\\x9d28.\\n\\nAnother example of a chatbot that learns is Robby Garner\\xe2\\x80\\x99s \\xe2\\x80\\x9cFunctional Re-\\nsponse Emulation Device\\xe2\\x80\\x9d (Fred), the ancestor of Albert One, the winner\\nof 1998 and 1999 Loebner Prize. Fred was a computer program that learned\\nfrom other people\\xe2\\x80\\x99s conversations in order to make its own conversations (Ca-\\nputo et al., 1997). Fred began with a library of basic responses, so that it\\ncould interact with users, and from then on, it learned new phrases with users\\nwilling to teach it29.\\n\\nAlthough such an (unsupervised) learning may lead to unexpected and un-\\ndesirable results, with the Internet growth and the possibility of having many\\npeople talking with the chatbots, one may foresee that these will quickly evolve.\\nWe will discuss this issue latter in Section 5.\\n\\n4 Towards the illusion of intelligence and/or the\\n\\nart of scripting\\n\\nChatbots go beyond writing good programs and developing algorithms, as in\\norder to create a chatbot, more than being a programmer, the botmaster must\\nbe an author. Juergen Pirner, creator of the 2003 Loebner prize winner Jab-\\nberwock30, emphasises the scripting process behind a chatbot, stating that\\nin the presence of possible failures, the one at fault is not the engine but its\\nauthor31.\\n\\nSince making a chatbot involves preparing it to the impossible mission of\\ngiving a plausible answer to all possible interactions, the botmasters usually\\ntake advance of several tricks to simulate understanding and intelligence in their\\nchatbots. For instance, Pirner describes basic techniques of scripted dialogs like\\n\\xe2\\x80\\x9chaving a set of responses for each scripted dialog sequence\\xe2\\x80\\x9d and \\xe2\\x80\\x9cending those\\nsame responses with a clue, a funny remark or a wordplay\\xe2\\x80\\x9d. With Eliza, we\\nlearnt that including the user\\xe2\\x80\\x99s string in its answers helps maintaining an illusion\\nof understanding (Mauldin, 1994). Other approaches focus on trying to guess\\nwhat the user might say, or forcing him/her to say something expected. In the\\nfollowing we survey other stratagems used by many botmasters.\\n\\n28http://www.icogno.com/a_very_personal_entertainment.html\\n29http://www.simonlaven.com/fred.htm\\n30http://www.abenteuermedien.de/jabberwock/\\n31http://www.abenteuermedien.de/jabberwock/how-jabberwock-works.pdf\\n\\n11\\n\\n\\x0c4.1 Giving the bot a personality\\n\\nWhereas personality has been a subject of study among the agent\\xe2\\x80\\x99s community,\\ndeeply exploited in all its complexity, the concept is kept as simple as possible\\nwithin chatbots. As we have seen, what is common is the association of an a\\npriori \\xe2\\x80\\x9cpersonality\\xe2\\x80\\x9d to a chatbot, which can justify some answers that other-\\nwise would be considered inappropriate. For instance, Rogerian mode of Eliza\\ncovers for its answers, as it leads to a conversation where the program never\\ncontradicts itself, never makes a\\xef\\xac\\x83rmations, and is free to know nothing or little\\nabout the real world without being suspicious. The same happens with Colby\\xe2\\x80\\x99s\\nParry: being a paranoid mental patient its changes in subject or incongruous\\nanswers are considered satisfactory and hide its absence of understanding. The\\naforementioned Eugene Goostman also follows along these lines. Vaselov ex-\\nplains his reasoning for such a character: \\xe2\\x80\\x9ca 13 years old is not too old to know\\neverything and not too young to know nothing\\xe2\\x80\\x9d32.\\n\\nThomas Whalen, winner of 1994 Loebner prize, took this a step further\\nwith Joe, the janitor. Whalen\\xe2\\x80\\x99s decision was related to the fact that contrary to\\nprevious editions of Loebner competitions, where the conversation was restricted\\nto a topic, in 1995 the judges could pose any question. Hence, Whalen decided\\nthat the best approach to deal with a non-topic situation, would be to present\\na system that \\xe2\\x80\\x9cwould not simply try to answer questions, but would try to\\nincorporate a personality, a personal history, and a unique view of the world\\xe2\\x80\\x9d33.\\nAnd so Joe was born.\\n\\nJoe was a night-worker janitor in the verge of being \\xef\\xac\\x81red. He was only\\n\\xe2\\x80\\x9cmarginally literate\\xe2\\x80\\x9d, and he did not read books, newspapers, or watch televi-\\nsion. These premises by themselves restricted the conversation by giving Joe a\\n\\xe2\\x80\\x9cfairly narrow worldview\\xe2\\x80\\x9d. Another trick was to use Joe\\xe2\\x80\\x99s eminent dismissal to\\nintroduce some stories revolving around it, which would, at the same time, pro-\\nvide a way of directing the conversation, the topic of the next section. However,\\ndespite the modelling of what Whalen considered to be the critical component of\\nhumanness \\xe2\\x80\\x93 the personality (along with the development of answers to common\\ntopics like weather) \\xe2\\x80\\x93, he did not won the competition. However, such occur-\\nrence can be justi\\xef\\xac\\x81ed because the judges asked questions that nobody would\\nremember to pose to someone recently met.\\n\\n4.2 Directing a conversation\\n\\nPersonality can justify some appropriate answers, but the best way to deal\\nwith unexpected interactions is to avoid them. Thus, being able to direct the\\nconversation is a trick used by many chatbots, including the simple forms used\\nby Eliza, where the usage of questions incited the user participation and made\\nhim/her keep the conversation with little contribution from the program.\\n\\nThe Converse (Batacharia et al., 1999), created by David Levy, was the\\n\\n32http://www.huffingtonpost.com/2012/06/27/eugene-goostman-2012-turing-test-\\n\\nwinner_n_1630412.html\\n\\n33http://hps.elte.hu/~gk/Loebner/story95.htm\\n\\n12\\n\\n\\x0c1997 winner of the Loebner competition, and did extremely well by using the\\nclever trick of controlling a conversation. Although directing a conversation by\\n\\xe2\\x80\\x9ctalking a lot about a prede\\xef\\xac\\x81ned topic\\xe2\\x80\\x9d was already used (Saygin et al., 2000),\\nConverse\\xe2\\x80\\x99s performance convinced a judge for the \\xef\\xac\\x81rst \\xef\\xac\\x81ve minutes that he\\nwas really human: after greeting the judge, Catherine (Converse\\xe2\\x80\\x99s charac-\\nter) asked the interrogator about something that had passed on the news the\\nprevious day and then kept talking about it, as can be seen in the transcripts34\\nin Example 5.\\n\\nConverse: Hi, my name is Catherine,\\n\\nwhat\\xe2\\x80\\x99s yours?\\n\\nJudge 4: My name is Elliott.\\nConverse: ell, it\\xe2\\x80\\x99s nice to meet you,\\n\\nElliott. Did you see that story\\non CNN last night about the\\nlesbian couple who came out at\\na White House party on Sunday?\\n\\nJudge 4: Yes, I did. I think it may all\\n\\nbe a publicity stunt for Ellen.\\n\\nConverse: Ellen Degeneres was one of them\\n\\n- she was kissing her lover in\\nfront of Bill Clinton. Do you\\nthink he was very embarrassed?\\n\\nJudge 4: I think he want the gay vote. He\\n\\nwill do anything to get it and\\nkeep it. He does not have a\\nconservative base.\\n\\nConverse: Are you still a Friend of Bill\\n\\nClinton?\\n\\n...\\n\\nExample 5.\\n\\nThis example also shows that besides controlling a conversation it is impor-\\ntant to appropriately choose its topic. David Levy\\xe2\\x80\\x99s won again the Loebner prize\\nin 2009 with Do-Much-More35, but this time the system was more \\xef\\xac\\x82exible in\\nthe range of topics and responses it covered.\\n\\n4.3 Paying attention to small talk\\n\\nSmall talk, also known as phatic communication (Malinowski, 1923), is an-\\nIt can be viewed as a \\xe2\\x80\\x9cneutral, non-\\nother hot topic in chatbots advances.\\n\\n34http://www.worldsbestchatbot.com/Corporate_History\\n35http://www.worldsbestchatbot.com/\\n\\n13\\n\\n\\x0ctask-oriented conversation about safe topics, where no speci\\xef\\xac\\x81c goals needs to\\nbe achieved\\xe2\\x80\\x9d (Endrass et al., 2011). Small talk can be used for two main pro-\\nposes (Schneider, 1988): establish a social relation by building rapport and\\navoiding (embarrassing) silence.\\n\\nLike stated by Bickmore and Cassell (1999), chatbots have been making\\nuse of the small talk mechanism. Such is brought to evidence when one looks\\nat the testimonials of persons establishing ongoing relationships with chatbots.\\nFor instance, Epstein (2007), an American psychologist, professor, author, and\\njournalist, went to an online dating service, and believed for several months that\\na chatbot, met in the dating service, was a \\xe2\\x80\\x9cslim, attractive brunette\\xe2\\x80\\x9d.\\n\\nIn brief, small talk is a constant in all chatbots programs, used in non-\\nsequiturs or canned responses.\\nIt not only allows to give the idea of under-\\nstanding, but also eases cooperation and facilitates human-like interaction by\\ngaining the user trust and developing a social relationship (Bickmore and Cas-\\nsell, 2000).\\n\\n4.4 Failing like a human\\n\\nAfter introducing the imitation game, Turing presented an example (Example 6)\\nof a possible conversation one could have with a machine (Turing, 1950).\\n\\nHuman: Add 34957 to 70764.\\n(after pause of about 30 seconds)\\nMachine: 105621.\\n\\nExample 6.\\n\\nObserving this example, besides the delay in providing the response, we\\ncan easily see that the answer is wrong. And this brings new insights to the\\nmodelling of human-computer communication. As Wallace wrote36, \\xe2\\x80\\x9cwe tend\\nto think of a computer\\xe2\\x80\\x99s replies ought to be fast, accurate, concise and above all\\ntruthful\\xe2\\x80\\x9d. However, human communication is not like that, containing errors,\\nmisunderstandings, dis\\xef\\xac\\x82uencies, rephrases, etc.\\n\\nThis is something that earlier chatbot\\xe2\\x80\\x99s writers already had in mind, as\\nsome already cared about simulated typing. For instance, Julia, Mauldin\\xe2\\x80\\x99s\\nChatterbot (Mauldin, 1994), simulated human typing by including delays and\\nleaving some errors. Simulated typing also proves to be useful in decreasing\\nmistakes by slowing down the interaction (Philip Maymin, a Loebner contestant\\nin 1995, slowed so much the typing speed of his program that a judge was not\\nable to pose more than one or two questions (Hutchens, 1997)).\\n\\n36http://www.alicebot.org/anatomy.html\\n\\n14\\n\\n\\x0c5 Chatbots and the human-computer communi-\\n\\ncation \\xef\\xac\\x81eld\\n\\nSeveral works from the human-computer communication \\xef\\xac\\x81eld use resources from\\nthe chatbots\\xe2\\x80\\x99 community and/or couple with strategies reported by chatbots\\xe2\\x80\\x99\\ndevelopers. However there are still some research challenges regarding the use\\nof some chatbots\\xe2\\x80\\x99 resources. We will discuss these issues in the following.\\n\\n5.1 Some works that merge both communities\\n\\nSome works take advantage of the scripting languages provided by the chatbots\\xe2\\x80\\x99\\ncommunity. An example is the conversational agent Edgar Smith (Fialho et al.,\\n2013), an old butler that answers questions about Monserrate\\xe2\\x80\\x99s palace, in Sintra,\\nPortugal, the place where it can be found (Figure 1), as part of its answers are\\nretrieved from an AIML database.\\n\\nFigure 1: Edgar Smith, in Monserrate, Sintra, Portugal.\\n\\nEdgar\\xe2\\x80\\x99s main knowledge base is constituted of question/answering pairs re-\\nlated with its domain of expertise (the palace), as the one of Sergeant Black-\\nwell, installed in the Cooper-Hewitt National Design Museum in New York (Su-\\nsan Robinson and Henderer, 2008), and the one of the twins, Ada and Grace, vir-\\ntual guides in the Boston Museum of Science (Traum et al., 2012). As Sergeant\\nBlackwell and the twins, the natural language interpretation module of Edgar\\ntargets to select the most likely answer from the agent\\xe2\\x80\\x99s main knowledge base,\\nbased on some classi\\xef\\xac\\x81cation process; however, Edgar falls into an AIML knowl-\\n\\n15\\n\\n\\x0cedge base, when no successful answer is found in the previous step. Its main\\nknowledge base was built by experts, but the AIML module allowed the fast\\ndevelopment of a secondary knowledge base, integrating some chat-based dia-\\nlogues, based on pattern matching.\\n\\nConsidering the idea of learning to chat by chatting, which is at the basis\\nof some chatbots, as we have previously seen, there are several recent works\\nthat explore it, by using large quantities of human interactions to build/train\\nconversational agents. For instance, both Shawar and Atwell (2003) and Shawar\\n(2005) are dedicated to the problem of retraining a chatbot with human dia-\\nlogue examples. Another example is the chatbot IRIS, presented by Banchs\\nand Li (2012), which was created based on Movie-DiC (Banchs, 2012), a corpus\\nextracted from movies\\xe2\\x80\\x99 scripts. Filipe is a chatbot that should also be men-\\ntioned, as it has a knowledge base built on a corpus, the Subtle corpus, built\\nwith movies\\xe2\\x80\\x99 subtitles (Magarreiro et al., 2014).\\n\\nAlso, many conversational agents also rely on tricks to simulate intelligence.\\nAn example is the 3D Hans Christian Andersen (HCA), a conversational agent\\ncapable of establishing multi-modal conversations about the namesake writer\\xe2\\x80\\x99s\\nlife and tales (Bernsen and Dybkj\\xc3\\xa6r, 2005), which changes topic when lost in\\nthe conversation, and has an \\xe2\\x80\\x9cexcuse\\xe2\\x80\\x9d for not answering some questions: it does\\nnot remember (yet) everything that the real HCA once knew. Another example\\nis, once again, the virtual butler Edgar Smith, as it suggests questions when it\\nis not able to understand an utterance, and it starts talking about the palace if\\nit does not understand the user repeatedly. A feature in its character de\\xef\\xac\\x81nition\\nalso \\xe2\\x80\\x9cexcuses\\xe2\\x80\\x9d some misunderstandings: as Edgar is an old \\xe2\\x80\\x9cperson\\xe2\\x80\\x9d, it does\\nnot have a very acute hearing. Both these examples, show how a \\xe2\\x80\\x9cpersonality\\xe2\\x80\\x9d\\nor, at least, some context, allows to \\xe2\\x80\\x9cforgive\\xe2\\x80\\x9d some lacks on the conversational\\nagent\\xe2\\x80\\x99s knowledge base or even some of its answers.\\n\\nFinally, there are also works that target to enhance chatbots\\xe2\\x80\\x99 resources. Ex-\\namples are the Persona-AIML architecture, that allows the creation of chatbots\\nin AIML, with a personality (Galv\\xcb\\x9cao et al., 2004), or the work described by Cho\\n(2007), where an emotion and personality model is added to A.l.i.c.e., allowing\\nits decisions to be based on its personality and emotions, as well.\\n\\n5.2 Main challenges\\n\\nAs previously said, Pandorabots reports over 3 billion conversational interac-\\ntions. Chatscript, although much more recent, provides more that 3 million\\ninteractions. Even if we just consider the contents of A.l.i.c.e\\xe2\\x80\\x99s brain, as well\\nas the logs collected by Bruce Wilcox (both can be freely obtained), we have\\nat hands extremely valuable resources, as they represent real interactions posed\\nby real people, thus, containing not only requests posed by real people, but also\\nanswers given by real people.\\n\\nThese requests can be extremely useful as, considering Zipf\\xe2\\x80\\x99s law, a program\\nthat receives a certain input has a non zero probability of having the same\\ninput entered later and, thus, by looking at requests that people usually pose\\nto chatbots, one can track patterns for which a speci\\xef\\xac\\x81c reply was not created\\n\\n16\\n\\n\\x0cyet. In other words, these requests are a way of having an idea of what people\\nwill ask. Moreover, they are the closest thing to the logs collected by Siri37,\\nCortana38 or Google Now39, to which the whole community has access to.\\n\\nConsidering the answers, some works already use corpora constituted of\\ninteractions to complement the agent\\xe2\\x80\\x99s knowledge base, and, in particular, to\\nprovide answers to out-of-domain interactions. The main motivation to \\xef\\xac\\x81nd\\nappropriate answers to these interactions (reported by all conversational agents\\ndevelopers) is that people become more engaged if out-of-domain requests are\\naddressed. Works like the ones described by Bickmore and Cassell (2000) and\\nPatel et al. (2006) validate this, as well as the fact that, in January 2013, Apple\\nwas asking for writers for Siri40. As it is impossible to prepare answers to\\nall the possible out-of-domain requests, and the majority of the conversational\\nagent\\xe2\\x80\\x99s developers cannot a\\xef\\xac\\x80ord to recruit writers, a solutions is to try to take\\nadvantage of those human dialogues that can be found in the web. An example\\nof a work that follows this approach is, again, the butler Edgar Smith.\\nIn\\nthe work reported by Ameixa et al. (2014), the previous mentioned Filipe\\xe2\\x80\\x99s\\ncorpus (Subtle), was used to answer out-of-domain requests posed to Edgar.\\nReported results say that 72% of the out-of-domain requests asked to Edgar\\nare now answered, and, from these, about 65% are considered to be appropriate\\nanswers.\\n\\nNevertheless, all these authors mention plenty of room for improvements.\\nMoreover, the previously mentioned corpora, made available by the chatbot\\xe2\\x80\\x99s\\ncommunity, were not properly explored yet. Thus, some research questions\\nremain to be answered:\\n\\n\\xe2\\x80\\xa2 How to \\xef\\xac\\x81lter these corpora in order to eliminate unwanted answers?\\n\\n\\xe2\\x80\\xa2 Which techniques should be used to detect paraphrases in these corpora,\\nas well as other semantic relations between requests and answers, in order\\nto organise such data?\\n\\n\\xe2\\x80\\xa2 How can the appropriate answer be chosen from the set of all possible an-\\nswers available in the corpus, in order to allow some level of automatic\\ncustomisation of the targeted agent?\\n\\n\\xe2\\x80\\xa2 How to guarantee that a pre-de\\xef\\xac\\x81ned answer makes sense in the context\\n\\nof a speci\\xef\\xac\\x81c dialogue?\\n\\nWe foresee these as interesting research challenges for the next years.\\n\\n37https://www.apple.com/ios/siri/\\n38http://www.windowsphone.com/en-us/how-to/wp8/cortana/\\n39http://www.google.com/landing/now/\\n40http://www.technologyreview.com/view/509961/apple-looks-to-improve-siris-\\n\\nscript/\\n\\n17\\n\\n\\x0c6 Conclusions and Future Challenges\\n\\nThe number of chatbots that can be found in the web increases every day. Al-\\nthough the majority of their developers do not have scienti\\xef\\xac\\x81c aspirations, the\\nfact is that, besides tools and corpora, the chatbots\\xe2\\x80\\x99 community has important\\nknow-how, which should not be neglected by researchers targeting advances in\\nhuman-computer communication. Therefore, in this paper we presented a brief\\nhistorical overview of chatbots, and described main resources and ideas. Fur-\\nthermore, we highlighted some chatbots, which have distinguished themselves\\nby introducing new paradigms and/or for being Loebner prize winners. How-\\never, it should be clear that these are only the tip of the iceberg of the panoply\\nof chatbots that currently exist.\\n\\nWe have seen that AIML and, more recently, Chatscript are widely used\\nlanguages that allow to code the chatbots\\xe2\\x80\\x99 knowledge sources, and that although\\nsome chatbots implement learning strategies, scripting is still at their core. We\\nhave also seen that a personality capable of justifying some of the chatbot\\xe2\\x80\\x99s\\nanswers, the capacity of directing a conversation and producing small talk, and\\nthe idea of failing like a human are some of the chatbots\\xe2\\x80\\x99 features that give the\\nillusion of intelligence.\\n\\nWe have also grasped that to create a chatbot, one \\xe2\\x80\\x9conly\\xe2\\x80\\x9d needs to think\\nabout a character, and enrich its knowledge bases with possible interactions.\\nEven better, that work does not need to be done from scratch as many plat-\\nforms already provide pre-de\\xef\\xac\\x81ned interactions, which can be adapted according\\nto the chatbot character. And this is the main richness of the chatbot\\xe2\\x80\\x99s com-\\nmunity: the immense amount of collected interactions, where the majority of\\nthem represent real human requests.\\n\\nA major future challenge is to be able to automatically use all this informa-\\ntion to build a credible chatbot. How to avoid contradictory answers? How to\\nchoose appropriated answers considering a chatbot\\xe2\\x80\\x99s character? And if we move\\nto other sources of dialogues, like the ones from books, theatre plays or movies\\nsubtitles, will we be able, one day, to integrate all that information simulating\\nreal human dialogues?\\n\\nReferences\\n\\nAmeixa, D., Coheur, L., Fialho, P., and Quaresma, P. (2014). Luke, I am Your\\nFather: Dealing with Out-of-Domain Requests by Using Movies Subtitles.\\nIn Intelligent Virtual Agents - 14th International Conference, IVA 2014,\\nBoston, MA, USA, August 27-29, 2014. Proceedings, pages 13\\xe2\\x80\\x9321. Springer.\\n\\nAngeli, A. D. and Brahnam, S. (2008). I hate you! Disinhibition with virtual\\n\\npartners. Interacting with Computers, 20:302\\xe2\\x80\\x93310.\\n\\nBanchs, R. E. (2012). Movie-DiC: a Movie Dialogue Corpus for Research and\\nDevelopment. In Proceedings of the 50th Annual Meeting of the Association\\n\\n18\\n\\n\\x0cfor Computational Linguistics (Volume 2: Short Papers), pages 203\\xe2\\x80\\x93207.\\nACL.\\n\\nBanchs, R. E. and Li, H. (2012). IRIS: a Chat-oriented Dialogue System based\\non the Vector Space Model. In ACL (System Demonstrations), pages 37\\xe2\\x80\\x9342.\\n\\nBatacharia, B., Levy, D., Catizone, R., Krotov, A., and Wilks, Y. (1999). Con-\\nverse: a conversational companion. In Wilks, Y., editor, Machine Conver-\\nsations, volume 511 of The Springer International Series in Engineering\\nand Computer Science, pages 205\\xe2\\x80\\x93215. Springer US.\\n\\nBernsen, N. O. and Dybkj\\xc3\\xa6r, L. (2005). Meet Hans Christian Anderson. In Pro-\\nceedings of the Sixth SIGdial Workshop on Discourse and Dialogue, pages\\n237\\xe2\\x80\\x93241.\\n\\nBickmore, T. and Cassell, J. (1999). Small talk and conversational storytelling\\nin embodied conversational interface agents. In Proceedings of the AAAI\\n1999 Fall Symposium on Narrative Intelligence, pages 87\\xe2\\x80\\x9392. AAAI Press.\\n\\nBickmore, T. and Cassell, J. (2000). How about this Weather? Social Dialogue\\nwith Embodied Conversational Agents. In Socially Intelligent Agents: The\\nHuman in the Loop, pages 4\\xe2\\x80\\x938. AAAI Press.\\n\\nCaputo, L., Garner, R., and Nathan, P. X. (1997). FRED, Milton and Barry:\\nthe evolution of intelligent agents for the Web. In Morabito, F. C., editor,\\nAdvances in Intelligent Systems, pages 400\\xe2\\x80\\x93407. IOS Press.\\n\\nCarpenter, R. and Freeman, J. (2005). Computing Machinery and the Individ-\\n\\nual: the Personal Turing Test. http://www.jabberwacky.com.\\n\\nCho, A. K. (2007). Emotional and Domain Concept Enhancements to Alicebot.\\n\\nMaster\\xe2\\x80\\x99s thesis, San Jose State University.\\n\\nColby, K. M. (1974). Ten criticisms of PARRY. SIGART Newsletter, pages 5\\xe2\\x80\\x939.\\n\\nDaden Limited (2010). Deploying chatbots to customer advantage. White Paper\\n\\n(http://issuu.com/dadenlimited/docs/whitepaper4/1).\\n\\nEndrass, B., Rehm, M., and Andr\\xc2\\xb4e, E. (2011). Planning Small Talk behav-\\nior with cultural in\\xef\\xac\\x82uences for multiagent systems. Computer Speech &\\nLanguage, 25(2):158\\xe2\\x80\\x93174.\\n\\nEpstein, R. (1992). The Quest for the Thinking Computer. AI Magazine, pages\\n\\n81\\xe2\\x80\\x9395.\\n\\nEpstein, R. (2007). From Russia, with Love. How I got fooled (and somewhat\\n\\nhumiliated) by a computer. Scienti\\xef\\xac\\x81c American Mind.\\n\\nFahlman, S. E. (2011). Using Scone\\xe2\\x80\\x99s Multiple-Context Mechanism to Emulate\\nHuman-Like Reasoning. In Advances in Cognitive Systems: Papers from\\nthe 2011 AAAI Fall Symposium, pages 98\\xe2\\x80\\x93105. AAAI Press.\\n\\n19\\n\\n\\x0cFellbaum, C., editor (1998). WordNet: An Electronic Lexical Database. MIT\\n\\nPress.\\n\\nFialho, P., Coheur, L., Curto, S., Cl\\xc2\\xb4audio, P., \\xcb\\x86Angela Costa, Abad, A., Meinedo,\\nH., and Trancoso, I. (2013). Meet Edgar, a tutoring agent at Monserrate.\\nIn Proceedings of the 51st Annual Meeting of the ACL: System Demonstra-\\ntions, pages 61\\xe2\\x80\\x9366.\\n\\nGalv\\xcb\\x9cao, A. M., Barros, F. A., Neves, A. M. M., and Ramalho, G. L. (2004).\\nPersona-AIML: An Architecture Developing Chatterbots with Personality.\\nIn Proceedings of the Third International Joint Conference on Autonomous\\nAgents and Multiagent Systems - Volume 3, AAMAS \\xe2\\x80\\x9904, pages 1266\\xe2\\x80\\x931267,\\nWashington, DC, USA. IEEE Computer Society.\\n\\nGebhard, P., Mehlmann, G., and Kipp, M. (2011). Visual SceneMaker-a tool\\nfor authoring interactive virtual characters. Journal on Multimodal User\\nInterfaces, 6:3\\xe2\\x80\\x9311.\\n\\nGrinberg, D., La\\xef\\xac\\x80erty, J. D., and Sleator, D. D. (1995). A Robust Parsing\\nAlgorithm For Link Grammars. In Proceedings of the Fourth International\\nWorkshop on Parsing Technologies, pages 111\\xe2\\x80\\x93125.\\n\\nHartholt, A., Traum, D., Marsella, S., Shapiro, A., Stratou, G., Leuski, A.,\\nMorency, L.-P., and Gratch, J. (2013). All together now. In Aylett, R.,\\nKrenn, B., Pelachaud, C., and Shimodaira, H., editors, Intelligent Virtual\\nAgents, volume 8108 of Lecture Notes in Computer Science, pages 368\\xe2\\x80\\x93381.\\nSpringer Berlin Heidelberg.\\n\\nHeller, B., Proctor, M., Mah, D., Jewell, L., and Cheung, B. (2005). Freud-\\nbot: An Investigation of Chatbot Technology in Distance Education. In\\nKommers, P. and Richards, G., editors, Proceedings of World Conference\\non Educational Multimedia, Hypermedia and Telecommunications, pages\\n3913\\xe2\\x80\\x933918. AACE.\\n\\nHutchens, J. L. (1997). How to Pass the Turing Test by Cheating. Technical\\n\\nreport, University of Western Australia.\\n\\nKazi, H., Chowdhry, B. S., and Memon, Z. (2012). MedChatBot: An UMLS\\nbased Chatbot for Medical Students. International Journal of Computer\\nApplications, 55(17):1\\xe2\\x80\\x935.\\n\\nKuipers, B., McCarthy, J., and Weizenbaum, J. (1976). Computer power and\\n\\nhuman reason. SIGART Bull., pages 4\\xe2\\x80\\x9313.\\n\\nLevesque, H., Davis, E., and Morgenstern, L. (2012). The Winograd Schema\\nChallenge. In Proceedings of the Thirteenth International Conference on\\nPrinciples of Knowledge Representation and Reasoning, pages 552\\xe2\\x80\\x93561.\\nAAAI.\\n\\nLevesque, H. J. (2014). On our best behaviour. Arti\\xef\\xac\\x81cial Intelligence, 212:27\\xe2\\x80\\x9335.\\n\\n20\\n\\n\\x0cMagarreiro, D., Coheur, L., and Melo, F. S. (2014). Using subtitles to deal\\nwith Out-of-Domain interactions. In Proceedings of 18th Workshop on the\\nSemantics and Pragmatics of Dialogue (SemDial), pages 98\\xe2\\x80\\x93106.\\n\\nMalinowski, B. (1923). The Meaning of Meaning, chapter The Problem of Mean-\\n\\ning in Primitive Socities, page 38. Harcourt Brace Jovanovich, Inc.\\n\\nMauldin, M. L. (1994). ChatterBots, TinyMuds, and the Turing test: entering\\nthe Loebner Prize competition. In Proceedings of the 12th National Con-\\nference on Arti\\xef\\xac\\x81cial Intelligence (vol. 1), AAAI \\xe2\\x80\\x9994, pages 16\\xe2\\x80\\x9321. AAAI\\nPress.\\n\\nMikic, F. A., Burguillo, J. C., Llamas, M., Rodriguez, D. A., and Rodriguez,\\nE. (2009). CHARLIE: An AIML-based Chatterbot which Works as an\\nInterface among INES and Humans. In 2009 EAEEIE Annual Conference,\\npages 1\\xe2\\x80\\x936. IEEE.\\n\\nPatel, R., Leuski, A., and Traum, D. (2006). Dealing with out of domain ques-\\ntions in virtual characters. In Gratch, J., Young, M., Aylett, R., Ballin, D.,\\nand Olivier, P., editors, Intelligent Virtual Agents, volume 4133 of Lecture\\nNotes in Computer Science, pages 121\\xe2\\x80\\x93131. Springer Berlin Heidelberg.\\n\\nPowers, D. M. W. (1998). The total Turing test and the Loebner prize.\\n\\nIn\\nProceedings of the Joint Conferences on New Methods in Language Pro-\\ncessing and Computational Natural Language Learning, NeMLaP3/CoNLL\\n\\xe2\\x80\\x9998, pages 279\\xe2\\x80\\x93280. ACL.\\n\\nSaygin, A. P., Cicekli, I., and Akman, V. (2000). Turing test: 50 years later.\\n\\nMinds and Machines, 10:2000.\\n\\nSchneider, K. (1988). Small Talk: Analyzing Phatic Discourse. Sprachwis-\\n\\nsenschaftliche Reihe. Hitzeroth.\\n\\nSchumaker, R. P., Ginsburg, M., Chen, H., and Liu, Y. (2007). An evaluation of\\nthe chat and knowledge delivery components of a low-level dialog system:\\nThe AZ-ALICE experiment. Decision Support Systems, 42(4):2236\\xe2\\x80\\x932246.\\n\\nShah, H. (2006). A.L.I.C.E.: an ACE in Digitaland. tripleC, 4(2):284\\xe2\\x80\\x93292.\\n\\nShawar, B. A. (2005). A Corpus Based Approach to Generalising a Chatbot\\n\\nSystem. PhD thesis, University of Leeds.\\n\\nShawar, B. A. and Atwell, E. (2003). Using dialogue corpora to train a chatbot.\\nIn Proceedings of CL2003: International Conference on Corpus Linguistics,\\npages 681\\xe2\\x80\\x93690. Lancaster University.\\n\\nSusan Robinson, David Traum, M. I. and Henderer, J. (2008). What would\\nyou Ask a conversational Agent? Observations of Human-Agent Dialogues\\nin a Museum Setting.\\nIn Proceedings of the Sixth International Confer-\\nence on Language Resources and Evaluation (LREC\\xe2\\x80\\x9908), pages 1125\\xe2\\x80\\x931131.\\nEuropean Language Resources Association (ELRA).\\n\\n21\\n\\n\\x0cTraum, D., Aggarwal, P., Artstein, R., Foutz, S., Gerten, J., Katsamanis, A.,\\nLeuski, A., Noren, D., and Swartout, W. (2012). Ada and Grace: Direct\\nInteraction with Museum Visitors.\\nIn Nakano, Y., Ne\\xef\\xac\\x80, M., Paiva, A.,\\nand Walker, M., editors, Intelligent Virtual Agents, volume 7502 of Lecture\\nNotes in Computer Science, pages 245\\xe2\\x80\\x93251. Springer Berlin Heidelberg.\\n\\nTuring, A. M. (1950). Computing Machinery and Intelligence. Mind, 59:433\\xe2\\x80\\x93\\n\\n460.\\n\\nWallace, R., Tomabechi, H., and Aimless, D. (2007). Chatterbots Go Native:\\nConsiderations for an eco-system fostering the development of arti\\xef\\xac\\x81cial life\\nforms in a human world. http://www.pandorabots.com/pandora/pics/\\nchatterbotsgonative.doc.\\n\\nWeizenbaum, J. (1966). ELIZA \\xe2\\x80\\x93 a computer program for the study of natural\\nlanguage communication between man and machine. Communications of\\nthe ACM, 9:36\\xe2\\x80\\x9345.\\n\\n22\\n\\n\\x0c',\n",
       " 'Detecting Context Dependent Messages in a Conversational Environment\\n\\nChaozhuo Li\\xe2\\x80\\xa0, Yu Wu\\xe2\\x80\\xa0, Wei Wu\\xe2\\x80\\xa1, Chen Xing\\xe2\\x8b\\x84, Zhoujun Li\\xe2\\x80\\xa0, Ming Zhou\\xe2\\x80\\xa1\\n\\n\\xe2\\x80\\xa0State Key Lab of Software Development Environment, Beihang University, Beijing, China\\n\\n\\xe2\\x80\\xa1 Microsoft Research, Beijing, China\\n\\xe2\\x8b\\x84 Nankai University, Tianjin, China\\n\\n{lichaozhuo,wuyu,lizj}@buaa.edu.cn {wuwei,v-chxing,mingzhou}@microsoft.com\\n\\nAbstract\\n\\nWhile automatic response generation for building chatbot systems has drawn a lot of attention\\nrecently, there is limited understanding on when we need to consider the linguistic context of\\nan input text in the generation process. The task is challenging, as messages in a conversational\\nenvironment are short and informal, and evidence that can indicate a message is context depen-\\ndent is scarce. After a study of social conversation data crawled from the web, we observed that\\nsome characteristics estimated from the responses of messages are discriminative for identifying\\ncontext dependent messages. With the characteristics as weak supervision, we propose using a\\nLong Short Term Memory (LSTM) network to learn a classi\\xef\\xac\\x81er. Our method carries out text\\nrepresentation and classi\\xef\\xac\\x81er learning in a uni\\xef\\xac\\x81ed framework. Experimental results show that the\\nproposed method can signi\\xef\\xac\\x81cantly outperform baseline methods on accuracy of classi\\xef\\xac\\x81cation.\\n\\n1 Introduction\\n\\nTogether with the rapid growth of social media such as Twitter and Weibo, the amount of conversation\\ndata on the web has tremendously increased. This makes building open domain chatbot systems with\\ndata-driven approaches possible. To carry on reasonable conversations with humans, a chatbot system\\nneeds to generate proper response with regard to users\\xe2\\x80\\x99 messages. Recently, with the large amount\\nof conversation data available, learning a response generator from data has drawn a lot of attention\\n(Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).\\n\\nA key step to coherent response generation is determining when to consider linguistic context of mes-\\nsages. Existing work on response generation, however, has overlooked this step. They either totally\\nignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply con-\\nsiders context for every message (Sordoni et al., 2015b; Serban et al., 2015). The former case is easy to\\nlead to irrelevant responses when users\\xe2\\x80\\x99 input messages rely on the context information in previous con-\\nversation turns, while the latter case is costly (e.g., on memory and responding time) for building a real\\nchatbot system and has the risk of bringing in noise to response generation especially when users want to\\nend the current conversation topic and start a new one. According to our observation, there are two types\\nof messages in a conversational environment. The \\xef\\xac\\x81rst type is context dependent message, which means\\nto reply to the message, one must consider previous utterances in the dialogue1, while the second type is\\ncontext independent message, which means even without the previous utterances, the message itself can\\nstill lead to a reasonable response. Table 1 compares the two types of messages using examples. In Case\\n1, \\xe2\\x80\\x9cwhy do you think so\\xe2\\x80\\x9d is a context dependent message. In order to reply to the message, one cannot\\nignore its linguistic context \\xe2\\x80\\x9cI think it will rain tomorrow\\xe2\\x80\\x9d. On the other hand, in Case 2, \\xe2\\x80\\x9cWell, what\\ntime is it now\\xe2\\x80\\x9d is a context independent message, as one can give a reasonable response without looking\\nat the previous turns. Distinguishing context dependent messages from context independent messages\\nis important for building a good response generator. Missing linguistic context for context dependent\\n\\n6\\n1\\n0\\n2\\n\\n \\n\\nv\\no\\nN\\n3\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n2\\nv\\n3\\n8\\n4\\n0\\n0\\n\\n.\\n\\n1\\n1\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence.\\n\\nLicence details:\\n\\nhttp://creativecommons.org/licenses/by/4.0/.\\n\\n1Broadly speaking, context may not be limited to linguistic context. For example, a user\\xe2\\x80\\x99s interest could also be a kind of\\n\\ncontext. As the \\xef\\xac\\x81rst step, in this work, we only focus on \\xe2\\x80\\x9clinguistic context\\xe2\\x80\\x9d.\\n\\n\\x0cTable 1: Two types of messages\\n\\n: What will the weather be like tomorrow? User\\n\\n: What are you doing?\\n\\nCase 1 : a context dependent message\\nUser\\nChatbot : I think it will rain tomorrow.\\nUser\\n\\n: Why do you think so?\\n\\nCase 2 : a context independent message\\n\\nChatbot : I am waiting for you to watch NBA.\\nUser\\n\\n: Well, what time is it now?\\n\\nmessages will lead to nonsense response. For example, \\xe2\\x80\\x9cbecause I love you\\xe2\\x80\\x9d could also be a response\\nfor the message \\xe2\\x80\\x9cwhy do you think so\\xe2\\x80\\x9d if we only look at the message itself, but it is nonsense appearing\\nin the dialogue of Case 1. Incorporating context information into context independent messages will\\nincrease the workload of a generation system and has the risk of bringing in noise to the generation pro-\\ncess. For example, if we consider the context \\xe2\\x80\\x9cNBA\\xe2\\x80\\x9d for the message \\xe2\\x80\\x9cWell, what time is it now\\xe2\\x80\\x9d, the\\nchatbot will probably say something about \\xe2\\x80\\x9cNBA\\xe2\\x80\\x9d rather than answer the question with a time answer.\\nAlthough detecting context dependent messages is crucial for building chatbot systems, there is limited\\nunderstanding about it.\\n\\nIn this paper, we study this important but less explored problem. Instead of answering how to incor-\\nporate context information, we try to understand when we need the information. Therefore, our effort\\nis complementary to the existing work on response generation. It can keep the existing generation algo-\\nrithms context-aware and improve their ef\\xef\\xac\\x81ciency and robustness to noise. The task is challenging, as\\nmessages in a conversational environment are usually short and informal, and evidence that can indicate\\na message is context dependent is scarce. For example, on 3 million post-response pairs crawled from\\nWeibo, the average length of messages is 4.65. On such short texts, classic NLP tools such as POS Tagger\\nand Parser suffer from bad performance (Derczynski et al., 2013; Foster et al., 2011) and it is dif\\xef\\xac\\x81cult to\\nexplicitly extract features that are discriminative on the two types of messages. More seriously, there are\\nno large scale annotations available for building a supervised learning procedure.\\n\\nWe consider leveraging the large amount of human-human conversation data available on the web\\nto learn a message classi\\xef\\xac\\x81er. Our intuition is that a context dependent message has different linguistic\\ncontext in different conversation sessions, therefore its responses could be more diverse on content than\\nresponses of a context independent message. To verify this idea, we study the distributions of responses\\nof messages using conversation data crawled from social media and \\xef\\xac\\x81nd that the length distribution of\\nresponses and the word distribution of responses are quite discriminative on the two types of messages.\\nBased on this observation, for each message in the crawled data, we estimate the average length of\\nresponses, the entropy of the word distribution of responses, and the maximum mass of the word distri-\\nbution of responses, and take these characteristics as weak supervision signals to learn a classi\\xef\\xac\\x81er. The\\nclassi\\xef\\xac\\x81er takes a message as input and can make prediction for any messages in a real conversation en-\\nvironment, even though the messages do not appear in the crawled data and characteristics like entropy\\nare not available for them. We propose using a Long Short Term Memory (LSTM) architecture to learn\\nthe classi\\xef\\xac\\x81er. Our model represents message texts in a continuous vector space using a one-layer LSTM\\nnetwork. The text vectors are then provided as input to a two-layer feed-forward neural network to per-\\nform classi\\xef\\xac\\x81cation. The neural network architecture carries out feature learning and model learning in\\na uni\\xef\\xac\\x81ed framework, and thus can avoid explicit feature extraction which is dif\\xef\\xac\\x81cult on short conversa-\\ntional messages. Our method leverages large scale weak supervision signals extracted from responses in\\nsocial conversation data and can reach a satisfactory accuracy with only a few human annotations.\\n\\nWe conduct experiments on large scale English and Chinese conversation data mined from Twitter\\nand Weibo respectively, and test the performance of our method on thousands of messages annotated\\nby human labelers. Experimental results show that our method can signi\\xef\\xac\\x81cantly outperform baseline\\nmethods on accuracy of message classi\\xef\\xac\\x81cation on both of the two data sets.\\n\\nWe make the following contributions in this paper: 1) proposal of detecting context dependent mes-\\nsages in a conversational environment; 2) proposal of learning weak supervision signals from responses\\nof messages using large scale conversation data; 3) proposal of using an LSTM architecture to learn a\\nmessage classi\\xef\\xac\\x81er; 4) empirical veri\\xef\\xac\\x81cation of the proposed method on human annotated data.\\n\\n\\x0c2 Related Work\\n\\nOur work lies in the path of building chatbot systems with data-driven approaches. Differing from\\ntraditional dialogue systems (cf., (Young et al., 2013)) which rely on hand-crafted features and rules\\nto generate reply sentences for speci\\xef\\xac\\x81c applications such as voice dialling (Williams, 2008) and ap-\\npointment scheduling (Janarthanam et al., 2011) etc., recent effort focuses on exploiting an end-to-\\nend approach to learn a response generator from social conversation data for open domain dialogue\\n(Koshinda et al., 2015; Higashinaka et al., 2016). For example, Ritter et al. (Ritter et al., 2011) em-\\nployed a phrase-based machine translation model for response generation.\\nIn (Shang et al., 2015;\\nVinyals and Le, 2015), neural network architectures were proposed to learning response generators from\\none-round conversation data. Based on these work, Sordoni et al. (Sordoni et al., 2015b) incorporated\\nlinguistic context into the learning of response generator. Serban et al. (Serban et al., 2015) proposed a\\nhierarchical neural network architecture to building context-aware response generation. In this paper, in-\\nstead of studying how to incorporate context into response generation, we consider the problem that when\\nwe need context in the process. Our work can keep the existing generation algorithms context-aware and\\nat the same time improve their ef\\xef\\xac\\x81ciency and robustness.\\n\\nWe employ a Recurrent Neural Network (RNN) architecture to learn a message classi\\xef\\xac\\x81er. RNN\\nmodels (Elman, 1990), due to their capability of modeling sequences with arbitrary length, have been\\nwidely used in many natural language processing tasks such as language modeling (Mikolov et al., 2010)\\nand tagging (Xu et al., 2015) etc. Recently, it is reported that Long Short Term Memory (LSTM)\\n(Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as two special\\nRNN models which can capture long term dependencies in sequences outperform state of the art methods\\non tasks like machine translation (Sutskever et al., 2014) and response generation (Shang et al., 2015). In\\nthis paper, we apply the LSTM architecture to the task of context dependent message detection. We ap-\\npend LSTM with a two-layer feed-forward neural network, thus feature learning and model learning can\\nbe carried out simultaneously.\\n\\nOur work belongs to the scope of short text classi\\xef\\xac\\x81cation (Song et al., 2014). Existing applica-\\ntions of short text classi\\xef\\xac\\x81cation include query classi\\xef\\xac\\x81cation (Kang and Kim, 2003), tweet classi\\xef\\xac\\x81cation\\n(Sriram et al., 2010), and question classi\\xef\\xac\\x81cation (Zhang and Lee, 2003). We study a new problem in\\nshort text classi\\xef\\xac\\x81cation: distinguishing context dependent messages from context independent messages\\nin a conversational environment. The task is important for building open domain chatbot systems and has\\nits unique challenges (e.g., new data structure). We tackle the challenges by leveraging the responses of\\nmessages and utilizing an LSTM network to conduct feature learning and model learning simultaneously.\\n\\n3 Learning to Detect Context Dependent Messages\\n\\nSuppose that we have a data set D = {(mi, yi)}N\\ni=1 where mi is a message composed of a sequence of\\nwords (wmi,1, . . . , wmi,ni) and yi is an indicator whose value re\\xef\\xac\\x82ects whether mi is context dependent\\nor not. Our goal is to learn a function g(\\xc2\\xb7) \\xe2\\x88\\x88 {\\xe2\\x88\\x921, 1} using D, thus for any new message m, g(\\xc2\\xb7) predicts\\nm a context dependent message if g(m) = 1. To this end, we need to answer two questions: 1) how to\\nconstruct D; 2) how to perform learning using D.\\n\\nFor the \\xef\\xac\\x81rst question, we can crawl conversation data from social media like Twitter and ask human\\nlabelers to annotate the messages in the data. The problem is that human annotation is expensive and time\\nconsuming and therefore we cannot obtain a large scale data set for learning. To solve the problem, we\\nautomatically learn some weak supervision signals using responses of messages in social conversation\\ndata, and take the signals as {yi} in D. For the second question, one straightforward way is \\xef\\xac\\x81rst extract-\\ning shallow features such as bag-of-words and syntax from messages and then employing off-the-shelf\\nmachine learning tools to learn a model. The problem is that shallow features are not effective enough\\non representing semantics in short conversation messages, which will be seen in our experiments. We\\npropose using a Long Short Term Memory (LSTM) architecture to learn a model from D. The advantage\\nof our approach is that it can avoid explicit feature extraction and large scale human annotations, and\\ncarry out feature learning and model learning in a uni\\xef\\xac\\x81ed framework.\\n\\n\\x0c3.1 Learning Weak Supervision Using Responses\\nInstead of requiring human annotations, we consider creating signals that are discriminative on the two\\ntypes of messages from large scale social conversation data available on the web. Our intuition is that a\\ncontext dependent message has different linguistic context in different conversation sessions, therefore,\\nits responses could be more diverse on content than responses of a context independent message (one\\nmessage may appear multiple times, and therefore it may correspond to multiple responses). Table\\n2 illustrates our idea with some examples from Twitter. The last column of the table represents the\\nfrequency of the message or the frequency of the response under the message. For each message, we\\nshow the top 5 most frequent responses. From the examples, we can see that a context dependent message\\ntends to have divergent and uniformly distributed responses corresponding to different linguistic context,\\nwhile the responses of a context independent message share relatively similar content and some content\\ndominates the distribution.\\n\\nTable 2: Responses of the two types of messages\\n\\nContext dependent message : why\\nResponse 1 : I am kidding\\nResponse 2 : He can be like mcdaniels for sixer\\nResponse 3 : Because I say no\\nResponse 4 : I am tired\\nResponse 5 : U will become dependent on them\\n\\n2196\\n\\n7\\n5\\n5\\n5\\n5\\n\\nContext independent message : Good night\\nResponse 1 : Good night\\nResponse 2 : Goodnight\\nResponse 3 : Night\\nResponse 4 : Sleep well\\nResponse 5 : Thank you\\n\\n644\\n47\\n44\\n23\\n10\\n9\\n\\nThe examples inspire us to investigate some statistical characteristics that can re\\xef\\xac\\x82ect the diversity of\\nresponses. These characteristics could be good indicators of context dependent messages, and we can\\nconstruct {yi} in D using the characteristics. We estimate the following statistical characteristics for each\\nmessage using its responses, and examine how the characteristics are discriminative on the two types of\\nmessages using 1000 labeled messages from Twitter and Weibo respectively. The details of the labeled\\ndata will be described in our experiments.\\n\\nEntropy: the \\xef\\xac\\x81rst characteristic we investigate is the entropy of the word distribution of responses,\\nwhich is a common measure for diversity. Given a word distribution P = (p1, p2, . . . , pn), the entropy\\nof the distribution is de\\xef\\xac\\x81ned as\\n\\nn\\n\\nE(P ) =\\n\\nXi=1\\n\\n\\xe2\\x88\\x92pi log2(pi).\\n\\n(1)\\n\\nThe maximum of the entropy is log2(n) which is reached when the distribution is uniform. Then, a\\nlarge entropy means a word distribution covers many words (i.e., n is big) and is close to a uniform\\ndistribution. Therefore, a context dependent message should have a larger entropy on responses than\\na context independent message (see the comparison in Table 2). We normalize the entropy to [0, 1] by\\nE(P )\\xe2\\x88\\x92min(E)\\nmax(E)\\xe2\\x88\\x92min(E), where max(E) and min(E) represent the maximum entropy and the minimum entropy\\nin the data set. Figure 1(a) shows the comparison of the two types of messages on normalized entropy\\nusing the Twitter labeled data. In the \\xef\\xac\\x81gure, each value on the x-axis represents an interval with a \\xef\\xac\\x81xed\\nlength 0.05. For example, 0.50 means an interval [0.5, 0.55). Each value on the y-axis represents the per-\\ncentage of messages in a speci\\xef\\xac\\x81c interval. For example, among messages falling in the interval [0.95, 1),\\nnearly 80% are labeled as context dependent and only about 20% are labeled as context independent.\\nFrom the \\xef\\xac\\x81gure, we can see that entropy is discriminative on the two types of messages: context de-\\npendent messages distributes on large entropy areas, while context independent messages tend to have\\nsmaller entropy.\\n\\nM(P): in addition to entropy, another characteristic that might re\\xef\\xac\\x82ect the diversity of responses could\\nbe the maximum mass of the word distribution of responses, as in diverse responses, words should be\\nuniformly distributed (stopwords are removed), while in less diverse responses, there may exit dominant\\nwords (e.g., \\xe2\\x80\\x9cnight\\xe2\\x80\\x9d in Table 2). Given a word distribution P = (p1, p2, . . . , pn), we de\\xef\\xac\\x81ne a character-\\nistic as\\n\\nM (P ) = 1 \\xe2\\x88\\x92 max\\n16i6n\\n\\npi\\n\\n(2)\\n\\nFigure 1(b) compares the two types of messages on M (P ) using the Twitter labeled data, in which values\\n\\n\\x0ce\\ng\\na\\nt\\nn\\ne\\nc\\nr\\ne\\nP\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\nContext Dependent\\nContext Independent\\n\\nContext Dependent\\nContext Independent\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\ne\\ng\\na\\nt\\nn\\ne\\nc\\nr\\ne\\nP\\n\\n0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95\\n\\nEntropy (Normalized)\\n\\n0.0\\n\\n0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75\\n\\nM(p)\\n\\n(a) Comparison on entropy\\n\\n(b) Comparison on M (P )\\n\\ne\\ng\\na\\nt\\nn\\ne\\nc\\nr\\ne\\nP\\n\\n1.0\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0.0\\n\\nContext Dependent\\nContext Independent\\n\\n0\\n\\n0.1\\nAverage Length (Normalized)\\n\\n0.2\\n\\n0.3\\n\\n(c) Comparison on average length of re-\\nsponses\\n\\nFigure 1: Comparison of the two types of messages on three characteristics.\\n\\non the x-axis and y-axis have the same meaning as those in Figure 1(a). From the \\xef\\xac\\x81gure, we can see that\\nsimilar to entropy, M (P ) is useful on distinguishing the two types of messages. Context dependent\\nmessages have larger M (P ) than context independent messages.\\n\\nAverage length of responses: \\xef\\xac\\x81nally, we consider the length distribution of responses. Since re-\\nsponses of context dependent messages are more diverse on content, they might be longer than responses\\nof context independent messages. We calculate the average length of responses for each message and\\nnormalize it to [0, 1] in the same way as entropy. Figure 1(c) compares the two types of messages on\\naverage length of responses using the Twitter labeled data, where values on the x-axis represent intervals\\nwith a length 0.1. The result supports our claim and clearly indicates that average length is discriminative\\non the two types of messages.\\n\\nWe combine the three characteristics using a linear SVM classi\\xef\\xac\\x81er learned with the 1000 labeled\\nmessages and take the output of the SVM (a real value) as {yi} in D. By this means, we can create\\na large scale training data set with only a little human labeling effort. Here, as a reference, we also\\nreport the classi\\xef\\xac\\x81cation accuracy of the three characteristics and the SVM classi\\xef\\xac\\x81er on the 1000 labeled\\ndata. Each characteristic corresponds to a threshold tuned on the 1000 labeled data with 5-fold cross\\nvalidation. If a value of a characteristic of a message is larger than the threshold, then the message will\\nbe predicted as context dependent. Table 3 shows the classi\\xef\\xac\\x81cation accuracy of 5-fold cross validation\\n(average of 5 results), where SVM (com) refers to the SVM classi\\xef\\xac\\x81er. Details of experiment setting will\\nbe described in Section 4. From Table 3, we can see that the numbers are consistent with Figure 1(a),\\n1(b), and 1(c).\\n\\nTable 3: Classi\\xef\\xac\\x81cation accuracy on 1000 labeled data\\n\\nEntropy\\nM (P )\\nAverage length of responses\\nSVM (com)\\n\\nWeibo\\nTwitter\\n72.6 % 70.5 %\\n72.6 % 69.8 %\\n72.8 % 68.5 %\\n73.8 % 71.2 %\\n\\n3.2 Model Learning\\nWe head for learning g(\\xc2\\xb7) using D constructed in Section 3.1. Note that g(\\xc2\\xb7) only takes a message m as\\ninput, and thus can make prediction for any messages in a real chatbot system even though the messages\\nare not in D and their entropy, M(P), and average length of responses are not available. Our idea is that\\nwe \\xef\\xac\\x81rst learn a regression model by \\xef\\xac\\x81tting {yi} in D through minimizing the sum of squared residuals\\nand then construct g(\\xc2\\xb7) by comparing the output of the regression model with a threshold. We can obtain\\nthe threshold by tuning it on a few labeled data (e.g., the 1000 labeled data). The key is how to learn the\\nregression model. We propose using a Recurrent Neural Network (RNN) architecture to embed messages\\ninto a continuous vector space and learning a regression model with the embedding of messages using a\\nfeed-forward neural network. The RNN model, which is capable of embedding sequences with arbitrary\\n\\n\\x0clength, can encode the order of words and the semantics of a message into a vector representation which\\nhas been recently proven effective on capturing similarity of short texts (Sordoni et al., 2015a). We take\\nthe output vector given by RNN as a feature representation of a message and feed it to a feed-forward\\nneural work. By this means, we can conduct feature learning and model learning in a uni\\xef\\xac\\x81ed framework\\nand jointly optimize the two components.\\n\\n\"!\\xe2\\x80\\x99(\\n(cid:179)\\n\\n%!\\xe2\\x80\\x99(\\n#!\\xe2\\x80\\x99(\\n(cid:179)  !\"# (cid:179)\\n\\n&!\\xe2\\x80\\x99(\\n\\n !\\xe2\\x80\\x99)\\n\\n$!\\xe2\\x80\\x99)\\n\\nxt-1\\n\\nht\\n\\n !\"#\\n\\n !\\n\\n$!\\n\\n\"!\\n(cid:179)\\n\\n&!\\n\\n%!\\n#!\\n(cid:179)  !\"# (cid:179)\\n\\nht-1\\n\\n !\"#\\n\\n !\\xe2\\x80\\x99(\\n\\n$!\\xe2\\x80\\x99(\\n\\nxt\\n\\n(cid:258)\\n\\n\"*\\n(cid:179)\\n\\n&*\\n\\n%*\\n#*\\n(cid:179)  !\"# (cid:179)\\n\\nxn\\n\\nhn\\n\\n !\"#\\n\\n(cid:258)\\n\\n $%\\n\\n \\n\\n(cid:258)\\n\\n\"\\n\\n$*\\n\\n+\\n\\nFigure 2: The architecture of our method\\n\\nGiven a message m which consists of n words, the RNN model reads the words one by one, and\\n\\nupdates a recurrent state ht for the t-th word wt by\\n\\nht = f (ht\\xe2\\x88\\x921, xt), h0 = 0,\\n\\n(3)\\n\\nwhere ht \\xe2\\x88\\x88 Rdh, xt \\xe2\\x88\\x88 Rdw is the vector representation of wt, and f is non-linear transformation. ht\\nacts as an encoding of the semantics of the word sequence up to position t, and the \\xef\\xac\\x81nal output hn\\nis a representation of message m. Both xt and ht are learned in the optimization of the RNN model.\\nWe select the Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as f , since it can\\nmodel long term dependencies in sequences with affordable complexity. LSTM controls the learning\\nof the representation of a sequence by gates. Speci\\xef\\xac\\x81cally, at position t, LSTM controls the information\\nthat should be kept from previous states by an input gate it, and the information that should be forgotten\\nby a forget gate ft. After memorizing and forgetting, the information is stored in a memory cell ct. ct\\ngenerates the recurrent state ht through an output gate ot. The speci\\xef\\xac\\x81c parameterization of LSTM is\\ngiven by\\n\\nit = \\xcf\\x83(W (i)xt + U (i)ht\\xe2\\x88\\x921 + b(i))\\nft = \\xcf\\x83(W (f )xt + U (f )ht\\xe2\\x88\\x921 + b(f ))\\not = \\xcf\\x83(W (o)xt + U (o)ht\\xe2\\x88\\x921 + b(o))\\nut = tanh(W (u)xt + U (u)ht\\xe2\\x88\\x921 + b(u))\\nct = it \\xe2\\x8a\\x97 ut + ft \\xe2\\x8a\\x97 c(t\\xe2\\x88\\x921)\\nht = ot \\xe2\\x8a\\x97 tanh(ct),\\n\\nwhere \\xcf\\x83(\\xc2\\xb7) is a sigmoid function and tanh(\\xc2\\xb7) is a hyperbolic tangent function. W (i), W (f ), W (o), W (u)\\n\\xe2\\x88\\x88 Rdh\\xc3\\x97dw , U (i), U (f ), U (o), U (u) \\xe2\\x88\\x88 Rdh\\xc3\\x97dh, and b(i), b(f ), b(o), b(u) \\xe2\\x88\\x88 Rdh\\xc3\\x971 are parameters. \\xe2\\x8a\\x97 means\\nelement-wise multiplication. After we get the \\xef\\xac\\x81nal state hn, we feed it to a two-layer feed-forward neural\\nnetwork to get an output s which is de\\xef\\xac\\x81ned by\\n\\ns = b2 + W2 (tanh(b1 + W1hn)) ,\\n\\n(4)\\n\\nwhere b1 \\xe2\\x88\\x88 Rds\\xc3\\x971, W1 \\xe2\\x88\\x88 Rds\\xc3\\x97dh, W2 \\xe2\\x88\\x88 R1\\xc3\\x97ds, and b2 \\xe2\\x88\\x88 R are parameters. Figure 2 illustrates the\\narchitecture of our method.\\n\\nFor each mi in D, we calculate an si using Equation (4) as an estimation of yi. We then learn the\\nparameters of the LSTM network and the feed-forward network by minimizing the sum of the squared\\nresiduals. Formally, our learning approach can be formulated as\\n\\narg min\\n\\ns\\n\\nN\\n\\nXi=1\\n\\n(yi \\xe2\\x88\\x92 si)2.\\n\\n(5)\\n\\n\\x0cAfter we obtain the parameters, we can calculate an sm for any message m using Equation (4). We then\\ntune a threshold T with a few labeled messages. The classi\\xef\\xac\\x81er g(\\xc2\\xb7) is given by\\n\\ng(m) = (cid:26) 1\\n\\n\\xe2\\x88\\x921\\n\\nif sm > T\\notherwise\\n\\n(6)\\n\\nThe gradients of the objective function (5) are computed using the back-propagation through\\ntime (BPTT) algorithm (Williams and Peng, 1990). We share the code for model\\nlearning at\\nhttps://github.com/whatsname1991/coling2016.\\n\\n4 Experiments\\n\\n4.1 Experiment Setup\\nWe constructed the conversation data for experiments from Weibo and Twitter. In each of the two social\\nmedia, two persons can communicate by replying to each other under a post. We crawled sequences\\nof reply with posts and extracted triples like \\xe2\\x80\\x9c(context, message, response)\\xe2\\x80\\x9d as experimental data. In a\\ntriple, \\xe2\\x80\\x9cmessage\\xe2\\x80\\x9d is a reply, \\xe2\\x80\\x9ccontext\\xe2\\x80\\x9d is the sentence in the previous turn of the message (a reply or a\\npost), and \\xe2\\x80\\x9cresponse\\xe2\\x80\\x9d is the sentence in the next turn (reply to the message). Note that in this work, we\\nrestrict the context of a message to a single sentence. This is a simpli\\xef\\xac\\x81cation of context in conversation.\\nIn real conversation, context could be more complicated and we leave the discussion of it as future work.\\nWe crawled 5.9 million English triples from Twitter, and 3.1 million Chinese triples from Weibo.\\nThe numbers of distinct messages in the Twitter data and in the Weibo data are 92, 755 and 112, 175\\nrespectively. On average, each Twitter message has 63.26 responses (some messages like \\xe2\\x80\\x9chello\\xe2\\x80\\x9d can\\nhave many different responses) and each Weibo message has 27.52 responses. The average word length\\nof Twitter message is 3.39 and the word average length of Weibo message is 4.65. English sentences\\nwere stemmed and stop words were removed, and Chinese sentences were segmented.\\n\\nWe constructed D = {(mi, yi)}N\\n\\ni=1 in Section 3.1 in the following way: we \\xef\\xac\\x81rst calculated entropy,\\nM (P ), and average length of responses for each message using the 5.9 million English triples and 3.1\\nmillion Chinese triples. Then from these data, we randomly sampled 1000 English triples and 1000\\nChinese triples as validation sets. For each triple in the validation data, we hid the response and recruited\\nhuman judges to label if the message is context dependent or not. Note that we hid responses when\\nlabeling messages because this is more close to the real case.\\nIn a real chatbot system, one has to\\ndetermine if a message is context dependent or not before generating a response. Each judge labeled\\na message with 1 if it is context dependent, otherwise the judge labeled the message with \\xe2\\x88\\x921. Each\\nmessage got three labels and the majority of the labels was taken as the \\xef\\xac\\x81nal decision for the message. In\\nthe Weibo data, there are 412 positive examples and 588 negative examples. In the Twitter data, the two\\nnumbers are 440 and 560, respectively. With the two validation data sets, we learned two SVM classi\\xef\\xac\\x81ers\\nin order to combine the three characteristics as described in Section 3.1. Parameters of SVMs were tuned\\nby 5-fold cross validation. Finally, we assigned a yi to each mi in the 112, 175 Twitter messages and\\n92, 755 Weibo messages by the output of the SVM classi\\xef\\xac\\x81ers, and formed D for both English data and\\nChinese data. We trained LSTM models using D.\\n\\nTo evaluate the performance of different models, we crawled another 3000 Chinese context-message\\npairs and 1000 English context-message pairs from Weibo and Twitter respectively, and followed the\\nsame way as the validation data to judge if the messages are context dependent or not. We used these\\ndata to simulate real context-message pairs in chatbot systems. In the Weibo data, there are 2715 unique\\nmessages and 1983 messages are not in D. The numbers of positive examples and negative examples are\\n1472 and 1528 respectively. In the Twitter data, the number of unique messages is 875 and 366 messages\\nare not included by D. The numbers of positive and negative examples are 464 and 536 respectively.\\nNote that for messages that are not included by D, their characteristics (i.e., entropy, M (P ), and average\\nlength of responses) are not available, and we can only use classi\\xef\\xac\\x81ers whose features are extracted from\\nmessages (like our LSTM models) to make prediction. This is close to a real situation in chatbots, and\\nwe took the two data sets as test sets.\\n\\nWe considered the following methods as baselines:\\n\\n\\x0cLength: intuitively, short messages tend to be context dependent (e.g., \\xe2\\x80\\x9cwhy\\xe2\\x80\\x9d in Table 2). Therefore,\\nwe employed length of a message as a baseline. A message shorter than a threshold will be predicted as\\na context dependent message.\\n\\nMDF: given a word, we estimated the number of messages that contain the word and named it \\xe2\\x80\\x9cdocu-\\nment frequency\\xe2\\x80\\x9d (DF). We constructed a list of words associated with DF using D. For a new message,\\nwe calculated the minimal DF of words in the message using the list. A context dependent message like\\n\\xe2\\x80\\x9cwhy do you think so\\xe2\\x80\\x9d may consist of common words, and thus correspond to a high minimal DF. We\\nconsidered minimal DF as a baseline. A message with a minimal DF larger than a threshold will be\\npredicted as a context dependent message.\\n\\nSVM (Length+MDF): we linearly combined Length and MDF by learning an SVM classi\\xef\\xac\\x81er on the\\n\\nvalidation data.\\n\\nSVM (classi\\xef\\xac\\x81cation):\\n\\nwe extracted unigrams,\\n\\nand frequencies of POS tags\\nand learned a linear SVM classi\\xef\\xac\\x81er on the validation data\\nas features\\nwith these features.\\nfor Chinese data were obtained using Stanford Parser\\n(http://nlp.stanford.edu/software/lex-parser.shtml) and POS tags for English\\ndata were obtained using TweetNLP (http://www.cs.cmu.edu/\\xcb\\x9cark/TweetNLP/).\\n\\nbigrams,\\n\\nfrom a message,\\n\\nPOS tags\\n\\nSVM (regression): instead of learning a classi\\xef\\xac\\x81er from annotations in the validation data, we \\xef\\xac\\x81tted\\n{yi} in D by learning an SVM regression model using the same features as SVM (classi\\xef\\xac\\x81cation) and\\nmade predictions on new messages by a threshold.\\n\\nAll SVM models were learned using SVM-Light (http://svmlight.joachims.org/). We\\n\\nemployed classi\\xef\\xac\\x81cation accuracy as an evaluation metric.\\n\\n4.2 Parameter Tuning\\nFor Length and MDF, the only parameter is a threshold. We tuned the thresholds on the validation\\ndata. For all SVM models, we selected the trade-off parameter in SVM from {0.01, 0.1, 1, 10, 100}\\nby 5-fold cross validation on the validation data. SVM (regression) also needs a threshold. We tuned\\nit on the validation data. The parameters of LSTM include the dimension of word vectors dw, the\\ndimension of hidden states dh, and the dimension of the \\xef\\xac\\x81rst layer of the feed-forward network ds. We\\nset dw = dh = 256, and ds = 100. Besides these parameters, we also set a dropout rate 0.1 in the\\nlearning of the feed-forward network as regularization.\\n\\nTable 4: Accuracy on two test sets\\n\\nLength\\nMDF\\nSVM (Length+MDF)\\nSVM (classi\\xef\\xac\\x81cation)\\nSVM (regression)\\nLSTM\\n\\nWeibo\\nTwitter\\n62.6 % 61.3 %\\n62.1 % 58.6 %\\n63.0 % 62.2 %\\n66.8 % 65.4 %\\n64.3 % 68.3 %\\n75.6 % 73.4 %\\n\\nTable 5: Comparison between LSTM, SVM (classi-\\n\\xef\\xac\\x81cation), and SVM (regression)\\nExample\\n\\ncontext : Have you heard\\nTaylor Swift\\xe2\\x80\\x99s new song?\\nmessage: Yep, I have heard\\nit on Saturday night.\\ncontext dependent\\ncontext independent\\ncontext independent\\ncontext dependent\\n\\nLabel\\nSVM (regression)\\nSVM (classi\\xef\\xac\\x81cation)\\nLSTM\\n\\n4.3 Quantitative Evaluation\\nTable 4 reports quantitative evaluation results on the test data. From the results, we can see that our\\nmethods outperform baseline methods. The improvement over the best performing baseline methods\\n(i.e., SVM (classi\\xef\\xac\\x81cation) on Webio and SVM(regression) on Twitter) is statistically signi\\xef\\xac\\x81cant (sign\\ntest, p-value < 0.01).\\n\\nLength and MDF are characteristics of messages. The results tell us that these characteristics are not so\\ndiscriminative on the two types of messages. The reason is easy to understand: we may think that context\\ndependent messages tend to be short and consist of common words, but the fact is that short messages\\ncomposed of common words could be context independent (e.g., \\xe2\\x80\\x9cGood night\\xe2\\x80\\x9d in Table 2) while long\\nmessages like \\xe2\\x80\\x9cYep, I have heard it on Saturday night\\xe2\\x80\\x9d (see the example in Table 5) could be context\\n\\n\\x0cdependent. Both SVM (classi\\xef\\xac\\x81cation) and SVM (regression) perform worse than our LSTM model,\\nindicating that shallow features are not effective enough to represent the semantics in short conversation\\nmessages. Our method outperforms the baseline methods on both data sets. The results veri\\xef\\xac\\x81ed our idea\\non leveraging responses for context dependent message detection, and demonstrates the power of big\\ndata and the advantage of LSTM on capturing semantics in short messages.\\n\\n4.4 Qualitative Evaluation\\n\\nWe use an example to further explain why our method is effective on distinguishing the two types of\\nmessages. Table 5 compares LSTM with SVM (classi\\xef\\xac\\x81cation) and SVM (regression). Both SVM (clas-\\nsi\\xef\\xac\\x81cation) and SVM (regression) rely on shallow features such as bag of words and pos tags to perform\\nlearning. These features, however, are not effective on representing the semantics of short messages. The\\nrepresentation is easily to be biased by some speci\\xef\\xac\\x81c words like \\xe2\\x80\\x9cSaturday night\\xe2\\x80\\x9d in the example. There-\\nfore, both SVM (classi\\xef\\xac\\x81cation) and SVM (regression) failed on this case. On the other hand, LSTM\\nmodels term dependencies in sequences with a memorizing-forgetting mechanism. It can capture the\\nsemantics in the message \\xe2\\x80\\x9cYep, I have heard it on Saturday night.\\xe2\\x80\\x9d and identify that it is similar to mes-\\nsages like \\xe2\\x80\\x9cYes, I did\\xe2\\x80\\x9d and \\xe2\\x80\\x9cYes, I have\\xe2\\x80\\x9d. For example, the cosine of the vector of \\xe2\\x80\\x9cYep, I have heard it\\non Saturday night.\\xe2\\x80\\x9d and the vector of \\xe2\\x80\\x9cYes, I have\\xe2\\x80\\x9d given by LSTM is 0.63. Since messages like \\xe2\\x80\\x9cYes, I\\ndid\\xe2\\x80\\x9d and \\xe2\\x80\\x9cYes, I have\\xe2\\x80\\x9d are common context dependent messages, LSTM can successfully recognize that\\nthe message in the example is also context dependent.\\n\\n5 Conclusion\\n\\nWe propose learning a LSTM network with weak supervision signals estimated from responses of mes-\\nsages to detecting context dependent messages in a conversational environment. Evaluation results show\\nthat the proposed method can signi\\xef\\xac\\x81cantly outperform baseline methods on distinguishing the two types\\nof messages.\\n\\nAcknowledgement\\n\\nThis work was supported by Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-\\n2016001), the National Natural Science Foundation of China (Grand Nos. 61370126, 61672081), Na-\\ntional High Technology Research and Development Program of China (No.2015AA016004),the Fund of\\nthe State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16).\\n\\nReferences\\n\\n[Cho et al.2014] Kyunghyun Cho, Bart Van Merri\\xc2\\xa8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for\\nstatistical machine translation. EMNLP.\\n\\n[Derczynski et al.2013] Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-\\n\\nspeech tagging for all: Overcoming sparse and noisy data. In RANLP, pages 198\\xe2\\x80\\x93206.\\n\\n[Elman1990] Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179\\xe2\\x80\\x93211.\\n\\n[Foster et al.2011] Jennifer Foster, \\xc2\\xa8Ozlem C\\xc2\\xb8 etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim\\nNivre, Deirdre Hogan, and Josef Van Genabith. 2011. # hardtoparse: Pos tagging and parsing the twitterverse.\\nIn AAAI 2011 Workshop on Analyzing Microtext, pages 20\\xe2\\x80\\x9325.\\n\\n[Higashinaka et al.2016] Ryuichiro Higashinaka, Nozomi Kobayashi, Toru Hirano, Chiaki Miyazaki, Toyomi Me-\\nguro, Toshiro Makino, and Yoshihiro Matsuo. 2016. Syntactic \\xef\\xac\\x81ltering and content-based retrieval of twitter\\nsentences for the generation of system utterances in dialogue systems.\\nIn Situated Dialog in Speech-Based\\nHuman-Computer Interaction, pages 15\\xe2\\x80\\x9326. Springer.\\n\\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and J\\xc2\\xa8urgen Schmidhuber. 1997. Long short-term memory.\\n\\nNeural computation, 9(8):1735\\xe2\\x80\\x931780.\\n\\n\\x0c[Janarthanam et al.2011] Srinivasan Janarthanam, Helen Hastie, Oliver Lemon, and Xingkun Liu. 2011. The day\\nafter the day after tomorrow?: a machine learning approach to adaptive temporal expression generation: training\\nand evaluation with real users. In Proceedings of the SIGDIAL 2011 Conference, pages 142\\xe2\\x80\\x93151. Association\\nfor Computational Linguistics.\\n\\n[Kang and Kim2003] In-Ho Kang and GilChang Kim. 2003. Query type classi\\xef\\xac\\x81cation for web document retrieval.\\n\\nIn SIGIR, pages 64\\xe2\\x80\\x9371. ACM.\\n\\n[Koshinda et al.2015] Makoto Koshinda, Michimasa Inaba, and Kenichi Takahashi. 2015. Machine-learned rank-\\ning based non-task-oriented dialogue agent using twitter data. In 2015 IEEE/WIC/ACM International Confer-\\nence on Web Intelligence and Intelligent Agent Technology (WI-IAT), volume 3, pages 5\\xe2\\x80\\x938. IEEE.\\n\\n[Mikolov et al.2010] Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc2\\xb4at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010.\\nRecurrent neural network based language model.\\nIn INTERSPEECH 2010, 11th Annual Conference of the\\nInternational Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages\\n1045\\xe2\\x80\\x931048.\\n\\n[Ritter et al.2011] Alan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven response generation in\\n\\nsocial media. In EMNLP, pages 583\\xe2\\x80\\x93593. Association for Computational Linguistics.\\n\\n[Serban et al.2015] Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau.\\n2015. Building end-to-end dialogue systems using generative hierarchical neural network models. arXiv\\npreprint arXiv:1507.04808.\\n\\n[Shang et al.2015] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text\\n\\nconversation. arXiv preprint arXiv:1503.02364.\\n\\n[Song et al.2014] Ge Song, Yunming Ye, Xiaolin Du, Xiaohui Huang, and Shifu Bie. 2014. Short text classi\\xef\\xac\\x81ca-\\n\\ntion: A survey. Journal of Multimedia, 9(5):635\\xe2\\x80\\x93643.\\n\\n[Sordoni et al.2015a] Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob Grue Simon-\\nsen, and Jian-Yun Nie. 2015a. A hierarchical recurrent encoder-decoder for generative context-aware query\\nsuggestion. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Man-\\nagement, pages 553\\xe2\\x80\\x93562. ACM.\\n\\n[Sordoni et al.2015b] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret\\nMitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015b. A neural network approach to context-sensitive\\ngeneration of conversational responses. arXiv preprint arXiv:1506.06714.\\n\\n[Sriram et al.2010] Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Ferhatosmanoglu, and Murat Demirbas.\\n\\n2010. Short text classi\\xef\\xac\\x81cation in twitter to improve information \\xef\\xac\\x81ltering. In SIGIR, pages 841\\xe2\\x80\\x93842. ACM.\\n\\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with\\n\\nneural networks. In Advances in neural information processing systems, pages 3104\\xe2\\x80\\x933112.\\n\\n[Vinyals and Le2015] Oriol Vinyals and Quoc Le.\\n\\narXiv:1506.05869.\\n\\n2015. A neural conversational model.\\n\\narXiv preprint\\n\\n[Williams and Peng1990] Ronald J Williams and Jing Peng. 1990. An ef\\xef\\xac\\x81cient gradient-based algorithm for on-\\n\\nline training of recurrent network trajectories. Neural Computation, 2(4):490\\xe2\\x80\\x93501.\\n\\n[Williams2008] Jason Williams. 2008. Demonstration of a pomdp voice dialer. In Proceedings of the 46th Annual\\nMeeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session,\\npages 1\\xe2\\x80\\x934. Association for Computational Linguistics.\\n\\n[Xu et al.2015] Wenduan Xu, Michael Auli, and Stephen Clark. 2015. Ccg supertagging with a recurrent neural\\n\\nnetwork. In ACL\\xe2\\x80\\x9915, volume 2, pages 250\\xe2\\x80\\x93255.\\n\\n[Young et al.2013] Stephanie Young, Milica Gasic, Blaise Thomson, and John D Williams. 2013. Pomdp-based\\n\\nstatistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160\\xe2\\x80\\x931179.\\n\\n[Zhang and Lee2003] Dell Zhang and Wee Sun Lee. 2003. Question classi\\xef\\xac\\x81cation using support vector machines.\\n\\nIn SIGIR, pages 26\\xe2\\x80\\x9332. ACM.\\n\\n\\x0c',\n",
       " 'Applying Chatbots to the Internet of Things: \\n\\nOpportunities and Architectural Elements\\n\\n \\n\\nRohan Kar1 \\n\\nHyderabad, India \\n\\nrohankar99@gmail.com \\n\\n \\n \\n \\n\\nAbstract\\xe2\\x80\\x94Internet of Things (IoT) is emerging as a significant \\ntechnology in shaping the future by connecting physical devices or \\nthings  with  internet.  It  also  presents  various  opportunities  for \\nintersection  of  other  technological  trends  which  can  allow  it  to \\nbecome even more intelligent and efficient. In this paper we focus \\nour  attention  on  the  integration  of  Intelligent  Conversational \\nSoftware  Agents  or  Chatbots  with  IoT.  Literature  surveys  have \\nlooked into various applications, features, underlying technologies \\nand  known  challenges  of  IoT.  On  the  other  hand,  Chatbots  are \\nbeing  adopted  in  greater  numbers  due  to  major  strides  in \\ndevelopment  of  platforms  and  frameworks.   The  novelty  of  this \\npaper  lies  in  the  specific  integration  of  Chatbots  in  the  IoT \\nscenario.  We  analyzed  the  shortcomings  of  existing  IoT  systems \\nand put forward ways to tackle them by incorporating chatbots. A \\ngeneral architecture is proposed for implementing such a system, \\nas well as platforms and frameworks \\xe2\\x80\\x93 both commercial and open \\nsource  \\xe2\\x80\\x93  which  allow  for  implementation  of  such  systems. \\nIdentification  of  the  newer  challenges  and  possible  future \\ndirections with this new integration, have also been addressed. \\n\\nKeywords\\xe2\\x80\\x94Internet  of  Things,  Chatbots,  Human-\\nComputer  Interaction,  Conversational  User  Interfaces, \\nSoftware Agents \\n\\nI. \\n\\nINTRODUCTION \\n\\nThe  Internet  of  Things  (IoT)  is  not  just  a  well-recognized \\nphenomenon  but  one  that  is  shaping  the  digital  age.  It \\nintroduces  an  era  of  interconnected  smart  objects  or  \\xe2\\x80\\x98things\\xe2\\x80\\x99 \\ndeveloped upon existing internet architectures. By using unique \\naddressing  schemes  and  standard  communication  protocols, \\nIoT  interconnects  these  things  or  objects  thereby  creating  a \\nvaried range of technologies are able to interact with each other \\nand reach common goals [1]. \\n \\nAn  essential  goal  of  connecting  various  sensors, \\nactuators and services and collecting/processing data from them \\nis  to  generate  situational  awareness  and  enable  machines  and \\nhuman users to make sense of themselves and their surrounding \\nenvironments.  \\n\\nin  our  daily \\n\\nThe  proliferation  of  IoT  can  be  seen  through  adoption  of \\nlife  which  include \\nthese  \\xe2\\x80\\x9csmart  devices\\xe2\\x80\\x9d \\napplications \\nin  Manufacturing,  Agriculture,  Medical  and \\nHealthcare, Transportation, Building and Home Automation and \\nEnergy  Management  among  others.  A  report  by  Gartner \\nestimates that there will be over 20 Billion connected things in \\nactivity by 2020 with Cisco estimating the number to be over 50 \\n\\n \\n \\n\\n \\n\\nRishin Haldar2 \\n\\nSchool of Computing Sciences and Engineering, \\n\\n VIT University,  \\n\\nVellore, India \\n\\nrishinhaldar@vit.ac.in \\n\\n \\n \\n\\nBillion [2, 3]. Among them more than half of all IoT endpoints \\nin the consumer space alone. Hence IoT is a phenomenon which \\nis certain to play a major  role in our daily interaction with the \\ndigitally connected world. \\n\\n \\n\\nA.  Scope of Internet of Things \\n\\nThe literature presents various ways to define the Internet of \\nThings. The RFID group defines Internet of Things as \\xe2\\x80\\x9cworld-\\nwide  network  of  interconnected  objects  uniquely  addressable, \\nbased on standard communication protocols\\xe2\\x80\\x9d. ITU [4] defines it \\nas \\xe2\\x80\\x9ca global infrastructure for the information society, enabling \\nadvanced  services  by  interconnecting  (physical  and  virtual) \\nthings based on existing and evolving interoperable information \\nand communication technologies\\xe2\\x80\\x9d.  \\n\\nWhile considering the broad vision of IoT, this paper focuses \\non the perspective of connected things and applications for those \\nthings.  To  do  this  we  simply  create  a  separation  of  concern \\nbetween  the  fragmented  lower  Open  System  Interconnection \\n(OSI) layers of IoT and the unifying adopted upper layers of IoT \\ncommunication  which  uses  the  World  Wide  Web  and  its \\nstandard network protocols.  \\n\\nThe  entire  IoT  system  consists  of  Sensors  (temperature, \\nlight,  motion,  etc.),  Actuators  (displays,  sound,  motors,  etc.), \\nComputation  (programs  and \\nlogic),  and  Communication \\ninterfaces (wired or  wireless).   However, based on  established \\nadvantages presented in the literature [5, 6, 7, 8], our scope will \\nbe  limited  to  interaction  with  IoT  through  Web  Application \\nProgramming  Interfaces  (API)  and  in  particular  Hypertext \\nTransfer Protocol (HTTP) based Representational State Transfer \\n(REST)  Architectures.  A  popular  approach  of  Web  of  Things \\nhas been illustrated in Fig 1 based on [6].  \\n\\n  The  Evans  Data  Corporation  (EDC)  Report:  Internet  of \\nThings - Vertical Research Service study [9] reveals that more \\nthan half of IoT developers connect to devices primarily through \\nthe  cloud.  The  massive  growth  and  acceptance  of  these  cloud \\nbased platforms such as IBM IoT Platform, AWS IoT, Microsoft \\nAzure IoT and Cisco IoT show that the new generation of IoT \\napplications  concentrate  on  cloud  based  platforms  with  the \\nlower  layers  (Transfer,  Transport  and  Network).  Hence,  this \\npaper also proposes the use of IoT cloud based platforms in our \\narchitectural design. This is discussed further in Section 4. \\n\\n\\x0c \\n\\n \\n\\nFig. 1.  Web of Things as shown in [6] \\n\\nB.  Scope of Chatbots \\n\\n  This  paper  proposes  the  use  of  Intelligent  Conversational \\nAgents.  We  refer  to  these  as  simply  Chatbots  (also  known  as \\nChatterbots  or  bots  in  general).  Interestingly,  there  are  many \\ndefinitions for Chatbots in close relation with Software Agents \\n(SA),  Virtual  Agents  (VA)  or  Intelligent  Personal  Assistants \\n(IPA) in literature and these have often been used in conjunction \\nwith each other. The term \\xe2\\x80\\x9cAgents\\xe2\\x80\\x9d itself has many definitions \\nbut among the earliest and most well-known uses of the term is \\n[10] - \"A self-contained, interactive and concurrently-executing \\nobject, possessing internal state and communication capability.\" \\n\\n  The  scope  of  Software  Agents  can  be  most  closely \\nassociated  with  Chatbots  and  has  been  well  documented  in \\nliterature  [11].  The  following  key  properties  have  been \\nassociated  with  Software  Agents  [12]:  (1)  reactive,  (2)  pro-\\nactive  and  goal-oriented,  (3)  deliberative  (4)  continual  (5) \\nadaptive (6) communicative, and (7) mobile. The purpose of this \\npaper is not to explore the various types of Software Agents and \\nagent  based  systems  or  its  properties  but  rather  propose  the \\nsolution  to  challenges  faced  in  IoT  through  the  use  of  the \\numbrella  term  for  these  Intelligent  Conversational  Agents, \\nSoftware  Agents  or  Chatbots  as  we  refer  to  them.  It  is  also \\nimportant to note that Software Agent distinguishes itself from \\nIntelligent Agents (also known as Rational Agents). Intelligent \\nagents  are  not  only  computer  programs.  They  can  also  be \\nmachines, humans or anything that is capable of a goal directed \\nbehavior [13]. \\n\\n  Typically  Chatbots  are  classified  into  two  types:  (1) \\nChatbots that function based on Rules (2) Chatbots that function \\nbased on Artificial Intelligence. Chatbots that function on rules \\nare  often  limited  as  they  are  only  as  smart  as  they  are \\nprogrammed.  On  the  other  hand,  AI  based  Chatbots  give  the \\nimpression  of  being  \\xe2\\x80\\x9cintelligent\\xe2\\x80\\x9d  as  they  are  capable  of \\nunderstanding natural language,  not just pre-defined commands \\nbut  get  smarter  as  they  interact  more  due  to  their  ability  to \\nmaintain different states. Based on this, concepts such as Virtual \\nAgents and Intelligent Personal Assistants (IPA) have come up, \\nwhich  uses  natural  language  processing,  as  well  as  speech \\nrecognition  techniques.  For  example,  Apple  Siri,  Amazon \\nAlexa, Microsoft Cortana and Google Assistant. \\n\\n \\nIn this paper we present a novel paradigm combining these \\ntwo disparate concepts in a single solution. However, the studies \\n\\n \\n\\nof  these  paradigms  have  largely  been  separate  endeavors.  We \\ndiscuss  how  using  chatbots  as  intelligent  conversational \\ninterfaces can be used to address critical problems in IoT. We \\nalso propose a high level conceptual architecture and discuss key \\nelements  involved  in  communicating  with  an  IoT  system \\nthrough  Chatbots.  To  explain  in  the  context  of  real  world \\napplicability,  we  put  forth  existing  solutions  to  each  of  the \\ncomponents in the architecture including frameworks, platforms \\nand specify open-source tools which can be used to build such a \\nsystem. \\n\\n \\n\\nThe rest of this paper is organized as follows: In section II \\nwe discuss our motivation for introducing this novel concept of \\nChatbots in Internet of Things and discuss other literature work \\nthat has helped shape this concept. In Section III, we evaluate \\nand  examine  the  shortcomings  and  challenges  of  current  IoT \\nsystems  and  the  opportunity  for  chatbots  to  address  them. \\nSection IV proposes a system design and the key architectural \\nelements.  Finally,  we  present  our  concluding  remarks  by \\nassessing  opportunities  and  scope  for  future  research  and \\ndevelopment in Section V. \\n\\nII.  MOTIVATION AND BACKGROUND STUDY \\n\\n \\nThe key to the massive adoption and diffusion of IoT \\nis  the  proliferation  of  Internet  in  our  daily  lives.  We  use  the \\ninternet  to  search  for  information,  check  emails,  consume \\nmedia,  and  connect  with  people  via  social  networks  and  so \\nmuch  more.  With  around  40%  of  the  global  population  (3.4 \\nBillion)  currently  using  the  world  wide  web,  this  number  is \\nestimated to increase to 7.6 billion global internet users in 2020, \\na  majority  of  which  use  mobile  devices  (phones,  tablets, \\nwearables etc.) [14]. Hence the internet has played a vital role \\nas a global backbone for information sharing, interconnection \\nof physical objects with computing/networking capabilities for \\napplications  and  services  spanning  numerous  use  cases. \\nInternet alone, however, cannot address all issues of IoT. First, \\nwe will briefly discuss the challenges in IoT, and then mention \\nthe  motivation \\nintelligent  conversational \\ninterfaces. \\n\\nfor  choosing \\n\\nA.  Challenges in IoT \\n\\n \\nDespite the wide scale efforts to popularize IoT, it still \\noffers  many  practical  challenges.   Primarily,  IoT  systems \\noperate  in  isolated  technology  or  vendor  specific  silos  which \\ninhibits  capability,  value,  and  interoperability  and  create  a \\nwidely  disparate  area  [15].  Specifically,  by  restricting \\nheterogeneous  devices  (home  appliances,  mobile  phones, \\nembedded devices etc.), sensors and services to communicate \\nwith each other across interconnected networks, possibilities of \\ncountless applications are hindered. \\n\\nSecondly,  the  sheer  number  of  connected  things  has \\nalready  started  to  create  problems  in  application,  device  and \\ndata  management  in  IoT  [16].  To  address  this  issue,  IoT \\nplatforms (such as Cisco IoT, IBM IoT, Microsoft Azure IoT, \\nAWS  IoT)  offer  scalable,  distributed  cloud  based  services  in \\norder to allow businesses to quickly connect to an established \\ninfrastructure,  service  or  software  without  having  to  worry \\nabout the backend complexities. While IoT Cloud is a step in \\nthe  right  direction,  offering  many  advantages,  it  still  presents \\n\\n \\n\\n\\x0cmany challenges particularly in interoperability which has led \\nto the issues of platform fragmentation [17, 18]. \\n \\n\\nIoT  systems  also  face  a  challenge  of  unifying  User \\nInterfaces  (UI).  It  becomes  increasingly  difficult  on  users  to \\nkeep  track  and  access  multiple  applications,  dashboards  for \\nevery new \\xe2\\x80\\x9cIoT object\\xe2\\x80\\x9d in their ecosystem [19]. Hence unifying \\nexperiences  across  multiple  connected  things  and  providing \\nthem  with  a  high  degree  of  smartness  for  improved  user \\nexperience is a key challenge. \\n\\nB.  Relevance of Conversational User Interfaces \\n\\nAccording  to  reports,  Chat  interfaces  which  are  used  in \\nInstant  Messaging  (IM)  platforms  (Such  as  Facebook \\nMessenger,  Slack,  Kik,  and  Telegram)  have  been  immensely \\npopular and continue to show steady growth. IM services have \\nmore active users than any other internet application including \\nsocial  networks,  mailing  applications  [20].  This  report  shows \\nhow the top ten messaging platforms alone account for nearly \\n4 Billion users. The global acceptance of chat based interfaces \\nallows for ease of adoption and diffusion of newer technologies \\nbuilt  on  top  of  the  pre-existing  platforms  (such  as  Chatbot \\nApplications).  Therefore  the  global  proliferation  of  chat \\nplatforms  only  furthers  the  motivation  to  develop  interesting \\napplications and use cases with chatbots. \\n \\n\\nOn  a  different  note,  advancements  made  in  the  areas  of \\nArtificial Intelligence, especially Natural Language Processing \\nhave furthered the efficiency and quality of Chatbots in terms \\nof \\nits  conversational  simplicity,  easy  adaptability  and \\ncapabilities  in  allowing  the  user  to  make  complex  requests \\nthrough simple natural language. \\n \\n\\nHowever,  the  similarity  of  both  Chatbots  and  IoT  lies  in \\nadoption  of  their  services  through  relatively  simple,  often \\nRESTful Web APIs. The rationale behind it has been based on \\nthe following observations: \\n\\n(1)  Developers  can  take  an  API  or  service-oriented \\napproach  to  development  for  both  IoT  as  well  as \\nChatbots.  This  means  that  application  development \\nmethodologies would be the same with both embedded \\ndevices as  with any  web service (including Chatbots) \\nthat  use  Web  APIs  and  in  particular  using  RESTful \\narchitectures.  \\n\\n(2)  Chatbot applications just like IoT applications can be \\ndesigned and deployed on cloud platforms which allow \\nsimple development and deployment without concern \\nabout  the  underlying  technologies  such  as  Transport \\nlayer, Storage, Processing etc.  \\n\\n(3)  Owing to HTTP RESTful standards and protocols,  it \\nbecomes \\nto \\nintegrate  chatbot  applications  into  IoT  systems  using \\napplication layer as the only concerned medium \\n\\ntechnologically  feasible  and  simple \\n\\n \\nThis  ease  of  integration  is  a  key  motivation  to  develop \\nplatforms  and  frameworks  which  can  synchronize  chatbot \\napplications within IoT platforms and frameworks. \\n\\n \\n\\nFig. 2.  Sample of a IoT Chatbot-User conversation \\n\\n \\n\\nIII.  OPPORTUNITIES FOR CHATBOTS IN IOT \\n\\nThe  shortcomings  of  modern  IoT  systems  can  be  broadly \\nclassified into two types: (i) Technology Centric Challenges and \\n(ii) Human Centric Challenges. With the help of sample chatbot-\\nuser conversations given in Fig. 2, we discuss the opportunities \\nfor  Chatbots  and  demonstrate  ways  in  which  Chatbots  can \\novercome challenges in IoT. \\n\\nUse Case (A) \\nUser: \\xe2\\x80\\x9cKeep the living room temperature comfortable\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cThe weather outside is a cool 17 degrees Celsius.  \\nSetting temperature in the living room to 21.4 degree Celsius.\\xe2\\x80\\x9d \\nUse Case (B) \\nUser: \\xe2\\x80\\x9cHow much is my car charged\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cThe Tesla Model S is currently 40% charged. 3 Hours 10 \\nminutes to full charge.\\xe2\\x80\\x9d \\nUse Case (C) \\nUser: \\xe2\\x80\\x9cTurn the light on in the guest bedroom\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cWhich light would you like to have turned ON? The \\nLamp or Table Light?\\xe2\\x80\\x9d \\nUser: \\xe2\\x80\\x9cBoth\\xe2\\x80\\x9d \\nUse Case (D) \\nUser: \\xe2\\x80\\x98Help me setup my new device\\xe2\\x80\\x99 \\nChatbot: \\xe2\\x80\\x9cHere is some help to guide you through the setup\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cWhich device would you like to setup? \\n 1) Smart Lock 2) Smart Kettle 3) Smart light?\\xe2\\x80\\x9d \\nUser: 1 \\nChatbot: \\xe2\\x80\\x9cOk, Enter your secret passcode for the smart lock\\xe2\\x80\\x9d \\nUser: \\xe2\\x80\\x9c*****\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cDone. Smart Lock is now setup.\\xe2\\x80\\x9d \\nUse Case (E) \\nChatbot: \\xe2\\x80\\x9cThe monitoring service indicates that the smart lock has \\nbeen offline for over 24 hours.\\xe2\\x80\\x9d \\nChatbot: \\xe2\\x80\\x9cWould you like me to report the issue to the Smart Lock \\nVendor?\\xe2\\x80\\x9d \\nUser: \\xe2\\x80\\x9cNo, I want to talk to a human\\xe2\\x80\\x9d \\nHuman-Operator: \\xe2\\x80\\x9cI can see the issue you are facing. I will try to \\nresolve it remotely.\\xe2\\x80\\x9d \\n\\nA.  Technology Centric Challenges of IoT \\n\\n1)  Data Management \\n\\n \\nA key challenge in the realm of IoT, is managing the \\nvast  amount  of  big  data  being  generated,  as  IoT  sensors  are \\nbecoming easily affordable.  Not only is the data generated by \\nthe sensors large but also diverse (varying in quality and type) \\n\\n \\n\\n\\x0cand  multimodal  (temperature,  light,  sound,  video,  etc.)  in \\nnature.  While  data  deluge  is  one  challenge,  drawing  insights \\nfrom  the  data  and  being  able  to  present  it  in  a  timely, \\nunderstandable way is a much larger challenge. \\n  \\nThe situation can be best illustrated by the well-known \\n \\n(Data, \\nKnowledge  Hierarchy  also  called \\nInformation,  Knowledge,  Wisdom)  Pyramid  in  the  context  of \\nIoT  [21,  22]  which  calls  for  solutions  to  tackle  the  difficult \\nchallenges  as  one  moves  up  in  the  pyramid.  The  data  gets \\nsmaller  but  becomes  more  difficult  to  gain  abstractions  and \\nperceptions  (Knowledge),  which \\nto  derive \\nactionable  intelligence  (Wisdom).  Chatbots  are  attempting  to \\nsolve  the  problems  of  data  and  information  management  by \\nmainly addressing the upper layers of the DIKW pyramid. \\n\\nthe  DIKW \\n\\nis  required \\n\\na) Data Context \\n\\nProcessing and analyzing of IoT data can be solved through \\nthe many \\xe2\\x80\\x9cbig data\\xe2\\x80\\x9d solutions and cloud platforms which offer \\nstorage  and  computing  infrastructure  to  accomplish  the  task. \\nThese  existing  IoT  cloud  solutions  handle  Data  source  and \\ntransmission challenges. However, a major challenge of existing \\nIoT systems is conveying data about the various interconnected \\ndevices  (sensors,  objects  etc.)  back  to  the  user  in  a  simple \\nhumanly  understandable  way.  This  requires  context,  which  is \\nachieved by enabling Chatbots to understand the true intent of \\nthe  user  query  and  collect  and  process  information  from  their \\nenvironments.  Moreover,  Chatbots  have  access  to  a  global \\nnetwork  of  information  via  the  internet  and  can  be  easily \\nprogrammed  to  retrieve  information  in  real-time  which  can \\nimprove the context. \\n\\n \\nIn practical  terms,  Chatbots  simplify  the  way  we  consume \\ninformation from multiple screens and heavy data and graphics \\nto simple conversational interfaces capable of delivering highly \\ncontextual  and  intelligible  information  within  the  flow  of  the \\nchat  app  itself.  Achieving  this  high-level  of  abstraction  can \\ndeliver actionable intelligence (wisdom) with domain and user \\nknowledge to maximize the full potential of IoT. For example, \\nin  use  case  (A),  the  query  was  relatively  vague.  The  Chatbot \\ncould have used contextual information from Real time outside \\ntemperature  along  with  knowledge  from  historical  user \\npreferences to perform a specific action. \\n\\nb) Information Retrieval \\n\\n \\nIoT  dashboards  are  often  saturated  with  various \\nmetrics,  data  points,  charts  and  tables  making  it  difficult  for \\nusers to find the required information. Chatbots can effectively \\nsolve this problem by responding quickly to direct queries with \\nhighly  accurate  information.  By  understanding  the  specific \\nintent  of  the  user  they  limit  the  scope  of  information  for \\npresentation.   In  terms  of  the  knowledge  hierarchy,  Chatbots \\nperform lookup and abstraction on IoT data. By automatically \\nproviding  IoT  data  as  well  as  user-contextual  data  to  an \\nanalytics,  Chatbots  can  also  derive  its  own  knowledge.  For \\nexample,  in  use  case  (B)  the  query  only  asked  for  Battery \\nCharge  related  information  and  nothing  else.  The  Chatbot \\nlimited the response accordingly. \\n \\n \\n\\n \\n\\n \\n\\n \\n\\n2)  Device and Application Management \\nA  key  challenge  of  IoT  has  been  the  fragmentation  of \\ntechnology  [17,  18].  Having  application  interoperability \\nbetween  heterogeneous  devices  from  a  single  remote  (mobile \\ndevice  or  operation  terminal)  is  especially  uncommon.  For \\nExample,  consider  the  situation  where  a  smart  light  and  a \\nHeating  Ventilation  and  Air  Conditioning  (HVAC)  system \\nbelong  to  the  same  network  and  environment  yet  may  have \\ndifferent user control terminals which are mutually independent \\nentities,  unaware  of  each  other  nor  able  to  control  or \\ncommunicate with each other. \\n \\n \\nChatbots are built on IM platforms (such as Facebook \\nMessenger and Slack) which support multiple different chatbot \\napplications.  A  single  chatbot  application  as  well  can  use \\nunique HTTP REST APIs pertaining to different IoT devices. \\nChatbots can thus act as a single interface for communication \\nbetween  single  purpose  devices  (eg.  Controlling  two  smart \\nlights), heterogeneous devices (eg. Controlling a HVAC and a \\nSmart Car) and even different IoT ecosystems (eg. Controlling \\nSmart  home  devices  and  Smart  Retail  devices)  in  the  case  of \\ncloud based IoT. For example, in the Use cases above, the same \\nchatbot is being used to converse with multiple heterogeneous \\ndevices.  Provided  the  right  permissions  are  available  it  can \\ncommunicate with Public IoT devices. \\n \\n\\n3)  Bridging Data across Platforms and Services \\n\\n \\nIoT  platforms  can  be  seen  as  software  development \\nenvironments  which  handle  Device  management,  Application \\nmanagement,  Connection  Management,  Dashboard  and \\nAnalytics.  Yet  owing  to  platform  fragmentation  [17,  18], \\nsharing  of  data  across  platforms  is  still  uncommon.  One \\nsolution is to solve the issue  at the application level by using \\n3rd  party  services,  which  through  APIs  can  access  data  from \\neach platform. The data can be either processed on the various \\nplatforms or extracted into another service and used to deliver \\nsomething  of  value  which  can  then  be  presented  through  a \\nsingle Chatbot interface. \\n \\n\\n4)  Search and Discoverability \\n\\n \\nA key attribute in IoT is the natural tendency of objects \\nto be dispersed in the environment while being interconnected \\nand identifiable at class-level (i.e. common information across \\nthe  same  class)  or  serial-level  (i.e.  unique  to  an  individual \\nobject) [23]. \\n\\nBased  on  the  permissions  of  the  requester  and  the \\navailability  of  the  connected  objects  in  the  scope  of  the \\nenvironment,  IoT  requires  lookup  and  discovery  services  to \\neffectively find and control these objects. Such services include \\navailability of sensors and actuators which the Chatbot would \\nbe able to retrieve from the entities and convey to the user at \\nthe appropriate times. \\n \\n \\n \\n \\n\\n\\x0c \\n\\n5)  Monitoring and Reporting \\n\\n \\nFrom IoT wearables such as health monitoring devices \\nto  industrial  sensors  which  convey  information  in  real  time, \\nmonitoring  and  reporting  are  key  aspects  of  IoT  systems. \\nChatbots can also be effectively used to perform as monitoring \\nservices  by  integrating  with  solutions  such  as  Application \\nPerformance  Management  (APM).  Accessing  data  from \\nvarious  IoT  systems  is  a  key  advantage  which  is  unique  to \\nChatbots in this scenario. Similarly Chatbot services can utilize \\nits  own  reporting  services  and  present \\nthe  abstracted \\ninformation to the user in an actionable and timely manner. \\n \\n\\nB.  Human Centric Challenges of IoT  \\n\\nChatbots  were  created  with  the  primary  purpose  of \\nimproving  the  human-computer  user  experience.  As  such, \\nsolving the user experience shortcomings of IoT systems can be \\nan  important  opportunity  for  chatbots.  IoT,  with  its  complex \\nsystem of applications, sensors, actuators and services presents \\na daunting challenge of gaining technical knowledge to interact \\nwith  these  various  components.  Hence  exposing  settings  and \\nconfigurations  to  users  presents  an  obvious  and  unfriendly \\nburden that is far from ideal. \\n \\n\\n1)  Cognitive Burden \\n\\n \\n\\nThe changing technology landscape of IoT is both imminent \\nand  rapid.  Furthermore,  as  newer  features  and  use  cases  are \\nintroduced, there is an added responsibility to educate the end \\nusers  which  can  be  burdensome  for  both  the  users  and  the \\ndevelopers  of \\nthe  system.  Complicated  systems  cause \\nin  adoption  and  diffusion.  As  an  assistive \\ndifficulties \\ntechnology,  chatbots  can  simplify  the  learning  curve  by  the \\nfollowing ways: \\n\\na) Help Features: IoT enabled Chatbots can feature help \\ntexts  which  clarify  the  user  request  to  ensure  that  the  action \\nperformed is same as the one intended. \\n\\nb) Recommendations: Chatbots can recommend possible \\nactions  to  the  user  which  can  be  made  more  intelligent  and \\ncontext aware depending on user preferences and the dynamics \\nof the environment. \\n\\nc) Automation:  Chatbots  are  good  at  automating \\ncommon cyclic, tasks and can perform certain actions such as \\nmonitoring availability of sensors (uptime, downtime etc.) and \\nothers  through  routine  API  calls,  Websockets  or  Publisher-\\nSubscriber methods. \\n\\nd) Better Quality of Service (QoS): Feedback loops can \\nbe easily integrated within chatbots to aggregate most frequent \\nqueries and data from the process can be used to improve the \\nfuture Quality of Service (QoS). \\n \\n\\n \\n \\n \\n \\n\\n \\n\\nAs  more  use  cases  are  discovered,  chatbots  can  make  the \\nadoption and diffusion of IoT systems significantly easier and \\nreduce  the  cognitive  burden  required  to  understand  the \\nfunctionalities of these systems. \\n \\n\\n2)  User Interface Opportunities \\nGraphical  User  Interfaces  (GUI)  for  IoT  are  largely \\nfunctional in nature. While it achieves simplicity by displaying \\nvirtual  switches,  sliders  and  buttons  rather  than  passing \\ncomplex commands, it still has some shortcomings which Chat \\ninterfaces  can  solve.  (1)   Chat  interfaces  understand  natural \\nlanguage which makes interaction with the system as simple as \\nasking  queries  and  receiving  answers.  There  is  no  need  for \\nnavigation  of  menus  and  finding  the  right  icon/button  to \\nperform a task. (2) Chatbots use  machine learning techniques \\nto learn about an individual user and can personalize the service \\nto that user. In this  way, they can understand the unique  way \\nthe user converses with while maintaining the natural flow of \\nthe conversation (3) They are also highly contextual interfaces \\nand  can  understand  the  intent  in  the  scope  of  the  past \\ninteractions  which  is  unique  to  chat  based  interfaces  (also \\nspeech).  (4)  Chat  based  interfaces  concern  mostly  textual \\ninformation  thereby  simple  log  files  can  be  maintained  and \\nconsequently analyzed to make debugging easier. \\n \\n\\n3)  Configuration Challenges \\nApart  from  the  knowledge  required  to  adapt  to  the  new \\nsystems and ease cognitive burden, each IoT device has its own \\nunique setup and configuration in terms of software, network, \\nfirmware etc. As the number of different IoT devices increase, \\nit  becomes  difficult  and  burdensome  at  best  to  navigate  the \\ninterfaces  of  various  applications  and  properly  configure  the \\nsystem. Often technicians are involved to configure and explain \\nthe uses of the system. \\n\\nUsing  Chatbots,  users  can  be  guided  and  advised  on  the \\nright  configurations  for  their  system  by  creating  step-by-step \\nsetup processes using predefined configuration APIs. This also \\nreduces human effort in setting up the system. For example: A \\nnew device was configured in use case (D). \\n \\n\\n4)  Lack of Automated Error Reporting \\nThe distributed nature of most IoT systems implies that user \\nreport  databases  of  IoT  errors  are  spread  across  multiple \\norganizations,  Operating  System  (OS)  vendors,  ISPs,  and \\ndevice  vendors  which  makes  automated  problem  reporting  a \\nmajor  challenge.  Furthermore,  users  themselves  are  uncertain \\nwhich  organization  to  report  the  particular  issue  to.  Thus, \\nvarious stakeholders in the system have a limited understanding \\nof the true nature of the problem and avoid sharing information \\nwith  each  other.  Chatbots,  in  this  scenario,  can  access  these \\nreported problems and by integrating other services, be able to \\nnot  only  retrieve  information  from  the  IoT  system  but  send \\ninformation  to  it.  In  Use  Case  (E),  the  chatbot  identified  the \\ncorrect stakeholder to send the error. \\n\\n \\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\nA.  IoT System \\n\\n1)  IoT Devices \\n\\n \\nIn the context of the paper, we refer to an IoT device \\nin the broad scope of the term as a \\xe2\\x80\\x9cuniquely identifiable IoT \\nendpoint which can be accessed and controlled using RESTful \\nWeb APIs\\xe2\\x80\\x9d. In the situation an embedded device does not have \\nAPIs, there are existing solutions to easily create APIs for them. \\nFor Example: Using platforms such as Zetta, one can create IoT \\ncloud  based  systems  with  full-fledged  APIs.  In  the  presented \\nsystem  we  consider  a  Home  automation  system  consisting  of \\nSmart lights (eg. Philips Hue) and Smart HVAC (consisting of \\na  smart  thermostat  eg.  Nest)  as  well  as  a  Connected  Car  or \\nSmart Car (eg. Tesla). However, on principle, any IoT device \\nmay be considered for interfacing with Chatbots. \\n \\n\\n2)  IoT Cloud Platform \\nIoT  Cloud  based  Platforms  is  an  important  enabling \\ntechnology in many IoT systems today. They deal with various \\nfragmented  technologies  in  embedded  devices  from  access \\nprotocols (eg. Message Queuing Telemetry Transport (MQTT), \\nHTTP etc.), wireless protocols (Zigbee, Bluetooth Low Energy \\n(BLE)) to various services, SDKs and integrations.  There has \\nbeen  positive  reports  on  the  established  advantages  of  Cloud \\nbased  IoT  platforms  [25].  Our  system  design  stresses  on \\naccessing  and  controlling  the  embedded  devices  in  question, \\nsuch  as  Smart  Car,  Light,  Thermostat  etc.,  through  the  API \\nManagement/Gateway of the IoT Cloud Platform, regardless of \\nthe standards and protocols of the individual embedded devices. \\nOn  the  other  hand,  complex  components  in  Networking  and \\nComputing infrastructure for the Cloud platform have not been \\naddressed  in  this  paper.  Popular  IoT  platforms  today  include \\nMicrosoft  Azure  IoT,  IBM  IoT,  APIGEE  IoT,  Cisco  IoT, \\namong others. \\n\\nB.  Chatbot System \\n\\n1)  Chatbot Channels and Platforms \\n\\nChatbot Channels are applications which run Chatbots \\n \\non  supported  Mobile  devices  (eg.  Smartphones,  Tablets)  or \\nTerminals (eg. Desktop Applications).They are typically built \\non  top  of  the  existing  instant  messaging  platforms.  Popular \\nChatbot  channels \\ninclude  Facebook  Messenger,  Slack, \\nTelegram, Kik, Skype, Line and Twilio SMS. These channels \\nare essentially the Chatbot applications in which a user interacts \\nwith  the  bot.  The  area  of  Chatbot  development  is  still  in  its \\ninfancy  and \\nthere  can  be  many  different  architectural \\napproaches in implementing Chatbots.  \\n \\nIn  some  approaches,  the  channels  are  interfaced \\nseparately  from  the  Chatbot  Platforms  through  connectors. \\nHere, the Chatbot Platforms are hosted on cloud services which \\ncan  use  Webhooks  to  communicate  with  the  Channel.  In  the \\ncontext of this paper, we consider the integration of Chatbots as \\ntext based inputs to IoT. By using Software Development Kits \\n(SDK)  it  is  possible  to  integrate  IoT  to  voice/speech  based \\nIntelligent  Personal  Assistants  such  as  Amazon  Echo  (using \\nAlexa SDK) and Google Home (using Google Assistant SDK). \\n \\n \\n\\n \\n\\n5)  Support Challenges \\n\\n \\n\\nRemedying  hardware  and  software  issues  in  modern \\nconsumer IoT systems can be an irksome task. The recourse is \\nto  call  the  service  provider  for  technical  support  or  in  many \\ncases return the product. Either way it is an unnecessary burden \\non  the  user  as  well  as  the  support  vendors  in  today\\xe2\\x80\\x99s  cost \\nstructure. \\n\\nSmart  Chatbots  often  have  support  services  built  into  their \\nfunctionality. It can even integrate human-in-the-loop systems \\nto handle situations the Chatbot is not trained or authorized to \\nperform, in real-time. In this manner, users need not go beyond \\nthe scope of the chatbot application to look for product support. \\nAny software issue or hardware malfunction can be monitored \\nand  Over  the  Air  (OTA)  software  repairs  can  be  performed. \\nChatbots can also be used to schedule technical repairs making \\nit a convenient and fast solution to customer support [24]. For \\nexample,  in  Use  case  (E),  a  human  operator  was  made  to \\nintervene. \\n\\nIV.  SYSTEM DESIGN AND ARCHITECTURAL ELEMENTS \\n\\n \\nWe present a conceptual system design which will aid \\nin  building  Chatbot  systems  for  IoT.  Fig.  3  presents  the  high \\nlevel  view  of  the  overall  architecture  consisting  of  the  IoT \\nsystem and the Chatbot system. \\n \\n\\nFig. 3.  Proposed System Design of IoT-Chatbot System \\n\\n \\n\\n \\n\\n \\n\\n\\x0c \\n\\n2)  Chatbot Engine \\nPerhaps the most important component of a Chatbot is the \\nengine,  often  referred  to  as  Natural  Language  Understanding \\n(NLU) engine. It is responsible for translating natural language \\ninto machine understandable action. Chatbot engines are often \\nhighly  complex,  using  various  Natural  Language  Processing \\n(NLP)  models  and  Machine  Learning  (ML)  techniques  to \\nprovide  acceptable  levels  of  accuracy.  To  make  it  easier  for \\nChatbot  developers,  many  companies  offer  the  processing \\ncapability  of \\nthe  Chatbot  engine  as  a  Software-as-a-\\nService(SaaS)  or  \\xe2\\x80\\x98AI-as-a-service\\xe2\\x80\\x99  which  are  applied  to \\nChatbot  applications  using  APIs.  For  example,  Wit.ai  and \\nMicrosoft LUIS.  \\n\\nThis paper is primarily focused on listing the relevant key \\nfunctional components of the engine in the context of IoT, not \\non designing the NLP techniques for the Chatbot engine. Next, \\nwe  include  key  concepts  typically  associated  with  chatbot \\nengines [26, 27]: \\n \\n\\na) Entities:  \\nEntities are domain specific information extracted from \\nthe  utterance  that  maps  the  natural  language  phrases  to  their \\ncanonical phrases in order to understand the intent. They help \\nin  identifying  the  parameters  which  are  required  to  take  a \\nspecific action. To train the chatbot engine, entities which are \\nexpected  to  give  the  same  actions  are  typically  grouped \\ntogether.   Common  entities  can  be  predefined  as  they  can  be \\nused in many different scenarios. For example: Money, Color, \\nDate time, Location, Number. Domain specific entities can be \\ntrained to recognize similar phrases. IoT devices are one such \\ndomain specific entity.  \\n\\nFor  example,  for  the  utterance:  \\xe2\\x80\\x9cThermostat\\xe2\\x80\\x9d,  the \\nacceptable phrases may be trained to recognize \"Thermostat\", \\n\"heat\",  \\xe2\\x80\\x9cheating\\xe2\\x80\\x9d,     \\xe2\\x80\\x9cAC\\xe2\\x80\\x9d  ,\"air  conditioning\"    and  will  be \\ndecoded  as  {\\xe2\\x80\\x9ctype\\xe2\\x80\\x9d:iot,  \\xe2\\x80\\x9cdevice\\xe2\\x80\\x9d:  \\xe2\\x80\\x9cThermostat\\xe2\\x80\\x9d}  where  the \\nentity  is  IoT.  Entities  may  also  have  its  own  attributes.  For \\nexample,  the  Utterance:  \\xe2\\x80\\x9c$15\\xe2\\x80\\x9d  can  be  decoded  as  {\\xe2\\x80\\x9ctype\\xe2\\x80\\x9d: \\n\\xe2\\x80\\x9cmoney\\xe2\\x80\\x9d, \\xe2\\x80\\x9camount\\xe2\\x80\\x9d:15, \\xe2\\x80\\x9ccurrency\\xe2\\x80\\x9d: \\xe2\\x80\\x9cdollars\\xe2\\x80\\x9d} where the entity \\nis Money in JSON format. \\n \\n\\nb) Context: \\n\\nDetermining \\n\\nthe  Context  of \\n\\n  \\nthe  current  user \\nexpression is an important feature of modern Chatbots. It can \\nbe used to handle situations where the utterances may be vague \\nand have multiple meanings depending upon the history of the \\nconversation.  Contexts  represent  the  ability  of  agents  to \\nmaintain state (also called lifespan or the number of utterances \\nafter which the context will be removed) and match the required \\nintent.  They  may  also  use \\ninformation  from  external \\nsources.  For example, if the user was asking about the living \\nroom (location) in the first utterance and then after the passing \\nof a few more utterances, mentions a vague statement such as: \\n\\xe2\\x80\\x9cmake  it  colder\\xe2\\x80\\x9d,  the  Chatbot  uses  context  to  understand  the \\nearlier reference to the temperature of the living room. \\n\\n \\n \\n\\n \\n\\n \\n\\nc) Intents:  Intents  are  the  crux  of  conversational  UI  in \\nchatbots.  The  intents  represent  what  the  users  are  looking  to \\naccomplish: get status updates, turn on/off devices, ask for help \\netc.  The  message  passed  from  the  user  (utterance)  in  natural \\nlanguage is first analyzed for the intent. This means mapping a \\nphrase  to  a  specific  action  that  should  be  taken  by  the  IoT \\nsystem  as  well  as  the  specific  dialog  to  be  returned  from  the \\nChatbot. The information contained in an intent would be the \\ncontext and action. \\n\\n \\n\\nd) Action:  Action refers to the steps that the IoT device \\nwill take when the intent of the user input is recognized. Actions \\nhave  specified  parameters  which  categorize  details  about  it. \\nActions will be triggered once they are recognized by the intent \\nFor  example \\nin  a  Smart  Home  may  be \\nsmartHome.lightsOn, \\nand \\nsmartHome.getStatus.  In  this  scenario,  other  parameters  may \\nalso  be  defined  such  as  location  (eg.  Dining  room),  time \\nstart/end (eg. 10am, Thursday etc.), schedule (eg. every hour, \\nevery minute) etc. \\n\\nsmartHome.doorLock, \\n\\nthe  actions \\n\\n \\nOnce the action has been set and the require parameters have \\nalso been defined, the correct intent can be mapped to an IoT \\nAPI endpoint and a HTTP request is made. \\n \\n\\nChatbots  can  also  be  built  using  existing  frameworks \\nsimplify  the  end-to-end  process  of  creating  and  integrating \\nChatbots \\ninto  messaging  and  IoT  platforms.  The  key \\nadvantages  of  using  a  Chatbot  framework  are:  (1)  Ease  of \\nDevelopment from pre-defined actions,  integrations and SDK \\nsupport  for  various  IoT  systems  (2)  Ability  to  \\xe2\\x80\\x98write  once \\ndeploy  anywhere\\xe2\\x80\\x99  through  integrations  with  multiple  Chatbot \\nChannels.  (3)  Ability  to  use  AI-as-a-service.  For  example, \\nLUIS.ai in the case of Microsoft Bot Framework. Other popular \\nChatbot frameworks include API.ai, Microsoft Bot Framework \\nand IBM Watson Conversation Service. \\n\\nV.  FUTURE RESEARCH AREAS AND CONCLUSION \\n\\n \\nIoT  is  poised  to  become  intrinsic  to  the  day  to  day \\nactivities in the future, however the dynamic nature of IoT has \\nits  share  of  difficulties,  and  this  paper  has  put  forward  the \\nconcept of using Chatbots to address some of the challenges in \\nIoT.  Through  this  initial  endeavor  we  can  identify  possible \\nareas  that  can  be  worked  on  for  the  future  which  show  great \\npotential: \\n\\n1)  Stronger AI based Agents \\n\\n \\nAs  more  advances  are  made  in  the  field  of  AI, \\nSoftware Agents will also grow to become more intelligent in \\nthe  future.  The  goal  of  Strong  AI  has  been  to  match  the \\nmachine\\xe2\\x80\\x99s intellectual capability to a human being. Immediate \\nresearch challenges includes improving decision making ability \\nto create more autonomous Chatbots and better NLP as well as \\nNatural  Language  Generation  models  to  create  more  natural \\nflows of conversation between humans and bots. Chatbots will \\nplay an important role in the research areas of Intelligent Agents \\nas well as Machine-to-Machine (M2M) research in IoT. \\n \\n\\n\\x0c2)  Cyber Physical Systems and IoT \\n\\n \\nCyber Physical Systems refer to more advanced, next \\ngeneration  embedded \\nInformation  and  Communications \\nTechnology (ICT) systems. They share many similarities with \\nIoT  but  with  higher  combination  and  coordination  between \\nphysical  and  computational  elements  [28].  The  US  National \\nScience  Foundation  (NSF)  identified  cyber-physical  systems \\namong the key research areas in the foreseeable future [29]. IoT \\nwill play a major role in the transition to CPS as one of the key \\nenabling  technologies  [16,  30].  Further  advancements  in  AI \\naspects  of  Chatbots  in  IoT  will  be  closely  related  to  the \\nConversion, Cyber, Cognition and Configuration levels of the \\n5C CPS Architecture [31]. \\n \\n\\n3)  Wisdom of Things \\n\\n \\nThe  progress  of  Chatbots  in  IoT  introduces  the \\nparadigm  of  human-in-the-loop  systems  which  has  exciting \\nresearch  challenges  in  the  areas  of  Wisdom  of  Crowds.  The \\nconcept  of  wisdom  of  crowd  suggests  that  aggregation  of \\ninformation  can  result  in  decisions  that  are  better  than  what \\ncould have been achieved by any individual in the group [32]. \\nIn the context of IoT, the sharing of big data from billions of \\nsensors  and  devices  creates  more  value  in  the  ecosystem  as \\ncompared \\nrequires  data \\ninteroperability  rather  than  simply  accumulating  multiple \\ndisparate  data  sources  which  are  incompatible  or  have  no \\nsimilarities. Hence Chatbots in IoT systems can use techniques \\nsuch  as  Human  Swarming,  an  approach  that  uses  real-time \\nfeedback loops from groups of users to make accurate insights. \\nThere  are  plenty  of  interesting  research  opportunities  in \\nacquiring accurate values from the crowd. \\n \\n\\nto  not  sharing.  However \\n\\nit \\n\\n4)  Evolution of the Semantic Web \\n\\nimpact  on  areas \\n\\n \\nAs  the  Internet  itself  changes,  there  are  many  more \\nopportunities  for  exciting  research  in  the  areas  of  IoT  and \\nSoftware Agents. The development of a Web 3.0 or Semantic \\nWeb and its impact on the future of Software Agents has been \\nclearly  described  in  the  literature  [33].  This  poses  a  great \\nopportunity  for  research  in  IoT  in  terms  of  Semantic \\ninteroperability  which  can  have  major  impact  on  the  IoT \\nparadigm itself. The evolution of the Semantic Web will have \\nmajor \\nin  pervasive  computing,  M2M \\ntechnologies  resulting  in  Software  Agents  being  able  to draw \\nmore value and achieve a higher level of wisdom than before. \\n \\nDevelopment in the field of IoT has been phenomenal \\n \\nin recent times. Similarly, Chatbot systems are also becoming \\nmore intelligent and sophisticated as the days progress. To the \\nbest  of  our  knowledge,  no  work  has  been  published  detailing \\nthe  specific  integration  of  Chatbots  to  IoT.  This  paper  has \\nattempted to integrate these two fields together by enlisting the \\nkey  architectural  components  required  and  envision  possible \\nways to address some of the present challenges in IoT. We hope \\nthat  this  endeavor  will  lead  to  more  intelligent  and  effective \\nintegrated IoT systems. \\n\\n \\n\\nVI.  REFERENCES \\n\\n \\n\\n[1]  Atzori,  L.,  Iera,  A.  and  Morabito,  G.,  2010.  The  internet  of  things:  A \\nsurvey.Computer  networks,  54(15),  pp.2787-2805  J.  Clerk  Maxwell,  A \\nTreatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, \\n1892, pp.68\\xe2\\x80\\x9373. \\n\\n[2]  Evans, D. \"The Internet of Things How the Next Evolution of the Internet \\n\\nis Changing Everything (April 2011).\" (2012): 346-360. \\n\\n[3]  van der Meulen, R., 2015. Gartner Says 6.4 Billion Connected \\xe2\\x80\\x98Things\\xe2\\x80\\x99 \\n\\nWill Be in Use in 2016, Up 30 Percent From 2015. Stamford, Conn  \\n\\n[4] \\n\\nITU-T  Recommendation \\nITU, \\nAvailable:http://handle.itu.int/11.1002/1000/11559  \\n\\ndatabase\", \\n\\n2016. \\n\\n[Online] \\n\\n[5]  Guinard, D., Trifa, V., Mattern, F., & Wilde, E. (2011). From the internet \\nof  things  to  the  web  of  things:  Resource-oriented  architecture  and  best \\npractices.  In  Architecting  the  Internet  of  Things  (pp.  97-129).  Springer \\nBerlin Heidelberg.  \\n\\n[6]  Guinard,  Dominique;  Vlad,  Trifa  (2015). Building  the  Web  of  Things. \\n\\nManning. ISBN 9781617292682. \\n\\n[7]  Vermesan, O.,et al.., 2011. Internet of things strategic research roadmap. \\nO. Vermesan, P. Friess, P. Guillemin, S. Gusmeroli, H. Sundmaeker, A. \\nBassi,  et  al.,  Internet  of  Things:  Global  Technological  and  Societal \\nTrends, 1, pp.9-52 \\n\\n[8]  Guinard,  D.,  Ion,  I.  and  Mayer,  S.,  2011,  December.  In  search  of  an \\ninternet  of  things  service  architecture:  REST  or  WS-*?  A  developers\\xe2\\x80\\x99 \\nperspective. International Conference on Mobile and Ubiquitous Systems: \\nComputing,  Networking,  and  Services  (pp.  326-337).  Springer  Berlin \\nHeidelberg \\n\\n[9]  Evansdata.com.  (2016).  Evans  Data  Corporation  |  Internet  of  Things  \\xe2\\x80\\x93 \\nat: \\n\\nVertical \\nhttp://www.evansdata.com/reports/viewRelease.php?reportID=38 \\n\\nAvailable \\n\\nResearch \\n\\nService. \\n\\n[10]  Hewitt,  C.,  1977.  Viewing  control  structures  as  patterns  of  passing \\n\\nmessages. Artificial intelligence, 8(3), pp.323-364 \\n\\n[11]  Nwana,  Hyacinth  S.  \"Software  agents:  An  overview.\"  The  knowledge \\n\\nengineering review 11, no. 03 (1996): 205-244 \\n\\n[12]  Schermer,  Bart  Willem.  Software  agents,  surveillance,  and  the  right  to \\nprivacy:  a  legislative  framework  for  agent-enabled  surveillance.  Leiden \\nUniversity Press, 2007 \\n\\n[13]  Russell, Stuart Jonathan, Peter Norvig, John F. Canny, Jitendra M. Malik, \\nand  Douglas  D.  Edwards.  \\xe2\\x80\\x9cArtificial  intelligence:  a  modern  approach\\xe2\\x80\\x9d. \\nVol. 2. Upper Saddle River: Prentice hall, 2003. \\n\\n[14]  Broadband Commission, 2014. The state of broadband 2014: Broadband \\n\\nfor all. Geneva, Switzerland: The United Nations \\n\\n[15]  S. Liang, \"SensorThings API - connecting IoT devices, their location and \\ntheirdata,\"2016.Available:http://www.eclipse.org/community/eclipse_ne\\nwsletter/2016/march/article2.php \\n\\n[16]  Miorandi, D., Sicari, S., De Pellegrini, F. and Chlamtac, I., 2012. Internet \\nof  things:  Vision,  applications  and  research  challenges.  Ad  Hoc \\nNetworks, 10(7), pp.1497-1516 \\n\\n[17]  Celesti, Antonio, Maria Fazio, Maurizio Giacobbe, Antonio Puliafito, and \\nMassimo Villari. \"Characterizing Cloud Federation in IoT.\" In 2016 30th \\nInternational  Conference  on  Advanced  Information  Networking  and \\nApplications Workshops (WAINA), pp. 93-98. IEEE, 2016 \\n\\n[18]  M.  Wallace,  \"Fragmentation  is  the  enemy  of  the  Internet  of  Things  | \\n\\nQualcomm\",Qualcomm, 2016 \\n\\n[19]  M.  Littman  and  S.  Kortchmar,  \"The  path  to  A  programmable  world,\" \\n2014.Available:http://footnote1.com/the-path-to-a-programmable-world/ \\n\\n[20]  W. Mckitterick, \"The Messaging App Report: How instant Messaging can \\n\\nbe monetized,\" Business Insider \\n\\n[21]  Rowley, Jennifer E. \"The wisdom hierarchy: representations of the DIKW \\n\\nhierarchy.\" Journal of information science (2007) \\n\\n[22]  Barnaghi, P., Wang, W., Henson, C. and Taylor, K., 2012. Semantics for \\nthe Internet of Things: early progress and back to the future. International \\nJournal on Semantic Web and Information Systems (IJSWIS), 8(1), pp.1-\\n21 \\n\\n \\n\\n\\x0c[23]  Bandyopadhyay,  Debasis,  and  Jaydip  Sen.  \"Internet  of \\n\\nthings: \\nApplications and challenges in technology and standardization.\" Wireless \\nPersonal Communications 58, no. 1 (2011): 49-69 \\n\\n[29]  Wolf,  Wayne  (November  2007).  \"The  Good  News  and  the  Bad  News \\n(Embedded  Computing  Column\".IEEE  Computer.  40  (11):  104\\xe2\\x80\\x93\\n105.doi:10.1109/MC.2007.404 \\n\\n \\n\\n[24]  Vinyals,  Oriol,  and  Quoc  Le.  \"A  neural  conversational  model.\"  arXiv \\n\\npreprint arXiv:1506.05869 (2015) \\n\\n[25]  Google,  \"Overview  of  Internet  of  things,\"  Google  Developers,  2016. \\n\\nAvailable: https://cloud.google.com/solutions/iot-overview \\n\\n[26]  Microsoft, \"LUIS: Help,\" 2016. Available: https://www.luis.ai/Help \\n\\n[27]  API.ai,\"Api.ai\"2016. Available: https://docs.api.ai \\n\\n[28]  Rad,  C.R.,  Hancu,  O.,  Takacs,  I.A.  and  Olteanu,  G.,  2015.  Smart \\nmonitoring of potato crop: a cyber-physical system architecture model in \\nthe  field  of  precision  agriculture.  Agriculture  and  Agricultural  Science \\nProcedia, 6, pp.73-79. \\n\\n[30]  Wan, J., Chen, M., Xia, F., Li, D. and Zhou, K., 2013. From machine-to-\\nmachine communications towards cyber-physical systems. Comput. Sci. \\nInf. Syst., 10(3), pp.1105-1128 \\n\\n[31]  Lee,  J.,  Bagheri,  B.  and  Kao,  H.A.,  2015.  A  cyber-physical  systems \\nsystems. \\n\\n4.0-based  manufacturing \\n\\nfor \\n\\narchitecture \\nManufacturing Letters, 3, pp.18-23 \\n\\nindustry \\n\\n[32]  Joe  Barkai.  (2016).  Wisdom  of  Things  -  Joe  Barkai.  Available  at: \\n\\nhttp://joebarkai.com/wisdom-of-things \\n\\n[33]  Berners-Lee,  T.,  Hendler,  J.  and  Lassila,  O.,  2001.  The  semantic  web. \\n\\nScientific american, 284(5), pp.28-37 \\n\\n \\n\\n \\n\\n\\x0c',\n",
       " 'Visual Dialog\\n\\nAbhishek Das1, Satwik Kottur2, Khushi Gupta2\\xe2\\x88\\x97, Avi Singh3\\xe2\\x88\\x97, Deshraj Yadav1, Jos\\xc3\\xa9 M.F. Moura2,\\n\\n1Virginia Tech, 2Carnegie Mellon University, 3UC Berkeley, 4Georgia Institute of Technology\\n\\nDevi Parikh4, Dhruv Batra4\\n\\n6\\n1\\n0\\n2\\n \\nc\\ne\\nD\\n5\\n\\n \\n\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n2\\nv\\n9\\n6\\n6\\n8\\n0\\n\\n.\\n\\n1\\n1\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\n1{abhshkdz, deshraj}@vt.edu\\n\\n2{skottur, khushig, moura}@andrew.cmu.edu\\n\\n3avisingh@cs.berkeley.edu\\n\\n4{parikh, dbatra}@gatech.edu\\n\\nvisualdialog.org\\n\\nAbstract\\n\\nWe introduce the task of Visual Dialog, which requires an\\nAI agent to hold a meaningful dialog with humans in natu-\\nral, conversational language about visual content. Speci\\xef\\xac\\x81-\\ncally, given an image, a dialog history, and a question about\\nthe image, the agent has to ground the question in image,\\ninfer context from history, and answer the question accu-\\nrately. Visual Dialog is disentangled enough from a speci\\xef\\xac\\x81c\\ndownstream task so as to serve as a general test of ma-\\nchine intelligence, while being grounded in vision enough\\nto allow objective evaluation of individual responses and\\nbenchmark progress. We develop a novel two-person chat\\ndata-collection protocol to curate a large-scale Visual Dia-\\nlog dataset (VisDial). Data collection is underway and on\\ncompletion, VisDial will contain 1 dialog with 10 question-\\nanswer pairs on all \\xe2\\x88\\xbc200k images from COCO, with a total\\nof 2M dialog question-answer pairs.\\nWe introduce a family of neural encoder-decoder models for\\nVisual Dialog with 3 encoders \\xe2\\x80\\x93 Late Fusion, Hierarchical\\nRecurrent Encoder and Memory Network \\xe2\\x80\\x93 and 2 decoders\\n(generative and discriminative), which outperform a num-\\nber of sophisticated baselines. We propose a retrieval-based\\nevaluation protocol for Visual Dialog where the AI agent is\\nasked to sort a set of candidate answers and evaluated on\\nmetrics such as mean-reciprocal-rank of human response.\\nWe quantify gap between machine and human performance\\non the Visual Dialog task via human studies. Our dataset,\\ncode, and trained models will be released publicly. Putting\\nit all together, we demonstrate the \\xef\\xac\\x81rst \\xe2\\x80\\x98visual chatbot\\xe2\\x80\\x99!\\n\\n1. Introduction\\n\\nWe are witnessing unprecedented advances in computer vi-\\nsion (CV) and arti\\xef\\xac\\x81cial intelligence (AI) \\xe2\\x80\\x93 from \\xe2\\x80\\x98low-level\\xe2\\x80\\x99\\nAI tasks such as image classi\\xef\\xac\\x81cation [14], scene recogni-\\n\\n\\xe2\\x88\\x97Work done while KG and AS were interns at Virginia Tech.\\n\\nFigure 1: We introduce a new AI task \\xe2\\x80\\x93 Visual Dialog, where an AI\\nagent must hold a dialog with a human about visual content. We\\nintroduce a large-scale dataset (VisDial), an evaluation protocol,\\nand novel encoder-decoder models for this task.\\n\\ntion [55], object detection [27] \\xe2\\x80\\x93 to \\xe2\\x80\\x98high-level\\xe2\\x80\\x99 AI tasks\\nsuch as learning to play Atari video games [35] and Go [47],\\nanswering reading comprehension questions by understand-\\ning short stories [15, 57], and even answering questions\\nabout images [4, 32, 41] and videos [49, 50]!\\nWhat lies next for AI? We believe that the next genera-\\ntion of visual intelligence systems will need to posses the\\nability to hold a meaningful dialog with humans in natural\\nlanguage about visual content. Applications include:\\n\\xe2\\x80\\xa2 Aiding visually impaired users in understanding their sur-\\nroundings [5] or social media content [58] (AI: \\xe2\\x80\\x98John just\\nuploaded a picture from his vacation in Hawaii\\xe2\\x80\\x99, Human:\\n\\xe2\\x80\\x98Great, is he at the beach?\\xe2\\x80\\x99, AI: \\xe2\\x80\\x98No, on a mountain\\xe2\\x80\\x99).\\n\\xe2\\x80\\xa2 Aiding analysts in making decisions based on large quan-\\ntities of surveillance data (Human: \\xe2\\x80\\x98Did anyone enter this\\n\\n1\\n\\n\\x0c\\xe2\\x80\\x98What is she doing?\\xe2\\x80\\x99\\n\\nConsider the Visual Dialog examples in Fig. 2. The ques-\\ntion \\xe2\\x80\\x98What is the gender of the one in the white shirt?\\xe2\\x80\\x99\\nrequires the machine to selectively focus and direct atten-\\ntion to a relevant region.\\nrequires\\nco-reference resolution (whom does the pronoun \\xe2\\x80\\x98she\\xe2\\x80\\x99 re-\\nfer to?), \\xe2\\x80\\x98Is that a man to her right?\\xe2\\x80\\x99 further requires the\\nmachine to have visual memory (which object in the im-\\nage were we talking about?). Such systems also need to\\nbe consistent with their outputs \\xe2\\x80\\x93 \\xe2\\x80\\x98How many people are\\nin wheelchairs?\\xe2\\x80\\x99, \\xe2\\x80\\x98Two\\xe2\\x80\\x99, \\xe2\\x80\\x98What are their genders?\\xe2\\x80\\x99, \\xe2\\x80\\x98One\\nmale and one female\\xe2\\x80\\x99 \\xe2\\x80\\x93 note that the number of genders be-\\ning speci\\xef\\xac\\x81ed should add up to two. Such dif\\xef\\xac\\x81culties make\\nthe problem a highly interesting and challenging one.\\nWhy do we talk to machines? Prior work in language-only\\n(non-visual) dialog can be arranged on a spectrum with the\\nfollowing two end-points:\\ngoal-driven dialog (e.g. booking a \\xef\\xac\\x82ight for a user) \\xe2\\x86\\x90\\xe2\\x86\\x92\\ngoal-free dialog (or casual \\xe2\\x80\\x98chit-chat\\xe2\\x80\\x99 with chatbots).\\nThe two ends have vastly differing purposes and con\\xef\\xac\\x82icting\\nevaluation criteria. Goal-driven dialog is typically evaluated\\non task-completion rate (how frequently was the user able to\\nbook their \\xef\\xac\\x82ight) or time to task completion [9,36] \\xe2\\x80\\x93 clearly,\\nthe shorter the dialog the better. In contrast, for chit-chat the\\nlonger the user engagement and interaction, the better. For\\ninstance, the goal of the 2017 $2.5 Million Amazon Alexa\\nPrize is to \\xe2\\x80\\x9ccreate a socialbot that converses coherently and\\nengagingly with humans on popular topics for 20 minutes.\\xe2\\x80\\x9d\\nWe believe our instantiation of Visual Dialog hits a sweet\\nIt is disentangled enough from a\\nspot on this spectrum.\\nspeci\\xef\\xac\\x81c downstream task so as to serve as a general test of\\nmachine intelligence, while being grounded enough in vi-\\nsion to allow objective evaluation of individual responses\\nand benchmark progress. The former discourages task-\\nengineered bots for \\xe2\\x80\\x98slot \\xef\\xac\\x81lling\\xe2\\x80\\x99 [23] and the latter discour-\\nages bots that put on a personality to avoid answering ques-\\ntions while keeping the user engaged [56].\\nContributions. We make the following contributions:\\n\\xe2\\x80\\xa2 We propose a new AI task:Visual Dialog,where a machine\\nmust hold a dialog with a human about visual content.\\n\\xe2\\x80\\xa2 We develop a novel two-person chat data-collection pro-\\ntocol to curate a large-scale Visual Dialog dataset (Vis-\\nDial). Upon completion1, VisDial will contain 1 dialog\\nwith 10 question-answer pairs on all 200k images from\\nCOCO [25], for a total of 2M dialog question-answer\\npairs. When compared to the popular VQA dataset [4],\\nVisDial studies a signi\\xef\\xac\\x81cantly richer task (dialog), over-\\ncomes a \\xe2\\x80\\x98visual priming bias\\xe2\\x80\\x99 in VQA (in VisDial, the per-\\nson asking the questions does not see the image), contains\\nfree-form longer answers (mean length: 3.1 words in Vis-\\nDial vs. 1.1 in VQA), and is an order of magnitude larger.\\n\\n1Data collection is underway. As of this submission, VisDial has 68k\\n\\ndialogs on 68k images.\\n\\nFigure 2: Differences between image captioning, Visual Question\\nAnswering (VQA) and Visual Dialog. Two (partial) dialogs are\\nshown from our VisDial dataset, which is curated from a live chat\\nbetween two Amazon Mechanical Turk workers (Sec. 3).\\n\\nroom last week?\\xe2\\x80\\x99, AI: \\xe2\\x80\\x98Yes, 27 instances logged on cam-\\nera\\xe2\\x80\\x99, Human: \\xe2\\x80\\x98Were any of them carrying a black bag?\\xe2\\x80\\x99),\\n\\xe2\\x80\\xa2 Interacting with an AI assistant (Human: \\xe2\\x80\\x98Alexa \\xe2\\x80\\x93 can\\nyou see the baby in the baby monitor?\\xe2\\x80\\x99, AI: \\xe2\\x80\\x98Yes, I can\\xe2\\x80\\x99,\\nHuman: \\xe2\\x80\\x98Is he sleeping or playing?\\xe2\\x80\\x99).\\n\\xe2\\x80\\xa2 Robotics applications (e.g. search and rescue missions)\\nwhere the operator may be \\xe2\\x80\\x98situationally blind\\xe2\\x80\\x99 and oper-\\nating via language [33] (Human: \\xe2\\x80\\x98Is there smoke in any\\nroom around you?\\xe2\\x80\\x99, AI: \\xe2\\x80\\x98Yes, in one room\\xe2\\x80\\x99, Human: \\xe2\\x80\\x98Go\\nthere and look for people\\xe2\\x80\\x99).\\nDespite rapid progress at the intersection of vision and lan-\\nguage \\xe2\\x80\\x93 in particular, in image captioning and visual ques-\\ntion answering (VQA) \\xe2\\x80\\x93 it is clear that we are far from this\\ngrand goal of an AI agent that can \\xe2\\x80\\x98see\\xe2\\x80\\x99 and \\xe2\\x80\\x98communicate\\xe2\\x80\\x99.\\nIn captioning, the human-machine interaction consists of\\nthe machine simply talking at the human (\\xe2\\x80\\x98Two people are\\nin a wheelchair and one is holding a racket\\xe2\\x80\\x99), with no dia-\\nlog or input from the human. While VQA takes a signi\\xef\\xac\\x81cant\\nstep towards human-machine interaction, it still represents\\nonly a single round of a dialog \\xe2\\x80\\x93 unlike in human conver-\\nsations, there is no scope for follow-up questions, no mem-\\nory in the system of previous questions asked by the user\\nnor consistency with respect to previous answers provided\\nby the system (Q: \\xe2\\x80\\x98How many people on wheelchairs?\\xe2\\x80\\x99, A:\\n\\xe2\\x80\\x98Two\\xe2\\x80\\x99; Q: \\xe2\\x80\\x98How many wheelchairs?\\xe2\\x80\\x99, A: \\xe2\\x80\\x98One\\xe2\\x80\\x99).\\nAs a step towards conversational visual AI, we introduce\\na novel task \\xe2\\x80\\x93 Visual Dialog \\xe2\\x80\\x93 along with a large-scale\\ndataset, an evaluation protocol, and novel deep models.\\nTask De\\xef\\xac\\x81nition. The concrete task in Visual Dialog is the\\nfollowing \\xe2\\x80\\x93 given an image I, a history of a dialog con-\\nsisting of a sequence of question-answer pairs (Q1: \\xe2\\x80\\x98How\\nmany people are in wheelchairs?\\xe2\\x80\\x99, A1: \\xe2\\x80\\x98Two\\xe2\\x80\\x99, Q2: \\xe2\\x80\\x98What\\nare their genders?\\xe2\\x80\\x99, A2: \\xe2\\x80\\x98One male and one female\\xe2\\x80\\x99), and\\na natural language follow-up question (Q3: \\xe2\\x80\\x98Which one is\\nholding a racket?\\xe2\\x80\\x99), the task for the machine is to answer the\\nquestion in free-form natural language (A3: \\xe2\\x80\\x98The woman\\xe2\\x80\\x99).\\nThis task is the visual analogue of the Turing Test.\\n\\n\\x0c\\xe2\\x80\\xa2 We introduce a family of neural encoder-decoder models\\nfor Visual Dialog with 3 novel encoders\\n\\xe2\\x80\\x93 Late Fusion: that embeds the image, history, and ques-\\ntion into vector spaces separately and performs a \\xe2\\x80\\x98late\\nfusion\\xe2\\x80\\x99 of these into a joint embedding.\\n\\xe2\\x80\\x93 Hierarchical Recurrent Encoder: that contains a dialog-\\nlevel Recurrent Neural Network (RNN) sitting on top of\\na question-answer (QA)-level recurrent block. In each\\nQA-level recurrent block, we also include an attention-\\nover-history mechanism to choose and attend to the\\nround of the history relevant to the current question.\\n\\xe2\\x80\\x93 Memory Network: that treats each previous QA pair as\\na \\xe2\\x80\\x98fact\\xe2\\x80\\x99 in its memory bank and learns to \\xe2\\x80\\x98poll\\xe2\\x80\\x99 the stored\\nfacts and the image to develop a context vector.\\nWe train all these encoders with 2 decoders (generative\\nand discriminative) \\xe2\\x80\\x93 all settings outperform a number of\\nsophisticated baselines, including our adaption of state-of-\\nart VQA models to VisDial.\\n\\xe2\\x80\\xa2 We propose a retrieval-based evaluation protocol for Vi-\\nsual Dialog where the AI agent is asked to sort a list of\\ncandidate answers and evaluated on metrics such as mean-\\nreciprocal-rank of the human response.\\n\\xe2\\x80\\xa2 We conduct studies to quantify human performance on\\nthis task.\\n\\xe2\\x80\\xa2 Putting it all together, we demonstrate a visual chatbot in\\na video at https://goo.gl/yjlHxY that answers a\\nsequence of questions from a user about an image!\\n\\n2. Related Work\\n\\nVision and Language. A number of problems at the inter-\\nsection of vision and language have recently gained promi-\\nnence \\xe2\\x80\\x93 image captioning [10, 11, 20, 54], video/movie\\ndescription [43, 51, 52], text-to-image coreference/ground-\\ning [16, 22, 37, 39, 42], Visual Madlibs [60], and of course,\\nvisual question answering (VQA) [4, 12, 31, 32, 41]. How-\\never, all of these works involve a single-shot natural lan-\\nguage interaction with visual content \\xe2\\x80\\x93 there is no dialog.\\nVisual Turing Test. Perhaps the most closely related to\\nour work is that of Geman et al. [13], who proposed a \\xe2\\x80\\x98Vi-\\nsual Turing Test\\xe2\\x80\\x99 in a fairly restricted setting. First, ques-\\ntions in [13] were automatically generated from 4 kinds of\\n\\xef\\xac\\x81xed templates (existence, uniqueness, attributes, relation-\\nships), while our dataset has free form, open-ended natu-\\nral language questions collected via two workers chatting\\non Amazon Mechanical Turk (AMT), resulting in a much\\nmore realistic and diverse dataset (see Fig. 5). Second, all\\nquestions in [13] were binary (yes/no), while our chat inter-\\nface results in a rich set of free-form responses (see Fig. 5).\\nThird, the dataset in [13] only contains street scenes, while\\nour dataset has considerably more variety since it uses im-\\nages from the COCO dataset [25]. Moreover, our dataset is\\nplanned to be two orders of magnitude larger \\xe2\\x80\\x93 2,591 im-\\n\\nages in [13] vs \\xe2\\x88\\xbc200k images, 10 question-answer pairs, a\\ntotal of \\xe2\\x88\\xbc2M question-answer pairs. Finally, the focus of\\nGeman et al. [13] is a statistical templated-question gener-\\nator and not an actual visual dialog system. We propose\\nseveral deep models for Visual Dialog, an evaluation proto-\\ncol, and conduct human accuracy studies for this task.\\nText-based Question Answering. Our work is related\\nto text-based question answering or \\xe2\\x80\\x98reading comprehen-\\nsion\\xe2\\x80\\x99 tasks studied in the NLP community. Some recent\\nlarge-scale datasets in this domain include the 30M Fac-\\ntoid Question-Answer corpus [44], 100K SimpleQuestions\\ndataset [6], DeepMind Q&A dataset [15], the 20 arti\\xef\\xac\\x81cial\\ntasks in the bAbI dataset [57], and the SQuAD dataset for\\nreading comprehension [38]. VisDial can be viewed as a\\nfusion of reading comprehension and VQA. In VisDial, the\\nmachine must comprehend the history of the past dialog and\\nthen understand the image to answer the question. By de-\\nsign, the answer to any question in VisDial is not present in\\nthe past dialog \\xe2\\x80\\x93 if it were, the question would not be asked.\\nThe history of the dialog contextualizes the question \\xe2\\x80\\x93 the\\nquestion \\xe2\\x80\\x98what else is she holding?\\xe2\\x80\\x99 requires a machine to\\ncomprehend the history to realize who the question is talk-\\ning about and what has been excluded, and then understand\\nthe image to answer the question.\\nConversational Modeling and Chatbots. Visual Dialog is\\nthe visual analogue of text-based dialog and conversation\\nmodeling. While some of the earliest developed chatbots\\nwere rule-based [56], end-to-end learning based approaches\\nare now being actively explored [7,9,19,24,45,46,53]. A re-\\ncent large-scale conversation dataset is the Ubuntu Dialogue\\nCorpus [28], which contains about 500K dialogs extracted\\nfrom the Ubuntu channel on Internet Relay Chat (IRC). Liu\\net al. [26] perform a study of problems in existing evalua-\\ntion protocols for free-form dialog. One important differ-\\nence between free-form textual dialog and VisDial is that in\\nVisDial, the two participants are not symmetric \\xe2\\x80\\x93 one per-\\nson (the \\xe2\\x80\\x98questioner\\xe2\\x80\\x99) asks questions about an image that\\nthey do not see; the other person (the \\xe2\\x80\\x98answerer\\xe2\\x80\\x99) sees the\\nimage and only answers the questions (in otherwise uncon-\\nstrained text, but no counter-questions allowed). This role\\nassignment gives a sense of purpose to the interaction (why\\nare we talking? To help the questioner build a mental model\\nof the image), and allows objective evaluation of individual\\nresponses. Next, we describe our data collection protocol.\\n\\n3. The Visual Dialog Dataset (VisDial)\\n\\nWe now describe our VisDial dataset. We begin by describ-\\ning the chat interface and data-collection process on AMT,\\nanalyze the dataset,and then discuss the evaluation protocol.\\nConsistent with previous data collection efforts, we collect\\nvisual dialog data on images from the Common Objects in\\n\\n\\x0c(a) What the \\xe2\\x80\\x98questioner\\xe2\\x80\\x99 sees.\\n\\n(c) Example dialog from our VisDial dataset.\\nFigure 3: Collecting visually-grounded dialog data on Amazon Mechanical Turk via a live chat interface where one person is assigned the\\nrole of \\xe2\\x80\\x98questioner\\xe2\\x80\\x99 and the second person is the \\xe2\\x80\\x98answerer\\xe2\\x80\\x99. We show the \\xef\\xac\\x81rst two questions being collected via the interface as Turkers\\ninteract with each other in Fig. 3a and Fig. 3b. Remaining questions are shown in Fig. 3c.\\n\\n(b) What the \\xe2\\x80\\x98answerer\\xe2\\x80\\x99 sees.\\n\\nContext (COCO) [25] dataset, which contains multiple ob-\\njects in everyday scenes. The visual complexity of these\\nimages allows for engaging and diverse conversations to be\\nheld about them.\\nLive Chat Interface. Good data for this task should in-\\nclude dialogs that have (1) temporal continuity, (2) ground-\\ning in the image, and (3) mimic natural \\xe2\\x80\\x98conversational\\xe2\\x80\\x99\\nexchanges. To elicit such responses, we paired 2 work-\\ners on AMT to chat with each other in real-time (Fig. 3).\\nEach worker was assigned a speci\\xef\\xac\\x81c role. One worker (the\\n\\xe2\\x80\\x98questioner\\xe2\\x80\\x99) sees only a single line of text describing an\\nimage (caption from COCO); the image remains hidden to\\nthe questioner. Their task is to ask questions about this hid-\\nden image so as to \\xe2\\x80\\x98imagine the scene better\\xe2\\x80\\x99. The sec-\\nond worker (the \\xe2\\x80\\x98answerer\\xe2\\x80\\x99) sees the image and the cap-\\ntion. Their task is to answer the questions asked by their\\nchat partner. Unlike VQA [4], answers are not restricted\\nto be short or concise, instead workers will be encouraged\\nto reply as naturally and \\xe2\\x80\\x98conversationally\\xe2\\x80\\x99 as possible. An\\nexample dialog is shown in Fig. 3c.\\nThis process is an unconstrained \\xe2\\x80\\x98live\\xe2\\x80\\x99 chat, with the only\\nexception that the questioner must wait to receive an answer\\nbefore posting the next question. The workers are allowed\\nto end the conversation after 20 messages are exchanged (10\\npairs of questions and answers). Further details about our\\n\\xef\\xac\\x81nal interface can be found in the supplement.\\nWe also piloted a different setup where the questioner saw a\\nhighly blurred version of the image, instead of the caption.\\nThe conversations seeded with blurred images resulted in\\nquestions that were essentially \\xe2\\x80\\x98blob recognition\\xe2\\x80\\x99 \\xe2\\x80\\x93 \\xe2\\x80\\x98What\\nis the pink patch at the bottom right?\\xe2\\x80\\x99. For our full-scale\\ndata-collection, we decided to seed with just the captions\\nsince it resulted in more \\xe2\\x80\\x98natural\\xe2\\x80\\x99 questions and more\\nclosely modeled the real-world applications discussed in\\nSection 1 where no visual signal is available to the human.\\n\\nBuilding a 2-person chat on AMT. Despite the popular-\\n\\nity of AMT as a data collection platform in computer vi-\\nsion, our setup had to design for and overcome some unique\\nchallenges \\xe2\\x80\\x93 the key issue being that AMT is simply not\\ndesigned for multi-user Human Intelligence Tasks (HITs).\\nHosting a live two-person chat on AMT meant that none of\\nthe Amazon tools could be used and we developed our own\\nbackend messaging and data-storage infrastructure based on\\nRedis messaging queues and Node.js. To support data qual-\\nity, we ensured that a worker could not chat with themselves\\n(using say, two different browser tabs) by maintaining a\\npool of worker IDs paired. To minimize wait time for one\\nworker while the second was being searched for, we ensured\\nthat there was always a signi\\xef\\xac\\x81cant pool of available HITs. If\\none of the workers abandoned a HIT (or was disconnected)\\nmidway, automatic conditions in the code kicked in asking\\nthe remaining worker to either continue asking questions or\\nproviding facts (captions) about the image (depending on\\ntheir role) till 10 messages were sent by them. Workers who\\ncompleted the task in this way were fully compensated, but\\nour backend discarded this data and automatically launched\\na new HIT on this image so a real two-person conversation\\ncould be recorded. Our entire data-collection infrastructure\\n(front-end UI, chat interface, backend storage and messag-\\ning system, error handling protocols) will be publicly avail-\\nable to help future efforts.\\n\\n4. VisDial Dataset Analysis\\n\\nWe now analyze the v0.5 subset of our VisDial dataset col-\\nlected so far \\xe2\\x80\\x93 it contains 1 dialog (10 question-answer\\npairs) on 68k images from COCO (58k train and 10k\\nval), or a total of 680,000 QA pairs.\\n\\n4.1. Analyzing VisDial Questions\\n\\nVisual Priming Bias. One key difference between VisDial\\nand previous image question-answering datasets (VQA [4],\\nVisual 7W [62], Baidu mQA [12]) is the lack of a \\xe2\\x80\\x98vi-\\nsual priming bias\\xe2\\x80\\x99 in VisDial. Speci\\xef\\xac\\x81cally, in all previ-\\n\\n\\x0cties:\\n\\n\\xe2\\x80\\x98Is it full grown?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is it facing the camera?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 asking about the scene category or the picture setting:\\n\\xe2\\x80\\x98Is this indoors or outdoors?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is this a zoo?\\xe2\\x80\\x99,\\n\\n\\xe2\\x80\\xa2 the weather:\\n\\xe2\\x80\\xa2 simply exploring the scene:\\n\\n\\xe2\\x80\\x98Is it snowing?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is it sunny?\\xe2\\x80\\x99,\\n\\n\\xe2\\x80\\x98Are there people?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is there shelter for elephant?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 and asking follow-up questions about the new visual en-\\ntities discovered from these explorations:\\n\\n\\xe2\\x80\\x98There\\xe2\\x80\\x99s a blue fence in background, like an enclosure\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Is the enclosure inside or outside?\\xe2\\x80\\x99.\\n\\nSuch a line of questioning does not exist in the VQA dataset,\\nwhere the subjects were shown the questions already asked\\nabout an image, and explicitly instructed to ask about dif-\\nferent entities [4].\\n\\n4.2. Analyzing VisDial Answers\\n\\nAnswer Lengths. Fig. 7a shows the distribution of answer\\nlengths. Unlike previous datasets, answers in VisDial are\\nlonger and more descriptive \\xe2\\x80\\x93 mean-length 3.1 words (Vis-\\nDial) vs 1.1 (VQA), 2.0 (Visual 7W), 2.8 (Visual Madlibs).\\nFig. 7b shows the cumulative coverage of all answers (y-\\naxis) by the most frequent answers (x-axis). The difference\\nbetween VisDial and VQA is stark \\xe2\\x80\\x93 the top-1000 answers\\nin VQA cover \\xe2\\x88\\xbc83% of all answers, while in VisDial that\\n\\xef\\xac\\x81gure is only \\xe2\\x88\\xbc58%. There is a signi\\xef\\xac\\x81cant heavy tail in Vis-\\nDial \\xe2\\x80\\x93 most long strings are unique, and thus the coverage\\ncurve in Fig. 7b becomes a straight line with slope 1. In\\ntotal, there are 171,502 unique answers in VisDial.\\nAnswer Types. Since the answers in VisDial are longer\\nstrings, we can visualize their distribution based on the\\nstarting few words (Fig. 9). An interesting category of an-\\nswers emerges \\xe2\\x80\\x93 \\xe2\\x80\\x98I think so\\xe2\\x80\\x99, \\xe2\\x80\\x98I can\\xe2\\x80\\x99t tell\\xe2\\x80\\x99, or \\xe2\\x80\\x98I can\\xe2\\x80\\x99t see\\xe2\\x80\\x99 \\xe2\\x80\\x93\\nexpressing doubt, uncertainty, or lack of information. This\\nis a consequence of the questioner not being able to see the\\nimage \\xe2\\x80\\x93 they are asking contextually relevant questions, but\\nnot all questions may be answerable with certainty from\\nthat image. We believe this is rich data for building more\\nhuman-like AI that refuses to answer questions it doesn\\xe2\\x80\\x99t\\nhave enough information to answer. See [40] for a related,\\nbut complementary effort on question relevance in VQA.\\nBinary Questions vs Binary Answers.\\nIn VQA, binary\\nquestions are simply those with \\xe2\\x80\\x98yes\\xe2\\x80\\x99, \\xe2\\x80\\x98no\\xe2\\x80\\x99, \\xe2\\x80\\x98maybe\\xe2\\x80\\x99 as an-\\nswers [4]. In VisDial, we must distinguish between binary\\nquestions and binary answers. Binary questions are those\\nstarting in \\xe2\\x80\\x98Do\\xe2\\x80\\x99, \\xe2\\x80\\x98Did\\xe2\\x80\\x99, \\xe2\\x80\\x98Have\\xe2\\x80\\x99, \\xe2\\x80\\x98Has\\xe2\\x80\\x99, \\xe2\\x80\\x98Is\\xe2\\x80\\x99, \\xe2\\x80\\x98Are\\xe2\\x80\\x99, \\xe2\\x80\\x98Was\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Were\\xe2\\x80\\x99, \\xe2\\x80\\x98Can\\xe2\\x80\\x99, \\xe2\\x80\\x98Could\\xe2\\x80\\x99. Answers to such questions can (1)\\ncontain only \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99, (2) begin with \\xe2\\x80\\x98yes\\xe2\\x80\\x99, \\xe2\\x80\\x98no\\xe2\\x80\\x99, and\\ncontain additional information or clari\\xef\\xac\\x81cation, (3) involve\\n\\n(a)\\n\\n(b)\\n\\nFigure 4: Distribution of lengths for questions and answers (left);\\nand percent coverage of unique answers over all answers from the\\ntrain dataset (right), compared to VQA. For a given coverage, Vis-\\nDial has more unique answers indicating greater answer diversity.\\n\\nous datasets, subjects saw an image while asking questions\\nabout it. As described in [61], this leads to a particular\\nbias in the questions \\xe2\\x80\\x93 people only ask \\xe2\\x80\\x98Is there a clock-\\ntower in the picture?\\xe2\\x80\\x99 on pictures actually containing clock\\ntowers. This allows language-only models to perform re-\\nmarkably well on VQA and results in an in\\xef\\xac\\x82ated sense of\\nprogress [61]. As one particularly perverse example \\xe2\\x80\\x93 for\\nquestions in the VQA dataset starting with \\xe2\\x80\\x98Do you see a\\n. . . \\xe2\\x80\\x99, blindly answering \\xe2\\x80\\x98yes\\xe2\\x80\\x99 without reading the rest of the\\nquestion or looking at the associated image results in an av-\\nerage VQA accuracy of 87%! In VisDial, questioners do\\nnot see the image. As a result, this bias is reduced.\\nDistributions. Fig. 7a shows the distribution of question\\nlengths in VisDial \\xe2\\x80\\x93 we see that most questions range from\\nfour to ten words. Fig. 5 shows \\xe2\\x80\\x98sunbursts\\xe2\\x80\\x99 visualizing the\\ndistribution of questions (based on the \\xef\\xac\\x81rst four words) in\\nVisDial vs. VQA. While there are a lot of similarities, some\\ndifferences immediately jump out. There are more binary\\nquestions2 in VisDial as compared to VQA \\xe2\\x80\\x93 the most fre-\\nquent \\xef\\xac\\x81rst question-word in VisDial is \\xe2\\x80\\x98is\\xe2\\x80\\x99 vs. \\xe2\\x80\\x98what\\xe2\\x80\\x99 in\\nVQA. A detailed comparison of the statistics of VisDial vs.\\nother datasets is available in Table 1 in the supplement.\\nFinally, there is a stylistic difference in the questions that\\nis dif\\xef\\xac\\x81cult to capture with the simple statistics above.\\nIn\\nVQA, subjects saw the image and were asked to stump a\\nsmart robot. Thus, most queries involve speci\\xef\\xac\\x81c details, of-\\nten about the background (Q:\\xe2\\x80\\x98What program is being uti-\\nlized in the background on the computer?\\xe2\\x80\\x99).\\nIn VisDial,\\nquestioners did not see the original image and were asking\\nquestions to build a mental model of the scene. Thus, the\\nquestions tend to be open-ended, and often follow a pattern:\\n\\xe2\\x80\\xa2 Generally starting with the entities in the caption:\\n\\n\\xe2\\x80\\x98An elephant walking away from a pool in an exhibit\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Is there only 1 elephant?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 digging deeper into their parts, attributes, or proper-\\n\\n2 Questions starting in \\xe2\\x80\\x98Do\\xe2\\x80\\x99, \\xe2\\x80\\x98Did\\xe2\\x80\\x99, \\xe2\\x80\\x98Have\\xe2\\x80\\x99, \\xe2\\x80\\x98Has\\xe2\\x80\\x99, \\xe2\\x80\\x98Is\\xe2\\x80\\x99, \\xe2\\x80\\x98Are\\xe2\\x80\\x99, \\xe2\\x80\\x98Was\\xe2\\x80\\x99,\\n\\n\\xe2\\x80\\x98Were\\xe2\\x80\\x99, \\xe2\\x80\\x98Can\\xe2\\x80\\x99, \\xe2\\x80\\x98Could\\xe2\\x80\\x99.\\n\\n\\x0c(a) VisDial Questions\\n\\n(b) VQA Questions\\n\\n(c) VisDial Answers\\n\\nFigure 5: Distribution of \\xef\\xac\\x81rst n-grams for (left to right) VisDial questions, VQA questions and VisDial answers. The ordering of the words\\nstarts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas\\nare words with contributions too small to show.\\n\\nambiguity (\\xe2\\x80\\x98It\\xe2\\x80\\x99s hard to see\\xe2\\x80\\x99, \\xe2\\x80\\x98Maybe\\xe2\\x80\\x99), or (4) answer the\\nquestion without explicitly saying \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99 (Q: \\xe2\\x80\\x98Is there\\nany type of design or pattern on the cloth?\\xe2\\x80\\x99, A: \\xe2\\x80\\x98There are\\ncircles and lines on the cloth\\xe2\\x80\\x99). We call answers that con-\\ntain \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99 as binary answers \\xe2\\x80\\x93 149,367 and 76,346\\nanswers in subsets (1) and (2) from above respectively. Bi-\\nnary answers in VQA are biased towards \\xe2\\x80\\x98yes\\xe2\\x80\\x99 [4, 61] \\xe2\\x80\\x93\\n61.40% of yes/no answers are \\xe2\\x80\\x98yes\\xe2\\x80\\x99. In VisDial, the trend\\nis reversed. Only 46.96% are \\xe2\\x80\\x98yes\\xe2\\x80\\x99 for all yes/no responses.\\nThis is understandable since workers did not see the image,\\nand were more likely to end up with negative responses.\\n\\n4.3. Analyzing VisDial Dialog\\n\\nIn Section 4.1, we discussed a typical \\xef\\xac\\x82ow of dialog in Vis-\\nDial. We analyze two quantitative statistics here.\\nCoreference in dialog. Since language in VisDial is the re-\\nsult of a sequential conversation, it naturally contains pro-\\nnouns \\xe2\\x80\\x93 \\xe2\\x80\\x98he\\xe2\\x80\\x99, \\xe2\\x80\\x98she\\xe2\\x80\\x99, \\xe2\\x80\\x98his\\xe2\\x80\\x99, \\xe2\\x80\\x98her\\xe2\\x80\\x99, \\xe2\\x80\\x98it\\xe2\\x80\\x99, \\xe2\\x80\\x98their\\xe2\\x80\\x99, \\xe2\\x80\\x98they\\xe2\\x80\\x99, \\xe2\\x80\\x98this\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98that\\xe2\\x80\\x99, \\xe2\\x80\\x98those\\xe2\\x80\\x99, etc. In total, 38% of questions, 22% of an-\\nswers, and nearly all (99%) dialogs contain at least one\\npronoun, thus con\\xef\\xac\\x81rming that a machine will need to over-\\ncome coreference ambiguities to be successful on this task.\\nWe \\xef\\xac\\x81nd that pronoun usage is low in the \\xef\\xac\\x81rst round (as ex-\\npected) and then picks up in frequency. A \\xef\\xac\\x81ne-grained per-\\nround analysis is available in the supplement.\\nTemporal Continuity in Dialog Topics. It is natural for\\nconversational dialog data to have continuity in the \\xe2\\x80\\x98top-\\nics\\xe2\\x80\\x99 being discussed. We have already discussed qualitative\\ndifferences in VisDial questions vs. VQA. In order to quan-\\ntify the differences, we performed a human study where we\\nmanually annotated question \\xe2\\x80\\x98topics\\xe2\\x80\\x99 for 40 images (a total\\nof 400 questions), chosen randomly from the val set. The\\ntopic annotations were based on human judgement with a\\nconsensus of 4 annotators, with topics such as: asking about\\n\\na particular object (\\xe2\\x80\\x98What is the man doing?\\xe2\\x80\\x99) , scene (\\xe2\\x80\\x98Is it\\noutdoors or indoors?\\xe2\\x80\\x99), weather (\\xe2\\x80\\x9cIs the weather sunny?\\xe2\\x80\\x99),\\nthe image (\\xe2\\x80\\x98Is it a color image?\\xe2\\x80\\x99), and exploration (\\xe2\\x80\\x98Is there\\nanything else?\\xe2\\x80\\x9d). We performed similar topic annotation\\nfor questions from VQA for the same set of 40 images, and\\ncompared topic continuity in questions. Across 10 rounds,\\nVisDial question have 4.55 \\xc2\\xb1 0.17 topics on average, con-\\n\\xef\\xac\\x81rming that these are not independent questions. Recall\\nthat VisDial has 10 questions per image as opposed to 3 for\\nVQA. Therefore, for a fair comparison, we compute aver-\\nage number of topics in VisDial over all subsets of 3 succes-\\nsive questions. For 500 bootstrap samples of batch size 40,\\nVisDial has 2.14\\xc2\\xb1 0.05 topics while VQA has 2.53\\xc2\\xb1 0.09.\\nLower mean suggests there is more continuity in VisDial\\nbecause questions do not change topics as often.\\n\\n4.4. VisDial Evaluation Protocol\\n\\nOne fundamental challenge in dialog systems is evaluation.\\nIf the response by a system at a particular round is incorrect,\\nhow does the dialog proceed? Moreover, similar to the state\\nof affairs in captioning and machine translation, it is an open\\nproblem to automatically evaluate the quality of long free-\\nform answers since existing metrics such as BLEU, ME-\\nTEOR, and ROUGE are known to correlate poorly with hu-\\nman judgement in evaluating dialog responses [26].\\nInstead of evaluating success on a downstream task [7]\\nor holistically evaluating the entire conversation [3] (as in\\ngoal-free chit-chat), we evaluate individual responses inde-\\npendently at each round (t = 1, 2, . . . , 10) in a retrieval or\\nmultiple-choice setup.\\nSpeci\\xef\\xac\\x81cally, at test time, a VisDial system is given an im-\\nage I, the \\xe2\\x80\\x98ground-truth\\xe2\\x80\\x99 dialog history (including the im-\\nage caption) C, (Q1, A1), . . . , (Qt\\xe2\\x88\\x921, At\\xe2\\x88\\x921), the question\\nQt, and a list of N = 100 candidate answers, and asked\\n\\n\\x0cto return a sorting of the candidate answers. The model is\\nevaluated on retrieval metrics \\xe2\\x80\\x93 (1) rank of human response\\n(lower is better), (2) recall@k, i.e. existence of the human\\nresponse in top-k ranked responses, and (3) mean reciprocal\\nrank (MRR) of the human response (higher is better).\\nThe evaluation protocol is compatible with both discrimi-\\nnative models (that simply score the input candidates, e.g.\\nvia a softmax over the options, and cannot generate new\\nanswers), and generative models (that generate an answer\\nstring, e.g. via Recurrent Neural Networks) by ranking the\\ncandidates by the model\\xe2\\x80\\x99s log-likelihood scores.\\nCandidate Answers. We generate a candidate set of cor-\\nrect and incorrect answers from four sets:\\nCorrect: The ground-truth human response to the question.\\nPlausible: Answers to 50 most similar questions. Simi-\\nlar questions are those that start with similar tri-grams and\\nmention similar semantic concepts in the rest of the ques-\\ntion. To capture this, all questions are embedded into a vec-\\ntor space by concatenating the GloVe embeddings of the\\n\\xef\\xac\\x81rst three words with the averaged GloVe embeddings of\\nthe remaining words in the questions. Euclidean distances\\nare used to compute neighbors. Since these neighboring\\nquestions were asked on different images, their answers\\nserve as \\xe2\\x80\\x98hard negatives\\xe2\\x80\\x99.\\nPopular: The 30 most popular answers from the dataset \\xe2\\x80\\x93\\ne.g. \\xe2\\x80\\x98yes\\xe2\\x80\\x99, \\xe2\\x80\\x98no\\xe2\\x80\\x99, \\xe2\\x80\\x982\\xe2\\x80\\x99, \\xe2\\x80\\x981\\xe2\\x80\\x99, \\xe2\\x80\\x98white\\xe2\\x80\\x99, \\xe2\\x80\\x983\\xe2\\x80\\x99, \\xe2\\x80\\x98grey\\xe2\\x80\\x99, \\xe2\\x80\\x98gray\\xe2\\x80\\x99, \\xe2\\x80\\x984\\xe2\\x80\\x99, \\xe2\\x80\\x98yes\\nit is\\xe2\\x80\\x99. The inclusion of popular answers forces the machine\\nto pick between likely a priori responses and plausible re-\\nsponses for the question, thus increasing the task dif\\xef\\xac\\x81culty.\\nRandom: The remaining are answers to random questions\\nin the dataset. To generate 100 candidates, we \\xef\\xac\\x81rst \\xef\\xac\\x81nd the\\nunion of the correct, plausible, and popular answers, and\\ninclude random answers until a unique set of 100 is found.\\n\\n5. Neural Visual Dialog Models\\n\\n(cid:124)\\n\\n(cid:123)(cid:122)\\n\\nHt\\xe2\\x88\\x921\\n\\n(cid:125)\\n\\nH0\\n\\n(cid:125)\\n\\n(cid:124)\\n\\n, . . . , A(100)\\n\\nt\\n\\nt\\n\\n(cid:123)(cid:122)\\n\\nH1\\n\\n, (Q1, A1)\\n\\n} \\xe2\\x80\\x93 and asked to return a sorting of At.\\n\\nimage caption) H = ( C(cid:124)(cid:123)(cid:122)(cid:125)\\n\\nIn this section, we develop a number of novel neural Visual\\nDialog models. Recall that the model is given as input \\xe2\\x80\\x93\\nan image I, the \\xe2\\x80\\x98ground-truth\\xe2\\x80\\x99 dialog history (including the\\n),\\n, . . . , (Qt\\xe2\\x88\\x921, At\\xe2\\x88\\x921)\\nthe question Qt, and a list of 100 candidate answers At =\\n{A(1)\\nAt a high level, all our models follow the encoder-decoder\\nframework, i.e. factorize into two parts \\xe2\\x80\\x93 (1) an encoder that\\nconverts the input (I, H, Qt) into a vector space, and (2) a\\ndecoder that converts the embedded vector into an output.\\nWe describe choices for each component next and present\\nexperiments with all encoder-decoder combinations.\\nDecoders: We use two types of decoders:\\n\\xe2\\x80\\xa2 Generative (LSTM) decoder: where the encoded vector\\nis set as the initial state of the Long Short-Term Mem-\\n\\nory (LSTM) RNN language model. During training, we\\nmaximize the log-likelihood of the ground truth answer\\nsequence given its corresponding encoded representation\\n(trained end-to-end). To evaluate, we use the model\\xe2\\x80\\x99s log-\\nlikelihood scores and rank candidate answers.\\nNote that this decoder does not need to score options dur-\\ning training. As a result, such models do not exploit the\\nbiases in option creation and typically underperform mod-\\nels that do [18], but it is debatable whether exploiting such\\nbiases is really indicative of progress. Moreover, genera-\\ntive decoders are more practical in that they can actually\\nbe deployed in realistic applications.\\n\\xe2\\x80\\xa2 Discriminative (softmax) decoder: computes dot product\\nsimilarity between the input encoding and an LSTM en-\\ncoding of each of the answer options. These dot products\\nare fed into a softmax to compute the posterior probability\\nover the options. During training, we maximize the log-\\nliklihood of the correct option. During evaluation, options\\nare simply ranked based on their posterior probabilities.\\nEncoders: We develop 3 different encoders (listed below)\\nthat convert inputs (I, H, Qt) into a joint representation.\\nIn all cases, we represent I via the (cid:96)2-normalized activa-\\ntions from the penultimate layer of VGG-16 [48]. For each\\nencoder E, we experiment with all possible ablated ver-\\nsions: E(Qt), E(Qt, I), E(Qt, H), E(Qt, I, H) (for some\\nencoders, not all combinations are \\xe2\\x80\\x98valid\\xe2\\x80\\x99; details below).\\n\\xe2\\x80\\xa2 Late Fusion (LF) Encoder: In this encoder, we treat H\\nas a long string with the entire history (H0, . . . , Ht\\xe2\\x88\\x921)\\nconcatenated. Qt and H are separately encoded with 2\\ndifferent LSTMs, and individual representations of par-\\nticipating inputs (I, H, Qt) are concatenated and linearly\\ntransformed to a desired size of joint representation.\\n\\xe2\\x80\\xa2 Hierarchical Recurrent Encoder (HRE): In this en-\\ncoder, we capture the intuition that there is a hierarchical\\nnature to our problem \\xe2\\x80\\x93 each question Qt is a sequence of\\nwords that need to be embedded, and the dialog as a whole\\nis a sequence of question-answer pairs (Qt, At). Thus,\\nsimilar to [46], as shown in Fig. 6, we propose an HRE\\nmodel that contains a dialog-RNN sitting on top of a recur-\\nrent block (Rt). The recurrent block Rt embeds the ques-\\ntion and image jointly via an LSTM (early fusion), embeds\\neach round of the history Ht, and passes a concatenation\\nof these to the dialog-RNN above it. The dialog-RNN pro-\\nduces both an encoding for this round (Et in Fig. 6) and a\\ndialog context to pass onto the next round. We also add an\\nattention-over-history (\\xe2\\x80\\x98Attention\\xe2\\x80\\x99 in Fig. 6) mechanism\\nallowing the recurrent block Rt to choose and attend to\\nthe round of the history relevant to the current question.\\nThis attention mechanism consists of a softmax over pre-\\nvious rounds (0, 1, . . . , t \\xe2\\x88\\x92 1) computed from the history\\nand question+image encoding.\\n\\xe2\\x80\\xa2 Memory Network (MN) Encoder: We develop a MN\\nencoder that maintains each previous question and answer\\n\\n\\x0cModel\\n\\nMRR R@1 R@5 R@10 Mean\\n31.56\\n0.311 19.85 39.14\\n30.88\\n0.392 30.54 46.99\\n0.385 29.71 46.57\\n30.90\\n\\n44.28\\n49.98\\n49.86\\n\\nn\\ni\\nl\\ne\\ns\\na\\nB\\n\\ne\\nv\\ni\\nt\\na\\nr\\ne\\nn\\ne\\nG\\n\\nNN-Q\\nNN-QI\\n\\ne\\xef\\xa3\\xb1\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\xef\\xa3\\xb4\\xef\\xa3\\xb3 Answer prior\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\nA(cid:8)\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\n\\nHuman-Q\\nHuman-QH\\nHuman-QI\\nHuman-QIH\\n\\ne\\nv\\ni\\nt\\na\\nn\\ni\\nm\\n\\ni\\nr\\nc\\ns\\ni\\nD\\n\\nQ\\nV\\n\\nn\\na\\nm\\nu\\nH\\n\\nMN-QH-G\\nMN-QIH-G\\n\\n0.403 29.74 50.10\\nLF-Q-G\\n0.425 32.49 51.56\\nLF-QH-G\\n0.437 34.06 52.50\\nLF-QI-G\\n0.430 33.27 51.96\\nLF-QIH-G\\n0.430 32.84 52.36\\nHRE-QH-G\\nHRE-QIH-G\\n0.442 34.37 53.40\\nHREA-QIH-G 0.442 34.47 53.43\\n0.434 33.12 53.14\\n0.443 34.62 53.74\\n0.482 34.29 63.42\\nLF-Q-D\\n0.505 36.21 66.56\\nLF-QH-D\\n0.502 35.76 66.59\\nLF-QI-D\\n0.511 36.72 67.46\\nLF-QIH-D\\n0.489 34.74 64.25\\nHRE-QH-D\\nHRE-QIH-D\\n0.502 36.26 65.67\\nHREA-QIH-D 0.508 36.76 66.54\\n0.524 36.84 67.78\\n0.529 37.33 68.47\\n0.506 36.21 67.08\\nHieCoAtt-QI-D 0.509 35.54 66.79\\n\\nMN-QH-D\\nMN-QIH-D\\nSAN1-QI-D\\n\\n56.32\\n57.80\\n58.89\\n58.09\\n58.64\\n59.74\\n59.73\\n59.61\\n60.18\\n74.31\\n77.31\\n77.61\\n78.30\\n75.40\\n77.05\\n77.75\\n78.92\\n79.54\\n78.16\\n77.94\\n\\nHuman Accuracies\\n\\n0.441 25.10 67.37\\n0.485 30.31 70.53\\n0.619 46.12 82.54\\n0.635 48.03 83.76\\n\\n-\\n-\\n-\\n-\\n\\n24.06\\n23.11\\n22.31\\n23.04\\n22.59\\n21.75\\n21.83\\n22.14\\n21.69\\n8.87\\n7.89\\n7.72\\n7.63\\n8.32\\n7.79\\n7.59\\n7.25\\n7.03\\n7.74\\n7.68\\n\\n4.19\\n3.91\\n2.92\\n2.83\\n\\nTable 1: Performance of methods on our VisDial dataset, mea-\\nsured by mean reciprocal rank (MRR), recall@k for k =\\n{1, 5, 10} and mean rank. Note that higher is better for MRR and\\nrecall@k, while lower is better for mean rank. Memory Network\\nhas the best performance in both discriminative and generative set-\\ntings.\\n\\nwe rank options by their mean-similarity to answers to these\\nk questions. We use k = 20, K = 100.\\nFinally, we adapt several (near) state-of-art VQA models\\n(SAN [59], HieCoAtt [30]) to Visual Dialog. Since VQA\\nis posed as classi\\xef\\xac\\x81cation, we \\xe2\\x80\\x98chop\\xe2\\x80\\x99 the \\xef\\xac\\x81nal VQA-answer\\nsoftmax from these models, feed these activations to our\\ndiscriminative decoder (Section 5), and train end-to-end on\\nVisDial. Note that our LF-QI-D model is similar to that in\\n[29]. Altogether, these form fairly sophisticated baselines.\\nResults. Tab. 1 shows the results for our proposed mod-\\nels and baselines. A few key takeaways \\xe2\\x80\\x93 First, as ex-\\npected, all learning based models signi\\xef\\xac\\x81cantly outperform\\nnon-learning baselines. Second, all discriminative mod-\\nels signi\\xef\\xac\\x81cantly outperform generative models, which as\\nwe discussed is expected since discriminative models can\\ntune to the biases in the answer options. This improve-\\nment comes with the signi\\xef\\xac\\x81cant limitation of not being able\\n\\nFigure 6: Architecture of HRE encoder with attention. At the cur-\\nrent round Rt, the model has the capability to choose and attend to\\nrelevant history from previous rounds, based on the current ques-\\ntion. This attention-over-history feeds into a dialog-RNN along\\nwith question to generate joint representation Et for the decoder.\\n\\nas a \\xe2\\x80\\x98fact\\xe2\\x80\\x99 in its memory bank and learns to \\xe2\\x80\\x98poll\\xe2\\x80\\x99 the\\nstored facts and the image to answer the question. Specif-\\nically, we encode Qt with an LSTM to get a 512-d vector,\\nencode each previous round of history (H0, . . . , Ht\\xe2\\x88\\x921)\\nwith another LSTM to get a t \\xc3\\x97 512 matrix. We com-\\npute inner product of question vector with each history\\nvector to get scores over previous rounds, which are fed to\\na softmax to get attention-over-history probabilities. Con-\\nvex combination of history vectors using these attention\\nprobabilities gives us the \\xe2\\x80\\x98context vector\\xe2\\x80\\x99, which is passed\\nthrough an fc-layer, added to the question vector, and fol-\\nlowed by another fc-layer to construct the MN encoding.\\nIn the language of Memory Network [7], this is a \\xe2\\x80\\x981-hop\\xe2\\x80\\x99\\nencoding.\\nWe use a \\xe2\\x80\\x98[encoder]-[input]-[decoder]\\xe2\\x80\\x99 convention to refer\\nto model-input combinations. For example, \\xe2\\x80\\x98LF-QI-D\\xe2\\x80\\x99 has\\na Late Fusion encoder with question+image inputs (no his-\\ntory), and a discriminative decoder. Implementation details\\nabout the models can be found in the supplement.\\n\\n6. Experiments\\n\\nSplits. VisDial v0.5 contains 58k dialogs on COCO-train\\nand 10k on COCO-val images. We split the 58k into 50k\\nfor training, 8k for validation, and use the 10k as test.\\nData preprocessing, choice of hyperparameters and training\\ndetails are included in the supplement.\\nBaselines We compare to a number of baselines: Answer\\nPrior: Answer options to a test question are encoded with\\nan LSTM and scored by a linear classi\\xef\\xac\\x81er. This captures\\nranking by frequency of answers in our training set with-\\nout resolving to exact string matching. NN-Q: Given a test\\nquestion, we \\xef\\xac\\x81nd k nearest neighbor questions (in GloVe\\nspace) from train, and score answer options by their mean-\\nsimilarity with these k answers. NN-QI: First, we \\xef\\xac\\x81nd K\\nnearest neighbor questions for a test question. Then, we \\xef\\xac\\x81nd\\na subset of size k based on image feature similarity. Finally,\\n\\n\\x0cwork was funded in part by NSF CAREER awards to DP\\nand DB, an ONR YIP award to DP, ONR Grant N00014-14-\\n1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards\\nto DB and DP, a Allen Distinguished Investigator award\\nto DP from the Paul G. Allen Family Foundation, ICTAS\\nJunior Faculty awards to DB and DP, Google Faculty Re-\\nsearch Awards to DP and DB, Amazon Academic Research\\nAwards to DP and DB, AWS in Education Research grant\\nto DB, and NVIDIA GPU donations to DB. SK was sup-\\nported by ONR Grant N00014-12-1-0903. The views and\\nconclusions contained herein are those of the authors and\\nshould not be interpreted as necessarily representing the of-\\n\\xef\\xac\\x81cial policies or endorsements, either expressed or implied,\\nof the U.S. Government, or any sponsor.\\n\\nto actually generate responses, and we recommend the two\\ndecoders be viewed as separate use cases. Third, our best\\ngenerative and discriminative models are MN-QIH-G with\\n0.44 MRR, and MN-QIH-D with 0.53 MRR that outper-\\nform a suite of models and sophisticated baselines. Fourth,\\nwe observe that models with H perform better than Q-only\\nmodels, highlighting the importance of history in VisDial.\\nAdditionally, modeling history through HRE improves per-\\nformance of generative decoders, while discriminative de-\\ncoders do not seem to bene\\xef\\xac\\x81t much. Fifth, models look-\\ning at I signi\\xef\\xac\\x81cantly outperform both the blind models (Q,\\nQH) by at least 2% on recall@1 in both decoders. This im-\\nprovement con\\xef\\xac\\x81rms that VisDial is indeed grounded in the\\nimage. Finally, models that use both H and I have best per-\\nformance. The gap between QIH and Q-only blind models\\nis only likely to increase as VisDial becomes larger.\\nHuman Studies. We conduct studies on AMT to quantita-\\ntively evaluate human performance on this task for all com-\\nbinations of {with image, without image}\\xc3\\x97{with history,\\nwithout history}. Speci\\xef\\xac\\x81cally, we show humans top-9 pre-\\ndicted respones from our \\xe2\\x80\\x98LF-QIH-D\\xe2\\x80\\x99 model and ground\\ntruth answer in jumbled order, and ask them to rank re-\\nsponses. Results of our study are included in Tab. 1. Note\\nthat these numbers are not directly comparable to machine\\nperformance as models are tasked with ranking 100 re-\\nsponses, as opposed to 10 for humans. The former would be\\ntoo cumbersome for humans. We \\xef\\xac\\x81nd that without image,\\nhumans perform better when they have access to dialog his-\\ntory. As expected, this gap narrows down when they have\\naccess to the image. Explicit human-machine comparison\\ncan be found in supplement.\\n\\n7. Conclusions\\n\\nTo summarize, we introduce a new AI task \\xe2\\x80\\x93 Visual Dialog,\\nwhere an AI agent must hold a dialog with a human about\\nvisual content. We develop a novel two-person chat data-\\ncollection protocol to curate a large-scale dataset (VisDial),\\npropose retrieval-based evaluation protocol, and develop a\\nfamily of encoder decoder models for Visual Dialog with\\nnovel encoders, which model the particular features of this\\ntask and consequently outperform sophisticated baselines.\\nWe conduct human studies to quantify human performance\\non this task. Putting it all together, in the supplement we\\ndemonstrate the \\xef\\xac\\x81rst visual chatbot. Our results and analy-\\nsis indicates that there is signi\\xef\\xac\\x81cant scope for improvement,\\nand we believe this task can serve as a testbed for measuring\\nprogress towards visual intelligence.\\nAcknowledgements. We thank Harsh Agrawal and Jiasen\\nLu for help on the AMT data collection interface. We also\\nthank Xiao Lin, Ramprasaath Selvaraju and Latha Pemula\\nfor model discussions. Finally, we are grateful to the devel-\\nopers of Torch [2] for building an excellent framework. This\\n\\n\\x0cAppendix Overview\\n\\nThis supplementary document is organized as follows:\\n\\n\\xe2\\x80\\xa2 Sec. A studies how and why VisDial is more than just a\\n\\ncollection of independent Q&As.\\n\\n\\xe2\\x80\\xa2 Sec. B shows qualitative examples from our dataset.\\n\\n\\xe2\\x80\\xa2 Sec. C presents detailed human studies along with com-\\nparisons to machine accuracy. The interface for human\\nstudies is demonstrated in a video3.\\n\\n\\xe2\\x80\\xa2 Sec. D shows snapshots of our two-person chat data-\\ncollection interface on Amazon Mechanical Turk. The in-\\nterface is also demonstrated in the video3.\\n\\n\\xe2\\x80\\xa2 Sec. E presents further analysis of VisDial, such as ques-\\ntion types, question and answer lengths per question type.\\nA video with an interactive sunburst visualization of the\\ndataset is included3.\\n\\n\\xe2\\x80\\xa2 Sec. F presents implementation-level training details in-\\n\\ncluding data preprocessing, and model architectures.\\n\\n\\xe2\\x80\\xa2 Putting it all together, we compile a video demonstrating\\nour visual chatbot3 that answers a sequence of questions\\nfrom a user about an image. This demo uses one of our\\nbest generative models from the main paper, MN-QIH-G,\\nand uses sampling (without any beam-search) for infer-\\nence in the LSTM decoder. Note that these videos demon-\\nstrate an \\xe2\\x80\\x98unscripted\\xe2\\x80\\x99 dialog \\xe2\\x80\\x93 in the sense that the partic-\\nular QA sequence is not present in VisDial and the model\\nis not provided with any list of answer options.\\n\\nA. In what ways are dialogs in VisDial more\\n\\nthan just 10 visual Q&As?\\n\\nIn this section, we lay out an exhaustive list of differences\\nbetween VisDial and existing image question-answering\\ndatasets, with the VQA dataset [4] serving as the represen-\\ntative.\\nIn essence, we characterize what makes an instance in Vis-\\nDial more than a collection of 10 independent question-\\nanswer pairs about an image \\xe2\\x80\\x93 what makes it a dialog.\\nIn order to be self-contained and an exhaustive list, some\\nparts of this section repeat content from the main document.\\n\\nA.1. VisDial has longer free-form answers\\n\\nFig. 7a shows the distribution of answer lengths in VisDial.\\nand Tab. 2 compares statistics of VisDial with existing im-\\nage question answering datasets. Unlike previous datasets,\\n\\n3https://goo.gl/yjlHxY\\n\\nanswers in VisDial are longer, conversational, and more de-\\nscriptive \\xe2\\x80\\x93 mean-length 3.1 words (VisDial) vs 1.1 (VQA),\\n2.0 (Visual 7W), 2.8 (Visual Madlibs). Moreover, 42.2% of\\nanswers in VisDial are longer than 2 words while the VQA\\ndataset has only 3.8% answers longer than 2 words.\\n\\n(a)\\n\\n(b)\\n\\nFigure 7: Distribution of lengths for questions and answers (left);\\nand percent coverage of unique answers over all answers from the\\ntrain dataset (right), compared to VQA. For a given coverage, Vis-\\nDial has more unique answers indicating greater answer diversity.\\n\\nFig. 7b shows the cumulative coverage of all answers (y-\\naxis) by the most frequent answers (x-axis). The difference\\nbetween VisDial and VQA is stark \\xe2\\x80\\x93 the top-1000 answers\\nin VQA cover \\xe2\\x88\\xbc83% of all answers, while in VisDial that\\n\\xef\\xac\\x81gure is only \\xe2\\x88\\xbc58%. There is a signi\\xef\\xac\\x81cant heavy tail of an-\\nswers in VisDial \\xe2\\x80\\x93 most long strings are unique, and thus the\\ncoverage curve in Fig. 7b becomes a straight line with slope\\n1. In total, there are 171,502 unique answers in VisDial (out\\nof the 680,000 answers currently in the dataset).\\n\\nA.2. VisDial has co-references in dialogs\\n\\nPeople conversing with each other tend to use pronouns to\\nrefer to already mentioned entities. Since language in Vis-\\nDial is the result of a sequential conversation, it naturally\\ncontains pronouns \\xe2\\x80\\x93 \\xe2\\x80\\x98he\\xe2\\x80\\x99, \\xe2\\x80\\x98she\\xe2\\x80\\x99, \\xe2\\x80\\x98his\\xe2\\x80\\x99, \\xe2\\x80\\x98her\\xe2\\x80\\x99, \\xe2\\x80\\x98it\\xe2\\x80\\x99, \\xe2\\x80\\x98their\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98they\\xe2\\x80\\x99, \\xe2\\x80\\x98this\\xe2\\x80\\x99, \\xe2\\x80\\x98that\\xe2\\x80\\x99, \\xe2\\x80\\x98those\\xe2\\x80\\x99, etc.\\nIn total, 38% of ques-\\ntions, 22% of answers, and nearly all (99%) dialogs contain\\nat least one pronoun, thus con\\xef\\xac\\x81rming that a machine will\\nneed to overcome coreference ambiguities to be successful\\non this task. As a comparison, only 9% of questions and\\n0.25% of answers in VQA contain at least one pronoun.\\nIn Fig. 8, we see that pronoun usage is lower in the \\xef\\xac\\x81rst\\nround compared to other rounds, which is expected since\\nthere are fewer entities to refer to in the earlier rounds. The\\npronoun usage is also generally lower in answers than ques-\\ntions, which is also understandable since the answers are\\ngenerally shorter than questions and thus less likely to con-\\ntain pronouns. In general, the pronoun usage is fairly con-\\nsistent across rounds (starting from round 2) for both ques-\\ntions and answers.\\n\\n\\x0cDAQUAR [31]\\nVisual Madlibs [60]\\nCOCO-QA [41]\\nBaidu [12]\\nVQA [4]\\nVisual7W [62]\\nVisDial (Ours)\\n\\n# QA # Images Q Length A Length A Length > 2 Top-1000 A Human Accuracy\\n12,468\\n56,468\\n117,684\\n316,193\\n614,163\\n327,939\\n680,000\\nTable 2: Comparison of existing image question answering datasets with VisDial\\n\\n11.5\\xc2\\xb1 2.4\\n4.9\\xc2\\xb1 2.4\\n8.7\\xc2\\xb1 2.7\\n6.2\\xc2\\xb1 2.0\\n6.9\\xc2\\xb1 2.4\\n4.64\\xc2\\xb1 0\\n\\n1,447\\n9,688\\n69,172\\n316,193\\n204,721\\n47,300\\n68,000\\n\\n1.2\\xc2\\xb1 0.5\\n2.8\\xc2\\xb1 2.0\\n1.0\\xc2\\xb1 0\\n1.1\\xc2\\xb1 0.4\\n2.0\\xc2\\xb1 1.4\\n3.08\\xc2\\xb1 0\\n\\n-\\n\\n82.7%\\n63.5%\\n58.1%\\n\\n3.4%\\n47.4%\\n0.0%\\n\\n-\\n\\n3.8%\\n27.6%\\n42.2%\\n\\n-\\n\\n96.4%\\n57.9%\\n100%\\n\\n-\\n\\n-\\n-\\n-\\n-\\n(cid:88)\\n(cid:88)\\n(cid:88)\\n\\n\\xe2\\x80\\x98There\\xe2\\x80\\x99s a blue fence in background, like an enclosure\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Is the enclosure inside or outside?\\xe2\\x80\\x99.\\n\\nSuch a line of questioning does not exist in the VQA dataset,\\nwhere the subjects were shown the questions already asked\\nabout an image, and explicitly instructed to ask about dif-\\nferent entities [4].\\n\\nCounting the Number of Topics.\\nIn order to quantify\\nthese qualitative differences, we performed a human study\\nwhere we manually annotated question \\xe2\\x80\\x98topics\\xe2\\x80\\x99 for 40 im-\\nages (a total of 400 questions), chosen randomly from the\\nval set. The topic annotations were based on human judge-\\nment with a consensus of 4 annotators, with topics such as:\\nasking about a particular object (\\xe2\\x80\\x98What is the man doing?\\xe2\\x80\\x99),\\nthe scene (\\xe2\\x80\\x98Is it outdoors or indoors?\\xe2\\x80\\x99), the weather (\\xe2\\x80\\x9cIs the\\nweather sunny?\\xe2\\x80\\x99), the image (\\xe2\\x80\\x98Is it a color image?\\xe2\\x80\\x99), and ex-\\nploration (\\xe2\\x80\\x98Is there anything else?\\xe2\\x80\\x9d). We performed similar\\ntopic annotation for questions from VQA for the same set\\nof 40 images, and compared topic continuity in questions.\\nAcross 10 rounds, VisDial questions have 4.55 \\xc2\\xb1 0.17 top-\\nics on average, con\\xef\\xac\\x81rming that these are not 10 independent\\nquestions. Recall that VisDial has 10 questions per image\\nas opposed to 3 for VQA. Therefore, for a fair compari-\\nson, we compute average number of topics in VisDial over\\nall \\xe2\\x80\\x98sliding windows\\xe2\\x80\\x99 of 3 successive questions. For 500\\nbootstrap samples of batch size 40, VisDial has 2.14 \\xc2\\xb1 0.05\\ntopics while VQA has 2.53 \\xc2\\xb1 0.09. Lower mean number of\\ntopics suggests there is more continuity in VisDial because\\nquestions do not change topics as often.\\n\\nTransition Probabilities over Topics. We can take this\\nanalysis a step further by computing topic transition proba-\\nbilities over topics as follows. For a given sequential dialog\\nexchange, we now count the number of topic transitions be-\\ntween consecutive QA pairs, normalized by the total num-\\nber of possible transitions between rounds (9 for VisDial\\nand 2 for VQA). We compute this \\xe2\\x80\\x98topic transition proba-\\nbility\\xe2\\x80\\x99 (how likely are two successive QA pairs to be about\\ntwo different topics) for VisDial and VQA in two different\\nsettings \\xe2\\x80\\x93 (1) in-order and (2) with a permuted sequence\\nof QAs. Note that if VisDial were simply a collection of\\n\\nFigure 8: Percentage of QAs with pronouns for different rounds.\\nIn round 1, pronoun usage in questions is low (in fact, slightly\\nlower than usage in answers). From rounds 2 through 10, pronoun\\nusage is higher in questions and fairly consistent across rounds.\\n\\nA.3. VisDial has smoothness/continuity in \\xe2\\x80\\x98topics\\xe2\\x80\\x99\\n\\nQualitative Example of Topics. There is a stylistic dif-\\nference in the questions asked in VisDial (compared to the\\nquestions in VQA) due to the nature of the task assigned to\\nthe subjects asking the questions. In VQA, subjects saw the\\nimage and were asked to \\xe2\\x80\\x9cstump a smart robot\\xe2\\x80\\x9d. Thus, most\\nqueries involve speci\\xef\\xac\\x81c details, often about the background\\n(Q: \\xe2\\x80\\x98What program is being utilized in the background on\\nthe computer?\\xe2\\x80\\x99).\\nIn VisDial, questioners did not see the\\noriginal image and were asking questions to build a mental\\nmodel of the scene. Thus, the questions tend to be open-\\nended, and often follow a pattern:\\n\\xe2\\x80\\xa2 Generally starting with the entities in the caption:\\n\\n\\xe2\\x80\\x98An elephant walking away from a pool in an exhibit\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Is there only 1 elephant?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 digging deeper into their parts, attributes, or proper-\\nties:\\n\\n\\xe2\\x80\\x98Is it full grown?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is it facing the camera?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 asking about the scene category or the picture setting:\\n\\xe2\\x80\\x98Is this indoors or outdoors?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is this a zoo?\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Is it snowing?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is it sunny?\\xe2\\x80\\x99,\\n\\n\\xe2\\x80\\xa2 the weather:\\n\\xe2\\x80\\xa2 simply exploring the scene:\\n\\n\\xe2\\x80\\x98Are there people?\\xe2\\x80\\x99, \\xe2\\x80\\x98Is there shelter for elephant?\\xe2\\x80\\x99,\\n\\xe2\\x80\\xa2 and asking follow-up questions about the new visual en-\\ntities discovered from these explorations:\\n\\n\\x0c10 independent QAs as opposed to a dialog, we would ex-\\npect the topic transition probabilities to be similar for in-\\norder and permuted variants. However, we \\xef\\xac\\x81nd that for\\n1000 permutations of 40 topic-annotated image-dialogs, in-\\norder-VisDial has an average topic transition probability of\\n0.61, while permuted-VisDial has 0.76 \\xc2\\xb1 0.02. In contrast,\\nVQA has a topic transition probability of 0.80 for in-order\\nvs. 0.83 \\xc2\\xb1 0.02 for permuted QAs.\\n(1) In-order transition\\nThere are two key observations:\\nprobability is lower for VisDial than VQA (i.e. topic transi-\\ntion is less likely in VisDial), and (2) Permuting the order\\nof questions results in a larger increase for VisDial, around\\n0.15, compared to a mere 0.03 in case of VQA (i.e. in-order-\\nVQA and permuted-VQA behave signi\\xef\\xac\\x81cantly more simi-\\nlarly than in-order-VisDial and permuted-VisDial).\\nBoth these observations establish that there is smoothness in\\nthe temporal order of topics in VisDial, which is indicative\\nof the narrative structure of a dialog, rather than indepen-\\ndent question-answers.\\n\\nA.4. VisDial has the statistics of an NLP dialog dataset\\n\\nIn this analysis, our goal is to measure whether VisDial be-\\nhaves like a dialog dataset.\\nIn particular, we compare VisDial, VQA, and Cornell\\nMovie-Dialogs Corpus [8]. The Cornell Movie-Dialogs\\ncorpus is a text-only dataset extracted from pairwise inter-\\nactions between characters from approximately 617 movies,\\nand is widely used as a standard dialog corpus in the natural\\nlanguage processing (NLP) and dialog communities.\\nOne popular evaluation criteria used in the dialog-systems\\nresearch community is the perplexity of language models\\ntrained on dialog datasets \\xe2\\x80\\x93 the lower the perplexity of a\\nmodel, the better it has learned the structure in the dialog\\ndataset.\\nFor the purpose of our analysis, we pick the popular\\nsequence-to-sequence (Seq2Seq) language model [17] and\\nuse the perplexity of this model trained on different datasets\\nas a measure of temporal structure in a dataset.\\nAs is standard in the dialog literature, we train the Seq2Seq\\nmodel to predict the probability of utterance Ut given the\\nprevious utterance Ut\\xe2\\x88\\x921, i.e. P(Ut | Ut\\xe2\\x88\\x921) on the Cornell\\ncorpus. For VisDial and VQA, we train the Seq2Seq model\\nto predict the probability of a question Qt given the previous\\nquestion-answer pair, i.e. P(Qt | (Qt\\xe2\\x88\\x921, At\\xe2\\x88\\x921)).\\nFor each dataset, we used its train and val splits for\\ntraining and hyperparameter tuning respectively, and report\\nresults on test. At test time, we only use conversations\\nof length 10 from Cornell corpus for a fair comparison to\\nVisDial (which has 10 rounds of QA).\\nFor all three datasets, we created 100 permuted versions of\\ntest, where either QA pairs or utterances are randomly\\n\\nDataset\\nVQA\\n\\nShuf\\xef\\xac\\x82ed\\n8.16 \\xc2\\xb1 0.02\\n85.31 \\xc2\\xb1 1.51\\n7.28 \\xc2\\xb1 0.01\\n\\nPerplexity Per Token Classi\\xef\\xac\\x81cation\\nOrig\\n52.8 \\xc2\\xb1 0.9\\n7.83\\n61.0 \\xc2\\xb1 0.6\\n82.31\\nCornell (10)\\n73.3 \\xc2\\xb1 0.4\\n6.61\\nVisDial (Ours)\\nTable 3: Comparison of sequences in VisDial, VQA, and Cor-\\nnell Movie-Dialogs corpus in their original ordering vs. permuted\\n\\xe2\\x80\\x98shuf\\xef\\xac\\x82ed\\xe2\\x80\\x99 ordering. Lower is better for perplexity while higher\\nis better for classi\\xef\\xac\\x81cation accuracy. Left:\\nthe absolute increase\\nin perplexity from natural to permuted ordering is highest in the\\nCornell corpus (3.0) followed by VisDial with 0.7, and VQA at\\n0.35, which is indicative of the degree of linguistic structure in\\nthe sequences in these datasets. Right: The accuracy of a simple\\nthreshold-based classi\\xef\\xac\\x81er trained to differentiate between the orig-\\ninal sequences and their permuted or shuf\\xef\\xac\\x82ed versions. A higher\\nclassi\\xef\\xac\\x81cation rate indicates the existence of a strong temporal con-\\ntinuity in the conversation, thus making the ordering important.\\nWe can see that the classi\\xef\\xac\\x81er on VisDial achieves the highest ac-\\ncuracy (73.3%), followed by Cornell (61.0%). Note that this is a\\nbinary classi\\xef\\xac\\x81cation task with the prior probability of each class\\nby design being equal, thus chance performance is 50%. The clas-\\nsi\\xef\\xac\\x81er on VQA performs close to chance.\\n\\nshuf\\xef\\xac\\x82ed to disturb their natural order. This allows us to\\ncompare datasets in their natural ordering w.r.t. permuted\\norderings. Our hypothesis is that since dialog datasets have\\nlinguistic structure in the sequence of QAs or utterances\\nthey contain, this structure will be signi\\xef\\xac\\x81cantly affected by\\npermuting the sequence. In contrast, a collection of inde-\\npendent question-answers (as in VQA) will not be signi\\xef\\xac\\x81-\\ncantly affected by a permutation.\\nTab. 3 compares the original, unshuf\\xef\\xac\\x82ed test with the\\nshuf\\xef\\xac\\x82ed testsets on two metrics:\\n\\nPerplexity: We compute the standard metric of perplex-\\nity per token, i.e. exponent of the normalized negative-log-\\nprobability of a sequence (where normalized is by the length\\nof the sequence). Tab. 3 shows these perplexities for the\\noriginal unshuf\\xef\\xac\\x82ed test and permuted test sequences.\\nWe notice a few trends.\\nFirst, we note that the absolute perplexity values are higher\\nfor the Cornell corpus than QA datasets. We hypothesize\\nthat this is due to the broad, unrestrictive dialog generation\\ntask in Cornell corpus, which is a more dif\\xef\\xac\\x81cult task than\\nquestion prediction about images, which is in comparison a\\nmore restricted task.\\nSecond, in all three datasets, the shuf\\xef\\xac\\x82ed test has statis-\\ntically signi\\xef\\xac\\x81cant higher perplexity than the original test,\\nwhich indicates that shuf\\xef\\xac\\x82ing does indeed break the linguis-\\ntic structure in the sequences.\\nThird, the absolute increase in perplexity from natural to\\npermuted ordering is highest in the Cornell corpus (3.0) fol-\\n\\n\\x0clowed by our VisDial with 0.7, and VQA at 0.35, which\\nis indicative of the degree of linguistic structure in the se-\\nquences in these datasets. Finally, the relative increases in\\nperplexity are 3.64% in Cornell, 10.13% in VisDial, and\\n4.21% in VQA \\xe2\\x80\\x93 VisDial suffers the highest relative in-\\ncrease in perplexity due to shuf\\xef\\xac\\x82ing, indicating the exis-\\ntence of temporal continuity that gets disrupted due to shuf-\\n\\xef\\xac\\x82ing.\\n\\nsociated image results in an average VQA accuracy of 87%!\\nIn VisDial, questioners do not see the image. As a result,\\nthis bias is reduced.\\nThis lack of visual priming bias (i.e. not being able to see\\nthe image while asking questions) and holding a dialog with\\nanother person while asking questions results in the follow-\\ning two unique features in VisDial.\\n\\nClassi\\xef\\xac\\x81cation: As our second metric to compare datasets\\nin their natural vs. permuted order, we test whether we can\\nreliably classify a given sequence as natural or permuted.\\nOur classi\\xef\\xac\\x81er is a simple threshold on perplexity of a se-\\nquence. Speci\\xef\\xac\\x81cally, given a pair of sequences, we compute\\nthe perplexity of both from our Seq2Seq model, and predict\\nthat the one with higher perplexity is the sequence in per-\\nmuted ordering, and the sequence with lower perplexity is\\nthe one in natural ordering. The accuracy of this simple\\nclassi\\xef\\xac\\x81er indicates how easy or dif\\xef\\xac\\x81cult it is to tell the dif-\\nference between natural and permuted sequences. A higher\\nclassi\\xef\\xac\\x81cation rate indicates the existence of a strong tempo-\\nral continuity in the conversation, thus making the ordering\\nimportant.\\nTab. 3 shows the classi\\xef\\xac\\x81cation accuracies achieved on all\\ndatasets. We can see that the classi\\xef\\xac\\x81er on VisDial achieves\\nthe highest accuracy (73.3%), followed by Cornell (61.0%).\\nNote that this is a binary classi\\xef\\xac\\x81cation task with the prior\\nprobability of each class by design being equal, thus chance\\nperformance is 50%. The classi\\xef\\xac\\x81ers on VisDial and Cornell\\nboth signi\\xef\\xac\\x81cantly outperforming chance. On the other hand,\\nthe classi\\xef\\xac\\x81er on VQA is near chance (52.8%), indicating a\\nlack of general temporal continuity.\\nTo summarize this analysis, our experiments show that\\nVisDial is signi\\xef\\xac\\x81cantly more dialog-like than VQA, and\\nbehaves more like a standard dialog dataset, the Cornell\\nMovie-Dialogs corpus.\\n\\nA.5. VisDial eliminates visual priming bias in VQA\\n\\nOne key difference between VisDial and previous image\\nquestion answering datasets (VQA [4], Visual 7W [62],\\nBaidu mQA [12]) is the lack of a \\xe2\\x80\\x98visual priming bias\\xe2\\x80\\x99 in\\nVisDial. Speci\\xef\\xac\\x81cally, in all previous datasets, subjects saw\\nan image while asking questions about it. As described in\\n[61], this leads to a particular bias in the questions \\xe2\\x80\\x93 people\\nonly ask \\xe2\\x80\\x98Is there a clocktower in the picture?\\xe2\\x80\\x99 on pictures\\nactually containing clock towers. This allows language-\\nonly models to perform remarkably well on VQA and re-\\nsults in an in\\xef\\xac\\x82ated sense of progress [61]. As one particu-\\nlarly perverse example \\xe2\\x80\\x93 for questions in the VQA dataset\\nstarting with \\xe2\\x80\\x98Do you see a . . . \\xe2\\x80\\x99, blindly answering \\xe2\\x80\\x98yes\\xe2\\x80\\x99\\nwithout reading the rest of the question or looking at the as-\\n\\nFigure 9: Distribution of answers in VisDial by their \\xef\\xac\\x81rst four\\nwords. The ordering of the words starts towards the center and\\nradiates outwards. The arc length is proportional to the number of\\nquestions containing the word. White areas are words with contri-\\nbutions too small to show.\\n\\nUncertainty in Answers in VisDial. Since the answers\\nin VisDial are longer strings, we can visualize their distri-\\nbution based on the starting few words (Fig. 9). An inter-\\nesting category of answers emerges \\xe2\\x80\\x93 \\xe2\\x80\\x98I think so\\xe2\\x80\\x99, \\xe2\\x80\\x98I can\\xe2\\x80\\x99t\\ntell\\xe2\\x80\\x99, or \\xe2\\x80\\x98I can\\xe2\\x80\\x99t see\\xe2\\x80\\x99 \\xe2\\x80\\x93 expressing doubt, uncertainty, or lack\\nof information. This is a consequence of the questioner\\nnot being able to see the image \\xe2\\x80\\x93 they are asking contex-\\ntually relevant questions, but not all questions may be an-\\nswerable with certainty from that image. We believe this\\nis rich data for building more human-like AI that refuses to\\nanswer questions it doesn\\xe2\\x80\\x99t have enough information to an-\\nswer. See [40] for a related, but complementary effort on\\nquestion relevance in VQA.\\n\\nBinary Questions (cid:54)= Binary Answers in VisDial.\\nIn\\nVQA, binary questions are simply those with \\xe2\\x80\\x98yes\\xe2\\x80\\x99, \\xe2\\x80\\x98no\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98maybe\\xe2\\x80\\x99 as answers [4].\\nIn VisDial, we must distinguish\\nbetween binary questions and binary answers. Binary ques-\\ntions are those starting in \\xe2\\x80\\x98Do\\xe2\\x80\\x99, \\xe2\\x80\\x98Did\\xe2\\x80\\x99, \\xe2\\x80\\x98Have\\xe2\\x80\\x99, \\xe2\\x80\\x98Has\\xe2\\x80\\x99, \\xe2\\x80\\x98Is\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Are\\xe2\\x80\\x99, \\xe2\\x80\\x98Was\\xe2\\x80\\x99, \\xe2\\x80\\x98Were\\xe2\\x80\\x99, \\xe2\\x80\\x98Can\\xe2\\x80\\x99, \\xe2\\x80\\x98Could\\xe2\\x80\\x99. Answers to such\\nquestions can (1) contain only \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99, (2) begin with\\n\\xe2\\x80\\x98yes\\xe2\\x80\\x99, \\xe2\\x80\\x98no\\xe2\\x80\\x99, and contain additional information or clari\\xef\\xac\\x81ca-\\n\\n\\x0ction (Q: \\xe2\\x80\\x98Are there any animals in the image?\\xe2\\x80\\x99, A: \\xe2\\x80\\x98yes, 2\\ncats and a dog\\xe2\\x80\\x99), (3) involve ambiguity (\\xe2\\x80\\x98It\\xe2\\x80\\x99s hard to see\\xe2\\x80\\x99,\\n\\xe2\\x80\\x98Maybe\\xe2\\x80\\x99), or (4) answer the question without explicitly say-\\ning \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99 (Q: \\xe2\\x80\\x98Is there any type of design or pat-\\ntern on the cloth?\\xe2\\x80\\x99, A: \\xe2\\x80\\x98There are circles and lines on the\\ncloth\\xe2\\x80\\x99). We call answers that contain \\xe2\\x80\\x98yes\\xe2\\x80\\x99 or \\xe2\\x80\\x98no\\xe2\\x80\\x99 as binary\\nanswers \\xe2\\x80\\x93 149,367 and 76,346 answers in subsets (1) and\\n(2) from above respectively. Binary answers in VQA are\\nbiased towards \\xe2\\x80\\x98yes\\xe2\\x80\\x99 [4,61] \\xe2\\x80\\x93 61.40% of yes/no answers are\\n\\xe2\\x80\\x98yes\\xe2\\x80\\x99. In VisDial, the trend is reversed. Only 46.96% are\\n\\xe2\\x80\\x98yes\\xe2\\x80\\x99 for all yes/no responses. This is understandable since\\nworkers did not see the image, and were more likely to end\\nup with negative responses.\\n\\nB. Qualitative Examples from VisDial\\n\\nFig. 10 shows random samples of dialogs from the VisDial\\ndataset.\\n\\nC. Human-Machine Comparison\\n\\n\\xef\\xa3\\xb1\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\ne(cid:40) HREA-QIH-G 0.477 31.64 61.61\\n\\nMRR R@1 R@5 Mean\\nModel\\n4.19\\n0.441 25.10 67.37\\nHuman-Q\\n3.91\\n0.485 30.31 70.53\\nHuman-QH\\n0.619 46.12 82.54\\n2.92\\nHuman-QI\\n2.83\\nHuman-QIH 0.635 48.03 83.76\\n4.42\\n4.47\\n3.48\\n\\n0.481 32.16 61.94\\n0.553 36.86 69.39\\n\\nMN-QIH-G\\nMN-QIH-D\\n\\nn\\na\\nm\\nu\\nH\\n\\nn\\ni\\nh\\nc\\na\\n\\nM\\n\\nTable 4: Human-machine performance comparison on our VisDial\\ndataset, measured by mean reciprocal rank (MRR), recall@k for\\nk = {1, 5} and mean rank. Note that higher is better for MRR and\\nrecall@k, while lower is better for mean rank.\\n\\nWe conducted studies on AMT to quantitatively evaluate\\nhuman performance on this task for all combinations of\\n{with image, without image}\\xc3\\x97{with history, without his-\\ntory} on 100 random images at each of the 10 rounds.\\nSpeci\\xef\\xac\\x81cally, in each setting, we show human subjects a\\njumbled list of 10 candidate answers for a question \\xe2\\x80\\x93 top-9\\npredicted responses from our \\xe2\\x80\\x98LF-QIH-D\\xe2\\x80\\x99 model and the 1\\nground truth answer \\xe2\\x80\\x93 and ask them to rank the responses.\\nEach task was done by 3 human subjects.\\nResults of this study are shown in the top-half of Tab. 4.\\nWe \\xef\\xac\\x81nd that without access to the image, humans perform\\nbetter when they have access to dialog history \\xe2\\x80\\x93 compare\\nthe Human-QH row to Human-Q (R@1 of 30.31 vs. 25.10).\\nAs perhaps expected, this gap narrows down when humans\\nhave access to the image \\xe2\\x80\\x93 compare Human-QIH to Human-\\nQI (R@1 of 48.03 vs. 46.12).\\nNote that these numbers are not directly comparable to ma-\\nchine performance reported in the main paper because mod-\\n\\nels are tasked with ranking 100 responses, while humans\\nare asked to rank 10 candidates. This is because the task of\\nranking 100 candidate responses would be too cumbersome\\nfor humans.\\nTo compute comparable human and machine performance,\\nwe evaluate our best discriminative (MN-QIH-D) and gen-\\nerative (HREA-QIH-G, MN-QIH-G)4 models on the same\\n10 options that were presented to humans. Note that in this\\nsetting, both humans and machines have R@10 = 1.0, since\\nthere are only 10 options.\\nTab. 4 bottom-half shows the results of this comparison. We\\ncan see that, as expected, humans with full information (i.e.\\nHuman-QIH) perform the best with a large gap in human\\nand machine performance (compare R@5: Human-QIH\\n83.76% vs. MN-QIH-D 69.39%). This gap is even larger\\nwhen compared to generative models, which unlike the dis-\\ncriminative models are not actively trying to exploit the bi-\\nases in the answer candidates (compare R@5: Human-QIH\\n83.76% vs. HREA-QIH-G 61.61%).\\nFurthermore, we see that humans outperform the best ma-\\nchine even when not looking at the image, simply on the\\nbasis of the context provided by the history (compare R@5:\\nHuman-QH 70.53% vs. MN-QIH-D 69.39%).\\nPerhaps as expected, with access to the image but not the\\nhistory, humans are signi\\xef\\xac\\x81cantly better than the best ma-\\nchines (compare R@5: Human-QI 82.54% vs. MN-QIH-D\\n69.39%). With access to history humans perform even bet-\\nter.\\nFrom in-house human studies and worker feedback on\\nAMT, we \\xef\\xac\\x81nd that the dialog history plays the following\\nroles for humans: (1) provides a context for the question\\nand paints a picture of the scene, which helps eliminate cer-\\ntain answer choices (especially when the image is not avail-\\nable), (2) gives cues about the answerer\\xe2\\x80\\x99s response style,\\nwhich helps identify the right answer among similar answer\\nchoices, and (3) disambiguates amongst likely interpreta-\\ntions of the image (i.e., when objects are small or occluded),\\nagain, helping identify the right answer among multiple\\nplausible options.\\n\\nD. Interface\\n\\nIn this section, we show our interface to connect two Ama-\\nzon Mechanical Turk workers live, which we used to collect\\nour data.\\nInstructions. To ensure quality of data, we provide detailed\\ninstructions on our interface as shown in Fig. 11a. Since the\\nworkers do not know their roles before starting the study, we\\nprovide instructions for both questioner and answerer roles.\\n\\n4 We use both HREA-QIH-G, MN-QIH-G since they have similar ac-\\n\\ncuracies.\\n\\n\\x0c(a)\\n\\n(c)\\n\\n(e)\\n\\n(b)\\n\\n(d)\\n\\n(f)\\n\\nFigure 10: Examples from VisDial\\n\\nAfter pairing: Immediately after pairing two workers, we\\nassign them roles of a questioner and a answerer and display\\n\\nrole-speci\\xef\\xac\\x81c instructions as shown in Fig. 11b. Observe that\\nthe questioner does not see the image while the answerer\\n\\n\\x0c(a) Detailed instructions for Amazon Mechanical Turkers on our interface\\n\\n(b) Left: What questioner sees; Right: What answerer sees.\\n\\nFigure 11\\n\\ndoes have access to it. Both questioner and answerer see\\nthe caption for the image.\\n\\nplanatory and long. Across question types, answers tend to\\nbe the longest in the middle of conversations.\\n\\nE. Additional Analysis of VisDial\\n\\nIn this section, we present additional analyses characteriz-\\ning our VisDial dataset.\\n\\nE.1. Question and Answer Lengths\\n\\nFig. 12 shows question lengths by type and round. Aver-\\nage length of question by type is consistent across rounds.\\nQuestions starting with \\xe2\\x80\\x98any\\xe2\\x80\\x99 (\\xe2\\x80\\x98any people?\\xe2\\x80\\x99, \\xe2\\x80\\x98any other\\nfruits?\\xe2\\x80\\x99, etc.) tend to be the shortest. Fig. 13 shows answer\\nlengths by type of question they were said in response to and\\nround. In contrast to questions, there is signi\\xef\\xac\\x81cant variance\\nin answer lengths. Answers to binary questions (\\xe2\\x80\\x98Any peo-\\nple?\\xe2\\x80\\x99, \\xe2\\x80\\x98Can you see the dog?\\xe2\\x80\\x99, etc.) tend to be short while\\nanswers to \\xe2\\x80\\x98how\\xe2\\x80\\x99 and \\xe2\\x80\\x98what\\xe2\\x80\\x99 questions tend to be more ex-\\n\\nE.2. Question Types\\n\\nFig. 14 shows round-wise coverage by question type. We\\nsee that as conversations progress, \\xe2\\x80\\x98how\\xe2\\x80\\x99 and \\xe2\\x80\\x98what\\xe2\\x80\\x99 ques-\\ntions reduce while \\xe2\\x80\\x98can\\xe2\\x80\\x99, \\xe2\\x80\\x98does\\xe2\\x80\\x99, \\xe2\\x80\\x98do\\xe2\\x80\\x99, \\xe2\\x80\\x98any\\xe2\\x80\\x99 questions occur\\nmore often. Questions starting with \\xe2\\x80\\x98Is\\xe2\\x80\\x99 are the most popu-\\nlar in the dataset.\\n\\nF. Experimental Details\\n\\nIn this section, we describe details about our models, data\\npreprocessing, training procedure and hyperparameter se-\\nlection.\\n\\n\\x0cFigure 15: Most frequent answer responses except for \\xe2\\x80\\x98yes\\xe2\\x80\\x99/\\xe2\\x80\\x98no\\xe2\\x80\\x99\\n\\nFigure 12: Question lengths by type and round. Average length\\nof question by type is fairly consistent across rounds. Questions\\nstarting with \\xe2\\x80\\x98any\\xe2\\x80\\x99 (\\xe2\\x80\\x98any people?\\xe2\\x80\\x99, \\xe2\\x80\\x98any other fruits?\\xe2\\x80\\x99, etc.) tend to\\nbe the shortest.\\n\\nFigure 13: Answer lengths by question type and round. Across\\nquestion types, average response length tends to be longest in the\\nmiddle of the conversation.\\n\\nF.1. Models\\n\\nLate Fusion (LF) Encoder. We encode the image with\\na VGG-16 CNN, question and concatenated history with\\nseparate LSTMs and concatenate the three representations.\\nThis is followed by a fully-connected layer and tanh non-\\nlinearity to a 512-d vector, which is used to decode the re-\\nsponse. Fig. 16a shows the model architecture for our LF\\nencoder.\\n\\nHierarchical Recurrent Encoder (HRE).\\nIn this en-\\ncoder, the image representation from VGG-16 CNN is early\\nfused with the question. Speci\\xef\\xac\\x81cally, the image representa-\\ntion is concatenated with every question word as it is fed\\nto an LSTM. Each QA-pair in dialog history is indepen-\\ndently encoded by another LSTM with shared weights. The\\nimage-question representation, computed for every round\\nfrom 1 through t, is concatenated with history representa-\\ntion from the previous round and constitutes a sequence of\\nquestion-history vectors. These vectors are fed as input to a\\n\\n\\x0calways are these attention weights interpretable.\\n\\nF.2. Training\\n\\nSplits. Recall that VisDial v0.5 contained 58k dialogs on\\nCOCO-train and 10k on COCO-val images, at the time\\nof submission. We split the 58k into 50k for training, 8k for\\nvalidation, and use the 10k as test.\\n\\nPreprocessing. We spell-correct VisDial data using the\\nBing API [34]. Following VQA, we lowercase all questions\\nand answers, convert digits to words, and remove contrac-\\ntions, before tokenizing using the Python NLTK [1]. We\\nthen construct a dictionary of words that appear at least \\xef\\xac\\x81ve\\ntimes in the train set, giving us a vocabulary of around 7.5k.\\n\\nHyperparameters. All our models are implemented in\\nTorch [2]. Model hyperparameters are chosen by early stop-\\nping on val based on the Mean Reciprocal Rank (MRR)\\nmetric. All LSTMs/GRUs are 2-layered with 512-dim hid-\\nden states. We learn 300-dim embeddings for words and im-\\nages. These word embeddings are shared across question,\\nhistory, and decoder LSTMs. For models with generative\\ndecoders, we use Adam [21] with a learning rate of 10\\xe2\\x88\\x923.\\nFor discriminative decoders, we use SGD+Nesterov mo-\\nmentum with a learning rate of 0.1. Gradients at each iter-\\nations are clamped to [\\xe2\\x88\\x925, 5] to avoid explosion. Our code,\\narchitectures, and trained models will be publicly available.\\n\\nFigure 14: Percentage coverage of question types per round. As\\nconversations progress, \\xe2\\x80\\x98How\\xe2\\x80\\x99 and \\xe2\\x80\\x98What\\xe2\\x80\\x99 questions reduce while\\n\\xe2\\x80\\x98Can\\xe2\\x80\\x99, \\xe2\\x80\\x98Does\\xe2\\x80\\x99, \\xe2\\x80\\x98Do\\xe2\\x80\\x99, \\xe2\\x80\\x98Any\\xe2\\x80\\x99 questions occur more often. Questions\\nstarting with \\xe2\\x80\\x98Is\\xe2\\x80\\x99 are the most popular in the dataset.\\n\\ndialog-level GRU, whose output state at t is used to decode\\nthe response to Qt. Fig. 16b shows the model architecture\\nfor our HRE.\\n\\nMemory Network. The image is encoded with a VGG-\\n16 CNN and question with an LSTM. We concatenate the\\nrepresentations and follow it by a fully-connected layer and\\ntanh non-linearity to get a \\xe2\\x80\\x98query vector\\xe2\\x80\\x99. Each caption/QA-\\npair (or \\xe2\\x80\\x98fact\\xe2\\x80\\x99) in dialog history is encoded independently\\nby an LSTM with shared weights. The query vector is then\\nused to compute attention over the t facts by inner product.\\nConvex combination of attended history vectors is passed\\nthrough a fully-connected layer and tanh non-linearity, and\\nadded back to the query vector. This combined represen-\\ntation is then passed through another fully-connected layer\\nand tanh non-linearity and then used to decode the response.\\nThe model architecture is shown in Fig. 16c. Fig. 17 shows\\nsome examples of attention over history facts from our MN\\nencoder. We see that the model learns to attend to facts\\nrelevant to the question being asked. For example, when\\nasked \\xe2\\x80\\x98What color are kites?\\xe2\\x80\\x99, the model attends to \\xe2\\x80\\x98A lot\\nof people stand around \\xef\\xac\\x82ying kites in a park.\\xe2\\x80\\x99 For \\xe2\\x80\\x98Is any-\\none on bus?\\xe2\\x80\\x99, it attends to \\xe2\\x80\\x98A large yellow bus parked in\\nsome grass.\\xe2\\x80\\x99 Note that these are selected examples, and not\\n\\n\\x0c(a) Late Fusion Encoder\\n\\n(b) Hierarchical Recurrent Encoder\\n\\n(c) Memory Network Encoder\\n\\nFigure 16\\n\\n\\x0cFigure 17: Selected examples of attention over history facts from our Memory Network encoder. The intensity of color in each row\\nindicates the strength of attention placed on that round by the model.\\n\\n\\x0cReferences\\n[1] NLTK. http://www.nltk.org/. 18\\n[2] Torch. http://torch.ch/. 9, 18\\n[3] Amazon. Alexa. http://alexa.amazon.com/. 6\\n[4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.\\nZitnick, and D. Parikh. VQA: Visual Question Answering.\\nIn ICCV, 2015. 1, 2, 3, 4, 5, 6, 10, 11, 13, 14\\n\\n[5] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C.\\nMiller, R. Miller, A. Tatarowicz, B. White, S. White, and\\nT. Yeh. VizWiz: Nearly Real-time Answers to Visual Ques-\\ntions. In UIST, 2010. 1\\n\\n[6] A. Bordes, N. Usunier, S. Chopra, and J. Weston. Large-\\nscale Simple Question Answering with Memory Networks.\\narXiv preprint arXiv:1506.02075, 2015. 3\\n\\n[7] A. Bordes and J. Weston.\\n\\nLearning End-to-End Goal-\\nOriented Dialog. arXiv preprint arXiv:1605.07683, 2016.\\n3, 6, 8\\n\\n[8] C. Danescu-Niculescu-Mizil and L. Lee. Chameleons in\\nimagined conversations: A new approach to understanding\\nIn Proceedings\\ncoordination of linguistic style in dialogs.\\nof the Workshop on Cognitive Modeling and Computational\\nLinguistics, ACL 2011, 2011. 12\\n\\n[9] J. Dodge, A. Gane, X. Zhang, A. Bordes, S. Chopra,\\nA. Miller, A. Szlam, and J. Weston. Evaluating Prerequi-\\nsite Qualities for Learning End-to-End Dialog Systems. In\\nICLR, 2016. 2, 3\\n\\n[10] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,\\nS. Venugopalan, K. Saenko, and T. Darrell. Long-term Re-\\ncurrent Convolutional Networks for Visual Recognition and\\nDescription. In CVPR, 2015. 3\\n\\n[11] H. Fang, S. Gupta, F. N. Iandola, R. K. Srivastava, L. Deng,\\nP. Doll\\xc3\\xa1r, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zit-\\nnick, and G. Zweig. From Captions to Visual Concepts and\\nBack. In CVPR, 2015. 3\\n\\n[12] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.\\nAre You Talking to a Machine? Dataset and Methods for\\nMultilingual Image Question Answering. In NIPS, 2015. 3,\\n4, 11, 13\\n\\n[13] D. Geman, S. Geman, N. Hallonquist, and L. Younes. A\\nVisual Turing Test for Computer Vision Systems. In PNAS,\\n2014. 3\\n\\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning\\n\\nfor Image Recognition. In CVPR, 2016. 1\\n\\n[15] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt,\\nW. Kay, M. Suleyman, and P. Blunsom. Teaching machines\\nto read and comprehend. In NIPS, 2015. 1, 3\\n\\n[16] R. Hu, M. Rohrbach, and T. Darrell. Segmentation from\\n\\nnatural language expressions. In ECCV, 2016. 3\\n\\n[17] Q. V. L. Ilya Sutskever, Oriol Vinyals. Sequence to Sequence\\n\\nLearning with Neural Networks. In NIPS, 2014. 12\\n\\n[18] A. Jabri, A. Joulin, and L. van der Maaten. Revisiting visual\\n\\nquestion answering baselines. In ECCV, 2016. 7\\n\\n[19] A. Kannan, K. Kurach, S. Ravi, T. Kaufmann, A. Tomkins,\\nB. Miklos, G. Corrado, L. Luk\\xc3\\xa1cs, M. Ganea, P. Young, et al.\\nSmart Reply: Automated Response Suggestion for Email. In\\nKDD, 2016. 3\\n\\n[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-\\nIn CVPR, 2015.\\n\\nments for generating image descriptions.\\n3\\n\\n[21] D. Kingma and J. Ba. Adam: A Method for Stochastic Opti-\\n\\nmization. In ICLR, 2015. 18\\n\\n[22] C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What\\nare you talking about? text-to-image coreference. In CVPR,\\n2014. 3\\n\\n[23] O. Lemon, K. Georgila, J. Henderson, and M. Stuttle. An\\nISU dialogue system exhibiting reinforcement learning of di-\\nalogue policies: generic slot-\\xef\\xac\\x81lling in the TALK in-car sys-\\ntem. In EACL, 2006. 2\\n\\n[24] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Juraf-\\nsky. Deep Reinforcement Learning for Dialogue Generation.\\nIn EMNLP, 2016. 3\\n\\n[25] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\\nmanan, P. Doll\\xc3\\x83\\xc2\\xa1r, and C. L. Zitnick. Microsoft COCO:\\nCommon Objects in Context. In ECCV, 2014. 2, 3, 4\\n\\n[26] C.-W. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Charlin,\\nand J. Pineau. How NOT To Evaluate Your Dialogue System:\\nAn Empirical Study of Unsupervised Evaluation Metrics for\\nDialogue Response Generation. In EMNLP, 2016. 3, 6\\n\\n[27] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.\\nFu, and A. C. Berg. SSD: Single Shot MultiBox Detector. In\\nECCV, 2016. 1\\n\\n[28] R. Lowe, N. Pow, I. Serban, and J. Pineau. The Ubuntu Dia-\\nlogue Corpus: A Large Dataset for Research in Unstructured\\nMulti-Turn Dialogue Systems. In SIGDIAL, 2015. 3\\n\\n[29] J. Lu, X. Lin, D. Batra, and D. Parikh.\\n\\nDeeper\\nLSTM and Normalized CNN Visual Question Answering\\nmodel.\\nhttps://github.com/VT-vision-lab/\\nVQA_LSTM_CNN, 2015. 8\\n\\n[30] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical\\nQuestion-Image Co-Attention for Visual Question Answer-\\ning. In NIPS, 2016. 8\\n\\n[31] M. Malinowski and M. Fritz. A Multi-World Approach to\\nQuestion Answering about Real-World Scenes based on Un-\\ncertain Input. In NIPS, 2014. 3, 11\\n\\n[32] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu-\\nrons: A neural-based approach to answering questions about\\nimages. In ICCV, 2015. 1, 3\\n\\n[33] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and\\nwalk: Neural mapping of navigational instructions to action\\nsequences. In AAAI, 2016. 2\\n\\n[34] Microsoft. Bing Spell Check API.\\n\\nhttps://www.\\n\\nmicrosoft.com/cognitive-services/en-us/\\nbing-spell-check-api/documentation. 18\\n\\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\\nFidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik,\\nI. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,\\nand D. Hassabis. Human-level control through deep rein-\\nforcement learning. Nature, 518(7540):529\\xe2\\x80\\x93533, 02 2015.\\n1\\n\\n[36] T. Paek. Empirical methods for evaluating dialog systems.\\nIn Proceedings of the workshop on Evaluation for Language\\nand Dialogue Systems-Volume 9, 2001. 2\\n\\n[37] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,\\nJ. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-\\nlecting region-to-phrase correspondences for richer image-\\nto-sentence models. In ICCV, 2015. 3\\n\\n[38] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD:\\n\\n\\x0c100,000+ Questions for Machine Comprehension of Text. In\\nEMNLP, 2016. 3\\n\\n[39] V. Ramanathan, A. Joulin, P. Liang, and L. Fei-Fei. Linking\\npeople with \"their\" names using coreference resolution. In\\nECCV, 2014. 3\\n\\n[40] A. Ray, G. Christie, M. Bansal, D. Batra, and D. Parikh.\\nQuestion Relevance in VQA: Identifying Non-Visual And\\nFalse-Premise Questions. In EMNLP, 2016. 5, 13\\n\\n[41] M. Ren, R. Kiros, and R. Zemel. Exploring Models and Data\\n\\nfor Image Question Answering. In NIPS, 2015. 1, 3, 11\\n\\n[42] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and\\nB. Schiele. Grounding of textual phrases in images by re-\\nconstruction. In ECCV, 2016. 3\\n\\n[43] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A\\n\\ndataset for movie description. In CVPR, 2015. 3\\n\\n[44] I. V. Serban, A. Garc\\xc3\\xada-Dur\\xc3\\xa1n, \\xc3\\x87. G\\xc3\\xbcl\\xc3\\xa7ehre, S. Ahn, S. Chan-\\ndar, A. C. Courville, and Y. Bengio. Generating Factoid\\nQuestions With Recurrent Neural Networks: The 30M Fac-\\ntoid Question-Answer Corpus. In ACL, 2016. 3\\n\\n[45] I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and\\nJ. Pineau. Building End-To-End Dialogue Systems Using\\nGenerative Hierarchical Neural Network Models. In AAAI,\\n2016. 3\\n\\n[46] I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,\\nA. Courville, and Y. Bengio. A Hierarchical Latent Variable\\nEncoder-Decoder Model for Generating Dialogues. arXiv\\npreprint arXiv:1605.06069, 2016. 3, 7\\n\\n[47] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,\\nG. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\\nV. Panneershelvam, M. Lanctot, et al. Mastering the game\\nof Go with deep neural networks and tree search. Nature,\\n529(7587):484\\xe2\\x80\\x93489, 2016. 1\\n\\n[48] K. Simonyan and A. Zisserman. Very deep convolutional\\nnetworks for large-scale image recognition. In ICLR, 2015.\\n7\\n\\n[49] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Ur-\\ntasun, and S. Fidler. MovieQA: Understanding Stories in\\nMovies through Question-Answering. In CVPR, 2016. 1\\n\\n[50] K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S. C. Zhu.\\n\\nJoint Video and Text Parsing for Understanding Events and\\nAnswering Queries. IEEE MultiMedia, 2014. 1\\n\\n[51] S. Venugopalan, M. Rohrbach, J. Donahue, R. J. Mooney,\\nT. Darrell, and K. Saenko. Sequence to Sequence - Video to\\nText. In ICCV, 2015. 3\\n\\n[52] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. J.\\nMooney, and K. Saenko. Translating Videos to Natural Lan-\\nguage Using Deep Recurrent Neural Networks. In NAACL\\nHLT, 2015. 3\\n\\n[53] O. Vinyals and Q. Le. A Neural Conversational Model. arXiv\\n\\npreprint arXiv:1506.05869, 2015. 3\\n\\n[54] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and\\n\\ntell: A neural image caption generator. In CVPR, 2015. 3\\n\\n[55] L. Wang, S. Guo, W. Huang, Y. Xiong, and Y. Qiao.\\nKnowledge Guided Disambiguation for Large-Scale Scene\\nClassi\\xef\\xac\\x81cation with Multi-Resolution CNNs. arXiv preprint\\narXiv:1610.01119, 2016. 1\\n\\n[56] J. Weizenbaum. ELIZA. http://psych.fullerton.\\n\\nedu/mbirnbaum/psych101/Eliza.htm. 2, 3\\n\\n[57] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards\\nAI-Complete Question Answering: A Set of Prerequisite Toy\\nTasks. In ICLR, 2016. 1, 3\\n\\n[58] S. Wu, H. Pique, and J. Wieland.\\nto Help Blind People\\n\\nIntelligence\\nhttp://newsroom.fb.com/news/2016/04/using-arti\\xef\\xac\\x81cial-\\nintelligence-to-help-blind-people-see-facebook/,\\n1\\n\\nUsing Arti\\xef\\xac\\x81cial\\n\\xe2\\x80\\x98See\\xe2\\x80\\x99 Facebook.\\n\\n2016.\\n\\n[59] Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola. Stacked\\nIn\\n\\nAttention Networks for Image Question Answering.\\nCVPR, 2016. 8\\n\\n[60] L. Yu, E. Park, A. C. Berg, and T. L. Berg. Visual Madlibs:\\nFill in the blank Image Generation and Question Answering.\\nIn ICCV, 2015. 3, 11\\n\\n[61] P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and\\nD. Parikh. Yin and Yang: Balancing and Answering Binary\\nVisual Questions. In CVPR, 2016. 5, 6, 13, 14\\n\\n[62] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W:\\nGrounded Question Answering in Images. In CVPR, 2016.\\n4, 11, 13\\n\\n\\x0c',\n",
       " 'Sequential Match Network: A New Architecture for Multi-turn Response\\n\\nSelection in Retrieval-based Chatbots\\n\\nYu Wu\\xe2\\x80\\xa0\\xe2\\x88\\x97 , Wei Wu\\xe2\\x80\\xa1\\n\\n, Zhoujun Li\\xe2\\x80\\xa0\\n\\n, Ming Zhou\\xe2\\x80\\xa1\\n\\n\\xe2\\x80\\xa0State Key Lab of Software Development Environment, Beihang University, Beijing, China\\n\\n\\xe2\\x80\\xa1 Microsoft Research, Beijing, China\\n\\n{wuyu,lizj}@buaa.edu.cn {wuwei,mingzhou}@microsoft.com\\n\\n6\\n1\\n0\\n2\\n \\nc\\ne\\nD\\n6\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n7\\n2\\n6\\n1\\n0\\n\\n.\\n\\n2\\n1\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe study response selection for multi-turn\\nconversation in retrieval based chatbots.\\nExisting works either ignores relationships\\namong utterances, or misses important in-\\nformation in context when matching a re-\\nsponse with a highly abstract context vec-\\ntor \\xef\\xac\\x81nally. We propose a new session\\nbased matching model\\nto address both\\nproblems. The model \\xef\\xac\\x81rst matches a re-\\nsponse with each utterance on multiple\\ngranularities, and distills important match-\\ning information from each pair as a vec-\\ntor with convolution and pooling opera-\\ntions. The vectors are then accumulated\\nin a chronological order through a recur-\\nrent neural network (RNN) which mod-\\nels the relationships among the utterances.\\nThe \\xef\\xac\\x81nal matching score is calculated with\\nthe hidden states of the RNN. Empirical\\nstudy on two public data sets shows that\\nour model can signi\\xef\\xac\\x81cantly outperform the\\nstate-of-the-art methods for response se-\\nlection in multi-turn conversation.\\n\\n1\\n\\nIntroduction\\n\\nTraditional research in human-computer conversa-\\ntion focused on building task-oriented dialog sys-\\ntems in vertical domains to help people complete\\nspeci\\xef\\xac\\x81c tasks such as ordering and tutoring etc\\n(Boden, 2006; Wallace, 2009; Young et al., 2010).\\nRecently, with the large amount of conversation\\ndata available on Internet, there has been a surge\\nof interest in building non-task-oriented chatbots\\nthat can naturally and meaningfully converse with\\nhumans on open domain topics (Jafarpour et al.,\\n2010; Ritter et al., 2011). Existing work on build-\\ning chatbots includes generation based methods\\n\\xe2\\x88\\x97The work was done when the \\xef\\xac\\x81rst author was an intern\\n\\nin Microsoft Research Asia.\\n\\nTable 1: An example of multi-turn conversation\\nContext 1: I am going to hold a drum class\\nin Shanghai. Anyone wants to join? The\\nlocation is near the Bound.\\nContext 2: Interesting! Do you have coaches\\nwho can help me practice drum?\\nContext 3: Of course.\\nMessage: Can I have a free \\xef\\xac\\x81rst lesson?\\n\\nand retrieval based methods.\\nIn this work, we\\nstudy retrieval based chatbots, because they select\\nresponses from an index of existing conversation\\nand thus can leverage the existing search power\\nand always return \\xef\\xac\\x82uent responses. While most\\nexisting work on retrieval based chatbots stud-\\nies response selection for single-turn conversation\\n(Wang et al., 2013; Wang et al., 2015), we con-\\nsider the problem in a multi-turn scenario which\\nis the nature of conversation but has not been well\\nexplored yet.\\n\\nDifferent from response selection in single-turn\\nconversation in which one only needs to match a\\nresponse with a single input message, response se-\\nlection in multi-turn conversation requires match-\\ning between a response and a conversation session\\nin which one needs to consider not only the match-\\ning between the response and the input message\\nbut also the matching between the response and\\nthe utterances in previous turns as context. The\\nchallenges of the task include (1) how to iden-\\ntify important information (words, phrases, and\\nsentences) in context that is crucial to selecting a\\nproper response for the session and how to lever-\\nage the information in matching; and (2) how to\\nmodel relationships among the utterances. Ta-\\nble 1 illustrates the challenges with an example.\\nFirst, \\xe2\\x80\\x9chold a drum class\\xe2\\x80\\x9d and \\xe2\\x80\\x9cdrum\\xe2\\x80\\x9d in the con-\\ntext are very important. Without them, one may\\n\\xef\\xac\\x81nd responses relevant to the message but non-\\nsense in the session (e.g., \\xe2\\x80\\x9cwhat lessons do you\\n\\n\\x0cwant?\\xe2\\x80\\x9d). On the other hand, although \\xe2\\x80\\x9cShanghai\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9cthe Bund\\xe2\\x80\\x9d, and \\xe2\\x80\\x9ccoaches\\xe2\\x80\\x9d are also keywords in\\ntheir utterances, they are useless and even noise to\\nresponse selection. It is crucial yet non-trivial to\\nextract the important information from the context\\nand leverage them in matching while circumvent\\nthe noise. Second, the message highly depends on\\nContext 1, and the order of the utterances matters\\nin response selection: exchanging Context 2 and\\nthe message may lead to different responses. Ex-\\nisting work, however, either ignores relationships\\namong utterances (Lowe et al., 2015; Yan et al.,\\n2016), or loses important information in context\\nin the process of converting the whole session to a\\nvector without enough supervision from responses\\n(Lowe et al., 2015; Zhou et al., 2016).\\n\\nWe propose a new session based matching\\nmodel which can tackle both challenges in an end-\\nto-end way. One major problem suffered by the\\nexisting models is that responses in matching can-\\nnot meet the session until the \\xef\\xac\\x81nal step, which re-\\nsults in information loss. To overcome this draw-\\nback, our model matches a response with each ut-\\nterance in the session (message and context) at the\\nvery beginning. For each utterance-response pair,\\nthe model constructs a word-word similarity ma-\\ntrix and a sequence-sequence similarity matrix by\\nthe embedding of words and the hidden states of a\\nrecurrent neural network with gated unites (GRU)\\n(Chung et al., 2014) respectively. The two matri-\\nces capture important matching information in the\\npair on a word level and a segment level respec-\\ntively, and the information is distilled and fused as\\na matching vector through an alternation of con-\\nvolution and pooling operations on the matrices.\\nBy this means, important information in context is\\nrecognized under suf\\xef\\xac\\x81cient supervision from the\\nresponse and carried into matching with minimal\\nloss. The matching vectors are then uploaded to\\na GRU to form a matching score for the session\\nand the response. The GRU accumulates the pair\\nmatching in its hidden states in the chronological\\norder of the utterances in the session. It models\\nthe relationships and the dependencies among the\\nutterances in a matching fashion and has the ut-\\nterance order supervise the accumulation of pair\\nmatching. The gate mechanism of the GRU helps\\nselect important pairs and \\xef\\xac\\x81lter out noise. The\\nmatching degree of the session and the response\\nis computed by a logit model with the hidden\\nstates of the GRU. Our model extends the pow-\\n\\nerful \\xe2\\x80\\x9c2D\\xe2\\x80\\x9d matching paradigm in text pair match-\\ning for single-turn conversation to session based\\nmatching for multi-turn conversation, and enjoys\\nthe advantage that both important information in\\nutterance-response pairs and relationships among\\nutterances are suf\\xef\\xac\\x81ciently preserved and leveraged\\nin matching.\\n\\nWe test our model on the Ubuntu dialogue\\ncorpus (Lowe et al., 2015) which is a large\\npublic English data set for research in multi-turn\\nconversation. The results show that our model\\ncan signi\\xef\\xac\\x81cantly outperform the state-of-the-art\\nmethods, and improvement to the best baseline\\nmodel on R10@1 is over 6%. One problem with\\nthe Ubuntu data is that negative examples are\\nrandomly sampled which might oversimplify\\nthe multi-turn problem in a real retrieval based\\nchatbot. To further verify the ef\\xef\\xac\\x81cacy of the\\nproposed model in a real situation, we simulate\\nthe procedure of a retrieval based chatbot and\\ncreate a large scale Chinese test set.\\nInstead of\\nnegative sampling, labels in the data are generated\\nby 3 human judges. On this data, our model im-\\nproves the best baseline model over 4% on P@1\\n(equivalent to R10@1). We published the data\\nat\\nhttps://github.com/MarkWuNLP/\\nMultiTurnResponseSelection.\\n\\nOur contributions in this paper are three-folds:\\n(1) proposal of a new session based matching\\nmodel for multi-turn response selection in retrieval\\nbased chatbots; (2) empirical veri\\xef\\xac\\x81cation of the ef-\\nfectiveness of the model on public data sets; (3)\\npublication of a large human labeled data set to\\nresearch communities.\\n\\n2 Related Work\\n\\nEarly work (Weizenbaum, 1966) on chatbots\\nexploits hand crafted templates to generate re-\\nsponses, which requires huge human effort and\\nis not scalable. Recently, data driven approaches\\n(Ritter et al., 2011; Higashinaka et al., 2014) have\\ndrawn a lot of attention. Existing work along this\\nline includes retrieval based methods and genera-\\ntion based methods. The former selects a proper\\nresponse from an index based on matching be-\\ntween the response and an input message with or\\nwithout context (Hu et al., 2014; Ji et al., 2014;\\nWang et al., 2015; Yan et al., 2016; Wu et al.,\\n2016; Zhou et al., 2016), while the latter employs\\nstatistical machine translation techniques (Ritter et\\nal., 2011) or the sequence to sequence framework\\n\\n\\x0c(Shang et al., 2015; Serban et al., 2015; Vinyals\\nand Le, 2015; Li et al., 2015; Li et al., 2016;\\nXing et al., 2016; Serban et al., 2016) to gener-\\nate responses. Our work belongs to retrieval based\\nmethods, and we study response selection with\\ncontext information.\\n\\nEarly studies of retrieval based chatbots focus\\non response selection for single-turn conversation\\n(Wang et al., 2013; Ji et al., 2014; Wang et al.,\\n2015; Wu et al., 2016). Recently, researchers be-\\ngin to pay attention to multi-turn conversation. For\\nexample, Lowe et al. (Lowe et al., 2015) match a\\nresponse with the literal concatenation of context\\nutterances. Yan et al. (Yan et al., 2016) concate-\\nnate context utterances with the input message as\\nreformulated queries and perform matching with\\na deep neural network architecture. Zhou et al.\\n(Zhou et al., 2016) improve multi-turn response\\nselection with a multi-view model including an ut-\\nterance view and a word view. The stark difference\\nbetween our model and the existing models is that\\nour model matches a response with each utterance\\nat the very \\xef\\xac\\x81rst and matching information instead\\nof sentences is accumulated in a temporal manner\\nthrough a GRU.\\n\\n3 Matching Approach\\n3.1 Problem Formalization\\nSuppose that we have a data set D =\\n{(yi, si, ri)}N\\ni=1, where si = {ui,1, . . . , ui,ni}\\nrepresents\\nwith\\na\\nconversation\\n{ui1, . . . , ui,ni\\xe2\\x88\\x921} utterances\\nin context and\\nui,ni an input message. ri is a response candidate\\nand yi \\xe2\\x88\\x88 {0, 1} denotes a label. yi = 1 means\\nri is a proper response for si, otherwise yi = 0.\\nOur goal is to learn a matching model g(\\xc2\\xb7,\\xc2\\xb7) with\\nD. For any session-response pair (s, r), g(s, r)\\nmeasures the matching degree between s and r.\\n\\nsession\\n\\n3.2 Model Overview\\nFigure 1 gives the architecture of our model. The\\nmodel \\xef\\xac\\x81rst decomposes session-response match-\\ning into several utterance-response pair matching\\nand then all pair matching are accumulated as a\\nsession based matching through a recurrent neu-\\nral network. Speci\\xef\\xac\\x81cally, the model consists of\\ntwo layers. The \\xef\\xac\\x81rst layer matches a response\\ncandidate with each utterance (context and mes-\\nsage) in the session on a word level and a segment\\nlevel. An utterance-response pair is transformed\\nto a word-word similarity matrix and a sequence-\\n\\nsequence similarity matrix, and important match-\\ning information in the pair is distilled from the two\\nmatrices and encoded in a matching vector. The\\nmatching vectors are then fed to the second layer\\nwhere they are accumulated in the hidden states\\nof a recurrent neural network with gated unites\\n(GRU) following the chronological order of the ut-\\nterances in the session. The matching degree of\\nthe session and the response is calculated with the\\nhidden states of the GRU.\\n\\nOur model enjoys several advantages over the\\nexisting models. First, a response candidate can\\nmeet each utterance in the session at the very be-\\nginning of the whole matching procedure, thus\\nmatching information in every utterance-response\\npair can be suf\\xef\\xac\\x81ciently extracted and carried to the\\n\\xef\\xac\\x81nal matching score with minimal loss. Second,\\ninformation extraction from each utterance is con-\\nducted on different granularities and under suf\\xef\\xac\\x81-\\ncient supervision from the response, thus seman-\\ntic structures that are useful to response selection\\nin each utterance can be well identi\\xef\\xac\\x81ed and ex-\\ntracted. Third, matching and utterance relation-\\nships are coupled rather than separately modeled,\\nthus utterance relationships (e.g., order), as a kind\\nof knowledge, can supervise the formation of the\\nmatching score.\\n\\nBy taking utterance relationships into consid-\\neration, our model extends the \\xe2\\x80\\x9c2D\\xe2\\x80\\x9d matching\\nwhich has proven effective in text pair match-\\ning for single-turn response selection to sequen-\\ntial \\xe2\\x80\\x9c2D\\xe2\\x80\\x9d matching for session based matching\\nin response selection for multi-turn conversation.\\nWe name our model \\xe2\\x80\\x9cSequential Match Network\\xe2\\x80\\x9d\\n(SMN). In the following sections, we will describe\\ndetails of the two layers.\\n\\n3.3 Utterance-Response Matching\\n\\nAt the \\xef\\xac\\x81rst layer, given an utterance u in a ses-\\nsion s and a response candidate r, the model looks\\nup an embedding table and represents u and r as\\nU = [eu,1, . . . , eu,nu] and R = [er,1, . . . , er,nr ]\\nrespectively, where eu,i, er,i \\xe2\\x88\\x88 Rd are the em-\\nbeddings of the i-th word of u and r respec-\\ntively. U \\xe2\\x88\\x88 Rd\\xc3\\x97nu and R \\xe2\\x88\\x88 Rd\\xc3\\x97nr are then\\nused to construct a word-word similarity matrix\\nM1 \\xe2\\x88\\x88 Rnu\\xc3\\x97nr and a sequence-sequence similarity\\nmatrix M2 \\xe2\\x88\\x88 Rnu\\xc3\\x97nr which are two input chan-\\nnels of a convolutional neural network (CNN). The\\nCNN distills important matching information from\\nthe matrices and encodes the information into a\\n\\n\\x0cFigure 1: Architecture of SMN\\n\\nmatching vector v.\\n\\nSpeci\\xef\\xac\\x81cally, \\xe2\\x88\\x80i, j, the (i, j)-th element of M1\\n\\nis de\\xef\\xac\\x81ned by\\n\\ne1,i,j = e(cid:62)\\n\\nu,i \\xc2\\xb7 er,j.\\n\\n(1)\\n\\nM1 models the matching between u and r on a\\nword level.\\n\\nTo construct M2, we \\xef\\xac\\x81rst employ a recurrent\\nneural network with gated units (GRU) (Chung et\\nal., 2014) to transform U and R to hidden vectors.\\nSuppose that Hu = [hu,1, . . . , hu,nu] is the hidden\\nvectors of U, then \\xe2\\x88\\x80i, hu,i \\xe2\\x88\\x88 Rm is de\\xef\\xac\\x81ned by\\n\\nzi = \\xcf\\x83(Wzeu,i + Uzhu,i\\xe2\\x88\\x921)\\nri = \\xcf\\x83(Wreu,i + Urhu,i\\xe2\\x88\\x921)\\n\\n(cid:101)hu,i = tanh(Wheu,i + Uh(ri (cid:12) hu,i\\xe2\\x88\\x921))\\nhu,i = zi (cid:12)(cid:101)hu,i + (1 \\xe2\\x88\\x92 zi) (cid:12) hu,i\\xe2\\x88\\x921,\\n\\n(2)\\n\\nwhere hu,0 = 0, zi and ri are an update gate and a\\nreset gate respectively, \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\nand Wz, Wh, Wr, Uz, Ur,Uh are parameters.\\nSimilarly, we have Hr = [hr,1, . . . , hr,nr ] as the\\nhidden vectors of R. Then, \\xe2\\x88\\x80i, j, the (i, j)-th ele-\\nment of M2 is de\\xef\\xac\\x81ned by\\ne2,i,j = h(cid:62)\\n\\n(3)\\nwhere W1 \\xe2\\x88\\x88 Rm\\xc3\\x97m is a linear transformation.\\n\\xe2\\x88\\x80i, GRU models the sequential relationship and\\nthe dependency among words up to position i and\\nencodes the text segment until the i-th word to a\\nhidden vector. Therefore, M2 models the match-\\ning between u and r on a segment level.\\n\\nu,iW1hr,j,\\n\\nM1 and M2 are then processed by a CNN\\n\\xe2\\x88\\x80i = 1, 2, CNN regards Mi as\\n\\nto form v.\\n\\n(cid:104)\\n\\n(cid:105)\\n\\nI (l,f )\\xc3\\x97J (l,f )\\n\\nan input channel, and alternates convolution and\\nmax-pooling operations. Suppose that z(l,f ) =\\nz(l,f )\\ndenotes the output of feature\\ni,j\\nmaps of type-f on layer-l, where z(0,f ) = Mf ,\\n\\xe2\\x88\\x80f = 1, 2. On the convolution layer, we employ\\na 2D convolution operation with a window size\\nw \\xc3\\x97 r(l,f )\\nr(l,f )\\nFl\\xe2\\x88\\x921(cid:88)\\n\\n, and de\\xef\\xac\\x81ne z(l,f )\\n\\nw(cid:88)\\n\\nh(cid:88)\\n\\nas\\n\\n(l,f )\\n\\n(l,f )\\n\\ni,j\\n\\n\\xc2\\xb7 z(l\\xe2\\x88\\x921,f(cid:48))\\n\\ni+s,j+t + bl,k), (4)\\n\\nz(l,f )\\ni,j = \\xcf\\x83(\\n\\nW(l,f )\\n\\ns,t\\n\\nh\\n\\nr\\n\\nr\\n\\nf(cid:48)=0\\n\\ns=0\\n\\nt=0\\n\\nw \\xc3\\x97r(l,f )\\n\\nwhere \\xcf\\x83(\\xc2\\xb7) is a ReLU, W(l,f ) \\xe2\\x88\\x88 Rr(l,f )\\nand\\nbl,k are parameters, and Fl\\xe2\\x88\\x921 is the number of fea-\\nture maps on the (l \\xe2\\x88\\x92 1)-th layer. An max pooling\\noperation follows a convolution operation and can\\nbe formulated as\\n\\nh\\n\\nz(l,f )\\ni,j = max\\n\\n(l,f )\\n\\nw >s\\xe2\\x89\\xa50\\n\\np\\n\\nmax\\nh >t\\xe2\\x89\\xa50\\n\\n(l,f )\\n\\np\\n\\nzi+s,j+t,\\n\\n(5)\\n\\nh\\n\\nw\\n\\nand p(l,f )\\n\\nwhere p(l,f )\\nare the width and the height\\nof the 2D pooling respectively. The output of the\\n\\xef\\xac\\x81nal feature maps are concatenated and mapped to\\na low dimensional space with a linear transforma-\\ntion as the matching vector v \\xe2\\x88\\x88 Rq.\\n\\nFrom Equation (1), (3), (4), and (5), we can see\\nthat by learning word embedding and parameters\\nof GRU from training data, words or segments in\\nan utterance that are useful to recognize the appro-\\npriateness of a response may have high similarity\\nwith some words or segments in the response and\\nresult in high value areas in the similarity matri-\\nces. These areas will be transformed and selected\\nby convolution and pooling operations and carry\\n\\n .... .... ....Score12,MMConvolutionPoolingMatching Acclumulation()L ....1,,,,Tijuirjeee\\xef\\x80\\xbd2,,,1,TijuirjehWh\\xef\\x80\\xbd .... ....1u1nu\\xef\\x80\\xadnurWord EmbeddingGRU1Sim MatricesGRU2CNN\\x0cthe important information in the utterance to the\\nmatching vector. This is how our model identi\\xef\\xac\\x81es\\nimportant information in context and leverage it\\nin matching under the supervision of the response.\\nWe consider multiple channels because we want to\\ncapture important matching information on multi-\\nple granularities of text.\\n\\n3.4 Matching Accumulation\\nSuppose that [v1, . . . , vn] is the output of the \\xef\\xac\\x81rst\\nlayer (corresponding to n pairs), at the second\\nlayer, a GRU takes [v1, . . . , vn] as an input and en-\\ncodes the matching sequence into its hidden states\\nHm = [h1, . . . , hn] \\xe2\\x88\\x88 Rq\\xc3\\x97n with a detailed pa-\\nrameterization similar to Equation (2). This layer\\nhas two functions: (1) it models the dependency\\nand the temporal relationship of utterances in the\\nsession; (2) it leverages the temporal relationship\\nto supervise the accumulation of the pair match-\\ning as a session based matching. Moreover, from\\nEquation (2), we can see that the reset gate (i.e.,\\nri) and the update gate (i.e., zi) control how much\\ninformation from the previous hidden state and\\nthe current input \\xef\\xac\\x82ows to the current hidden state,\\nthus important matching vectors (corresponding to\\nimportant utterances) can be accumulated while\\nnoise in the vectors can be \\xef\\xac\\x81ltered out.\\n\\nWith Hm, we de\\xef\\xac\\x81ne g(s, r) as\\n\\ng(s, r) = sof tmax(W2L[h1, . . . , hn] + b2),\\n\\n(6)\\nwhere W2 and b2 are parameters. We con-\\nsider three parameterizations for L[h1, . . . , hn]:\\n(1) only the last hidden state is used. Then\\n(cid:80)n\\nL[h1, . . . , hn] = hn.\\n(2) the hidden states\\nare linearly combined. Then, L[h1, . . . , hn] =\\ni=1 wihi, where wi \\xe2\\x88\\x88 R. (3) we follow (Yang\\net al., 2016) and employ an attention mechanism\\nto combine the hidden states. Then, L[h1, . . . , hn]\\nis de\\xef\\xac\\x81ned as\\n\\nti = tanh(W3hi + b3),\\n\\n(cid:80)\\nexp(t(cid:62)\\ni ts)\\nn(cid:88)\\ni(exp(t(cid:62)\\n\\ni ts))\\n\\n,\\n\\n\\xce\\xb1i =\\n\\nL[h1, . . . , hn] =\\n\\n\\xce\\xb1ihi,\\n\\ni=1\\n\\nwhere W3 \\xe2\\x88\\x88 Rq\\xc3\\x97q and b3 \\xe2\\x88\\x88 Rq are parameters.\\nts \\xe2\\x88\\x88 Rq is a high level virtual context vector which\\nis randomly initialized and jointly learned in train-\\ning.\\n\\ni=1\\n\\n[yilog(g(si, ri)) + (1 \\xe2\\x88\\x92 yi)log(1 \\xe2\\x88\\x92 g(si, ri))] , (7)\\n\\nBoth (2) and (3) aim to learn weights for\\n{h1, . . . , hn} from training data and dynamically\\nhighlight the effect of important matching vec-\\ntors in the \\xef\\xac\\x81nal matching score. The difference\\nis that weights in (2) are nonparametric and un-\\nnormalized, while in (3) they are parametric and\\nnormalized. We denote our model with the three\\nparameterizations of L[h1, . . . , hn] as SMNlast,\\nSMNnon\\xe2\\x88\\x92para, and SMNpara respectively, and\\nempirically compare them in experiments.\\nWe learn g(\\xc2\\xb7,\\xc2\\xb7) by minimizing cross entropy\\nwith D. Let \\xce\\x98 denote the parameters of our model,\\nthen the objective function L(D, \\xce\\x98) of learning\\ncan be formulated as\\n\\xe2\\x88\\x92 N(cid:88)\\nwhere N in the number of instances in D.\\n4 Response Candidate Retrieval\\nIn practice of a retrieval based chatbot, to apply\\nthe matching approach to response selection, one\\nneeds to retrieve a bunch of response candidates\\nfrom an index beforehand. While candidate re-\\ntrieval is not the focus of the paper, it is an im-\\nportant step in a real system. In this work, we ex-\\nploit a heuristic method to obtain response can-\\ndidates from index. Given a message un with\\n{u1, . . . , un\\xe2\\x88\\x921} utterances in its previous turns,\\nwe extract top 5 keywords based on their tf-idf\\nvalues1 and expand un with the keywords. Then\\nwe send the expanded message to the index and\\nretrieve response candidates using the inline re-\\ntrieval algorithm of the index. Finally, we use\\ng(s, r) to re-rank the candidates and return the top\\none as a response to the session.\\n\\n5 Experiment\\nWe tested our model on a public English data set\\nand a Chinese data set we publish with this paper.\\n\\n5.1 Experiment setup\\nThe English data set is the Ubuntu Corpus (Lowe\\net al., 2015) which contains large scale multi-turn\\ndialogues collected from chat logs of Ubuntu Fo-\\nrum. The data set consists of 1 million session-\\nresponse pairs for training, 0.5 million pairs for\\nvalidation, and 0.5 million pairs for test. Posi-\\ntive responses are true responses from human, and\\n\\n1Tf is word frequency in the session, while idf is calcu-\\n\\nlated using the entire index.\\n\\n\\x0cnegative ones are randomly sampled. The ratio of\\nthe positive and the negative is 1:1 in training, and\\n1:9 in validation and test. We used the copy shared\\nby Xu et al. (2016)2 in which numbers, urls, and\\npaths are replaced by special placeholders.\\n\\nOne problem with the Ubuntu data is that nega-\\ntive examples is much easier to identify than those\\nin a real chatbot, because they are randomly sam-\\npled and most of them are far from the semantics\\nof the context. A better data set that can simulate\\nthe real scenario of a retrieval based chatbot must\\nhave responses generated following the procedure\\nof information retrieval and labels annotated by\\nhumans. As far as we know, however, there are\\nno such data sets publicly available. To test our\\nmodel in a setting closer to the real case and facil-\\nitate the research of multi-turn response selection,\\nwe created a new data set and publish it to research\\ncommunities with the paper. We crawled 15 mil-\\nlion post-reply pairs from Sina Weibo3 which is\\nthe largest microblogging service in China and in-\\ndexed the pairs with an open source Lucene4. We\\nthen crawled 1.1 million dyadic dialogues (con-\\nversation between two people) longer than 2 turns\\nfrom Douban group5 which is a popular forum in\\nChina. From the data, we randomly sampled 0.5\\nmillion dialogues for creating a training set, 25\\nthousand dialouges for creating a validation set,\\nand 1, 000 dialogues for creating a test set, and\\nmade sure that there is no overlap among the three\\nsets. For each dialogue in training and validation,\\nwe took the last turn as a positive response for\\nthe previous turns as a session and randomly sam-\\npled another response from the 1.1 million data\\nas a negative response. In total, there are 1 mil-\\nlion session-response pairs in the training set and\\n50 thousand pairs in the validation set. To cre-\\nate the test set, we took the last turn of each dia-\\nlogue as a message, retrieved 10 response candi-\\ndates from the index following the method in Sec-\\ntion 4, and \\xef\\xac\\x81nally formed a test set with 10, 000\\nsession-response pairs. We recruited three label-\\ners to judge if a candidate is a proper response to\\nthe session. A proper response means the response\\ncan naturally reply to the message given the con-\\ntext. Each pair received three labels and the ma-\\njority of the labels was taken as the \\xef\\xac\\x81nal decision.\\n\\n2https://www.dropbox.com/s/\\n\\n2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\\n\\n3http://weibo.com/\\n4https://lucenenet.apache.org/\\n5https://www.douban.com/group\\n\\n# response candidates per session\\n\\nAvg. # positive responses per session\\n\\n# session-response pairs\\n\\nTable 2: Statistics of the Chinese data set\\nval\\n50k\\n2\\n1\\n3\\n40\\n5.81\\n17.22\\n\\nMin. # turns per session\\nMax. # turns per session\\nAvg. # turns per session\\n\\ntrain\\n1M\\n2\\n1\\n3\\n53\\n6.03\\n16.75\\n\\nAvg. # words per utterance\\n\\ntest\\n10k\\n10\\n1.18\\n\\n3\\n44\\n5.95\\n17.17\\n\\nTable 2 gives the statistics of the three sets. Note\\nthat the Fleiss\\xe2\\x80\\x99 kappa (Fleiss, 1971) of the label-\\ning is 0.41, which indicates that the three labelers\\nreached a relatively high agreement.\\n\\nOn the Ubuntu data, we followed (Lowe et al.,\\n2015) and employed recall at position k in n can-\\ndidates (Rn@k) as evaluation metrics, and on the\\nhuman labeled data, we followed the convention\\nof information retrieval and employed mean aver-\\nage precision (MAP) (Baeza-Yates et al., 1999),\\nmean reciprocal rank (MRR) (Voorhees and oth-\\ners, 1999), and precision at position 1 (P@1) as\\nmetrics. Note that when using the labeled set, we\\nremoved sessions with all negative responses or\\nall positive responses, as models make no differ-\\nence on them. After that there are 6, 670 session-\\nresponse pairs left in test.\\n\\n5.2 Baseline\\nWe considered the following baselines:\\n\\nBasic models: models in (Lowe et al., 2015)\\nand (Kadlec et al., 2015) including TF-IDF, RNN,\\nCNN, LSTM and BiLSTM.\\n\\nMulti-view: the model proposed by Zhou et al.\\n(2016) who utilize a hierarchical recurrent neural\\nnetwork to model utterance relationships.\\n\\nDeep learning to respond (DL2R): the model\\n\\nproposed by Yan et al. (2016).\\n\\nAdvanced single-turn matching models:\\nsince LSTM and BiLSTM do not represent the\\nstate-of-the-art matching models, we concatenated\\nthe utterances in a session and matched the long\\ntext with a response candidate using more pow-\\nerful models including MV-LSTM (Wan et al.,\\n2016), Match-LSTM (Wang and Jiang, 2015), and\\nMulti-Channel which is described in Section 3.3.\\nMulti-Channel is a simple version of our model\\nwithout considering utterance relationships.\\n\\n5.3 Parameter Tuning\\nFor baseline models, if their results are available\\nin the existing literatures (e.g., those on Ubuntu\\nCorpus), we just copied the numbers, otherwise\\n\\n\\x0cTable 3: Evaluation results on the two data sets\\n\\nUbuntu data\\n\\nChinese data\\n\\nTF-IDF (Lowe et al., 2015)\\nRNN (Lowe et al., 2015)\\nCNN (Kadlec et al., 2015)\\nLSTM (Kadlec et al., 2015)\\nBiLSTM (Kadlec et al., 2015)\\nMulti-View (Zhou et al., 2016)\\nDL2R (Yan et al., 2016)\\nMV-LSTM (Wan et al., 2016)\\nMatch-LSTM (Wang and Jiang, 2015)\\nMulti-Channel\\nSMNlast\\nSMNnon\\xe2\\x88\\x92para\\nSMNpara\\n\\nR2@1 R10@1 R10@2 R10@5 MAP MRR P@1\\n0.179\\n0.659\\n0.768\\n0.208\\n0.226\\n0.848\\n0.320\\n0.901\\n0.313\\n0.895\\n0.908\\n0.342\\n0.330\\n0.899\\n0.348\\n0.906\\n0.904\\n0.345\\n0.349\\n0.904\\n0.392\\n0.923\\n0.387\\n0.927\\n0.926\\n0.372\\n\\n0.359\\n0.422\\n0.440\\n0.527\\n0.514\\n0.543\\n0.527\\n0.538\\n0.537\\n0.543\\n0.571\\n0.572\\n0.561\\n\\n0.708\\n0.819\\n0.896\\n0.949\\n0.944\\n0.951\\n0.944\\n0.946\\n0.944\\n0.942\\n0.956\\n0.962\\n0.960\\n\\n0.545\\n0.547\\n0.684\\n0.784\\n0.780\\n0.801\\n0.783\\n0.804\\n0.799\\n0.809\\n0.842\\n0.838\\n0.848\\n\\n0.331\\n0.390\\n0.417\\n0.485\\n0.479\\n0.505\\n0.488\\n0.498\\n0.500\\n0.506\\n0.526\\n0.523\\n0.517\\n\\n0.410\\n0.403\\n0.549\\n0.638\\n0.630\\n0.662\\n0.626\\n0.653\\n0.653\\n0.656\\n0.723\\n0.725\\n0.726\\n\\nTable 4: Evaluation results of model ablation\\n\\nUbuntu data\\n\\nChinese data\\n\\nReplaceM\\nReplaceS\\nOnly M1\\nOnly M2\\nSMNlast\\n\\nR2@1\\n0.905\\n0.918\\n0.919\\n0.921\\n0.923\\n\\nR10@1\\n0.661\\n0.716\\n0.704\\n0.715\\n0.723\\n\\nR10@2\\n0.799\\n0.832\\n0.832\\n0.836\\n0.842\\n\\nR10@5 MAP MRR\\n0.541\\n0.950\\n0.565\\n0.954\\n0.562\\n0.955\\n0.565\\n0.956\\n0.956\\n0.571\\n\\n0.503\\n0.522\\n0.518\\n0.521\\n0.526\\n\\nP@1\\n0.343\\n0.376\\n0.370\\n0.382\\n0.392\\n\\n(a) M1 of u1 and r\\n\\n(b) M1 of u3 and r\\n\\n(c) Update gate\\n\\nFigure 2: Model visualization\\n\\nwe implemented the models following the settings\\nin the literatures. All models were implemented\\nusing Theano (Theano Development Team, 2016).\\nWord embeddings were initialized by the results\\nof word2vec 6 run on the training data, and the\\ndimension of word vectors is 200. For Multi-\\nChannel and layer one of our model, we set the di-\\nmension of the hidden states of GRU as 200. We\\ntuned the window size in convolution and pool-\\ning in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) \\xef\\xac\\x81nally.\\nThe number of feature maps is 8.\\nIn layer two,\\n\\n6https://code.google.com/archive/p/\\n\\nword2vec/\\n\\nwe set the dimensions of matching vectors and\\nthe hidden states of GRU as 50. We optimized\\nthe objective function using back-propagation and\\nthe parameters were updated by stochastic gradi-\\nent descent with Adam algorithm (Kingma and\\nBa, 2014) on a single Tesla K80 GPU. The ini-\\ntial learning rate is 0.001, and the parameters of\\nAdam, \\xce\\xb21 and \\xce\\xb22, that control exponential de-\\ncay are 0.9 and 0.999 respectively. We employed\\nearly-stopping (Lawrence and Giles, 2000) as a\\nregularization strategy. Models were trained in\\nmini-batches with a batch size 200, and we padded\\nzeros if the length of an utterance exceeds 50.\\n\\nthenthecommandglebihanshouldextractthemallfrom/tothatdirectoryhowcanunzipmanyrar(_number_forexample)filesatonceValue of M_1 (u_1 and r)0.000.150.300.450.600.750.901.051.201.351.50valuethenthecommandglebihanshouldextractthemallfrom/tothatdirectoryokayhowValue of M_1 (u_3 and r)0.000.150.300.450.600.750.901.051.201.351.50value010203040u_1u_2u_3u_4u_5Value of update gate0.00.10.20.30.40.50.60.70.80.91.0value\\x0c5.4 Evaluation Results\\nTable 3 shows the evaluation results on the two\\ndata sets.\\nOur models outperform baselines\\ngreatly in terms of all metrics on both data sets,\\nand the improvements are statistically signi\\xef\\xac\\x81cant\\n(t-test with p-value \\xe2\\x89\\xa4 0.01). Even the state-of-\\nthe-art single-turn matching models perform much\\nworse than our models. The results demonstrate\\nthat one cannot neglect utterance relationships and\\nsimply perform multi-turn response selection by\\ntransforming it to a single-turn problem. Our mod-\\nels achieve signi\\xef\\xac\\x81cant improvements over Multi-\\nView, which justi\\xef\\xac\\x81ed our \\xe2\\x80\\x9cmatching \\xef\\xac\\x81rst\\xe2\\x80\\x9d strategy.\\nDL2R is also worse than our models, indicating\\nthat utterance reformulation with heuristic rules is\\nnot a good method to utilize context information.\\nNumbers on the Ubuntu data are much higher than\\nthose on the Chinese data (R10@1 and P@1 are\\nequivalent). The results showed the merit of our\\nnew data and supported our claim that the Ubuntu\\ndata oversimpli\\xef\\xac\\x81ed the problem of multi-turn re-\\nsponse selection. There is no signi\\xef\\xac\\x81cant differ-\\nence among our three models. The reason might\\nbe GRU has already selected useful signals from\\nthe matching sequence and accumulated them in\\nthe \\xef\\xac\\x81nal state with its gate mechanism, especially\\nwhen the sequence is not long, and there is no need\\nto equip another attention mechanism on top of it.\\n\\n5.5 Further Analysis\\nVisualization: we visualize the similarity matri-\\nces and the gates of GRU in layer two using an\\nexample from the Ubuntu Corpus to further clar-\\nify how our model identi\\xef\\xac\\x81es important informa-\\ntion in context and how it selects important match-\\ning vectors with the gate mechanism of GRU as\\ndescribed in Section 3.3 and Section 3.4. The ex-\\nample is {u1: how can unzip many rar ( number\\nfor example ) \\xef\\xac\\x81les at once; u2: sure you can do\\nthat in bash; u3: okay how? u4: are the \\xef\\xac\\x81les all\\nin the same directory? u5: yes they all are; r:\\nthen the command glebihan should extract them\\nall from/to that directory}. It is from the test set\\nand our model successfully ranked the correct re-\\nsponse to the top position. Due to space limitation,\\nwe only visualized M1 of u1 and r in Figure 2(a),\\nM1 of u3 and r in Figure 2(b), and the update gate\\n(i.e. z) in Figure 2(c). They are already enough\\nto support our analysis. In all pictures, darker ar-\\neas mean larger values. We can see that in u1\\nimportant words including \\xe2\\x80\\x9cunzip\\xe2\\x80\\x9d, \\xe2\\x80\\x9crar\\xe2\\x80\\x9d, \\xe2\\x80\\x9c\\xef\\xac\\x81les\\xe2\\x80\\x9d\\n\\nare recognized and carried to matching by \\xe2\\x80\\x9ccom-\\nmand\\xe2\\x80\\x9d, \\xe2\\x80\\x9cextract\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cdirectory\\xe2\\x80\\x9d in r, while u3\\nis almost useless and thus little information is ex-\\ntracted from it. u1 is crucial to response selection\\nand nearly all information from u1 and r \\xef\\xac\\x82ows to\\nthe hidden state of GRU, while other utterances\\nare less informative and the corresponding gates\\nare almost \\xe2\\x80\\x9cclosed\\xe2\\x80\\x9d to keep the information from\\nu1 and r until the \\xef\\xac\\x81nal state.\\n\\nFigure 3: Comparison across session length\\nModel ablation: we investigate the effect of\\ndifferent parts of our model by removing them one\\nby one. Table 4 reports the results. First, replac-\\ning the multi-channel \\xe2\\x80\\x9c2D\\xe2\\x80\\x9d matching with a neural\\ntensor network (NTN) (Socher et al., 2013) (de-\\nnoted as ReplaceM ) makes the performance drop\\ndramatically. This is because NTN only matches\\na pair by an utterance vector and a response vec-\\ntor and misses important information in the pair.\\nTogether with the visualization, we can conclude\\nthat \\xe2\\x80\\x9c2D\\xe2\\x80\\x9d matching plays a key role in the \\xe2\\x80\\x9cmatch-\\ning \\xef\\xac\\x81rst\\xe2\\x80\\x9d strategy as it can capture the impor-\\ntant matching information in each pair with mini-\\nmal loss. Second, the performance slightly drops\\nwhen replacing the GRU for matching accumu-\\nlation with a multi-layer perceptron (denoted as\\nReplaceS). This indicates that utterance relation-\\nships are also useful. Finally, we left only one\\nchannel in matching and found that M2 is a little\\nmore powerful than M1 and we can achieve the\\nbest results with both of them.\\n\\nSession length: we \\xef\\xac\\x81nally study how our model\\n(SMNlast) performs with respect to the length of\\nsessions. Figure 3 shows the comparison on MAP\\nin different length intervals on the Chinese data.\\nWe can see that our model consistently performs\\nbetter than the baselines, and when sessions be-\\ncome longer, the gap becomes larger. The re-\\nsults demonstrate that our model can well capture\\nthe dependencies, especially long dependencies,\\namong utterances in sessions.\\n\\n(2,5](5,10](10,)session length4045505560MAPLSTMMV-LSTMMulti-ViewSMN\\x0c6 Conclusion\\nWe present a new model for multi-turn response\\nselection in retrieval-based chatbots. Experiment\\nresults on public data sets show that the model can\\nsigni\\xef\\xac\\x81cantly outperform the state-of-the-art meth-\\nods.\\n\\nReferences\\n[Baeza-Yates et al.1999] Ricardo Baeza-Yates, Berthier\\nRibeiro-Neto, et al. 1999. Modern information re-\\ntrieval, volume 463. ACM press New York.\\n\\n[Boden2006] Margaret Ann Boden. 2006. Mind as ma-\\nchine: A history of cognitive science. Clarendon\\nPress.\\n\\n[Chung et al.2014] Junyoung Chung, Caglar Gulcehre,\\nKyungHyun Cho, and Yoshua Bengio. 2014. Em-\\npirical evaluation of gated recurrent neural net-\\narXiv preprint\\nworks on sequence modeling.\\narXiv:1412.3555.\\n\\n[Fleiss1971] Joseph L Fleiss. 1971. Measuring nomi-\\nnal scale agreement among many raters. Psycholog-\\nical bulletin, 76(5):378.\\n\\n[Higashinaka et al.2014] Ryuichiro Higashinaka, Kenji\\nImamura, Toyomi Meguro, Chiaki Miyazaki, No-\\nzomi Kobayashi, Hiroaki Sugiyama, Toru Hirano,\\nToshiro Makino, and Yoshihiro Matsuo. 2014. To-\\nwards an open-domain conversational system fully\\nbased on natural language processing. In COLING,\\npages 928\\xe2\\x80\\x93939.\\n\\n[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li,\\nand Qingcai Chen.\\n2014. Convolutional neural\\nnetwork architectures for matching natural language\\nsentences. In Advances in Neural Information Pro-\\ncessing Systems, pages 2042\\xe2\\x80\\x932050.\\n\\n[Jafarpour et al.2010] Sina Jafarpour, Christopher JC\\nBurges, and Alan Ritter. 2010. Filter, rank, and\\ntransfer the knowledge: Learning to chat. Advances\\nin Ranking, 10.\\n\\n[Ji et al.2014] Zongcheng Ji, Zhengdong Lu, and Hang\\nLi. 2014. An information retrieval approach to short\\ntext conversation. arXiv preprint arXiv:1408.6988.\\n\\n[Kadlec et al.2015] Rudolf Kadlec, Martin Schmid, and\\nJan Kleindienst. 2015.\\nImproved deep learning\\nbaselines for ubuntu corpus dialogs. arXiv preprint\\narXiv:1510.03753.\\n\\n[Kingma and Ba2014] Diederik Kingma and Jimmy\\nBa. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980.\\n\\n[Lawrence and Giles2000] Steve Lawrence and C Lee\\nGiles. 2000. Over\\xef\\xac\\x81tting and neural networks: con-\\njugate gradient and backpropagation. In Neural Net-\\nworks, 2000. IJCNN 2000, Proceedings of the IEEE-\\n\\nINNS-ENNS International Joint Conference on, vol-\\nume 1, pages 114\\xe2\\x80\\x93119. IEEE.\\n\\n[Li et al.2015] Jiwei Li, Michel Galley, Chris Brockett,\\nJianfeng Gao, and Bill Dolan. 2015. A diversity-\\npromoting objective function for neural conversation\\nmodels. arXiv preprint arXiv:1510.03055.\\n\\n[Li et al.2016] Jiwei Li, Michel Galley, Chris Brockett,\\nJianfeng Gao, and Bill Dolan. 2016. A persona-\\nbased neural conversation model. arXiv preprint\\narXiv:1603.06155.\\n\\n[Lowe et al.2015] Ryan Lowe, Nissan Pow, Iulian Ser-\\nban, and Joelle Pineau. 2015. The ubuntu dia-\\nlogue corpus: A large dataset for research in unstruc-\\ntured multi-turn dialogue systems. arXiv preprint\\narXiv:1506.08909.\\n\\n[Ritter et al.2011] Alan Ritter, Colin Cherry,\\n\\nand\\nWilliam B Dolan. 2011. Data-driven response gen-\\neration in social media. In Proceedings of the Con-\\nference on Empirical Methods in Natural Language\\nProcessing, pages 583\\xe2\\x80\\x93593. Association for Com-\\nputational Linguistics.\\n\\n[Serban et al.2015] Iulian V Serban, Alessandro Sor-\\ndoni, Yoshua Bengio, Aaron Courville, and Joelle\\nPineau. 2015. Building end-to-end dialogue sys-\\ntems using generative hierarchical neural network\\nmodels. arXiv preprint arXiv:1507.04808.\\n\\n[Serban et al.2016] Iulian Vlad Serban, Tim Klinger,\\nGerald Tesauro, Kartik Talamadupula, Bowen Zhou,\\nYoshua Bengio, and Aaron Courville. 2016. Mul-\\ntiresolution recurrent neural networks: An applica-\\ntion to dialogue response generation. arXiv preprint\\narXiv:1606.00776.\\n\\n[Shang et al.2015] Lifeng Shang, Zhengdong Lu, and\\nHang Li. 2015. Neural responding machine for\\nIn ACL 2015, July 26-31,\\nshort-text conversation.\\n2015, Beijing, China, Volume 1: Long Papers, pages\\n1577\\xe2\\x80\\x931586.\\n\\n[Socher et al.2013] Richard Socher, Danqi Chen,\\nChristopher D Manning, and Andrew Ng. 2013.\\nReasoning with neural tensor networks for knowl-\\nIn Advances in Neural\\nedge base completion.\\nInformation Processing Systems, pages 926\\xe2\\x80\\x93934.\\n\\n[Theano Development Team2016] Theano\\n\\nDevelop-\\nment Team. 2016. Theano: A Python framework\\nfor fast computation of mathematical expressions.\\narXiv e-prints, abs/1605.02688, May.\\n\\n[Vinyals and Le2015] Oriol Vinyals and Quoc Le.\\narXiv\\n\\n2015. A neural conversational model.\\npreprint arXiv:1506.05869.\\n\\n[Voorhees and others1999] Ellen M Voorhees et al.\\n1999. The trec-8 question answering track report.\\nIn Trec, volume 99, pages 77\\xe2\\x80\\x9382.\\n\\n[Wallace2009] Richard S Wallace. 2009. The anatomy\\n\\nof ALICE. Springer.\\n\\n\\x0c[Wan et al.2016] Shengxian Wan, Yanyan Lan, Jun\\nXu, Jiafeng Guo, Liang Pang, and Xueqi Cheng.\\n2016. Match-srnn: Modeling the recursive match-\\narXiv preprint\\ning structure with spatial rnn.\\narXiv:1604.04378.\\n\\n[Xu et al.2016] Zhen Xu, Bingquan Liu, Baoxun Wang,\\nChengjie Sun, and Xiaolong Wang. 2016. Incorpo-\\nrating loose-structured knowledge into lstm with re-\\ncall gate for conversation modeling. arXiv preprint\\narXiv:1605.05110.\\n\\n[Wang and Jiang2015] Shuohang Wang and Jing Jiang.\\n2015. Learning natural language inference with\\nlstm. arXiv preprint arXiv:1512.08849.\\n\\n[Wang et al.2013] Hao Wang, Zhengdong Lu, Hang Li,\\nand Enhong Chen. 2013. A dataset for research\\non short-text conversations. In EMNLP, pages 935\\xe2\\x80\\x93\\n945.\\n\\n[Wang et al.2015] Mingxuan Wang, Zhengdong Lu,\\nSyntax-based\\narXiv preprint\\n\\nHang Li, and Qun Liu.\\ndeep matching of short\\narXiv:1503.02427.\\n\\n2015.\\n\\ntexts.\\n\\n[Weizenbaum1966] Joseph Weizenbaum.\\n\\n1966.\\nEliza?a computer program for the study of nat-\\nural\\nlanguage communication between man and\\nmachine. Communications of the ACM, 9(1):36\\xe2\\x80\\x9345.\\n\\n[Wu et al.2016] Yu Wu, Wei Wu, Zhoujun Li, and Ming\\nZhou. 2016. Topic augmented neural network for\\nshort text conversation. CoRR, abs/1605.00090.\\n\\n[Xing et al.2016] Chen Xing, Wei Wu, Yu Wu, Jie\\nLiu, Yalou Huang, Ming Zhou, and Wei-Ying Ma.\\n2016. Topic augmented neural response generation\\nwith a joint attention mechanism. arXiv preprint\\narXiv:1606.08340.\\n\\n[Yan et al.2016] Rui Yan, Yiping Song, and Hua Wu.\\n2016. Learning to respond with deep neural net-\\nworks for retrieval-based human-computer conver-\\nIn Proceedings of the 39th Inter-\\nsation system.\\nnational ACM SIGIR conference on Research and\\nDevelopment in Information Retrieval, SIGIR 2016,\\nPisa, Italy, July 17-21, 2016, pages 55\\xe2\\x80\\x9364.\\n\\n[Yang et al.2016] Zichao Yang, Diyi Yang, Chris Dyer,\\nXiaodong He, Alex Smola, and Eduard Hovy. 2016.\\nHierarchical attention networks for document clas-\\nsi\\xef\\xac\\x81cation. In Proceedings of the 2016 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies.\\n\\n[Young et al.2010] Steve Young, Milica Ga\\xcb\\x87si\\xc2\\xb4c, Simon\\nKeizer, Franc\\xc2\\xb8ois Mairesse, Jost Schatzmann, Blaise\\nThomson, and Kai Yu.\\n2010. The hidden in-\\nformation state model: A practical framework for\\npomdp-based spoken dialogue management. Com-\\nputer Speech & Language, 24(2):150\\xe2\\x80\\x93174.\\n\\n[Zhou et al.2016] Xiangyang Zhou, Daxiang Dong,\\nHua Wu, Shiqi Zhao, R Yan, D Yu, Xuan Liu, and\\nH Tian. 2016. Multi-view response selection for\\nhuman-computer conversation. EMNLP16.\\n\\n\\x0c',\n",
       " 'Hierarchical Recurrent Attention Network for Response Generation\\nChen Xing12\\xe2\\x88\\x97 , Wei Wu3 , Yu Wu4 , Ming Zhou3 , Yalou Huang12 , Wei-Ying Ma3\\n\\n1College of Computer and Control Engineering, Nankai University, Tianjin, China\\n\\n2College of Software, Nankai University, Tianjin, China\\n\\n3 Microsoft Research, Beijing, China\\n\\n4State Key Lab of Software Development Environment, Beihang University, Beijing, China\\n\\n{v-chxing,wuwei,v-wuyu,mingzhou,wyma}@microsoft.com ylhuang@nankai.edu.cn\\n\\n7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n5\\n2\\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n9\\n4\\n1\\n7\\n0\\n\\n.\\n\\n1\\n0\\n7\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe study multi-turn response generation\\nin chatbots where a response is generated\\naccording to a conversation context. Ex-\\nisting work has modeled the hierarchy of\\nthe context, but does not pay enough at-\\ntention to the fact that words and utter-\\nances in the context are differentially im-\\nportant. As a result, they may lose im-\\nportant information in context and gen-\\nerate irrelevant responses. We propose\\na hierarchical recurrent attention network\\n(HRAN) to model both aspects in a uni-\\n\\xef\\xac\\x81ed framework. In HRAN, a hierarchical\\nattention mechanism attends to important\\nparts within and among utterances with\\nword level attention and utterance level at-\\ntention respectively. With the word level\\nattention, hidden vectors of a word level\\nencoder are synthesized as utterance vec-\\ntors and fed to an utterance level encoder\\nto construct hidden representations of the\\ncontext. The hidden vectors of the context\\nare then processed by the utterance level\\nattention and formed as context vectors for\\ndecoding the response. Empirical studies\\non both automatic evaluation and human\\njudgment show that HRAN can signi\\xef\\xac\\x81-\\ncantly outperform state-of-the-art models\\nfor multi-turn response generation.\\n\\n1\\n\\nIntroduction\\n\\nConversational agents include task-oriented dia-\\nlog systems which are built in vertical domains\\nfor speci\\xef\\xac\\x81c tasks (Young et al., 2013; Boden,\\n2006; Wallace, 2009; Young et al., 2010), and\\nnon-task-oriented chatbots which aim to realize\\nnatural and human-like conversations with people\\n\\xe2\\x88\\x97The work was done when the \\xef\\xac\\x81rst author was an intern\\n\\nin Microsoft Research Asia.\\n\\nFigure 1: An example of multi-turn conversation\\n\\nregarding to a wide range of issues in open do-\\nmains (Jafarpour et al., 2010). A common practice\\nto build a chatbot is to learn a response genera-\\ntion model within an encoder-decoder framework\\nfrom large scale message-response pairs (Shang\\net al., 2015; Vinyals and Le, 2015). Such mod-\\nels ignore conversation history when responding,\\nwhich is contradictory to the nature of real con-\\nversation between humans. To resolve the prob-\\nlem, researchers have taken conversation history\\ninto consideration and proposed response gener-\\nation for multi-turn conversation (Sordoni et al.,\\n2015; Serban et al., 2015; Serban et al., 2016b;\\nSerban et al., 2016c).\\n\\nIn this work, we study multi-turn response gen-\\neration for open domain conversation in chatbots\\nin which we try to learn a response generation\\nmodel from responses and their contexts. A con-\\ntext refers to a message and several utterances in\\nits previous turns.\\nIn practice, when a message\\ncomes, the model takes the context as input and\\ngenerate a response as the next turn. Multi-turn\\nconversation requires a model to generate a re-\\nsponse relevant to the whole context. The com-\\nplexity of the task lies in two aspects: 1) a conver-\\nsation context is in a hierarchical structure (words\\nform an utterance, and utterances form the con-\\ntext) and has two levels of sequential relationships\\namong both words and utterances within the struc-\\n\\n\\x0cture; 2) not all parts of the context are equally\\nimportant to response generation. Words are dif-\\nferentially informative and important, and so are\\nthe utterances. State-of-the-art methods such as\\nHRED (Serban et al., 2016a) and VHRED (Ser-\\nban et al., 2016c) focus on modeling the hierar-\\nchy of the context, whereas there is little explo-\\nration on how to select important parts from the\\ncontext, although it is often a crucial step for gen-\\nerating a proper response. Without this step, ex-\\nisting models may lose important information in\\ncontext and generate irrelevant responses1. Fig-\\nure 1 gives an example from our data to illustrate\\nthe problem. The context is a conversation be-\\ntween two speakers about height and boyfriend,\\ntherefore, to respond to the context, words like\\n\\xe2\\x80\\x9cgirl\\xe2\\x80\\x9d, \\xe2\\x80\\x9cboyfriend\\xe2\\x80\\x9d and numbers indicating height\\nsuch as \\xe2\\x80\\x9c160\\xe2\\x80\\x9d and \\xe2\\x80\\x9c175\\xe2\\x80\\x9d are more important than\\n\\xe2\\x80\\x9cnot good-looking\\xe2\\x80\\x9d. Moreover, u1 and u4 convey\\nmain semantics of the context, and therefore are\\nmore important than the others for generating a\\nproper response. Without modeling the word and\\nutterance importance, the state-of-the-art model\\nVHRED (Serban et al., 2016c) misses important\\npoints and gives a response \\xe2\\x80\\x9care you a man or\\na woman\\xe2\\x80\\x9d which is OK if there were only u3\\nleft, but nonsense given the whole context. Af-\\nter paying attention to the important words and ut-\\nterances, we can have a reasonable response like\\n\\xe2\\x80\\x9cNo, I don\\xe2\\x80\\x99t care much about height\\xe2\\x80\\x9d (the response\\nis generated by our model, as will be seen in ex-\\nperiments).\\n\\nWe aim to model the hierarchy and the impor-\\ntant parts of contexts in a uni\\xef\\xac\\x81ed framework. In-\\nspired by the success of the attention mechanism\\nin single-turn response generation (Shang et al.,\\n2015), we propose a hierarchical recurrent atten-\\ntion network (HRAN) for multi-turn response gen-\\neration in which we introduce a hierarchical atten-\\ntion mechanism to dynamically highlight impor-\\ntant parts of word sequences and the utterance se-\\nquence when generating a response. Speci\\xef\\xac\\x81cally,\\nHRAN is built in a hierarchical structure. At the\\nbottom of HRAN, a word level recurrent neural\\nnetwork (RNN) encodes each utterance into a se-\\nquence of hidden vectors. In generation of each\\nword in the response, a word level attention mech-\\n\\n1Note that one can simply concatenate all utterances and\\nemploys the classic sequence-to-sequence with attention to\\nmodel word importance in generation. This method, how-\\never, loses utterance relationships and results in bad genera-\\ntion quality, as will be seen in expeirments.\\n\\nanism assigns a weight to each vector in the hid-\\nden sequence of an utterance and forms an utter-\\nance vector by a linear combination of the vectors.\\nImportant hidden vectors correspond to important\\nparts in the utterance regarding to the generation\\nof the word, and contribute more to the forma-\\ntion of the utterance vector. The utterance vectors\\nare then fed to an utterance level RNN which con-\\nstructs hidden representations of the context. Dif-\\nferent from classic attention mechanism, the word\\nlevel attention mechanism in HRAN is dependent\\non both the decoder and the utterance level RNN.\\nThus, both the current generated part of the re-\\nsponse and the content of context can help select\\nimportant parts in utterances. At the third layer, an\\nutterance attention mechanism attends to impor-\\ntant utterances in the utterance sequence and sum-\\nmarizes the sequence as a context vector. Finally,\\nat the top of HRAN, a decoder takes the context\\nvector as input and generates the word in the re-\\nsponse. HRAN mirrors the data structure in multi-\\nturn response generation by growing from words\\nto utterances and then from utterances to the out-\\nput. It extends the architecture of current hierar-\\nchical response generation models by a hierarchi-\\ncal attention mechanism which not only results in\\nbetter generation quality, but also provides insight\\ninto which parts in an utterance and which utter-\\nances in context contribute to response generation.\\nWe conduct an empirical study on large scale\\nopen domain conversation data and compare our\\nmodel with state-of-the-art models using both au-\\ntomatic evaluation and side-by-side human com-\\nparison. The results show that on both met-\\nrics our model can signi\\xef\\xac\\x81cantly outperform ex-\\nisting models for multi-turn response genera-\\ntion. We release our source code and data at\\nhttps://github.com/LynetteXing1991/HRAN.\\n\\nThe contributions of the paper include (1) pro-\\nposal of attending to important parts in contexts\\nin multi-turn response generation; (2) proposal of\\na hierarchical recurrent attention network which\\nmodels hierarchy of contexts, word importance,\\nand utterance importance in a uni\\xef\\xac\\x81ed framework;\\n(3) empirical veri\\xef\\xac\\x81cation of the effectiveness of\\nthe model by both automatic evaluation and hu-\\nman judgment.\\n\\n2 Related Work\\n\\nMost existing effort on response generation is paid\\nto single-turn conversation. Starting from the ba-\\n\\n\\x0csic sequence to sequence model (Sutskever et al.,\\n2014), various models (Shang et al., 2015; Vinyals\\nand Le, 2015; Li et al., 2015; Xing et al., 2016;\\nLi et al., 2016; ?) have been proposed under\\nan encoder-decoder framework to improve genera-\\ntion quality from different perspectives such as rel-\\nevance, diversity, and personality. Recently, multi-\\nturn response generation has drawn attention from\\nacademia. For example, Sordoni et al.\\n(2015)\\nproposed DCGM where context information is en-\\ncoded with a multi-layer perceptron (MLP). Ser-\\nban et al. (2016a) proposed HRED which models\\ncontexts in a hierarchical encoder-decoder frame-\\nwork. Under the architecture of HRED, more vari-\\nants including VHRED (Serban et al., 2016c) and\\nMrRNN (Serban et al., 2016b) are proposed in or-\\nder to introduce latent and explicit variables into\\nthe generation process. In this work, we also study\\nmulti-turn response generation. Different from the\\nexisting models which do not model word and ut-\\nterance importance in generation, our hierarchical\\nrecurrent attention network simultaneously mod-\\nels the hierarchy of contexts and the importance\\nof words and utterances in a uni\\xef\\xac\\x81ed framework.\\n\\nAttention mechanism is \\xef\\xac\\x81rst proposed for ma-\\nchine translation (Bahdanau et al., 2014; Cho et\\nal., 2015), and is quickly applied to single-turn re-\\nsponse generation afterwards (Shang et al., 2015;\\nVinyals and Le, 2015). Recently, Yang et al.\\n(2016) proposed a hierarchical attention network\\nfor document classi\\xef\\xac\\x81cation in which two levels of\\nattention mechanisms are used to model the con-\\ntributions of words and sentences in classi\\xef\\xac\\x81cation\\ndecision. Seo et al. (2016) proposed a hierarchical\\nattention network to precisely attending objects of\\ndifferent scales and shapes in images. Inspired by\\nthese work, we extend the attention mechanism for\\nsingle-turn response generation to a hierarchical\\nattention mechanism for multi-turn response gen-\\neration. To the best of our knowledge, we are the\\n\\xef\\xac\\x81rst who apply the hierarchical attention technique\\nto response generation in chatbots.\\n\\n3 Problem Formalization\\n\\nSuppose that we have a data set D =\\n{(Ui, Yi)}N\\ni=1. \\xe2\\x88\\x80i, (Ui, Yi) consists of a re-\\nsponse Yi = (yi,1, . . . , yi,Ti) and its context\\nUi = (ui,1, . . . , ui,mi) with yi,j the j-th word,\\nui,mi the message, and (ui,1, . . . , ui,mi\\xe2\\x88\\x921) the ut-\\nterances in previous turns.\\nIn this work, we re-\\nquire mi (cid:62) 2 and thus each context has at\\n\\nui,j =(cid:0)wi,j,1, . . . , wi,j,Ti,j\\n\\n(cid:1) where wi,j,k is the k-\\n\\nleast one utterance as conversation history. \\xe2\\x88\\x80j,\\n\\nth word. We aim to estimate a generation prob-\\nability p(y1, . . . , yT|U) from D, and thus given\\na new conversation context U, we can gener-\\nate a response Y = (y1, . . . , yT ) according to\\np(y1, . . . , yT|U).\\nstruct p(y1, . . . , yT|U) and how to learn it.\\n\\nIn the following, we will elaborate how to con-\\n\\n4 Hierarchical Recurrent Attention\\n\\nNetwork\\n\\nWe propose a hierarchical recurrent attention net-\\nwork (HRAN) to model the generation probability\\np(y1, . . . , yT|U). Figure 2 gives the architecture\\nof HRAN. Roughly speaking, before generation,\\nHRAN employs a word level encoder to encode\\ninformation of every utterance in context as hidden\\nvectors. Then, when generating every word, a hi-\\nerarchical attention mechanism attends to impor-\\ntant parts within and among utterances with word\\nlevel attention and utterance level attention respec-\\ntively. With the two levels of attention, HRAN\\nworks in a bottom-up way: hidden vectors of ut-\\nterances are processed by the word level attention\\nand uploaded to an utterance level encoder to form\\nhidden vectors of the context. Hidden vectors of\\nthe context are further processed by the utterance\\nlevel attention as a context vector and uploaded to\\nthe decoder to generate the word.\\n\\nIn the following, we will describe details and\\n\\nthe learning objective of HRAN.\\n\\n4.1 Word Level Encoder\\n\\nGiven U = (u1, . . . , um), we employ a bidirec-\\ntional recurrent neural network with gated recur-\\nrent units (BiGRU) (Bahdanau et al., 2014) to en-\\ncode each ui, i \\xe2\\x88\\x88 {1, . . . , m} as hidden vectors\\n(hi,1, . . . , hi,Ti). Formally, suppose that ui =\\n(wi,1, . . . , wi,Ti), then \\xe2\\x88\\x80k \\xe2\\x88\\x88 {1, . . . , Ti}, hi,k is\\ngiven by\\n\\nhi,k = concat(\\n\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k,\\n\\n\\xe2\\x86\\x90\\xe2\\x88\\x92\\nh i,k),\\n\\n(1)\\n\\nwhere concat(\\xc2\\xb7,\\xc2\\xb7) is an operation de\\xef\\xac\\x81ned as con-\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\ncatenating the two arguments together,\\nh i,k is the\\n\\xe2\\x86\\x90\\xe2\\x88\\x92\\nk-th hidden state of a forward GRU (Cho et al.,\\n2014), and\\nh i,k is the k-th hidden state of a back-\\nward GRU. The forward GRU reads ui in its order\\n\\n\\x0cFigure 2: Hierarchical Recurrent Attention Network\\n\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k as\\n\\n(i.e., from wi,1 to wi,Ti), and calculates\\n\\n(2)\\n\\nh i,k\\xe2\\x88\\x921,\\n\\nrk = \\xcf\\x83(Wrei,k + Vr\\n\\nzk = \\xcf\\x83(Wzei,k + Vz\\n\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k\\xe2\\x88\\x921)\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k\\xe2\\x88\\x921)\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k\\xe2\\x88\\x921 \\xe2\\x97\\xa6 rk))\\nsk = tanh(Wsei,k + Vs(\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nh i,k = (1 \\xe2\\x88\\x92 zk) \\xe2\\x97\\xa6 sk + zk \\xe2\\x97\\xa6 \\xe2\\x88\\x92\\xe2\\x86\\x92\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nwhere\\nh i,0 is initialized with a isotropic Gaus-\\nsian distribution, ei,k is the embedding of wi,k,\\nzk and rk are an update gate and a reset gate\\nrespectively, \\xcf\\x83(\\xc2\\xb7) is a sigmoid function, and\\nWz, Wr, Ws, Vz, Vr, Vs are parameters. The\\nfrom wi,Ti to wi,1) and generates {\\xe2\\x86\\x90\\xe2\\x88\\x92\\nbackward GRU reads ui in its reverse order (i.e.,\\nk=1 with\\n\\nh i,k}Ti\\n\\na parameterization similar to the forward GRU.\\n\\nwill have, and the more contributions it will make\\nto the high level vector (i.e., the utterance vector\\nand the context vector). This is how the two levels\\nof attention attends to the important parts of utter-\\nances and the important utterances in generation.\\nMore speci\\xef\\xac\\x81cally, the utterance level encoder is\\na backward GRU which processes {ri,t}m\\ni=1 from\\nthe message rm,t to the earliest history r1,t. Simi-\\nlar to Equation (2), \\xe2\\x88\\x80i \\xe2\\x88\\x88 {m, . . . , 1}, li,t is calcu-\\nlated as\\n\\n(cid:48)\\ni = \\xcf\\x83(Wzlri,t + Vzlli+1,t)\\nz\\n(cid:48)\\ni = \\xcf\\x83(Wrlri,t + Vrlli+1,t)\\nr\\ni = tanh(Wslri,t + Vsl(li+1,t \\xe2\\x97\\xa6 r\\n(cid:48)\\ns\\nli,t = (1 \\xe2\\x88\\x92 z\\n\\ni) \\xe2\\x97\\xa6 s\\n(cid:48)\\n(cid:48)\\ni + z\\n\\ni \\xe2\\x97\\xa6 li+1,t,\\n(cid:48)\\n\\n(cid:48)\\ni))\\n\\n(5)\\n\\ni and r(cid:48)\\n\\nwhere lm+1,t is initialized with a isotropic Gaus-\\nsian distribution, z(cid:48)\\ni are the update gate and\\nthe reset gate of the utterance level GRU respec-\\ntively, and Wzl, Vzl, Wrl, Vrl, Wsl, Vsl are pa-\\nrameters.\\n\\nDifferent from the classic attention mechanism,\\nword level attention in HRAN depends on both\\nthe hidden states of the decoder and the hidden\\nstates of the utterance level encoder. It works in\\na reverse order by \\xef\\xac\\x81rst weighting {hm,j}Tm\\nj=1 and\\nthen moving towards {h1,j}T1\\nj=1 along the utter-\\nance sequence. \\xe2\\x88\\x80i \\xe2\\x88\\x88 {m, . . . , 1}, j \\xe2\\x88\\x88 {1, . . . , Ti},\\nweight \\xce\\xb1i,t,j is calculated as\\n\\nei,t,j = \\xce\\xb7(st\\xe2\\x88\\x921, li+1,t, hi,j);\\n\\n(cid:80)Ti\\n\\nexp(ei,t,j)\\nk=1 exp(ei,t,k)\\n\\n\\xce\\xb1i,t,j =\\n\\n,\\n\\n(6)\\n\\nwhere lm+1,t is initialized with a isotropic Gaus-\\nsian distribution, st\\xe2\\x88\\x921 is the (t\\xe2\\x88\\x92 1)-th hidden state\\nof the decoder, and \\xce\\xb7(\\xc2\\xb7) is a multi-layer perceptron\\n(MLP) with tanh as an activation function.\\n\\n4.2 Hierarchical Attention and Utterance\\n\\nEncoder\\n\\nSuppose that the decoder has generated t \\xe2\\x88\\x92 1\\nwords, at step t, word level attention calculates a\\nweight vector (\\xce\\xb1i,t,1, . . . , \\xce\\xb1i,t,Ti) (details are de-\\nscribed later) for {hi,j}Ti\\nj=1 and represents utter-\\nance ui as a vector ri,t. \\xe2\\x88\\x80i \\xe2\\x88\\x88 {1, . . . , m}, ri,t is\\nTi(cid:88)\\nde\\xef\\xac\\x81ned by\\n\\nri,t =\\n\\n\\xce\\xb1i,t,jhi,j.\\n\\n(3)\\n\\nj=1\\n\\n{ri,t}m\\ni=1 are then utilized as input of an utterance\\nlevel encoder and transformed to (l1,t, . . . , lm,t) as\\nhidden vectors of the context. After that, utterance\\nlevel attention assigns a weight \\xce\\xb2i,t to li,t (details\\nare described later) and forms a context vector ct\\nas\\n\\nm(cid:88)\\n\\nct =\\n\\n\\xce\\xb2i,tli,t.\\n\\n(4)\\n\\ni=1\\n\\nIn both Equation (3) and Equation (4), the more\\nimportant a hidden vector is, the larger weight it\\n\\numWord Level Attentionu2Word Level Attentionu1Word Level Attention\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x992,\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x991,\\xf0\\x9d\\x91\\xa1\\xe2\\x80\\xa6\\xe2\\x80\\xa6Utterance Level Attention\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\xa1\\xe2\\x88\\x921\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\xf0\\x9d\\x91\\xa0T\\xe2\\x80\\xa6WordLevelAttention:\\xf0\\x9d\\x91\\x991,\\xf0\\x9d\\x91\\xa1MLP\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\xa1\\xe2\\x88\\x921\\xe2\\x8a\\x95\\xce\\xb11,\\xf0\\x9d\\x91\\xa1,1\\xce\\xb11,\\xf0\\x9d\\x91\\xa1,2\\xf0\\x9d\\x91\\x992,\\xf0\\x9d\\x91\\xa1\\xe2\\x80\\xa6\\xe2\\x80\\xa6Decoder:\\xf0\\x9d\\x91\\x9f2,\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x9f1,\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x9f1,\\xf0\\x9d\\x91\\xa1h1,1h1,2h1,3h1,4h1,5h2,1h2,2h2,3h2,4h2,5h\\xf0\\x9d\\x91\\x9a,1h\\xf0\\x9d\\x91\\x9a,2h\\xf0\\x9d\\x91\\x9a,3h\\xf0\\x9d\\x91\\x9a,4h\\xf0\\x9d\\x91\\x9a,5h1,1h1,2Weights:Utterance Level EncoderWordLevel EncoderContext vector\\x0cNote that the word level attention and the utter-\\nance level encoding are dependent with each other\\nand alternatively conducted (\\xef\\xac\\x81rst attention then\\nencoding). The motivation we establish the depen-\\ndency between \\xce\\xb1i,t,j and li+1,t is that content from\\nthe context (i.e., li+1,t) could help identify im-\\nportant information in utterances, especially when\\nst\\xe2\\x88\\x921 is not informative enough (e.g., the generated\\npart of the response are almost function words).\\nWe require the utterance encoder and the word\\nlevel attention to work reversely, because we think\\nthat compared to history, conversation that hap-\\npened after an utterance in the context is more\\nlikely to be capable of identifying important in-\\nformation in the utterance for generating a proper\\nresponse to the context.\\n\\nWith {li,t}m\\n\\ni=1, the utterance level attention cal-\\n\\nculates a weight \\xce\\xb2i,t for li,t as\\n\\n(cid:48)\\ni,t = \\xce\\xb7(st\\xe2\\x88\\x921, li,t);\\ne\\nexp(e(cid:48)\\ni,t)\\ni=1 exp(e(cid:48)\\n\\n(cid:80)m\\n\\n\\xce\\xb2i,t =\\n\\ni,t)\\n\\n.\\n\\n(7)\\n\\n5 Experiments\\n\\nWe compared HRAN with state-of-the-art meth-\\nods by both automatic evaluation and side-by-side\\nhuman judgment.\\n\\n5.1 Data Set\\n\\nWe built a data set from Douban Group2 which is a\\npopular Chinese social networking service (SNS)\\nallowing users to discuss a wide range of topics\\nin groups through posting and commenting.\\nIn\\nDouban Group, regarding to a post under a speci\\xef\\xac\\x81c\\ntopic, two persons can converse with each other\\nby one posting a comment and the other quoting\\nit and posting another comment. We crawled 20\\nmillion conversations between two persons with\\nthe average number of turns as 6.32. The data\\ncovers many different topics and can be viewed\\nas a simulation of open domain conversations in a\\nchatbot. In each conversation, we treated the last\\nturn as response, and the remaining turns as con-\\ntext. As preprocessing, we \\xef\\xac\\x81rst employed Stan-\\nford Chinese word segmenter3 to tokenize each\\nutterance in the data. Then we removed the con-\\nversations whose response appearing more than\\n50 times in the whole data to prevent them from\\ndominating learning. We also removed the con-\\nversations shorter than 3 turns and the conversa-\\ntions with an utterance longer than 50 words. Af-\\nter the preprocessing, there are 1, 656, 652 conver-\\nsations left. From them, we randomly sampled\\n1 million conversations as training data, 10, 000\\nconversations as validation data, and 1, 000 con-\\nversations as test data, and made sure that there is\\nno overlap among them. In the test data, the con-\\ntexts were used to generate responses and their re-\\nsponses were used as ground truth to calculate per-\\nplexity of generation models. We kept the 40, 000\\nmost frequent words in the contexts of the training\\ndata to construct a context vocabulary. The vo-\\ncabulary covers 98.8% of words appearing in the\\ncontexts of the training data. Similarly, we con-\\nstructed a response vocabulary that contains the\\n40, 000 most frequent words in the responses of\\nthe training data which covers 99.0% words ap-\\npearing in the responses. Words outside the two\\nvocabularies were treated as \\xe2\\x80\\x9cUNK\\xe2\\x80\\x9d. The data will\\nbe publicly available.\\n\\nT(cid:89)\\n\\n4.3 Decoding the Response\\nThe decoder of HRAN is a RNN language model\\n(Mikolov et al., 2010) conditioned on the context\\nvectors {ct}T\\nt=1 given by Equation (4). Formally,\\nthe probability distribution p(y1, . . . , yT|U) is de-\\n\\xef\\xac\\x81ned as\\n\\np(y1, ..., yT|U) = p(y1|c1)\\n\\np(yt|ct, y1, ..., yt\\xe2\\x88\\x921).\\n\\n(8)\\n\\nwhere p(yt|ct, y1, ..., yt\\xe2\\x88\\x921) is given by\\n\\nt=2\\n\\nst = f (eyt\\xe2\\x88\\x921 , st\\xe2\\x88\\x921, ct)\\np(yt|ct, y1, ..., yt\\xe2\\x88\\x921) = Iyt \\xc2\\xb7 sof tmax(st, eyt\\xe2\\x88\\x921 ),\\n\\n(9)\\n\\nwhere st is the hidden state of the decoder at step t,\\neyt\\xe2\\x88\\x921 is the embedding of yt\\xe2\\x88\\x921, f is a GRU, Iyt is\\nthe one-hot vector for yt, and sof tmax(st, eyt\\xe2\\x88\\x921)\\nis a V -dimensional vector with V the response\\nvocabulary size and each element the generation\\nprobability of a word. In practice, we employ the\\nbeam search (Tillmann and Ney, 2003) technique\\nto generate the n-best responses.\\nthen we estimate \\xce\\x98 from D = {(Ui, Yi)}N\\nminimizing the following objective function:\\n\\nLet us denote \\xce\\x98 as the parameter set of HRAN,\\ni=1 by\\n\\n\\xe2\\x88\\x92 N(cid:88)\\n\\n\\xcb\\x86\\xce\\x98 = arg min\\n\\n\\xce\\x98\\n\\ni=1\\n\\nlog (p(yi,1, ..., yi,Ti|Ui))\\n\\n(10)\\n\\n2https://www.douban.com/group/explore\\n3http://nlp.stanford.edu/software/segmenter.shtml\\n\\n\\x0cModel\\nS2SA\\nHRED\\nVHRED\\nHRAN\\n\\nValidation Perplexity\\n\\n43.679\\n46.279\\n44.548\\n40.257\\n\\nTest Perplexity\\n\\n44.508\\n47.467\\n45.484\\n41.138\\n\\nTable 1: Perplexity results\\n\\n5.2 Baselines\\nWe compared HRAN with the following models:\\nS2SA: we concatenated all utterances in a con-\\ntext as a long sequence and treated the sequence\\nand the response as a message-response pair. By\\nthis means, we transformed the problem of multi-\\nturn response generation to a problem of single-\\nturn response generation and employed the stan-\\ndard sequence to sequence with attention (Shang\\net al., 2015) as a baseline.\\n\\nHRED:\\n\\nthe\\n\\nhierarchical\\n\\nencoder-decoder\\n\\nmodel proposed by (Serban et al., 2016a).\\n\\nVHRED: a modi\\xef\\xac\\x81cation of HRED (Serban et\\nal., 2016c) where latent variables are introduced\\nin to generation. In all models, we set the dimen-\\nsionality of hidden states of encoders and decoders\\nas 1000, and the dimensionality of word embed-\\nding as 620. All models were initialized with\\nisotropic Gaussian distributions X \\xe2\\x88\\xbc N (0, 0.01)\\nand trained with an AdaDelta algorithm (Zeiler,\\n2012) on a NVIDIA Tesla K40 GPU. The batch\\nsize is 128. We set the initial learning rate as 1.0\\nand reduced it by half if the perplexity on val-\\nidation began to increase. We implemented the\\nmodels with an open source deep learning tool\\nBlocks4.\\n\\n5.3 Evaluation Metrics\\nHow to evaluate a response generation model is\\nstill an open problem but not the focus of the pa-\\nper. We followed the existing work and employed\\nthe following metrics:\\n\\nPerplexity: following (Vinyals and Le, 2015),\\nwe employed perplexity as an evaluation metric.\\nPerplexity is de\\xef\\xac\\x81ned by Equation (11).\\nIt mea-\\nsures how well a model predicts human responses.\\nLower perplexity generally indicates better gener-\\nation performance. In our experiments, perplex-\\nity on validation was used to determine when to\\nstop training.\\nIf the perplexity stops decreasing\\nand the difference is smaller than 2.0 \\xef\\xac\\x81ve times in\\nvalidation, we think that the algorithm has reached\\nconvergence and terminate training. We tested the\\ngeneration ability of different models by perplex-\\n\\n4https://github.com/mila-udem/blocks\\n\\nity on the test data.\\n\\n(cid:26)\\n\\n(cid:27)\\n\\nModels\\n\\nWin\\n27.3\\nHRAN v.s. S2SA\\n27.2\\nHRAN v.s. HRED\\nHRAN v.s. VHRED 25.2\\n\\nKappa\\n0.37\\n0.35\\n0.34\\nTable 2: Human annotation results (in %)\\n\\nLoss\\n20.6\\n21.2\\n20.4\\n\\nTie\\n52.1\\n51.6\\n54.4\\n\\nP P L = exp\\n\\n\\xe2\\x88\\x92 1\\nN\\n\\ni=1 log(p(Yi|Ui))\\n\\xce\\xa3N\\n\\n.\\n\\n(11)\\n\\nSide-by-side human annotation: we also com-\\npared HRAN with every baseline model by side-\\nby-side human comparison. Speci\\xef\\xac\\x81cally, we re-\\ncruited three native speakers with rich Douban\\nGroup experience as human annotators. To each\\nannotator, we showed a context of a test example\\nwith two generated responses, one from HRAN\\nand the other one from a baseline model. Both\\nresponses are the top one results in beam search.\\nThe two responses were presented in random or-\\nder. We then asked the annotator to judge which\\none is better. The criteria is, response A is better\\nthan response B if (1) A is relevant, logically con-\\nsistent to the context, and \\xef\\xac\\x82uent, while B is either\\nirrelevant or logically contradictory to the context,\\nor it is dis\\xef\\xac\\x82uent (e.g., with grammatical errors or\\nUNKs); or (2) both A and B are relevant, consis-\\ntent, and \\xef\\xac\\x82uent, but A is more informative and in-\\nteresting than B (e.g., B is a universal reply like\\n\\xe2\\x80\\x9cI see\\xe2\\x80\\x9d). If the annotator cannot tell which one is\\nbetter, he/she was asked to label a \\xe2\\x80\\x9ctie\\xe2\\x80\\x9d. Each an-\\nnotator individually judged 1000 test examples for\\neach HRAN-baseline pair, and in total, each one\\njudged 3000 examples (for three pairs). Agree-\\nments among the annotators were calculated using\\nFleiss\\xe2\\x80\\x99 kappa (Fleiss and Cohen, 1973).\\n\\nNote that we do not choose BLEU (Papineni et\\nal., 2002) as an evaluation metric, because (1) Liu\\net al. (Liu et al., 2016) have proven that BLEU\\nis not a proper metric for evaluating conversa-\\ntion models as there is weak correlation between\\nBLEU and human judgment; (2) different from the\\nsingle-turn case, in multi-turn conversation, one\\ncontext usually has one copy in the whole data.\\nThus, without any human effort like what Sordoni\\net al. (Sordoni et al., 2015) did in their work, each\\ncontext only has a single reference in test. This\\nmakes BLEU even unreliable as a measurement\\nof generation quality in open domain conversation\\ndue to the diversity of responses.\\n\\n5.4 Evaluation Results\\nTable 1 gives the results on perplexity. HRAN\\nachieves the lowest perplexity on both validation\\n\\n\\x0cFigure 3: Case study (utterances between two persons in contexts are split by \\xe2\\x80\\x9c\\xe2\\x87\\x92\\xe2\\x80\\x9d)\\nTie\\n\\nModel\\n\\nNo UD Att\\nNo Word Att\\n\\nNo Utterance Att\\n\\nLoss\\n\\nWin\\nPPL\\n22.3% 24.8% 52.9% 41.54\\n20.4% 25.0% 50.6% 43.24\\n21.1% 23.7% 55.2% 47.35\\n\\nand test. We conducted t-test on test perplexity and\\nthe result shows that the improvement of HRAN\\nover all baseline models is statistically signi\\xef\\xac\\x81cant\\n(p-value < 0.01).\\n\\nTable 2 shows the human annotation results.\\nThe ratios were calculated by combining the anno-\\ntations from the three judges together. We can see\\nthat HRAN outperforms all baseline models and\\nall comparisons have relatively high kappa scores\\nwhich indicates that the annotators reached rela-\\ntively high agreements in judgment. Compared\\nwith S2SA, HRED, and VHRED, HRAN achieves\\npreference gains (win-loss) 6.7%, 6%, 4.8% re-\\nspectively. Sign test results show that the improve-\\nment is statistically signi\\xef\\xac\\x81cant (p-value < 0.01 for\\nHRAN v.s. S2SA and HRAN v.s. HRED, and p-\\nvalue < 0.05 for HRAN v.s. VHRED). Among\\nthe three baseline models, S2SA is the worst one,\\nbecause it loses relationships among utterances in\\nthe context. VHRED is the best baseline model,\\nwhich is consistent with the existing literatures\\n(Serban et al., 2016c). We checked the cases on\\nwhich VHRED loses to HRAN and found that\\non 56% cases, VHRED generated irrelevant re-\\nsponses while responses given by HRAN are rele-\\nvant, logically consistent, and \\xef\\xac\\x82uent.\\n\\n5.5 Discussions\\nCase study: Figure 3 lists some cases from the\\ntest set to compare HRAN with the best baseline\\nVHRED. We can see that HRAN not only can\\nanswer the last turn in the context (i.e., the mes-\\n\\nTable 3: Model ablation results\\n\\nsage) properly by understanding the context (e.g.,\\ncase 2), but also be capable of starting a new topic\\naccording to the conversation history to keep the\\nconversation going (e.g., case 1). In case 2, HRAN\\nunderstands that the message is actually asking\\n\\xe2\\x80\\x9cwhy can\\xe2\\x80\\x99t you come to have dinner with me?\\xe2\\x80\\x9d\\nand generates a proper response that gives a plau-\\nsible reason. In case 1, HRAN properly brings up\\na new topic by asking the \\xe2\\x80\\x9cbrand\\xe2\\x80\\x9d of the user\\xe2\\x80\\x99s \\xe2\\x80\\x9clo-\\ntion\\xe2\\x80\\x9d when the current topic \\xe2\\x80\\x9chow to exfoliate my\\nskin\\xe2\\x80\\x9d has come to an end. The new topic is based\\non the content of the context and thus can naturally\\nextends the conversation in the case.\\n\\nVisualization of attention: to further illustrate\\nwhy HRAN can generate high quality responses,\\nwe visualized the hierarchical attention for the\\ncases in Figure 3 in Figure 4. In every sub-\\xef\\xac\\x81gure,\\neach line is an utterance with blue color indicat-\\ning word importance. The leftmost column of\\neach sub-\\xef\\xac\\x81gure uses red color to indicate utterance\\nimportance. Darker color means more important\\nwords or utterances. The importance of a word or\\nan utterance was calculated by the average weight\\nof the word or the utterance assigned by attention\\nin generating the response given at the bottom of\\neach sub-\\xef\\xac\\x81gure.\\nIt re\\xef\\xac\\x82ects an overall contribu-\\ntion of the word or the utterance to generate the\\nresponse. Above each line, we gave the transla-\\n\\n\\x0c(a) Visualization of case 1\\n\\n(b) Visualization of case 2\\n\\n(c) Visualization of case 3\\n\\n(d) Visualization of case 4\\n\\nFigure 4: Attention visualization (the importance of a word or an utterance is calculated as their average\\nweights when generating the whole response)\\n\\ntion of the utterance, and below it, we translated\\nimportant words. Note that word-to-word trans-\\nlation may cause confusion sometimes, therefore,\\nwe left some words (most of them are function\\nwords) untranslated. We can see that the hierar-\\nchical attention mechanism in HRAN can attend\\nto both important words and important utterances\\nin contexts. For example, in Figure 4(c), words\\nincluding \\xe2\\x80\\x9cgirl\\xe2\\x80\\x9d and \\xe2\\x80\\x9cboyfriend\\xe2\\x80\\x9d and numbers in-\\ncluding \\xe2\\x80\\x9c160\\xe2\\x80\\x9d and \\xe2\\x80\\x9c175\\xe2\\x80\\x9d are highlighted, and u1\\nand u4 are more important than others. The result\\nmatches our intuition in introduction.\\nIn Figure\\n4(b), HRAN assigned larger weights to u1, u4 and\\nwords like \\xe2\\x80\\x9cdinner\\xe2\\x80\\x9d and \\xe2\\x80\\x9cwhy\\xe2\\x80\\x9d. This explains why\\nthe model can understand that the message is ac-\\ntually asking \\xe2\\x80\\x9cwhy can\\xe2\\x80\\x99t you come to have dinner\\nwith me?\\xe2\\x80\\x9d. The \\xef\\xac\\x81gures provide us insights on how\\nHRAN understands contexts in generation.\\n\\nModel ablation: we then examine the effect\\nof different components of HRAN by removing\\nthem one by one. We \\xef\\xac\\x81rst removed li+1 from\\n\\xce\\xb7(st\\xe2\\x88\\x921, li+1,t, hi,j) in Equation (6) (i.e., removing\\nutterance dependency from word level attention)\\nand denoted the model as \\xe2\\x80\\x9cNo UD Att\\xe2\\x80\\x9d, then we\\nremoved word level attention and utterance level\\nattention separately, and denoted the models as\\n\\xe2\\x80\\x9cNo Word Att\\xe2\\x80\\x9d and \\xe2\\x80\\x9cNo Utterance Att\\xe2\\x80\\x9d respec-\\ntively. We conducted side-by-side human compar-\\nison on these models with the full HRAN on the\\ntest data and also calculated their test perplexity\\n(PPL). Table 3 gives the results. We can see that\\n\\nall the components are useful because removing\\nany of them will cause performance drop. Among\\nthem, word level attention is the most important\\none as HRAN achieved the most preference gain\\n(4.6%) to No Word Att on human comparison.\\n\\nError analysis: we \\xef\\xac\\x81nally investigate how to\\nimprove HRAN in the future by analyzing the\\ncases on which HRAN loses to VHRED. The er-\\nrors can be summarized as: 51.81% logic con-\\ntradiction, 26.95% universal reply, 7.77% irrel-\\nevant response, and 13.47% others. Most bad\\ncases come from universal replies and responses\\nthat are logically contradictory to contexts. This\\nis easy to understand as HRAN does not explicitly\\nmodel the two issues. The result also indicates that\\n(1) although contexts provide more information\\nthan single messages, multi-turn response gener-\\nation still has the \\xe2\\x80\\x9csafe response\\xe2\\x80\\x9d problem as the\\nsingle-turn case; (2) although attending to impor-\\ntant words and utterances in generation can lead\\nto informative and logically consistent responses\\nfor many cases like those in Figure 3, it is still not\\nenough for fully understanding contexts due to the\\ncomplex nature of conversations. The irrelevant\\nresponses might be caused by wrong attention in\\ngeneration. Although the analysis might not cover\\nall bad cases (e.g., HRAN and VHRED may both\\ngive bad responses), it sheds light on our future di-\\nrections: (1) improving response diversity, e.g., by\\nintroducing extra content into generation like Xing\\net al. (Xing et al., 2016) and Mou et al. (Mou et al.,\\n\\n\\x0c2016) did for single-turn conversation; (2) model-\\ning logics in contexts; (3) improving attention.\\n\\n6 Conclusion\\nWe propose a hierarchical recurrent attention net-\\nwork (HRAN) for multi-turn response generation\\nin chatbots. Empirical studies on large scale con-\\nversation data show that HRAN can signi\\xef\\xac\\x81cantly\\noutperform state-of-the-art models.\\n\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\narXiv preprint\\nlearning to align and translate.\\narXiv:1409.0473.\\n\\nMargaret Ann Boden. 2006. Mind as machine: A his-\\n\\ntory of cognitive science. Clarendon Press.\\n\\nKyunghyun Cho, Bart van Merri\\xc2\\xa8enboer, Dzmitry Bah-\\ndanau, and Yoshua Bengio. 2014. On the properties\\nof neural machine translation: Encoder\\xe2\\x80\\x93decoder ap-\\nproaches. Syntax, Semantics and Structure in Statis-\\ntical Translation, page 103.\\n\\nKyunghyun Cho, Aaron Courville, and Yoshua Ben-\\ngio. 2015. Describing multimedia content using\\nattention-based encoder-decoder networks. Multi-\\nmedia, IEEE Transactions on, 17(11):1875\\xe2\\x80\\x931886.\\n\\nJoseph L Fleiss and Jacob Cohen. 1973. The equiv-\\nalence of weighted kappa and the intraclass corre-\\nlation coef\\xef\\xac\\x81cient as measures of reliability. Educa-\\ntional and psychological measurement.\\n\\nSina Jafarpour, Christopher JC Burges, and Alan Rit-\\nter. 2010. Filter, rank, and transfer the knowledge:\\nLearning to chat. Advances in Ranking, 10.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\nand Bill Dolan. 2015. A diversity-promoting objec-\\ntive function for neural conversation models. arXiv\\npreprint arXiv:1510.03055.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\nand Bill Dolan. 2016. A persona-based neural con-\\nversation model. arXiv preprint arXiv:1603.06155.\\n\\nChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael\\nNoseworthy, Laurent Charlin, and Joelle Pineau.\\n2016. How not to evaluate your dialogue system:\\nAn empirical study of unsupervised evaluation met-\\nrics for dialogue response generation. arXiv preprint\\narXiv:1603.08023.\\n\\nTomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc2\\xb4at, Lukas Burget, Jan\\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\\nrent neural network based language model. In IN-\\nTERSPEECH 2010, 11th Annual Conference of the\\nInternational Speech Communication Association,\\nMakuhari, Chiba, Japan, September 26-30, 2010,\\npages 1045\\xe2\\x80\\x931048.\\n\\nLili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,\\nand Zhi Jin. 2016. Sequence to backward and for-\\nward sequences: A content-introducing approach to\\ngenerative short-text conversation. arXiv preprint\\narXiv:1607.00970.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of\\nthe 40th annual meeting on association for compu-\\ntational linguistics, pages 311\\xe2\\x80\\x93318. Association for\\nComputational Linguistics.\\n\\nPaul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui\\nShen, and Bohyung Han. 2016. Hierarchical atten-\\ntion networks. arXiv preprint arXiv:1606.02393.\\n\\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\\nAaron Courville, and Joelle Pineau. 2015. Build-\\ning end-to-end dialogue systems using generative hi-\\nerarchical neural network models. arXiv preprint\\narXiv:1507.04808.\\n\\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\\nAaron Courville, and Joelle Pineau. 2016a. Build-\\ning end-to-end dialogue systems using generative hi-\\nerarchical neural network models. In Proceedings of\\nthe 30th AAAI Conference on Arti\\xef\\xac\\x81cial Intelligence\\n(AAAI-16).\\n\\nIulian Vlad Serban, Tim Klinger, Gerald Tesauro,\\nKartik Talamadupula, Bowen Zhou, Yoshua Ben-\\ngio, and Aaron Courville.\\n2016b. Multireso-\\nlution recurrent neural networks: An application\\narXiv preprint\\nto dialogue response generation.\\narXiv:1606.00776.\\n\\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\\nLaurent Charlin, Joelle Pineau, Aaron Courville,\\nand Yoshua Bengio. 2016c. A hierarchical latent\\nvariable encoder-decoder model for generating dia-\\nlogues. arXiv preprint arXiv:1605.06069.\\n\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015.\\nNeural responding machine for short-text conversa-\\ntion. arXiv preprint arXiv:1503.02364.\\n\\nAlessandro Sordoni, Michel Galley, Michael Auli,\\nChris Brockett, Yangfeng Ji, Margaret Mitchell,\\nJian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.\\nA neural network approach to context-sensitive gen-\\neration of conversational responses. arXiv preprint\\narXiv:1506.06714.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems, pages 3104\\xe2\\x80\\x933112.\\n\\nChristoph Tillmann and Hermann Ney. 2003. Word re-\\nordering and a dynamic programming beam search\\nalgorithm for statistical machine translation. Com-\\nputational linguistics, 29(1):97\\xe2\\x80\\x93133.\\n\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\n\\ntional model. arXiv preprint arXiv:1506.05869.\\n\\n\\x0cRichard S Wallace. 2009. The anatomy of ALICE.\\n\\nSpringer.\\n\\nChen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,\\nMing Zhou, and Wei-Ying Ma.\\nTopic\\naware neural response generation. arXiv preprint\\narXiv:1606.08340.\\n\\n2016.\\n\\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\\ncal attention networks for document classi\\xef\\xac\\x81cation.\\nIn Proceedings of the 2016 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies.\\n\\nSteve Young, Milica Ga\\xcb\\x87si\\xc2\\xb4c, Simon Keizer, Franc\\xc2\\xb8ois\\n\\nMairesse, Jost Schatzmann, Blaise Thomson, and\\nKai Yu. 2010. The hidden information state model:\\nA practical framework for pomdp-based spoken dia-\\nlogue management. Computer Speech & Language,\\n24(2):150\\xe2\\x80\\x93174.\\n\\nStephanie Young, Milica Gasic, Blaise Thomson, and\\nJohn D Williams. 2013. Pomdp-based statistical\\nspoken dialog systems: A review. Proceedings of\\nthe IEEE, 101(5):1160\\xe2\\x80\\x931179.\\n\\nMatthew D Zeiler. 2012. Adadelta: an adaptive learn-\\n\\ning rate method. arXiv preprint arXiv:1212.5701.\\n\\n\\x0c',\n",
       " '7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n6\\n2\\n\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n\\n2\\nv\\n4\\n7\\n2\\n7\\n0\\n\\n.\\n\\n1\\n0\\n7\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nDEEP REINFORCEMENT LEARNING: AN OVERVIEW\\n\\nYuxi Li (yuxili@gmail.com)\\n\\nABSTRACT\\n\\nWe give an overview of recent exciting achievements of deep reinforcement learn-\\ning (RL). We start with background of deep learning and reinforcement learning,\\nas well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and\\nits extensions, asynchronous methods, policy optimization, reward, and planning.\\nAfter that, we talk about attention and memory, unsupervised learning, and learn-\\ning to learn. Then we discuss various applications of RL, including games, in\\nparticular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine\\ntranslation, text sequence prediction, neural architecture design, personalized web\\nservices, healthcare, \\xef\\xac\\x81nance, and music generation. We mention topics/papers not\\nreviewed yet. After listing a collection of RL resources, we close with discussions.\\n\\n1\\n\\nINTRODUCTION\\n\\nReinforcement learning (RL) is usually about sequential decision making, solving problems in a\\nwide range of \\xef\\xac\\x81elds in science, engineering and arts (Sutton and Barto, 2017).\\n\\nThe integration of reinforcement learning and neural networks dated back to 1990s (Tesauro, 1994;\\nBertsekas and Tsitsiklis, 1996; Schmidhuber, 2015). With recent exciting achievements of deep\\nlearning (LeCun et al., 2015; Goodfellow et al., 2016), bene\\xef\\xac\\x81ting from big data, powerful compu-\\ntation and new algorithmic techniques, we have been witnessing the renaissance of reinforcement\\nlearning (Krakovsky, 2016), especially, the combination of reinforcement learning and deep neural\\nnetworks, i.e., deep reinforcement learning (deep RL).\\n\\nWe have been witnessing breakthroughs,\\nlike deep Q-network (Mnih et al., 2015), Al-\\nphaGo (Silver et al., 2016) and differentiable neural computer (Graves et al., 2016); and novel ar-\\nchitectures and applications, like asynchronous methods (Mnih et al., 2016), dueling network ar-\\nchitectures (Wang et al., 2016a), value iteration networks (Tamar et al., 2016), dual learning for\\nmachine translation (He et al., 2016a), spoken dialogue systems (Su et al., 2016b), information ex-\\ntraction (Narasimhan et al., 2016), guided policy search (Levine et al., 2016a), generative adver-\\nsarial imitation learning (Ho and Ermon, 2016), unsupervised reinforcement and auxiliary learn-\\ning (Jaderberg et al., 2017), and neural architecture design (Zoph and Le, 2017), etc.\\nIn this\\noverview, we mainly focus on work in recent couple of years, and by no means complete.1\\nWe refer readers to materials for further study: reinforcement learning (Sutton and Barto, 2017;\\nSzepesv\\xc2\\xb4ari, 2010; Bertsekas, 2012; Powell, 2011; Bertsekas and Tsitsiklis, 1996; Puterman, 2005;\\nLittman, 2015; Kaelbling et al., 1996); deep learning (LeCun et al., 2015; Goodfellow et al.,\\n2016; Bengio, 2009; Deng and Dong, 2014); machine learning (Jordan and Mitchell, 2015;\\nHastie et al., 2009; Bishop, 2011; Murphy, 2012; James et al., 2013); practical machine learn-\\ning advices (Domingos, 2012; Zinkevich, 2017); arti\\xef\\xac\\x81cial\\nintelligence (Russell and Norvig,\\n2009), deep learning in neural networks (Schmidhuber, 2015); natural language processing\\n(NLP) (Hirschberg and Manning, 2015; Deng and Liu, 2017); robotics (Kober et al., 2013); trans-\\nfer learning (Taylor and Stone, 2009; Pan and Yang, 2010; Weiss et al., 2016); semi-supervised\\nlearning (Zhu and Goldberg, 2009); Bayesian RL (Ghavamzadeh et al., 2015); spoken dialogue\\nsystems (Hinton et al., 2012; He and Deng, 2013; Young et al., 2013); AI safety (Amodei et al.,\\n2016; Garc`\\xc4\\xb1a and Fern`andez, 2015), Monte Carlo tree search (MCTS) (Browne et al., 2012;\\n\\n1We consider this overview as incomplete, for time and timing reasons, in the sense that we may not discuss\\nin depth all relevant work, and we will see fast and enormous growth in this \\xef\\xac\\x81eld in the next couple of years. Yet\\nwe decide to make this overview public available, hoping it would be helpful for some people in the community\\nand we would appreciate feedbacks for us to make improvements.\\n\\n1\\n\\n\\x0cGelly et al., 2012); multi-agent RL (Shoham et al., 2003; Busoniu et al., 2008); game the-\\nory (Leyton-Brown and Shoham, 2008), etc. We list RL resources in Section 23. See lists of RL\\napplications at: goo.gl/KoXIQC, and goo.gl/1Q1lzg.\\n\\nThe outline of this overview follows: background of deep learning and reinforcement learning,\\nas well as introduction of testbeds in Section 2; Deep Q-Network (DQN) and its extensions in\\nSection 3; asynchronous methods in Section 4; policy optimization in Section 5; reward in Section 6;\\nplanning in Section 7; attention and memory, in particular differentiable neural computer (DNC),\\nin Section 8; unsupervised learning in Section 9; learning to learn in Section 10; games, including\\nboard games, video games and imperfect information games, in Section 11; AlphaGo in Section 12;\\nrobotics in Section 13; spoken dialogue systems (a.k.a. chatbot) in Section 14; machine translation\\nin Section 15; text sequence prediction in Section 16; neural architecture design in Section 17;\\npersonalized web services in Section 18; healthcare in Section 19; \\xef\\xac\\x81nance in Section 20; music\\ngeneration in Section 21; a to-do list of topics/papers not reviewed yet in Section 22; and discussions\\nin Section 24.2\\nIn particular, we list a collection of RL resources including books, online courses, tutorials, con-\\nferences, journals and workshops, and blogs in Section 23. If picking a single RL resource, it is\\nProfessor Sutton\\xe2\\x80\\x99s RL book (Sutton and Barto, 2017), 2nd edition in progress. It covers RL funda-\\nmentals and re\\xef\\xac\\x82ects new progress, e.g., in deep Q-network, AlphaGo, policy gradient methods, as\\nwell as in psychology and neuroscience. A single pick for deep learning is Goodfellow et al. (2016).\\n\\n2 BACKGROUND\\n\\nIn this section, we brie\\xef\\xac\\x82y introduce concepts and fundamentals in deep learning (Goodfellow et al.,\\n2016) and reinforcement learning (Sutton and Barto, 2017).\\n\\n2.1 DEEP LEARNING\\n\\nDeep learning is in contrast to \\xe2\\x80\\x9dshallow\\xe2\\x80\\x9d learning. For many machine learning algorithms, e.g.,\\nlinear regression, logistic regression, support vector machines (SVMs), decision trees, and boosting,\\nwe have input layer and output layer, and the inputs may be transformed with manual feature en-\\ngineering before training. In deep learning, between input and output layers, we have one or more\\nhidden layers. At each layer except input layer, we compute the input to each unit, as the weighted\\nsum of units from the previous layer; then we usually use nonlinear transformation, or activation\\nfunction, such as logistic, tanh, or more popular recently, recti\\xef\\xac\\x81ed linear unit (ReLU), to apply to\\nthe input of a unit, to obtain a new representation of the input from previous layer. We have weights\\non links between units from layer to layer. After computations \\xef\\xac\\x82ow forward from input to output, at\\noutput layer and each hidden layer, we can compute error derivatives backward, and backpropagate\\ngradients towards the input layer, so that weights can be updated to optimize some loss function.\\n\\nA feedforward deep neural network or multilayer perceptron (MLP) is to map a set of input values\\nto output values with a mathematical function formed by composing many simpler functions at each\\nlayer. A convolutional neural network (CNN) is a feedforward deep network, with convolutional\\nlayers, pooling layers and fully connected layers. CNNs are designed to process data with multiple\\narrays, e.g., colour image, language, audio spectrogram, and video, bene\\xef\\xac\\x81t from the properties of\\nsuch signals: local connections, shared weights, pooling and the use of many layers, and are in-\\nspired by simple cells and complex cells in visual neuroscience (LeCun et al., 2015). A recurrent\\nneural network (RNN) is often used to process sequential inputs like speech and language, element\\n\\n2We discuss how/why we organize the overview from Section 3 to Section 21 in the current way: starting\\nwith RL fundamentals: value function/control, policy, reward, and planning (model in to-do list); next attention\\nand memory, unsupervised learning, and learning to learn, which, together with transfer/semi-supervised/one-\\nshot learning, etc, would be critical mechanisms for RL; then various applications.\\n\\nWe basically make a \\xef\\xac\\x82at organization of topics. Otherwise, there may be multiple ways to categorize the top-\\nics reviewed. For example, we can combine spoken dialogue systems, machine translation and text sequence\\nprediction as a single section about language models. Another way is to combine these topics, together with\\nlearning to learn, neural architecture design and music generation as a section about sequence modelling. Duel-\\ning architecture, Value Iteration Netwroks, and differentiable neural computer (DNC) are novel neural networks\\narchitectures for RL.\\n\\n2\\n\\n\\x0cby element, with hidden units to store history of past elements. A RNN can be seen as a multilayer\\nnetwork with all layers sharing the same weights, when being unfolded in time of forward compu-\\ntation. It is hard for RNN to store information for very long time and the gradient may vanish. Long\\nshort term memory networks (LSTM) and gated recurrent unit (GRU) were proposed to address\\nsuch issues, with gating mechanisms to manipulate information through recurrent cells. Gradient\\nbackpropagation or its variants can be used for training all above deep neural networks.\\n\\nDropout is a regularization strategy to train an ensemble of sub-networks by removing non-output\\nunits randomly from the original network. Batch normalization performs the normalization for each\\ntraining mini-batch, to accelerate training by reducing internal covariate shift, i.e., the change of\\nparameters of previous layers will change each layer\\xe2\\x80\\x99s inputs distribution.\\n\\nDeep neural networks learn representations automatically from raw inputs to recover the compo-\\nsitional hierarchies in many natural signals, i.e., higher-level features are composed of lower-level\\nones, e.g., in images, the hierarch of objects, parts, motifs, and local combinations of edges. Dis-\\ntributed representation is a central idea in deep learning, which implies that many features may\\nrepresent each input, and each feature may represent many inputs. The exponential advantages of\\ndeep, distributed representations combat the exponential challenges of the curse of dimensionality.\\nThe notion of end-to-end training refers to that a learning model uses raw inputs without manual\\nfeature engineering to generate outputs, e.g., AlexNet (Krizhevsky et al., 2012) with raw pixels for\\nimage classi\\xef\\xac\\x81cation, Seq2Seq (Sutskever et al., 2014) with raw sentences for machine translation,\\nand DQN (Mnih et al., 2015) with raw pixels and score to play games.\\n\\n2.2 REINFORCEMENT LEARNING\\n\\nReinforcement learning usually solves sequential decision making problems. An RL agent interacts\\nwith an environment over time. At each time step t, the agent receives a state st and selects an\\naction at from some action space A, following a policy \\xcf\\x80(at|st), which is the agent\\xe2\\x80\\x99s behavior,\\ni.e., a mapping from state st to actions at, receives a scalar reward rt, and transitions to the next\\nstate st+1, according to the environment dynamics, or model, for reward function R(s, a) and state\\ntransition probability P (st+1|st, at) respectively. In an episodic problem, this process continues\\nk=0 \\xce\\xb3krt+k is the\\ndiscounted, accumulated reward with the discount factor \\xce\\xb3 \\xe2\\x88\\x88 (0, 1]. The agent aims to maximize\\nthe expectation of such long term return from each state.\\n\\nuntil the agent reaches a terminal state and then it restarts. The return Rt = P\\xe2\\x88\\x9e\\n\\nA value function is a prediction of the expected, accumulative, discounted, future reward, measuring\\nhow good is each state, or state-action pair. The action value Q\\xcf\\x80(s, a) = E[Rt|st = s, at = a] is\\nthe expected return for selecting action a in state s and then following policy \\xcf\\x80. An optimal action\\nvalue function Q\\xe2\\x88\\x97(s, a) is the maximum action value achievable by any policy for state s and action\\na. We can de\\xef\\xac\\x81ne state value V \\xcf\\x80(s) and optimal state value V \\xe2\\x88\\x97(s) similarly.\\nTemporal difference (TD) learning is a central idea in RL. It learns value function V (s) directly\\nfrom experience with TD error, with bootstrapping, in a model-free, online, and fully incremental\\nway. The update rule is V (st) \\xe2\\x86\\x90 V (st) + \\xce\\xb1[rt + \\xce\\xb3V (st+1) \\xe2\\x88\\x92 V (st)], where \\xce\\xb1 is a learning rate, and\\nrt + \\xce\\xb3V (st+1) \\xe2\\x88\\x92 V (st) is called TD error. Similarly, Q-learning learns action value function, with\\nthe update rule, Q(st, at) \\xe2\\x86\\x90 Q(st, at) + \\xce\\xb1[r + \\xce\\xb3 maxat+1 Q(st+1, at+1) \\xe2\\x88\\x92 Q(st, at)]. Q-learning\\nis an off-policy control method. In contrast, SARSA, representing state, action, reward, (next) state,\\n(next) action, is an on-policy control method, with the update rule, Q(st, at) \\xe2\\x86\\x90 Q(st, at) + \\xce\\xb1[r +\\n\\xce\\xb3Q(st+1, at+1) \\xe2\\x88\\x92 Q(st, at)]. SARSA re\\xef\\xac\\x81nes the policy greedily with respect to action values. TD-\\nlearning, Q-learning and SARSA converge under certain conditions. From optimal action value\\nfunction, we can derive an optimal policy.\\n\\nThe above algorithms are referred to as TD(0) and Q(0), with one-step return. We have multi-step\\nreturn variants or Monte-Carlo approach in the forward view. The eligibility trace from the back-\\nward view provides an online, incremental implementation, resulting in TD(\\xce\\xbb) and Q(\\xce\\xbb) algorithms,\\nwhere \\xce\\xbb \\xe2\\x88\\x88 [0, 1]. When \\xce\\xbb = 1, it is the same as a Monte Carlo approach.\\nWe discuss the tabular cases above, where a value function or a policy is stored in a tabular form.\\nFunction approximation is a way for generalization when the state and/or action spaces are large or\\ncontinuous. Linear function approximation used to be a popular choice, esp. before the work of\\nDeep Q-Network (Mnih et al., 2015).\\n\\n3\\n\\n\\x0cIn contrast to value-based methods like TD learning and Q-learning, policy-based methods opti-\\nmize the policy \\xcf\\x80(a|s; \\xce\\xb8) (with function approximation) directly, and update the parameters \\xce\\xb8 by\\ngradient ascent on E[Rt]. REINFORCE is a policy gradient method, updating \\xce\\xb8 in the direc-\\ntion of \\xe2\\x88\\x87\\xce\\xb8 log \\xcf\\x80(at|st; \\xce\\xb8)Rt. Usually a baseline bt(st) is subtracted from the return to reduce\\nthe variance of gradient estimate, yet keeping its unbiasedness, to yield the gradient direction\\n\\xe2\\x88\\x87\\xce\\xb8 log \\xcf\\x80(at|st; \\xce\\xb8)(Rt \\xe2\\x88\\x92 bt(st)). Using V (st) as the baseline bt(st), we have the advantage func-\\ntion A(at, st) = Q(at, st) \\xe2\\x88\\x92 V (st), since Rt is an estimate of Q(at, st). In actor-critic algorithms,\\nthe critic updates action-value function parameters, and the actor updates policy parameters, in the\\ndirection suggested by the critic.\\n\\nWe obtain deep reinforcement learning (deep RL) methods when we use deep neural networks to\\napproximate any of the following component of reinforcement learning: value function, V (s; \\xce\\xb8) or\\nQ(s, a; \\xce\\xb8), policy \\xcf\\x80(a|s; \\xce\\xb8), and model (state transition and reward). Here, the parameters \\xce\\xb8 are\\nthe weights in deep neural networks. When we use \\xe2\\x80\\x9dshallow\\xe2\\x80\\x9d models, like linear function, deci-\\nsion trees, tile coding and so on as the function approximator, we obtain \\xe2\\x80\\x9dshallow\\xe2\\x80\\x9d RL, and the\\nparameters \\xce\\xb8 are the weight parameters in these models. Note, a shallow model, e.g., decision trees,\\nmay be non-linear. The distinct difference between deep RL and \\xe2\\x80\\x9dshallow\\xe2\\x80\\x9d RL is what function\\napproximator is used. This is similar to the difference between deep learning and \\xe2\\x80\\x9dshallow\\xe2\\x80\\x9d learn-\\ning. We usually utilize stochastic gradient descent to update weight parameters in deep RL. When\\noff-policy, function approximation, in particular, non-linear function approximation, and bootstrap-\\nping are combined together, instability and divergence may occur (Tsitsiklis and Van Roy, 1997).\\nHowever, recent work like Deep Q-Network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016)\\nstabilized the learning and achieved outstanding results.\\n\\nWe explain some terms in RL parlance. The prediction problem, or policy evaluation, is to compute\\nthe state or action value function for a policy. The control problem is to \\xef\\xac\\x81nd the optimal policy.\\nPlanning constructs a value function or a policy with a model. On-policy methods evaluate or\\nimprove the behavioural policy, e.g., SARSA \\xef\\xac\\x81ts the action-value function to the current policy,\\ni.e., SARSA evaluates the policy based on samples from the same policy, then re\\xef\\xac\\x81nes the policy\\ngreedily with respect to action values.\\nIn off-policy methods, an agent learns an optimal value\\nfunction/policy, maybe following an unrelated behavioural policy, e.g., Q-learning attempts to \\xef\\xac\\x81nd\\naction values for the optimal policy directly, not necessarily \\xef\\xac\\x81tting to the policy generating the data,\\ni.e., the policy Q-learning obtains is usually different from the policy that generates the samples.\\nThe notion of on-policy and off-policy can be understood as same-policy and different-policy.The\\nexploration-exploitation dilemma is about the agent needs to exploit the currently best action to\\nobtain rewards, yet it has to explore the environment to \\xef\\xac\\x81nd better actions. In model-free methods,\\nthe agent learns with trail-and-error from experience explicitly; the model (state transition function)\\nis not known or learned from experience. RL methods that use models are model-based methods. In\\nonline mode, training algorithms are executed on data acquired in sequence. In batch mode, models\\nare trained on the entire data set. With bootstrapping, an estimate of state or action value is updated\\nfrom subsequent estimates.\\n\\n2.3 TESTBEDS\\n\\nThe Arcade Learning Environment (ALE) (Bellemare et al., 2013) is a framework composed of\\nAtari 2600 games to develop and evaluate AI agents.\\n\\nDeepMind released a \\xef\\xac\\x81rst-person 3D game platform DeepMind Lab (Beattie et al., 2016).\\nDeepmind and Blizzard will collaborate to release the Starcraft II AI research environment\\n(goo.gl/Ptiwfg).\\n\\nOpenAI Gym (https://gym.openai.com) is a toolkit for the development of RL algorithms, consist-\\ning of environments, e.g., Atari games and simulated robots, and a site for the comparison and\\nreproduction of results.\\n\\nOpenAI Universe (https://universe.openai.com) is used to turn any program into a Gym environment.\\nUniverse has already integrated many environments, including Atari games, \\xef\\xac\\x82ash games, browser\\ntasks like Mini World of Bits and real-world browser tasks. Recently, GTA V was added to Universe\\nfor self-driving vehicle simulation.\\n\\n4\\n\\n\\x0cFAIR TorchCraft (Synnaeve et al., 2016) is a library for Real-Time Strategy (RTS) games such as\\nStarCraft: Brood War.\\n\\nViZDoom is a Doom-based AI research platform for visual RL (Kempka et al., 2016).\\n\\nTORCS is a car racing simulator (Bernhard Wymann et al., 2014).\\n\\nMuJoCo, Multi-Joint dynamics with Contact, is a physics engine. See http://www.mujoco.org.\\n\\nDuan et al. (2016) presented a benchmark for continuous control tasks. The open source is available\\nat: https://github.com/openai/rllab.\\n\\nNogueira and Cho (2016) presented WebNav Challenge for Wikipedia links navigation.\\n\\n3 DEEP Q-NETWORK\\n\\nMnih et al. (2015) introduced Deep Q-Network (DQN) and ignited the \\xef\\xac\\x81eld of deep RL. Before\\nDQN, it is well known that RL is unstable or even divergent when action-value Q function is ap-\\nproximated with a nonlinear function like neural networks. DQN made several important contribu-\\ntions: 1) stabilize the training of Q action value function approximation with deep neural networks\\n(CNN) using experience replay (Lin, 1992) and target network; 2) designing an end-to-end RL ap-\\nproach, with only the pixels and the game score as inputs, so that only minimal domain knowledge\\nis required; 3) training a \\xef\\xac\\x82exible network with the same algorithm, network architecture and hyper-\\nparameters to perform well on many different tasks, i.e., 49 Atari games (Bellemare et al., 2013),\\nand outperforming previous algorithms and performing comparably to a human professional tester.\\n\\nSee Chapter 16 in Sutton and Barto (2017) for a Sutton-style description of Deep Q-Network. See\\nDeepmind\\xe2\\x80\\x99s description of DQN at goo.gl/IWco9h. We present DQN pseudo code below.\\n\\nInput: the pixels and the game score\\nOutput: Q action value function (from which we obtain policy and select action)\\nInitialize replay memory D\\nInitialize action-value function Q with random weight \\xce\\xb8\\nInitialize target action-value function \\xcb\\x86Q with weights \\xce\\xb8\\xe2\\x88\\x92 = \\xce\\xb8\\nfor episode = 1 to M do\\n\\nInitialize sequence s1 = {x1} and preprocessed sequence \\xcf\\x861 = \\xcf\\x86(s1)\\nfor t = 1 to T do\\n\\nFollowing \\xc7\\xab-greedy policy, select at = (cid:26)a random action\\n\\narg maxa Q(\\xcf\\x86(st), a; \\xce\\xb8)\\n\\nwith probability \\xc7\\xab\\notherwise\\n\\nExecute action ai in emulator and observe reward rt and image xt+1\\nSet st+1 = st, at, xt+1 and preprocess \\xcf\\x86t+1 = \\xcf\\x86(st+1)\\nStore transition (\\xcf\\x86t, at, rt, \\xcf\\x86t+1) in D\\n// experience replay\\nSample random minibatch of transitions (\\xcf\\x86j, aj, rj , \\xcf\\x86j+1) from D\\nSet yj = (cid:26)rj\\n\\nrj + \\xce\\xb3 maxa\\xe2\\x80\\xb2 \\xcb\\x86Q(\\xcf\\x86j+1, a\\xe2\\x80\\xb2; \\xce\\xb8\\xe2\\x88\\x92) otherwise\\n\\nif episode terminates at step j + 1\\n\\nPerform a gradient descent step on (yj \\xe2\\x88\\x92 Q(\\xcf\\x86j, aj; \\xce\\xb8))2 w.r.t. the network parameter \\xce\\xb8\\n// periodic update of target network\\nEvery C steps reset \\xcb\\x86Q = Q, i.e., set \\xce\\xb8\\xe2\\x88\\x92 = \\xce\\xb8\\n\\nend\\n\\nend\\n\\nAlgorithm 1: Deep Q-Nework (DQN), adapted from Mnih et al. (2015)\\n\\n3.1 DOUBLE DQN\\n\\nvan Hasselt et al. (2016a) proposed Double DQN (D-DQN) to tackle the overestimate problem in\\nQ-learning. In standard Q-learning, as well as in DQN, the parameters are updated as follows:\\n\\n\\xce\\xb8t+1 = \\xce\\xb8t + \\xce\\xb1(yQ\\n\\nt \\xe2\\x88\\x92 Q(st, at; \\xce\\xb8t))\\xe2\\x88\\x87\\xce\\xb8t Q(st, at; \\xce\\xb8t),\\n\\n5\\n\\n\\x0cwhere\\n\\nyQ\\nt = rt+1 + \\xce\\xb3 max\\n\\na\\n\\nQ(st+1, a; \\xce\\xb8t),\\n\\nso that the max operator uses the same values to both select and evaluate an action. As a conse-\\nquence, it is more likely to select overestimated values, and results in overoptimistic value estimates.\\nvan Hasselt et al. (2016a) proposed to evaluate the greedy policy according to the online network,\\nbut to use the target network to estimate its value. This can be achieved with a minor change to the\\nDQN algorithm, replacing yQ\\n\\nt with\\n\\nyD\\xe2\\x88\\x92DQN\\nt\\n\\n= rt+1 + \\xce\\xb3Q(st+1, max\\n\\na\\n\\nQ(st+1, at; \\xce\\xb8t); \\xce\\xb8\\xe2\\x88\\x92\\n\\nt ),\\n\\nwhere \\xce\\xb8t is the parameter for online network and \\xce\\xb8\\xe2\\x88\\x92\\nt\\nence, yQ\\n\\nt can be written as\\n\\nis the parameter for target network. For refer-\\n\\nyQ\\nt = rt+1 + \\xce\\xb3Q(st+1, max\\n\\na\\n\\nQ(st+1, at; \\xce\\xb8t); \\xce\\xb8t).\\n\\nD-DQN found better policies than DQN on Atari games.\\n\\n3.2 PRIORITIZED EXPERIENCE REPLAY\\n\\nIn DQN, experience transitions are uniformly sampled from the replay memory, regardless of the\\nsigni\\xef\\xac\\x81cance of experiences. Schaul et al. (2016) proposed to prioritize experience replay, so that\\nimportant experience transitions can be replayed more frequently, to learn more ef\\xef\\xac\\x81ciently. The\\nimportance of experience transitions are measured by TD errors. The authors designed a stochastic\\nprioritization based on the TD errors, using importance sampling to avoid the bias in the update\\ndistribution. The authors used prioritized experience replay in DQN and D-DQN, and improved\\ntheir performance on Atari games.\\n\\n3.3 DUELING ARCHITECTURE\\n\\nWang et al. (2016b) proposed the dueling network architecture to estimate state value function V (s)\\nand associated advantage function A(s, a), and then combine them to estimate action value function\\nQ(s, a), to converge faster than Q-learning. In DQN, a CNN layer is followed by a fully connected\\n(FC) layer. In dueling architecture, a CNN layer is followed by two streams of FC layers, to estimate\\nvalue function and advantage function separately; then the two streams are combined to estimate\\naction function. Usually we use the following to combine V (s) and A(s, a) to obtain Q(s, a),\\n\\nQ(s, a; \\xce\\xb8, \\xce\\xb1, \\xce\\xb2) = V (s; \\xce\\xb8, \\xce\\xb2) +(cid:0)A(s, a; \\xce\\xb8, \\xce\\xb1) \\xe2\\x88\\x92 max\\n\\na\\xe2\\x80\\xb2\\n\\nA(s, a\\xe2\\x80\\xb2; \\xce\\xb8, \\xce\\xb1)(cid:1)\\n\\nwhere \\xce\\xb1 and \\xce\\xb2 are parameters of the two streams of FC layers. Wang et al. (2016b) proposed to\\nreplace max operator with average as following for better stability,\\n\\nQ(s, a; \\xce\\xb8, \\xce\\xb1, \\xce\\xb2) = V (s; \\xce\\xb8, \\xce\\xb2) +(cid:0)A(s, a; \\xce\\xb8, \\xce\\xb1) \\xe2\\x88\\x92\\n\\na\\n|A|\\n\\nA(s, a\\xe2\\x80\\xb2; \\xce\\xb8, \\xce\\xb1)(cid:1)\\n\\nDueling architecture implemented with D-DQN and prioritized experience replay improved previous\\nwork, DQN and D-DQN with prioritized experience replay, on Atari games.\\n\\n3.4 MORE EXTENSIONS\\n\\nMnih et al. (2016) proposed asynchronous methods for RL methods, in particular, the asynchronous\\nadvantage actor-critic (A3C) algorithm, as discussed in Section 4. Osband et al. (2016) designed\\nbetter exploration strategy to improve DQN. O\\xe2\\x80\\x99Donoghue et al. (2017) proposed policy gradient\\nand Q-learning (PGQ), as discussed in Section 5.6. He et al. (2017) proposed to accelerate DQN\\nby optimality tightening, a constrained optimization approach, to propagate reward faster, and to\\nimprove accuracy over DQN. Babaeizadeh et al. (2017) proposed a hybrid CPU/GPU implementa-\\ntion of A3C. Liang et al. (2016) attempted to understand the success of DQN and reproduced results\\nwith shallow RL.\\n\\n6\\n\\n\\x0c4 ASYNCHRONOUS METHODS\\n\\nMnih et al. (2016) proposed asynchronous methods for four RL methods, Q-learning, SARSA, n-\\nstep Q-learning and advantage actor-critic, and the asynchronous advantage actor-critic (A3C) algo-\\nrithm performs the best. Parallel actors employ different exploration policies to stabilize training, so\\nthat experience replay is not utilized. Different from most deep learning algorithms, asynchronous\\nmethods can run on a single multi-core CPU. For Atari games, A3C ran much faster yet performed\\nbetter than or comparably with DQN, Gorila, D-DQN, Dueling D-DQN, and Prioritized D-DQN.\\nA3C also succeeded on continuous motor control problems: TORCS car racing games and MujoCo\\nphysics manipulation and locomotion, and Labyrinth, a navigating task in random 3D mazes using\\nvisual inputs, in which an agent will face a new maze in each new episode, so that it needs to learn\\na general strategy to explore random mazes.\\n\\nWe present pseudo code for asynchronous advantage actor-critic for each actor-learner thread. A3C\\nmaintains a policy \\xcf\\x80(at|st; \\xce\\xb8) and an estimate of the value function V (st; \\xce\\xb8v), being updated with\\nn-step returns in the forward view, after every tmax actions or reaching a terminal state, similar to\\nusing minibatches. The gradient update can be seen as \\xe2\\x88\\x87\\xce\\xb8 \\xe2\\x80\\xb2 log \\xcf\\x80(at|st; \\xce\\xb8\\xe2\\x80\\xb2)A(st, at; \\xce\\xb8, \\xce\\xb8v), where\\ni=0 \\xce\\xb3irt+i + \\xce\\xb3kV (st+k; \\xce\\xb8v) \\xe2\\x88\\x92 V (st; \\xce\\xb8v) is an estimate of the advantage func-\\n\\nA(st, at; \\xce\\xb8, \\xce\\xb8v) = Pk\\xe2\\x88\\x921\\n\\ntion, with k upbounded by tmax.\\n\\nGlobal shared parameter vectors \\xce\\xb8 and \\xce\\xb8v, thread-speci\\xef\\xac\\x81c parameter vectors \\xce\\xb8\\xe2\\x80\\xb2 and \\xce\\xb8\\xe2\\x80\\xb2\\nv\\nGlobal shared counter T = 0, Tmax\\nInitialize step counter t \\xe2\\x86\\x90 1\\nfor T \\xe2\\x89\\xa4 Tmax do\\n\\nReset gradients, d\\xce\\xb8 \\xe2\\x86\\x90 0 and d\\xce\\xb8v \\xe2\\x86\\x90 0\\nSynchronize thread-speci\\xef\\xac\\x81c parameters \\xce\\xb8\\xe2\\x80\\xb2 = \\xce\\xb8 and \\xce\\xb8\\xe2\\x80\\xb2\\nSet tstart = t, get state st\\nfor st not terminal and t \\xe2\\x88\\x92 tstart \\xe2\\x89\\xa4 tmax do\\n\\nv = \\xce\\xb8v\\n\\nTake at according to policy \\xcf\\x80(at|st; \\xce\\xb8\\xe2\\x80\\xb2)\\nReceive reward rt and new state st+1\\nt \\xe2\\x86\\x90 t + 1, T \\xe2\\x86\\x90 T + 1\\n\\nend\\n\\nfor terminal st\\n\\nR = (cid:26)0\\nv) otherwise\\nfor i \\xe2\\x88\\x88 {t \\xe2\\x88\\x92 1, ..., tstart} do\\n\\nV (st, \\xce\\xb8\\xe2\\x80\\xb2\\n\\nR \\xe2\\x86\\x90 ri + \\xce\\xb3R\\naccumulate gradients wrt \\xce\\xb8\\xe2\\x80\\xb2: d\\xce\\xb8 \\xe2\\x86\\x90 d\\xce\\xb8 + \\xe2\\x88\\x87\\xce\\xb8 \\xe2\\x80\\xb2 log \\xcf\\x80(ai|si; \\xce\\xb8\\xe2\\x80\\xb2)(R \\xe2\\x88\\x92 V (si; \\xce\\xb8\\xe2\\x80\\xb2\\naccumulate gradients wrt \\xce\\xb8\\xe2\\x80\\xb2\\n\\nv: d\\xce\\xb8v \\xe2\\x86\\x90 d\\xce\\xb8v + \\xe2\\x88\\x87\\xce\\xb8 \\xe2\\x80\\xb2\\n\\nv (R \\xe2\\x88\\x92 V (si; \\xce\\xb8\\xe2\\x80\\xb2\\n\\nv))2\\n\\nv))\\n\\nend\\nUpdate asynchronously \\xce\\xb8 using d\\xce\\xb8, and \\xce\\xb8v using d\\xce\\xb8v\\n\\nend\\n\\nAlgorithm 2: A3C, each actor-learner thread, based on Mnih et al. (2016)\\n\\n5 POLICY OPTIMIZATION\\n\\nPolicies are usually stochastic. However, Silver et al. (2014) introduced the deterministic pol-\\nicy gradient (DPG) for ef\\xef\\xac\\x81cient estimation of policy gradient. Lillicrap et al. (2016) extended\\nDPG with deep neural networks. We also introduce several recent work, including Guided Policy\\nSearch (Levine et al., 2016a), Trust Region Policy Optimization (Schulman et al., 2015), benchmark\\nresults (Duan et al., 2016) and policy gradient and Q-learning (O\\xe2\\x80\\x99Donoghue et al., 2017).\\n\\n5.1 DETERMINISTIC POLICY GRADIENT\\n\\nSilver et al. (2014) introduced the deterministic policy gradient (DPG) algorithm for RL problems\\nwith continuous action spaces. The deterministic policy gradient is the expected gradient of the\\naction-value function, which integrates over the state space; whereas in the stochastic case, the pol-\\nicy gradient integrates over both state and action spaces. Consequently, the deterministic policy\\ngradient can be estimated more ef\\xef\\xac\\x81ciently than the stochastic policy gradient. The authors intro-\\n\\n7\\n\\n\\x0cduced an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory\\nbehaviour policy, and to ensure unbiased policy gradient with the compatible function approxima-\\ntion for deterministic policy gradients. Empirical results showed its superior to stochastic policy\\ngradients, in particular in high dimensional tasks, on several problems: a high-dimensional bandit;\\nstandard benchmark RL tasks of mountain car and pendulum and 2D puddle world with low dimen-\\nsional action spaces; and controlling an octopus arm with a high-dimensional action space. The\\nexperiments were conducted with tile-coding and linear function approximators.\\n\\n5.2 DEEP DETERMINISTIC POLICY GRADIENT\\n\\nLillicrap et al. (2016) proposed an actor-critic, model-free, deep deterministic policy gradient\\n(DDPG) algorithm in continuous action spaces, by extending DQN (Mnih et al., 2015) and\\nDPG (Silver et al., 2014). With actor-critic as in DPG, DDPG avoids the optimization of action at\\nevery time step to obtain a greedy policy as in Q-learning, which will make it infeasible in com-\\nplex action spaces with large, unconstrained function approximators like deep neural networks.\\nTo make the learning stable and robust, similar to DQN, DDPQ deploy experience replay and an\\nidea similar to target network, \\xe2\\x80\\x9dsoft\\xe2\\x80\\x9d target, which, rather than copying the weights directly as in\\nDQN, updates the soft target network weights \\xce\\xb8\\xe2\\x80\\xb2 slowly to track the learned networks weights \\xce\\xb8:\\n\\xce\\xb8\\xe2\\x80\\xb2 \\xe2\\x86\\x90 \\xcf\\x84 \\xce\\xb8 + (1 \\xe2\\x88\\x92 \\xcf\\x84 )\\xce\\xb8\\xe2\\x80\\xb2, with \\xcf\\x84 \\xe2\\x89\\xaa 1. The authors adapted batch normalization to handle the issue\\nthat the different components of the observation with different physical units. As an off-policy al-\\ngorithm, DDPG learns an actor policy from experiences from an exploration policy by adding noise\\nsampled from a noise process to the actor policy. More than 20 simulated physics tasks of varying\\ndif\\xef\\xac\\x81culty in the MuJoCo environment were solved with the same learning algorithm, network archi-\\ntecture and hyper-parameters, and obtained policies with performance competitive with those found\\nby a planning algorithm with full access to the underlying physical model and its derivatives. DDPG\\ncan solve problems with 20 times fewer steps of experience than DQN, although it still needs a large\\nnumber of training episodes to \\xef\\xac\\x81nd solutions, as in most model-free RL methods. It is end-to-end,\\nwith raw pixels as input. DDPQ paper also contains links to videos for illustration.\\n\\n5.3 GUIDED POLICY SEARCH\\n\\nLevine et al. (2016a) proposed to train the perception and control systems jointly end-to-end, to map\\nraw image observations directly to torques at the robot\\xe2\\x80\\x99s motors. The authors introduced guided pol-\\nicy search (GPS) to train policies represented as CNN, by transforming policy search into supervised\\nlearning to achieve data ef\\xef\\xac\\x81ciency, with training data provided by a trajectory-centric RL method\\noperating under unknown dynamics. GPS alternates between trajectory-centric RL and supervised\\nlearning, to obtain the training data coming from the policy\\xe2\\x80\\x99s own state distribution, to address\\nthe issue that supervised learning usually does not achieve good, long-horizon performance. GPS\\nutilizes pre-training to reduce the amount of experience data to train visuomotor policies. Good\\nperformance was achieved on a range of real-world manipulation tasks requiring localization, visual\\ntracking, and handling complex contact dynamics, and simulated comparisons with previous policy\\nsearch methods. As the authors mentioned, \\xe2\\x80\\x9dthis is the \\xef\\xac\\x81rst method that can train deep visuomotor\\npolicies for complex, high-dimensional manipulation skills with direct torque control\\xe2\\x80\\x9d.\\n\\n5.4 TRUST REGION POLICY OPTIMIZATION\\n\\nSchulman et al. (2015) introduced an iterative procedure to monotonically improve policies, and\\nproposed a practical algorithm, Trust Region Policy Optimization (TRPO), by making several ap-\\nproximations. The authors also uni\\xef\\xac\\x81ed policy iteration and policy gradient with analysis. In the\\nexperiments, TRPO methods performed well on simulated robotic tasks of swimming, hopping, and\\nwalking, as well as playing Atari games in an end-to-end manner directly from raw images.\\n\\n5.5 BENCHMARK RESULTS\\n\\nDuan et al. (2016) presented a benchmark for continuous control tasks, including classic tasks like\\ncart-pole, tasks with very large state and action spaces such as 3D humanoid locomotion and tasks\\nwith partial observations, and tasks with hierarchical structure, implemented various algorithms,\\nincluding batch algorithms: REINFORCE, Truncated Natural Policy Gradient (TNPG), Reward-\\n\\n8\\n\\n\\x0cWeighted Regression (RWR), Relative Entropy Policy Search (REPS), Trust Region Policy Opti-\\nmization (TRPO), Cross Entropy Method (CEM), Covariance Matrix Adaption Evolution Strategy\\n(CMA-ES); online algorithms: Deep Deterministic Policy Gradient (DDPG); and recurrent variants\\nof batch algorithms. The open source is available at: https://github.com/ rllab/rllab.\\n\\nDuan et al. (2016) compared various algorithms, and showed that DDPG, TRPO, and Truncated Nat-\\nural Policy Gradient (TNPG) (Schulman et al., 2015) are effective in training deep neural network\\npolicies, yet better algorithms are called for hierarchical tasks.\\n\\n5.6 COMBINING POLICY GRADIENT AND Q-LEARNING\\n\\nO\\xe2\\x80\\x99Donoghue et al. (2017) proposed to combine policy gradient with off-policy Q-learning (PGQ),\\nto bene\\xef\\xac\\x81t from experience replay. Usually actor-critic methods are on-policy. The authors also\\nshowed that action value \\xef\\xac\\x81tting techniques and actor-critic methods are equivalent, and interpreted\\nregularized policy gradient techniques as advantage function learning algorithms. Empirically, the\\nauthors showed that PGQ outperformed DQN and A3C on Atari games.\\n\\n6 REWARD\\n\\nInverse reinforcement learning (IRL) is the problem of determining a reward function given observa-\\ntions of optimal behaviour (Ng and Russell, 2000). In imitation learning, or apprenticeship learning,\\nan agent learns to perform a task from expert demonstrations, with samples of trajectories from the\\nexpert, without reinforcement signal, without additional data from the expert while training; two\\nmain approaches for imitation learning are behavioral cloning and inverse reinforcement learning;\\nbehavioral cloning is formulated as a supervised learning problem to map state-action pairs from\\nexpert trajectories to policy (Ho and Ermon, 2016).\\n\\n6.1 GENERATIVE ADVERSARIAL NETWORKS\\n\\nGoodfellow et al. (2014) proposed generative adversarial nets (GANs) to estimate generative models\\nvia an adversarial process by training two models simultaneously, a generative model G to capture\\nthe data distribution, and a discriminative model D to estimate the probability that a sample comes\\nfrom the training data but not the generative model G.\\n\\nGoodfellow et al. (2014) modelled G and D with multilayer perceptrons: G(z : \\xce\\xb8g) and D(x : \\xce\\xb8d),\\nwhere \\xce\\xb8g and \\xce\\xb8d are parameters, x are data points, and z are input noise variables. De\\xef\\xac\\x81ne a prior on\\ninput noise variable pz(z). G is a differentiable function and D(x) outputs a scalar as the probability\\nthat x comes from the training data rather than pg, the generative distribution we want to learn.\\nD will be trained to maximize the probability of assigning labels correctly to samples from both\\ntraining data and G. Simultaneously, G will be trained to minimize such classi\\xef\\xac\\x81cation accuracy,\\nlog(1 \\xe2\\x88\\x92 D(G(z))). As a result, D and G form the two-player minimax game as follows:\\n\\nmin\\n\\nG\\n\\nmax\\n\\nD\\n\\nEx\\xe2\\x88\\xbcpdata(x)[log D(x)] + Ez\\xe2\\x88\\xbcpz(z)[log(1 \\xe2\\x88\\x92 D(G(z)))]\\n\\nGoodfellow et al. (2014) showed that as G and D are given enough capacity, generative adversarial\\nnets can recover the data generating distribution, and provided a training algorithm with backpropa-\\ngation by minibatch stochastic gradient descent.\\n\\nGenerative adversarial networks have received much attention. See Goodfellow (2017) for Ian\\nGoodfellow\\xe2\\x80\\x99s summary of his NIPS 2016 Tutorial.\\n\\n6.2 GENERATIVE ADVERSARIAL IMITATION LEARNING\\n\\nWith IRL, an agent learns a reward function \\xef\\xac\\x81rst, then from which derives an optimal policy. Many\\nIRL algorithms have high time complexity, with an RL problem in the inner loop.\\n\\nHo and Ermon (2016) proposed generative adversarial imitation learning algorithm to learn policies\\ndirectly from data, bypassing the intermediate IRL step. Generative adversarial training was de-\\n\\n9\\n\\n\\x0cployed to \\xef\\xac\\x81t the discriminator, the distribution of states and actions that de\\xef\\xac\\x81nes expert behavior, and\\nthe generator, the policy.\\nGenerative adversarial imitation learning \\xef\\xac\\x81nds a policy \\xcf\\x80\\xce\\xb8 so that a discriminator DR can not distin-\\nguish states following the expert policy \\xcf\\x80E and states following the imitator policy \\xcf\\x80\\xce\\xb8, hence forcing\\nDR to take 0.5 in all cases and \\xcf\\x80\\xce\\xb8 not distinguishable from \\xcf\\x80E in the equillibrium. Such a game is\\nformulated as:\\n\\nmax\\n\\xcf\\x80\\xce\\xb8\\n\\nmin\\nDR\\n\\n\\xe2\\x88\\x92E\\xcf\\x80\\xce\\xb8 [log DR(s)] \\xe2\\x88\\x92 E\\xcf\\x80E [log(1 \\xe2\\x88\\x92 DR(s))]\\n\\nThe authors represented both \\xcf\\x80\\xce\\xb8 and DR as deep neural networks, and found an optimal solution\\nby repeatedly performing gradient updates on each of them. DR can be trained with supervised\\nlearning with a data set formed from traces from a current \\xcf\\x80\\xce\\xb8 and expert traces. For a \\xef\\xac\\x81xed DR, an\\noptimal \\xcf\\x80\\xce\\xb8 is sought. Hence it is a policy optimization problem, with \\xe2\\x88\\x92 log DR(s) as the reward.\\nThe authors trained \\xcf\\x80\\xce\\xb8 by trust region policy optimization (Schulman et al., 2015).\\nFinn et al. (2016) established a connection between GANs, IRL, and energy-based models.\\nPfau and Vinyals (2016) established the connection between GANs and actor-critic algorithms.\\n\\n7 PLANNING\\n\\nPlanning constructs a value function or a policy usually with a model. Tamar et al. (2016) intro-\\nduced Value Iteration Networks (VIN), a fully differentiable CNN planning module to approximate\\nthe value iteration algorithm, to learn to plan, e.g, policies in RL. In contrast to conventional plan-\\nning, VIN is model-free, where reward and transition probability are part of the neural network to be\\nlearned, so that it avoids issues with system identi\\xef\\xac\\x81cation. VIN can be trained end-to-end with back-\\npropagation. VIN can generalize in a diverse set of tasks: simple gridworlds, Mars Rover Naviga-\\ntion, continuous control and WebNav Challenge for Wikipedia links navigation (Nogueira and Cho,\\n2016). One merit of Value Iteration Network, as well as Dueling Network(Wang et al., 2016b), is\\nthat they design novel deep neural networks architectures for reinforcement learning problems. See\\na blog about VIN at goo.gl/Dr8gKL.\\n\\n8 ATTENTION AND MEMORY\\n\\nAttention and memory are two important mechanisms, which work together in many cases.\\n\\nMnih et al. (2014) introduced the recurrent attention model (RAM) to focus on selected sequence\\nof regions or locations from an image or video for image classi\\xef\\xac\\x81cation and object detection. The\\nauthors used RL methods, in particular, REINFORCE algorithm, to train the model, to overcome\\nthe issue that the model is non-differentiable, and experimented on an image classi\\xef\\xac\\x81cation task\\nand a dynamic visual control problem. Xu et al. (2015) integrated attention to image captioning,\\ntrained the hard version attention with the REINFORCE algorithm, and showed the effectiveness of\\nattention on Flickr8k, Flickr30k, and MS COCO datasets. The attention mechanism is also deployed\\nin NLP, e.g., in Bahdanau et al. (2015; 2017), and with external memory, in differentiable neural\\ncomputer (Graves et al., 2016).\\n\\nGraves et al. (2016) proposed differentiable neural computer (DNC), in which, a neural network can\\nread from and write to an external memory, so that DNC can solve complex, structured problems,\\nwhich a neural network without read-write memory can not solve. DNC minimizes memory alloca-\\ntion interference and enables long-term storage. Similar to a conventional computer, in a DNC, the\\nneural network is the controller and the external memory is the random-access memory; and a DNC\\nrepresents and manipulates complex data structures with the memory. Differently, a DNC learns\\nsuch representation and manipulation end-to-end with gradient descent from data in a goal-directed\\nmanner. When trained with supervised learning, a DNC can solve synthetic question answering\\nproblems, for reasoning and inference in natural language; it can solve the shortest path \\xef\\xac\\x81nding\\nproblem between two stops in transportation networks and the relationship inference problem in a\\nfamily tree. When trained with reinforcement learning, a DNC can solve a moving blocks puzzle\\nwith changing goals speci\\xef\\xac\\x81ed by symbol sequences. DNC outperformed normal neural network like\\n\\n10\\n\\n\\x0cLSTM or DNC\\xe2\\x80\\x99s precursor Neural Turing Machine (Graves et al., 2014); with harder problems, an\\nLSTM may simply fail. Although these experiments are relatively small-scale, we expect to see\\nfurther improvements and applications of DNC.\\n\\nSee Deepmind\\xe2\\x80\\x99s description of DNC at goo.gl/58mgoX. See more work on attention and/or\\nmemory, e.g., Ba et al. (2014; 2016); Chen et al. (2016a); Danihelka et al. (2016); Eslami et al.\\n(2016); Gregor et al. (2015); Jaderberg et al. (2015); Kaiser and Bengio (2016); Kadlec et al.\\n(2016); Oquab et al. (2015); Weston et al. (2015); Sukhbaatar et al. (2015); Yang et al. (2015);\\nZagoruyko and Komodakis (2017); Zaremba and Sutskever (2015).\\nSee goo.gl/ArW2nE and\\ngoo.gl/UukROv for blogs about attention and memory.\\n\\n9 UNSUPERVISED LEARNING\\n\\nJaderberg et al. (2017) proposed UNsupervised REinforcement and Auxiliary Learning (UNREAL)\\nto improve learning ef\\xef\\xac\\x81ciency by maximizing pseudo-reward functions, besides the usual cumulative\\nreward, while sharing a common representation. UNREAL bene\\xef\\xac\\x81ts from learning from the abundant\\npossible training signals, especially when the extrinsic reward signals are rarely observed. UNREAL\\nis composed of RNN-LSTM base agent, pixel control, reward prediction, and value function replay.\\nThe base agent is trained on-policy with A3C. Experiences of observations, rewards and actions\\nare stored in a reply buffer, for being used by auxiliary tasks. The auxiliary policies use the base\\nCNN and LSTM, together with a deconvolutional network, to maximize changes in pixel intensity\\nof different regions of the input images. The reward prediction module predicts short-term extrinsic\\nreward in next frame by observing the last three frames, to tackle the issue of reward sparsity. Value\\nfunction replay further trains the value function. UNREAL improved A3C\\xe2\\x80\\x99s performance on Atari\\ngames, and performed well on 3D Labyrinth game. See Deepmind\\xe2\\x80\\x99s description of UNREAL at\\ngoo.gl/zhqBGy.\\n\\nWe discuss robotics navigation with similar unsupervised auxiliary learning in Section 13, and\\ngenerative adversarial networks (GANs), a recent unsupervised learning framework, in Section 6.\\nSee Sutton et al. (2011) for Horde, a scalable real-time architecture for learning knowledge from\\nunsupervised sensorimotor interaction.\\n\\n10 LEARNING TO LEARN\\n\\nLearning to learn is related to transfer learning, multi-task learning or representation learning, and\\nis a core ingredient to achieve strong AI (Lake et al., 2016). Learning to learn is also related to meta\\nlearning or one-shot learning.\\n\\nDuan et al. (2017) and Wang et al. (2016a) proposed to learn a \\xef\\xac\\x82exible RNN model to handle a\\nfamily of RL tasks, to improve sample ef\\xef\\xac\\x81ciency, learn new tasks in a few samples, and bene\\xef\\xac\\x81t\\nfrom prior knowledge. The agent is modelled with RNN, with inputs of observations, rewards,\\nactions and termination \\xef\\xac\\x82ags; the weights of RNN are trained with RL,TRPO in Duan et al. (2017)\\nand A3C in Wang et al. (2016a), and achieve similar performance for various problems to speci\\xef\\xac\\x81c\\nRL algorithms. Duan et al. (2017) experimented with multi-arm bandits, tabular MDPs and visual\\nnavigation, and discussed that for larger problems, better RL algorithms are needed to train RNN.\\nWang et al. (2016a) experimented with bandits with independent arms, bandits with dependant arms,\\nrestless arms and MDPs. A future work is to improve scalability.\\n\\nLi and Malik (2017) proposed to automate unconstrained continuous optimization algorithms with\\nguided policy search (Levine et al., 2016a) by representing a particular optimization algorithm as a\\npolicy, and convergence rate as reward.\\n\\n11 GAMES\\n\\nGames provide excellent testbeds for RL/AI algorithms. We discuss Deep Q-Network (DQN) in\\nSection 3 and its extensions, all of which experimented with Atari games. We discuss Mnih et al.\\n(2016) in Section 4, Jaderberg et al. (2017) in Section 9, and Mirowski et al. (2017) in Section 13,\\nand they used Labyrinth as the testbed.\\n\\n11\\n\\n\\x0cBackgammon and Go are perfect information games. We discuss brie\\xef\\xac\\x82y Backgammon in Sec-\\ntion 11.1 about board games. We talk about video games like Doom in Section 11.2. We put\\npoker, a board game, under Section 11.3 about imperfect information games, where game theory\\nis concerned. Video games like Labyrinth and Doom are usually imperfect information games,\\nwhereas game theory is not (yet) used in these work to approach the problem. We single out Al-\\nphaGo (Silver et al., 2016) in Section 12, for its signi\\xef\\xac\\x81cance.\\n\\n11.1 BOARD GAMES\\n\\nBoard games, e.g., backgammon, Go, chess, checker and othello, are classical testbeds for RL/AI\\nalgorithms. Tesauro (1994) approached backgammon by using neural networks to approximate\\nvalue function learned with TD learning, and achieved human level performance.\\n\\n11.2 VIDEO GAMES\\n\\nWu and Tian (2017) deployed A3C with CNN to train an agent in a partially observable 3D envi-\\nronment, Doom, from recent four raw frames and game variables, to predict next action and value\\nfunction, following the curriculum learning (Bengio et al., 2009) approach of starting with simple\\ntasks and gradually transition to harder ones. It is nontrivial to apply A3C to such 3D games directly,\\npartly due to sparse and long term reward. The authors won the champion in Track 1 of ViZDoom\\nCompetition by a large margin, and plan the following future work: a map from an unknown envi-\\nronment, localization, a global plan to act, and visualization of the reasoning process.\\n\\nDosovitskiy and Koltun (2017) approached the problem of sensorimotor control in immersive en-\\nvironments with supervised learning, and won the Full Deathmatch track of the Visual Doom AI\\nCompetition. We list it here since it is usually an RL problem, yet it was solved with supervised\\nlearning. Lample and Chaplot (2016) also discussed how to tackle Doom.\\n\\nUsunier et al. (2016) studied StarCraft and Tessler et al. (2017) studied Minecraft.\\n\\n11.3\\n\\nIMPERFECT INFORMATION GAMES\\n\\nHeinrich and Silver (2016) proposed Neural Fictitious Self-Play (NFSP) to combine \\xef\\xac\\x81ctitious self-\\nplay with deep RL to learn approximate Nash equilibria for games of imperfect information in\\na scalable end-to-end approach without prior domain knowledge. NFSP was evaluated on two-\\nplayer zero-sum games. In Leduc poker, NFSP approached a Nash equilibrium, while common RL\\nmethods diverged. In Limit Texas Hold\\xe2\\x80\\x99em, a real-world scale imperfect-information game, NFSP\\nperformed similarly from scratch to state-of-the-art, superhuman algorithms which are based on\\nsigni\\xef\\xac\\x81cant domain expertise.\\n\\nHeads-up Limit Hold\\xe2\\x80\\x99em Poker was essentially solved (Bowling et al., 2015) with counterfactual\\nregret minimization (CFR), which is an iterative method to approximate a Nash equilibrium of an\\nextensive-form game with repeated self-play between two regret-minimizing algorithms.\\n\\nprogress\\n\\nhas\\n\\nsigni\\xef\\xac\\x81cant\\n\\nfor Heads-up No-Limit Hold\\xe2\\x80\\x99em\\nRecently,\\nPoker (Morav\\xcb\\x87c\\xc2\\xb4\\xc4\\xb1k et al., 2017),\\nthe DeepStack computer program defeated professional poker\\nplayers for the \\xef\\xac\\x81rst time. DeepStack utilized the recursive reasoning of CFR to handle information\\nasymmetry, focusing computation on speci\\xef\\xac\\x81c situations arising when making decisions and use\\nof value functions trained automatically, with little domain knowledge or human expert games,\\nwithout abstraction and of\\xef\\xac\\x82ine computation of complete strategies as before (Sandholm, 2015).\\n\\nbeen made\\n\\nImperfect information games, or game theory in general, have many applications, e.g., security and\\nmedical decision support (Sandholm, 2015). It is interesting to see more progress of deep RL in\\nsuch applications, and the full version of Texas Hold\\xe2\\x80\\x99em.\\n\\n12 ALPHAGO\\n\\nAlphaGo (Silver et al., 2016), a computer Go program, won the human European Go champion, 5\\ngames to 0, in October 2015, and became the \\xef\\xac\\x81rst computer Go program to won a human profes-\\nsional Go player without handicaps on a full-sized 19 \\xc3\\x97 19 board. Soon after that in March 2016,\\n\\n12\\n\\n\\x0cAlphaGo defeated Lee Sedol, an 18-time world champion Go player, 4 games to 1, making headline\\nnews worldwide. This set a landmark in AI. The challenge of solving Go comes from not only the\\ngigantic search space of about 250150, an astronomical number, but also the hardness of position\\nevaluation, which was successfully used in solving many other games, like backgammon and chess.\\n\\n12.1 TRAINING PIPELINE AND MCTS\\n\\nWe discuss brie\\xef\\xac\\x82y how AlphaGo works based on Silver et al. (2016) and Sutton and Barto (2017).\\nSee Chapter 16 in Sutton and Barto (2017) for a detailed and intuitive description of AlphaGo. See\\nDeepmind\\xe2\\x80\\x99s description of AlphaGo at goo.gl/lZoQ1d.\\n\\nAlphaGo was built with techniques of deep CNN, supervised learning, reinforcement learning, and\\nMonte Carlo tree search (MCTS) (Browne et al., 2012; Gelly et al., 2012). AlphaGo is composed\\nof two phases: neural network training pipeline and MCTS. The training pipeline phase includes\\ntraining a supervised learning (SL) policy network from expert moves, a fast rollout policy, an RL\\npolicy network, and an RL value network.\\n\\nThe SL policy network has convolutional layers, ReLU nonlinearities, and an output softmax layer\\nrepresenting probability distribution over legal moves. The inputs to the CNN are 19 \\xc3\\x97 19 \\xc3\\x97 48\\nimage stacks, where 19 is the dimension of a Go board and 48 is the number of features. State-\\naction pairs are sampled from expert moves to train the network with stochastic gradient ascent to\\nmaximize the likelihood of the move selected in a given state. The fast rollout policy uses a linear\\nsoftmax with small pattern features.\\n\\nThe RL policy network improves SL policy network, with the same network architecture, and the\\nweights of SL policy network as initial weight, and policy gradient for training. The reward function\\nis +1 for winning and -1 for losing in the terminal states, and 0 otherwise. Games are played between\\nthe current policy network and a random, previous iteration of the policy network, to stabilize the\\nlearning and to avoid over\\xef\\xac\\x81tting. Weights are updated by stochastic gradient ascent to maximize the\\nexpected outcome.\\n\\nThe RL value network still has the same network architecture as SL policy network, except the out-\\nput is a single scalar predicting the value of a position. The value network is learned in a Monte\\nCarlo policy evaluation approach. To tackle the over\\xef\\xac\\x81tting problem caused by strongly correlated\\nsuccessive positions in games, data are generated by self-play between the RL policy network and\\nitself until game termination. The weights are trained by regression on state-outcome pairs, us-\\ning stochastic gradient descent to minimize the mean squared error between the prediction and the\\ncorresponding outcome.\\n\\nIn MCTS phase, AlphaGo selects moves by lookahead search. It builds a partial game tree starting\\nfrom the current state, in the following stages: 1) select a promising node to explore further, 2)\\nexpand a leaf node guided by the SL policy network and collected statistics, 3) evaluate a leaf node\\nwith a mixture of the RL value network and the rollout policy, 4) backup evaluations to update the\\naction values. A move is then selected.\\n\\n12.2 DISCUSSIONS\\n\\nThe Deepmind team integrated several existing techniques together to engineered AlphaGo and it\\nhas achieved tremendous results. However, the RL policy network and RL value network are not\\nstrong/accurate enough, so that the RL value network, together with the SL policy network and the\\nrollout network, assist MCTS to search for the move. This might explain the one game loss against\\nLee Sedol. Moverover, AlphaGo still requires manually de\\xef\\xac\\x81ned features with human knowledge, so\\nit is not entirely an end-to-end solution yet; in contrast, DQN requires only raw pixels and scores\\nas inputs. Such a room for improvements would inspire intellectual inquisition for better computer\\nGo programs, potentially with deep RL only, without MCTS, like TD-Gammon (Sutton and Barto,\\n2017). This would be based on a novel RL algorithm, a novel deep neural network architecture, and\\npowerful computation. New RL algorithms are called for, for data ef\\xef\\xac\\x81ciency, and possibly for better\\nknowledge representation and reasoning. New deep neural network architectures are called for, for\\nthe sophistication to represent complex scenarios in Go and the elegance for learning in a reasonable\\ntime, so that an optimal policy and/or an optimal value function can be directly approximated to\\n\\n13\\n\\n\\x0cmake decisions without the help of MCTS to choose moves. Admittedly, such endeavour would be\\nillusive at large currently.\\n\\nBeing more practical, we expect more applications/extensions of techniques in Silver et al. (2016) in\\nsolving problems requiring titanic search spaces, like classical AI problems, e.g., planning, schedul-\\ning, and constraint satisfaction, etc.\\n\\n13 ROBOTICS\\n\\nAs we discuss in Section 5, Schulman et al. (2015) proposed Trust Region Policy Optimization\\n(TRPO), and experimented with simulated robotic tasks, and Levine et al. (2016a) proposed Guided\\nPolicy Search (GPS) to handle physical robots.\\n\\nMirowski et al. (2017) obtained the navigation ability by solving an RL problem maximizing cu-\\nmulative reward and jointly considering un/self-supervised tasks to improve data ef\\xef\\xac\\x81ciency and task\\nperformance. The authors addressed the sparse reward issues by augmenting the loss with two\\nauxiliary tasks, 1) unsupervised reconstruction of a low-dimensional depth map for representation\\nlearning to aid obstacle avoidance and short-term trajectory planning; 2) self-supervised loop clo-\\nsure classi\\xef\\xac\\x81cation task within a local trajectory. The authors incorporated a stacked LSTM to use\\nmemory at different time scales for dynamic elements in the environments. The proposed agent\\nlearn to navigate in complex 3D mazes end-to-end from raw sensory input, and performed similarly\\nto human level, even when start/goal locations change frequently.\\n\\nIn this approach, navigation is a by-product of the goal-directed RL optimization problem, in con-\\ntrast to conventional approaches such as Simultaneous Localisation and Mapping (SLAM), where\\nexplicit position inference and mapping are used for navigation. This may have the chance to replace\\nthe popular SLAM, which usually requires manual processing.\\n\\nSee more recent robotics papers, e.g., Chebotar et al. (2016); Finn and Levine (2016); Gu et al.\\n(2016a); Levine et al. (2016b); Yahya et al. (2016); Zhu et al. (2016). See Kober et al. (2013) for\\na survey of RL in robotics. See Science Robotics.\\n\\n14 SPOKEN DIALOGUE SYSTEMS\\n\\nIn spoken dialogue systems, conversational agent, or simply, chatbot, human and computer interacts\\nwith natural speech. There are usually two categories: chat-oriented and task-oriented systems; the\\nformer aims to converse with users in contextually reasonable way; the latter aims to assist users for\\nspeci\\xef\\xac\\x81c goals (Su et al., 2016b).\\n\\nLi et al. (2016c) proposed to use deep RL to generate dialogues to model future reward for better\\ninformativity, coherence, and ease of answering, to attempt to address the issues in the sequence\\nto sequence models based on Sutskever et al. (2014): the myopia and misalignment of maximizing\\nthe probability of generating a response given the previous dialogue turn, and the in\\xef\\xac\\x81nite loop of\\nrepetitive responses. The authors designed a reward function to re\\xef\\xac\\x82ect the above desirable properties,\\nand deployed policy gradient to optimize the long term reward. It would be interesting to investigate\\nthe reward model with the approach in Su et al. (2016b) below or with inverse RL and imitation\\nlearning as discussed in Section 6, although Su et al. (2016b) mentioned that such methods are\\ncostly, and humans may not act optimally.\\n\\nSu et al. (2016b) proposed an on-line learning framework to train the dialogue policy jointly with\\nthe reward model via active learning with a Gaussian process model, to tackle the issue that it is\\nunreliable and costly to use explicit user feedback as the reward signal. The authors showed em-\\npirically that the proposed framework reduced manual data annotations signi\\xef\\xac\\x81cantly and mitigated\\nnoisy user feedback in dialogue policy learning.\\n\\nLi et al. (2016d) designed a user simulator for movie booking, with both rules and collected data,\\nfor movie ticket booking and movie seeking. The source code is available at: goo.gl/jOv4AR.\\n\\nSome recent papers\\n(2016b), Dhingra et al.\\n\\nfollow:\\n(2016), Fatemi et al.\\n\\nAsri et al.\\n\\n(2016), Bordes and Weston (2016), Chen et al.\\n(2016),\\n\\n(2016a), Lipton et al.\\n\\n(2016), Li et al.\\n\\n14\\n\\n\\x0cMesnil et al. (2015), Mo et al. (2016), Shah et al. (2016), Su et al. (2016a), Wen et al. (2015a),\\nWilliams and Zweig (2016), Yang et al. (2016), Zhao and Eskenazi (2016).\\n\\nSee Li Deng\\xe2\\x80\\x99s recent talk at goo.gl/BqzeIZ. See conferences like SIGDIAL and INTERSPEECH.\\nSee NIPS 2016 Workshop on End-to-end Learning for Speech and Audio Processing, and NIPS\\n2015 Workshop on Machine Learning for Spoken Language Understanding and Interactions.\\n\\n15 MACHINE TRANSLATION\\n\\nHe et al. (2016a) proposed dual learning mechanism to tackle the data hunger issue in machine\\ntranslation, inspired by the observation that the information feedback between the primal, translation\\nfrom language A to language B, and the dual, translation from B to A, can help improve both\\ntranslation models, with a policy gradient method, using the language model likelihood as the reward\\nsignal. Experiments showed that, with only 10% bilingual data for warm start and monolingual\\ndata, the dual learning approach performed comparably with previous neural machine translation\\nmethods with full bilingual data in English to French tasks. The dual learning mechanism may\\nhave extensions to many tasks, if the task has a dual form, e.g., speech recognition and text to\\nspeech, image caption and image generation, question answering and question generation, search\\nand keyword extraction, etc.\\n\\nSee Sutskever et al. (2014); Bahdanau et al. (2015) for sequence to sequence neural machine trans-\\nlation. See Wu et al. (2016) for Google\\xe2\\x80\\x99s Neural Machine Translation System.\\n\\n16 TEXT SEQUENCE PREDICTION\\n\\nText generation models are usually based on n-gram, feed-forward neural networks, or recurrent\\nneural networks, trained to predict next word given the previous ground truth words as inputs; then\\nin testing, the trained models are used to generate a sequence word by word, using the generated\\nwords as inputs. The errors will accumulate on the way, causing the exposure bias issue. Moreover,\\nthese models are trained with word level losses, e.g., cross entropy, to maximize the probability of\\nnext word; however, the models are evaluated on a different metrics like BLEU.\\n\\nRanzato et al. (2016) proposed Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence\\nprediction, with incremental learning and a loss function combining both REINFORCE and cross-\\nentropy. MIXER is a sequence level training algorithm, aligning training and testing objective, such\\nas BLEU, rather than predicting the next word as in previous works.\\n\\nBahdanau et al. (2017) proposed an actor-critic algorithm for sequence prediction, attempting to\\nfurther improve Ranzato et al. (2016). The authors utilized a critic network to predict the value of a\\ntoken, i.e., the expected score following the sequence prediction policy, de\\xef\\xac\\x81ned by an actor network,\\ntrained by the predicted value of tokens. Some techniques are deployed to improve performance:\\nSARSA rather than Monter-Carlo method to lessen the variance in estimating value functions; target\\nnetwork for stability; sampling prediction from a delayed actor whose weights are updated more\\nslowly than the actor to be trained, to avoid the feedback loop when actor and critic need to be\\ntrained based on the output of each other; reward shaping to avoid the issue of sparse training signal.\\n\\nYu et al. (2017) proposed SeqGAN, sequence generative adversarial nets with policy gradient, in-\\ntegrating the adversarial scheme in Goodfellow et al. (2014). Li et al. (2017) proposed to improve\\nsequence generation by considering the knowledge about the future.\\n\\n17 NEURAL ARCHITECTURE DESIGN\\n\\nNeural networks architecture design is a notorious, nontrivial engineering issue. Neural architecture\\nsearch provides a promising avenue to explore.\\n\\nZoph and Le (2017) proposed the neural architecture search to generate neural networks architec-\\ntures with an RNN trained by RL, in particular, REINFORCE, searching from scratch in variable-\\nlength architecture space, to maximize the expected accuracy of the generated architectures on a\\nvalidation set. In the RL formulation, a controller generates hyperparameters as a sequence of to-\\nkens, which are actions chosen from hyperparameters spaces; each gradient update to the policy\\n\\n15\\n\\n\\x0cparameters corresponds to training one generated network to convergence; an accuracy on a valida-\\ntion set is the reward signal. The neural architecture search can generate convolutional layers, with\\nskip connections or branching layers, and recurrent cell architecture. The authors designed a param-\\neter server approach to speed up training. Comparing with state of the art methods, the proposed\\napproach achieved competitive results for an image classi\\xef\\xac\\x81cation task with CIFAR-10 dataset; and\\nbetter results for a language modeling task with Penn Treebank. See also Baker et al. (2017).\\n\\n18 PERSONALIZED WEB SERVICES\\n\\nLi et al. (2010) formulated personalized news articles recommendation as a contextual bandit prob-\\nlem, to learn an algorithm to select articles sequentially for users based on contextual information\\nof the user and articles, such as historical activities of the user and descriptive information and cate-\\ngories of content, and to take user-click feedback to adapt article selection policy to maximize total\\nuser clicks in the long run.\\n\\nTheocharous et al. (2015) formulated a personalized Ad recommendation systems as an RL problem\\nto maximize life-time value (LTV) with theoretical guarantees. This is in contrast to a myopic\\nsolution with supervised learning or contextual bandit formulation, usually with the performance\\nmetric of click through rate (CTR). As the models are hard to learn, the authors deployed a model-\\nfree approach to computes a lower-bound on the expected return of a policy to address the off-policy\\nevaluation problem, i.e., how to evaluate a RL policy without deployment.\\n\\nLi et al. (2015) also attempted to maximize lifetime value of customers. Silver et al. (2013) pro-\\nposed concurrent reinforcement learning for the customer interaction problem. See Chapter 16 in\\nSutton and Barto (2017) for a detailed and intuitive description of personalized web services.\\n\\n19 HEALTHCARE\\n\\nThere are many opportunities and challenges in healthcare for machine learning (Saria, 2014). Per-\\nsonalized medicine is getting popular in healthcare. It systematically optimizes the patient\\xe2\\x80\\x99s health\\ncare, in particular, for chronic conditions and cancers using individual patient information, poten-\\ntially from electronic health/medical record (EHR/EMR). Here dynamic treatment regimes (DTRs)\\nor adaptive treatment strategies are sequential decision making problems. Some issues in DTRs are\\nnot in standard RL. Shortreed et al. (2011) tackled the missing data problem, and designed meth-\\nods to quantify the evidence of the learned optimal policy. Goldberg and Kosorok (2012) proposed\\nmethods for censored data (patients may drop out during the trial) and \\xef\\xac\\x82exible number of stages.\\nSee Chakraborty and Murphy (2014) for a recent survey, and Kosorok and Moodie (2015) for an\\nedited book about recent progress in DTRs. Currently Q-learning is the RL method in DTRs. It is\\ninteresting to see the applications of deep RL methods in this \\xef\\xac\\x81eld.\\n\\nSome recent workshops at the intersection of machine learning and healthcare are: NIPS 2016\\nWorkshop on Machine Learning for Health (http://www.nipsml4hc.ws) and NIPS 2015 Workshop\\non Machine Learning in Healthcare (https://sites.google.com/site/nipsmlhc15/).\\n\\n20 FINANCE\\n\\nRL is a natural solution to some \\xef\\xac\\x81nance and economics problems (Hull, 2014; Luenberger, 1997),\\nlike option pricing (Longstaff and Schwartz, 2001; Tsitsiklis and Van Roy, 2001; Li et al., 2009),\\nand multi-period portfolio optimization (Brandt et al., 2005), where value function based RL meth-\\nods were used. Moody and Saffell (2001) proposed to utilize policy gradient to learn to trade;\\nDeng et al. (2016) extended it with deep neural networks. Deep (reinforcement) learning would pro-\\nvide better solutions in some issues in risk management (Hull, 2014; Yu et al., 2009). The market\\nef\\xef\\xac\\x81ciency hypothesis is fundamental in \\xef\\xac\\x81nance. However, there are well-known behavioral biases in\\nhuman decision-making under uncertainty. A reconciliation is the adaptive markets hypothesis (Lo,\\n2004), which may be approached by reinforcement learning.\\n\\nIt is nontrivial for \\xef\\xac\\x81nance and economics academia to accept blackbox methods like neural networks;\\nHeaton et al. (2016) may be regarded as an exception. However, there is a lecture in AFA 2017\\n\\n16\\n\\n\\x0cannual meeting: Machine Learning and Prediction in Economics and Finance (goo.gl/7xdePd). A\\n(obvious) factor is \\xef\\xac\\x81nancial \\xef\\xac\\x81rms would probably hold state-of-the-art research/application results.\\n\\n21 MUSIC GENERATION\\n\\nJaques et al. (2017) proposed to combine maximum likelihood estimation with RL training, using\\nRL to impose structure on an RNN trained on data by choosing reward functions, to attempt to ensure\\ncoherent global structure in multi-step generated sequences. A Note-RNN was trained to predict the\\nnext note in a musical sequence with a large corpus of songs. Then the Note-RNN was re\\xef\\xac\\x81ned using\\nRL to obtain RL Tuner, with a reward function considering both rules of music theory and output of\\nanother trained Note-RNN. RL Tuner produced more pleasant-sounding and subjectively pleasing\\nmelodies than alternative methods. The proposed approach has the potential for training sequence\\nmodels other than music, by allowing for encoding high-level domain knowledge into the RNN.\\n\\n22 TO-DO LIST\\n\\nWe list interesting and/or important directions/papers we have not discussed in this overview as\\nbelow, hoping it would provide pointers for those who may be interested in studying them further.3\\nThis would be part of our future work.4\\n\\n\\xe2\\x80\\xa2 understanding deep learning, Daniely et al. (2016); Li et al. (2016b); Zhang et al. (2017)\\n\\xe2\\x80\\xa2 exploration, e.g., Stadie et al. (2015); Bellemare et al. (2016); Kulkarni et al. (2016);\\n\\nOsband et al. (2016); Nachum et al. (2017)\\n\\n\\xe2\\x80\\xa2 model-based learning, e.g., Oh et al. (2015); Gu et al. (2016b)\\n\\xe2\\x80\\xa2 retrace algorithm, Munos et al. (2016)\\n\\xe2\\x80\\xa2 predictron, Silver et al. (2017)\\n\\xe2\\x80\\xa2 hierarchical RL, e.g., Kulkarni et al. (2016); Vezhnevets et al. (2016); Tessler et al. (2017);\\n\\nFlorensa et al. (2017)\\n\\n\\xe2\\x80\\xa2 transfer/multitask RL, e.g., Maurer et al. (2016); Mo et al. (2016); Parisotto et al. (2016),\\n\\nNIPS 2015 Transfer and Multi-Task Learning: Trends and New Perspectives Workshop\\n\\n\\xe2\\x80\\xa2 zero/one-shot learning, e.g., Vinyals et al. (2016); Lake et al. (2015); Johnson et al. (2016)\\n\\xe2\\x80\\xa2 semi-supervised RL, e.g., Finn et al. (2017)\\n\\xe2\\x80\\xa2 deep symbolic RL, Garnelo et al. (2016)\\n\\xe2\\x80\\xa2 intrinsic motivation, e.g., Stadie et al. (2015); Kulkarni et al. (2016); Oudeyer et al. (2016)\\n\\xe2\\x80\\xa2 hyperparameter learning, e.g. Andrychowicz et al. (2016)\\n\\xe2\\x80\\xa2 information extraction, e.g., Narasimhan et al. (2016)\\n\\xe2\\x80\\xa2 text games, e.g., He et al. (2016b); Narasimhan et al. (2015)\\n\\xe2\\x80\\xa2 language tree-structure learning, e.g., Yogatama et al. (2017)\\n\\xe2\\x80\\xa2 question answering, e.g., Shen et al. (2016); Trischler et al. (2016)\\n\\xe2\\x80\\xa2 large action space, e.g., Dulac-Arnold et al. (2016); He et al. (2016c)\\n\\xe2\\x80\\xa2 adaptive normalization, van Hasselt et al. (2016b)\\n\\xe2\\x80\\xa2 self-driving vehicle, e.g., Bojarski et al. (2016), NIPS 2016 Workshop on Machine Learn-\\n\\ning for Intelligent Transportation Systems\\n\\n\\xe2\\x80\\xa2 smart grid, e.g., Wen et al. (2015b)\\n\\xe2\\x80\\xa2 physics experiments, e.g., Denil et al. (2016)\\n\\n3Some topics/papers may not contain RL yet. However, we believe these are interesting and/or important\\n\\ndirections for RL in the sense of either theory or application.\\n\\n4It would be de\\xef\\xac\\x81nitely more desirable if we could \\xef\\xac\\x81nish reviewing these before publishing this overview.\\n\\nOne factor is we set the deadline for the \\xef\\xac\\x81rst version before January 28, 2017, the Chinese Spring Festival.\\n\\n17\\n\\n\\x0c\\xe2\\x80\\xa2 deep probabilistic programming, Tran et al. (2017)\\n\\xe2\\x80\\xa2 deep learning games, Schuurmans and Zinkevich (2016)\\n\\xe2\\x80\\xa2 program learning, e.g., Reed and de Freitas (2016)\\n\\xe2\\x80\\xa2 quantum RL, e.g., Crawford et al. (2016), NIPS 2015 Workshop on Quantum Machine\\n\\nLearning\\n\\n23 RESOURCES\\n\\nWe list some resources for Deep RL, which by no means are complete.\\n\\n23.1 BOOKS\\n\\n\\xe2\\x80\\xa2 The de\\xef\\xac\\x81nite and intuitive reinforcement learning book by Richard S. Sutton and Andrew\\n\\nG. Barto (Sutton and Barto, 2017)\\n\\n\\xe2\\x80\\xa2 Concise\\n\\nand\\n\\ntheoretical, Algorithms\\n\\nfor Reinforcement Learning by Csaba\\n\\nSzepesv\\xc2\\xb4ari (Szepesv\\xc2\\xb4ari, 2010)\\n\\n\\xe2\\x80\\xa2 A theoretical book about approximate dynamic programming by Dimitri P. Bert-\\n\\nsekas (Bertsekas, 2012)\\n\\n\\xe2\\x80\\xa2 An operations research oriented book, Approximate Dynamic Programming, by Warren B.\\n\\nPowell (Powell, 2011)\\n\\n\\xe2\\x80\\xa2 Deep\\n\\nlearning\\n\\nCourville (Goodfellow et al., 2016)\\n\\nbook\\n\\nby\\n\\nIan Goodfellow, Yoshua Bengio,\\n\\nand Aaron\\n\\n23.2 COURSES\\n\\n\\xe2\\x80\\xa2 David Silver, Reinforcement Learning, 2015, slides (goo.gl/UqaxlO), video-lectures\\n\\n(goo.gl/7BVRkT)\\n\\n\\xe2\\x80\\xa2 Sergey Levine, John Schulman and Chelsea Finn, CS 294: Deep Reinforcement Learning,\\n\\nSpring 2017, http://rll.berkeley.edu/deeprlcourse/\\n\\n\\xe2\\x80\\xa2 Charles Isbell, Michael Littman and Pushkar Kolhe, Udacity: Machine Learning: Rein-\\n\\nforcement Learning, goo.gl/eyvLfg\\n\\n\\xe2\\x80\\xa2 Fei-Fei Li, Andrej Karpathy and Justin Johnson, CS231n: Convolutional Neural Networks\\n\\nfor Visual Recognition, http://cs231n.stanford.edu\\n\\n\\xe2\\x80\\xa2 Richard Socher, CS224d:\\n\\nhttp://cs224d.stanford.edu\\n\\nDeep Learning for Natural Language Processing,\\n\\n\\xe2\\x80\\xa2 Nando de Freitas, Deep Learning Lectures, https://www.youtube.com/user/ProfNandoDF\\n\\n23.3 TUTORIALS\\n\\n\\xe2\\x80\\xa2 David Silver, Deep Reinforcement Learning, ICML 2016\\n\\xe2\\x80\\xa2 Pieter Abbeel and John Schulman, Deep Reinforcement Learning Through Policy Opti-\\n\\nmization, NIPS 2016\\n\\n\\xe2\\x80\\xa2 Andrew Ng, Nuts and Bolts of Building Applications using Deep Learning, NIPS 2016\\n\\xe2\\x80\\xa2 John Schulman, The Nuts and Bolts of Deep Reinforcement Learning Research, Deep Re-\\n\\ninforcement Learning Workshop, NIPS 2016\\n\\n\\xe2\\x80\\xa2 John Schulman, Deep Reinforcement Learning, Deep Learning School, 2016\\n\\xe2\\x80\\xa2 Pieter Abbeel, Deep Reinforcement Learning, Deep Learning Summer School, 2016;\\n\\nhttp://videolectures.net/deeplearning2016 abbeel deep reinforcement/\\n\\n\\xe2\\x80\\xa2 David Silver, Deep Reinforcement Learning, 2nd Multidisciplinary Conference\\non Reinforcement Learning and Decision Making (RLDM), Edmonton 2015;\\nhttp://videolectures.net/rldm2015 silver reinforcement learning/\\n\\n18\\n\\n\\x0c\\xe2\\x80\\xa2 Rich Sutton, Introduction to Reinforcement Learning with Function Approximation,\\n\\nhttps://www.microsoft.com/en-us/research/video/tutorial-introduction-to-reinforcement-\\nlearning-with-function-approximation/\\n\\n\\xe2\\x80\\xa2 Joelle Pineau, Introduction to Reinforcement Learning, Deep Learning Summer School,\\n\\n2016; http://videolectures.net/deeplearning2016 pineau reinforcement learning/\\n\\n\\xe2\\x80\\xa2 Deep Learning Summer School, 2016, 2015\\n\\n23.4 CONFERENCES, JOURNALS AND WORKSHOPS\\n\\n\\xe2\\x80\\xa2 NIPS: Neural Information Processing Systems\\n\\xe2\\x80\\xa2 ICML: International Conference on Machine Learning\\n\\xe2\\x80\\xa2 ICLR: International Conference on Learning Representation\\n\\xe2\\x80\\xa2 RLDM: Multidisciplinary Conference on Reinforcement Learning and Decision Making\\n\\xe2\\x80\\xa2 AAAI, IJCAI, ACL, EMNLP, SIGDIAL, ICRA, IROS, KDD, SIGIR, CVPR, etc.\\n\\xe2\\x80\\xa2 Science Robotics, JMLR, MLJ, AIJ, JAIR, PAMI, etc\\n\\xe2\\x80\\xa2 Nature May 2015, Science July 2015, survey papers on machine learning/AI\\n\\xe2\\x80\\xa2 Deep Reinforcement Learning Workshop, NIPS 2016, 2015; IJCAI 2016\\n\\xe2\\x80\\xa2 Deep Learning Workshop, ICML 2016\\n\\n23.5 BLOGS\\n\\n\\xe2\\x80\\xa2 Andrej Karpathy, karpathy.github.io, esp. goo.gl/1hkKrb\\n\\xe2\\x80\\xa2 Denny Britz, www.wildml.com, esp. goo.gl/MyrwDC\\n\\xe2\\x80\\xa2 Junling Hu, Reinforcement learning explained - learning to act based on long-term payoffs\\n\\xe2\\x80\\xa2 Li Deng, How deep reinforcement learning can help chatbots\\n\\xe2\\x80\\xa2 Christopher Olah, colah.github.io\\n\\nIn the current information/social media age, we are overwhelmed by information, e.g., from Twitter,\\nGoogle+, WeChat, arXiv, etc. The skill to ef\\xef\\xac\\x81ciently select the best information becomes essential.\\n\\n24 DISCUSSIONS\\n\\nIt is both the best and the worst of times for the \\xef\\xac\\x81eld of deep RL, for the same reason: it has been\\ngrowing so fast and so enormously. We have been witnessing breakthroughs, exciting new methods\\nand applications, and we expect to see much more and much faster. As a consequence, this overview\\nis incomplete, in the sense of both depth and width. However, we attempt to summarize important\\nachievements and discuss potential directions and applications in this amazing \\xef\\xac\\x81eld.\\n\\nWe have been witnessing breakthroughs, three papers about or using Deep RL published in Nature\\nin less than two years: deep Q-network (Mnih et al., 2015), AlphaGo (Silver et al., 2016) and differ-\\nentiable neural computer (Graves et al., 2016); We have already seen many extensions to, improve-\\nments for and applications of deep Q-network (Mnih et al., 2015). The mechanisms of attention and\\nmemory (Graves et al., 2016) has been attracting much attention.\\n\\nNovel architectures and applications using deep RL were recognized in top tier conferences as\\nbest (student) papers in 2016: dueling network architectures (Wang et al., 2016a) at ICML, spo-\\nken dialogue systems (Su et al., 2016b) at ACL (student), information extraction (Narasimhan et al.,\\n2016) at EMNLP, and value iteration networks (Tamar et al., 2016) at NIPS. Exciting achieve-\\nments abound: asynchronous methods (Mnih et al., 2016), dual learning for machine transla-\\ntion (He et al., 2016a), guided policy search (Levine et al., 2016a), generative adversarial imitation\\nlearning (Ho and Ermon, 2016), unsupervised reinforcement and auxiliary learning (Jaderberg et al.,\\n2017), and neural architecture design (Zoph and Le, 2017), etc.\\n\\nValue function is central to reinforcement learning, e.g., in deep Q-network and its many exten-\\ntions. Policy optimization approaches have been gaining traction, in many, diverse applications,\\n\\n19\\n\\n\\x0ce.g., robotics, neural architecture design, spoken dialogue systems, machine translation, attention,\\nand learning to learn, and this list is boundless. New learning mechanisms have emerged, e.g.,\\nusing unsupervised/semi-supervised/tranfer learning to improve the quality and speed of learn-\\ning, and more new mechanisms will be emerging. This is the renaissance of reinforcement learn-\\ning (Krakovsky, 2016). In fact, reinforcement learning and deep learning have been making steady\\nprogress even in the AI winter.\\n\\nIt is essential to consider issues of learning models, like stability, convergence, accuracy, data ef\\xef\\xac\\x81-\\nciency, scalability, speed, simplicity, interpretability, robustness, and safety, etc. It is important to\\ninvestigate comments/criticisms, e.g., from conginitive science, like intuitive physics, intuitive psy-\\nchology, causal model, compositionality, learning to learn, and act in real time (Lake et al., 2016),\\nfor stronger AI. See also Peter Norvig\\xe2\\x80\\x99s perspective at goo.gl/obvmVB.\\n\\nDeep learning, in this third wave of AI, will have deeper in\\xef\\xac\\x82uences, as we have already seen many\\nachievements. Reinforcement learning, as a more general learning and decision making paradigm,\\nwill deeply in\\xef\\xac\\x82uence deep learning, machine learning, and arti\\xef\\xac\\x81cial intelligence in general.5 It is\\ninteresting to mention that when Professor Rich Sutton started working in the University of Alberta\\nin 2003, he named his lab RLAI: Reinforcement Learning and Arti\\xef\\xac\\x81cial Intelligence.\\n\\nACKOWLEDGEMENT\\n\\nI appreciate comments from Baochun Bai, Junling Hu, Ruitong Huang, Lihong Li, Dale Schuur-\\nmans, David Silver, Rich Sutton, Csaba Szepesv\\xc2\\xb4ari, Yi Wan and Qing Yu. Any remaining issues and\\nerrors are my own. This document also bene\\xef\\xac\\x81ts from discussions during various seminars/webinars,\\nin particular, an AlphaGo seminar at MIT in April 2016, deep (reinforcement) learning seminars\\nat the University of Toronto, McGill University and the University of Alberta in October 2016 as\\npart of the North America tour of Synced (Jiqizhixin), and webinars using David Silver\\xe2\\x80\\x99s slides in\\nNovember and December 2016, and discussions in several WeChat groups.\\n\\nREFERENCES\\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man\\xc2\\xb4e, D. (2016). Concrete\\n\\nProblems in AI Safety. ArXiv e-prints.\\n\\nAndrychowicz, M., Denil, M., Colmenarejo, S. G., Hoffman, M. W., Pfau, D., Schaul, T., Shilling-\\nford, B., and de Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In\\nthe Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nAsri, L. E., He, J., and Suleman, K. (2016). A sequence-to-sequence model for user simulation\\nIn Annual Meeting of the International Speech Communication\\n\\nin spoken dialogue systems.\\nAssociation (INTERSPEECH).\\n\\nBa, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu, C. (2016). Using fast weights to attend to\\n\\nthe recent past. In the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nBa, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. In\\n\\nthe International Conference on Learning Representations (ICLR).\\n\\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. (2017). Reinforcement learning\\nthrough asynchronous advantage actor-critic on a gpu. Submitted to Int\\xe2\\x80\\x99l Conference on Learning\\nRepresentations.\\n\\nBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y.\\n(2017). An actor-critic algorithm for sequence prediction. Submitted to Int\\xe2\\x80\\x99l Conference on\\nLearning Representations.\\n\\n5It is worthwhile to envision deep RL considering perspectives of government, academia and industry on AI,\\ne.g., Arti\\xef\\xac\\x81cial Intelligence, Automation, and the economy, Executive Of\\xef\\xac\\x81ce of the President, USA; Arti\\xef\\xac\\x81cial\\nIntelligence and Life in 2030 - One Hundred Year Study on Arti\\xef\\xac\\x81cial Intelligence: Report of the 2015-2016\\nStudy Panel, Stanford University; and AI, Machine Learning and Data Fuel the Future of Productivity by The\\nGoldman Sachs Group, Inc., etc. See also the recent AI Frontiers Conference, https://www.aifrontiers.com.\\n\\n20\\n\\n\\x0cBahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to\\n\\nalign and translate. In the International Conference on Learning Representations (ICLR).\\n\\nBaker, B., Gupta, O., Naik, N., and Raskar, R. (2017). Designing neural network architectures using\\n\\nreinforcement learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., K\\xc2\\xa8uttler, H., Lefrancq, A., Green,\\nS., Vald\\xc2\\xb4es, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton,\\nA., Gaffney, S., King, H., Hassabis, D., Legg, S., and Petersen, S. (2016). DeepMind Lab. ArXiv\\ne-prints.\\n\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environ-\\nment: An evaluation platform for general agents. Journal of Arti\\xef\\xac\\x81cial Intelligence Research,\\n47:253?279.\\n\\nBellemare, M. G., Schaul, T., Srinivasan, S., Saxton, D., Ostrovski, G., and Munos, R. (2016).\\nUnifying count-based exploration and intrinsic motivation. In the Annual Conference on Neural\\nInformation Processing Systems (NIPS).\\n\\nBengio, Y. (2009). Learning deep architectures for ai. Foundations and trends R(cid:13)in Machine Learn-\\n\\ning, 2(1):1\\xe2\\x80\\x93127.\\n\\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In the Inter-\\n\\nnational Conference on Machine Learning (ICML).\\n\\nBernhard Wymann, E. E., Guionneau, C., Dimitrakakis, C., and R\\xc2\\xb4emi Coulom, A. S. (2014).\\n\\nTORCS, The Open Racing Car Simulator. \\xe2\\x80\\x9dhttp://www.torcs.org\\xe2\\x80\\x9d.\\n\\nBertsekas, D. P. (2012). Dynamic programming and optimal control (Vol. II, 4th Edition: Approxi-\\n\\nmate Dynamic Programming). Athena Scienti\\xef\\xac\\x81c, Massachusetts, USA.\\n\\nBertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scienti\\xef\\xac\\x81c.\\n\\nBishop, C. (2011). Pattern Recognition and Machine Learning. Springer.\\n\\nBojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort,\\nM., Muller, U., Zhang, J., Zhang, X., Zhao, J., and Zieba, K. (2016). End to End Learning for\\nSelf-Driving Cars. ArXiv e-prints.\\n\\nBordes, A. and Weston, J. (2016). Learning End-to-End Goal-Oriented Dialog. ArXiv e-prints.\\n\\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. (2015). Heads-up limit hold\\xe2\\x80\\x99em poker is\\n\\nsolved. Science, 347(6218):145\\xe2\\x80\\x93149.\\n\\nBrandt, M. W., Goyal, A., Santa-Clara, P., and Stroud, J. R. (2005). A simulation approach to\\ndynamic portfolio choice with an application to learning about return predictability. The Review\\nof Financial Studies, 18(3):831\\xe2\\x80\\x93873.\\n\\nBrowne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S.,\\nPerez, D., Samothrakis, S., and Colton, S. (2012). A survey of Monte Carlo tree search methods.\\nIEEE Transactions on Computational Intelligence and AI in Games, 4(1):1\\xe2\\x80\\x9343.\\n\\nBusoniu, L., Babuska, R., and Schutter, B. D. (2008). A comprehensive survey of multiagent rein-\\nforcement learning. IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications\\nand Reviews, 38(2).\\n\\nChakraborty, B. and Murphy, S. A. (2014). Dynamic treatment regimes. Annual Review of Statistics\\n\\nand Its Application, 1:447\\xe2\\x80\\x93464.\\n\\nChebotar, Y., Kalakrishnan, M., Yahya, A., Li, A., Schaal, S., and Levine, S. (2016). Path integral\\n\\nguided policy search. ArXiv e-prints.\\n\\nChen, Y.-N., Hakkani-Tur, D., Tur, G., Celikyilmaz, A., Gao, J., and Deng, L. (2016a). Knowledge\\n\\nas a Teacher: Knowledge-Guided Structural Attention Networks. ArXiv e-prints.\\n\\n21\\n\\n\\x0cChen, Y.-N. V., Hakkani-T\\xc2\\xa8ur, D., Tur, G., Gao, J., and Deng, L. (2016b). End-to-end memory\\nnetworks with knowledge carryover for multi-turn spoken language understanding. In Annual\\nMeeting of the International Speech Communication Association (INTERSPEECH).\\n\\nCrawford, D., Levit, A., Ghadermarzy, N., Oberoi, J. S., and Ronagh, P. (2016). Reinforcement\\n\\nLearning Using Quantum Boltzmann Machines. ArXiv e-prints.\\n\\nDaniely, A., Frostig, R., and Singer, Y. (2016). Toward deeper understanding of neural networks:\\nThe power of initialization and a dual view on expressivity. In the Annual Conference on Neural\\nInformation Processing Systems (NIPS).\\n\\nDanihelka, I., Wayne, G., Uria, B., Kalchbrenner, N., and Graves, A. (2016). Associative long\\n\\nshort-term memory. In the International Conference on Machine Learning (ICML).\\n\\nDeng, L. and Dong, Y. (2014). Deep Learning: Methods and Applications. Now Publishers Inc.\\n\\nDeng, L. and Liu, Y. (2017). Deep Learning in Natural Language Processing (edited book, sched-\\n\\nuled August 2017). Springer.\\n\\nDeng, Y., Bao, F., Kong, Y., Ren, Z., and Dai, Q. (2016). Deep direct reinforcement learning for\\n\\xef\\xac\\x81nancial signal representation and trading. IEEE Transactions on Neural Networks and Learning\\nSystems.\\n\\nDenil, M., Agrawal, P., Kulkarni, T. D., Erez, T., Battaglia, P., and de Freitas, N. (2016). Learning to\\nperform physics experiments via deep reinforcement learning. In NIPS 2016 Deep Reinforcement\\nLearning Workshop.\\n\\nDhingra, B., Li, L., Li, X., Gao, J., Chen, Y.-N., Ahmed, F., and Deng, L. (2016). End-to-End\\n\\nReinforcement Learning of Dialogue Agents for Information Access. ArXiv e-prints.\\n\\nDomingos, P. (2012). A few useful things to know about machine learning. Communications of the\\n\\nACM, 55(10):78\\xe2\\x80\\x9387.\\n\\nDosovitskiy, A. and Koltun, V. (2017). Learning to act by predicting the future. Submitted to Int\\xe2\\x80\\x99l\\n\\nConference on Learning Representations.\\n\\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep rein-\\nforcement learning for continuous control. In the International Conference on Machine Learning\\n(ICML).\\n\\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2017). RL2: Fast re-\\ninforcement learning via slow reinforcement learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning\\nRepresentations.\\n\\nDulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt, J., Mann, T., Weber,\\nT., Degris, T., and Coppin, B. (2016). Deep reinforcement learning in large discrete action spaces.\\nIn the International Conference on Machine Learning (ICML).\\n\\nEslami, S. M. A., Heess, N., Weber, T., Tassa, Y., Szepesv\\xc2\\xb4ari, D., Kavukcuoglu, K., and Hinton,\\nIn the\\n\\nG. E. (2016). Attend, infer, repeat: Fast scene understanding with generative models.\\nAnnual Conference on Neural Information Processing Systems (NIPS).\\n\\nFatemi, M., Asri, L. E., Schulz, H., He, J., and Suleman, K. (2016). Policy networks with two-\\nstage training for dialogue systems. In the Annual SIGdial Meeting on Discourse and Dialogue\\n(SIGDIAL).\\n\\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. (2016). A connection between GANs, inverse\\nreinforcement learning, and energy-based models. In NIPS 2016 Workshop on Adversarial Train-\\ning.\\n\\nFinn, C. and Levine, S. (2016). Deep visual foresight for planning robot motion. ArXiv e-prints.\\n\\nFinn, C., Yu, T., Fu, J., Abbeel, P., and Levine, S. (2017). Generalizing skills with semi-supervised\\n\\nreinforcement learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\n22\\n\\n\\x0cFlorensa, C., Duan, Y., and Abbeel, P. (2017). Stochastic neural networks for hierarchical reinforce-\\n\\nment learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nGarc`\\xc4\\xb1a, J. and Fern`andez, F. (2015). A comprehensive survey on safe reinforcement learning. The\\n\\nJournal of Machine Learning Research, 16:1437\\xe2\\x80\\x931480.\\n\\nGarnelo, M., Arulkumaran, K., and Shanahan, M. (2016). Towards Deep Symbolic Reinforcement\\n\\nLearning. ArXiv e-prints.\\n\\nGelly, S., Schoenauer, M., Sebag, M., Teytaud, O., Kocsis, L., Silver, D., and Szepesv\\xc2\\xb4ari, C. (2012).\\nThe grand challenge of computer go: Monte carlo tree search and extensions. Communications\\nof the ACM, 55(3):106\\xe2\\x80\\x93113.\\n\\nGhavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A. (2015). Bayesian reinforcement learning:\\n\\na survey. Foundations and Trends in Machine Learning, 8(5-6):359\\xe2\\x80\\x93483.\\n\\nGoldberg, Y. and Kosorok, M. R. (2012). Q-learning with censored data. Annals of Statistics,\\n\\n40(1):529\\xe2\\x80\\x93560.\\n\\nGoodfellow, I. (2017). NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv e-prints.\\n\\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.\\n\\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., ,\\nand Bengio, Y. (2014). Generative adversarial nets. In the Annual Conference on Neural Infor-\\nmation Processing Systems (NIPS), page 2672?2680.\\n\\nGraves, A., Wayne, G., and Danihelka, I. (2014). Neural Turing Machines. ArXiv e-prints.\\n\\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi\\xc2\\xb4nska, A., Col-\\nmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., nech Badia, A. P., Hermann, K. M.,\\nZwols, Y., Ostrovski, G., Cain, A., King, H., Summer\\xef\\xac\\x81eld, C., Blunsom, P., Kavukcuoglu, K., and\\nHassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory.\\nNature, 538:471\\xe2\\x80\\x93476.\\n\\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015). Draw: A recurrent\\nIn the International Conference on Machine Learning\\n\\nneural network for image generation.\\n(ICML).\\n\\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2016a). Deep reinforcement learning for robotic\\n\\nmanipulation with asynchronous off-policy updates. ArXiv e-prints.\\n\\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous deep q-learning with model-\\n\\nbased acceleration. In the International Conference on Machine Learning (ICML).\\n\\nHastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data\\n\\nMining, Inference, and Prediction. Springer.\\n\\nHe, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T.-Y., and Ma, W.-Y. (2016a). Dual learning\\nfor machine translation. In the Annual Conference on Neural Information Processing Systems\\n(NIPS).\\n\\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2017). Learning to play in a day: Faster deep\\nreinforcement learning by optimality tightening. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Rep-\\nresentations.\\n\\nHe, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2016b). Deep reinforcement\\nlearning with a natural language action space. In the Association for Computational Linguistics\\nannual meeting (ACL).\\n\\nHe, J., Ostendorf, M., He, X., Chen, J., Gao, J., Li, L., and Deng, L. (2016c). Deep reinforcement\\nlearning with a combinatorial action space for predicting popular reddit threads. In Conference\\non Empirical Methods in Natural Language Processing (EMNLP).\\n\\n23\\n\\n\\x0cHe, X. and Deng, L. (2013). Speech-centric information processing: An optimization-oriented\\n\\napproach. Proceedings of the IEEE \\xe2\\x80\\x94 Vol. 101, No. 5, May 2013, 101(5):1116\\xe2\\x80\\x931135.\\n\\nHeaton, J. B., Polson, N. G., and Witte, J. H. (2016). Deep learning for \\xef\\xac\\x81nance: deep portfolios.\\n\\nApplied Stochastic Models in Business and Industry.\\n\\nHeinrich, J. and Silver, D. (2016). Deep reinforcement learning from self-play in imperfect-\\n\\ninformation games. In NIPS 2016 Deep Reinforcement Learning Workshop.\\n\\nHinton, G., Deng, L., Yu, D., Dahl, G. E., rahman Mohamed, A., Jaitly, N., Senior, A., Vanhoucke,\\nV., Nguyen, P., Sainath, T. N., , and Kingsbury, B. (2012). Deep neural networks for acoustic\\nmodeling in speech recognition. IEEE Signal Processing Magazine, 82.\\n\\nHirschberg, J. and Manning, C. D. (2015). Advances in natural language processing. Science,\\n\\n349(6245):261\\xe2\\x80\\x93266.\\n\\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. In the Annual Conference\\n\\non Neural Information Processing Systems (NIPS).\\n\\nHull, J. C. (2014). Options, Futures and Other Derivatives (9th edition). Prentice Hall.\\n\\nJaderberg, M., Mnih, V., Czarnecki, W., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K.\\n(2017). Reinforcement learning with unsupervised auxiliary tasks. Submitted to Int\\xe2\\x80\\x99l Conference\\non Learning Representations.\\n\\nJaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K. (2015). Spatial transformer\\n\\nnetworks. In the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An Introduction to Statistical Learning\\n\\nwith Applications in R. Springer.\\n\\nJaques, N., Gu, S., Turner, R. E., and Eck, D. (2017). Tuning recurrent neural networks with\\n\\nreinforcement learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nJohnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Vi\\xc2\\xb4egas, F., Watten-\\nberg, M., Corrado, G., Hughes, M., and Dean, J. (2016). Google\\xe2\\x80\\x99s Multilingual Neural Machine\\nTranslation System: Enabling Zero-Shot Translation. ArXiv e-prints.\\n\\nJordan, M. I. and Mitchell, T. (2015). Machine learning: Trends, perspectives, and prospects. Sci-\\n\\nence, 349(6245):255\\xe2\\x80\\x93260.\\n\\nKadlec, R., Schmid, M., Bajgar, O., and Kleindienst, J. (2016). Text Understanding with the Atten-\\n\\ntion Sum Reader Network. ArXiv e-prints.\\n\\nKaelbling, L. P., Littman, M. L., and Moore, A. (1996). Reinforcement learning: A survey. Journal\\n\\nof Arti\\xef\\xac\\x81cial Intelligence Research, 4:237\\xe2\\x80\\x93285.\\n\\nKaiser, L. and Bengio, S. (2016). Can active memory replace attention? In the Annual Conference\\n\\non Neural Information Processing Systems (NIPS).\\n\\nKempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jas\\xc2\\xb4kowski, W. (2016). ViZDoom: A Doom-\\nbased AI research platform for visual reinforcement learning. In IEEE Conference on Computa-\\ntional Intelligence and Games.\\n\\nKober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey.\\n\\nInternational Journal of Robotics Research, 32(11):1238\\xe2\\x80\\x931278.\\n\\nKosorok, M. R. and Moodie, E. E. M. (2015). Adaptive Treatment Strategies in Practice: Plan-\\nning Trials and Analyzing Data for Personalized Medicine. ASA-SIAM Series on Statistics and\\nApplied Probability.\\n\\nKrakovsky, M. (2016). Reinforcement renaissance. Communications of the ACM, 59(8):12\\xe2\\x80\\x9314.\\n\\n24\\n\\n\\x0cKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classi\\xef\\xac\\x81cation with deep convo-\\nlutional neural networks. In the Annual Conference on Neural Information Processing Systems\\n(NIPS).\\n\\nKulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. (2016). Hierarchical deep\\nreinforcement learning: Integrating temporal abstraction and intrinsic motivation. In the Annual\\nConference on Neural Information Processing Systems (NIPS).\\n\\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through\\n\\nprobabilistic program induction. Science, 350(6266):1332\\xe2\\x80\\x931338.\\n\\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2016). Building Machines That\\n\\nLearn and Think Like People. ArXiv e-prints.\\n\\nLample, G. and Chaplot, D. S. (2016). Playing FPS Games with Deep Reinforcement Learning.\\n\\nArXiv e-prints.\\n\\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521:436\\xe2\\x80\\x93444.\\n\\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016a). End-to-end training of deep visuomotor\\n\\npolicies. The Journal of Machine Learning Research, 17:1\\xe2\\x80\\x9340.\\n\\nLevine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016b). Learning Hand-Eye Coordination\\n\\nfor Robotic Grasping with Deep Learning and Large-Scale Data Collection. ArXiv e-prints.\\n\\nLeyton-Brown, K. and Shoham, Y. (2008). Essentials of Game Theory: A Concise, Multidisciplinary\\n\\nIntroduction. Morgan & Claypool Publishers.\\n\\nLi, J., Monroe, W., and Jurafsky, D. (2016a). A Simple, Fast Diverse Decoding Algorithm for Neural\\n\\nGeneration. ArXiv e-prints.\\n\\nLi, J., Monroe, W., and Jurafsky, D. (2016b). Understanding Neural Networks through Representa-\\n\\ntion Erasure. ArXiv e-prints.\\n\\nLi, J., Monroe, W., and Jurafsky, D. (2017). Learning to Decode for Future Success. ArXiv e-prints.\\n\\nLi, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016c). Deep reinforcement\\nIn Conference on Empirical Methods in Natural Language\\n\\nlearning for dialogue generation.\\nProcessing (EMNLP).\\n\\nLi, K. and Malik, J. (2017). Learning to optimize. Submitted to Int\\xe2\\x80\\x99l Conference on Learning\\n\\nRepresentations.\\n\\nLi, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to person-\\n\\nalized news article recommendation. In the International World Wide Web Conference (WWW).\\n\\nLi, X., Li, L., Gao, J., He, X., Chen, J., Deng, L., and He, J. (2015). Recurrent Reinforcement\\n\\nLearning: A Hybrid Approach. ArXiv e-prints.\\n\\nLi, X., Lipton, Z. C., Dhingra, B., Li, L., Gao, J., and Chen, Y.-N. (2016d). A User Simulator for\\n\\nTask-Completion Dialogues. ArXiv e-prints.\\n\\nLi, Y., Szepesv\\xc2\\xb4ari, C., and Schuurmans, D. (2009). Learning exercise policies for American options.\\n\\nIn International Conference on Arti\\xef\\xac\\x81cial Intelligence and Statistics (AISTATS09).\\n\\nLiang, Y., Machado, M. C., Talvitie, E., and Bowling, M. (2016). State of the art control of atari\\nIn the International Conference on Autonomous\\n\\ngames using shallow reinforcement learning.\\nAgents & Multiagent Systems (AAMAS).\\n\\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\\n(2016). Continuous control with deep reinforcement learning. In the International Conference on\\nLearning Representations (ICLR).\\n\\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\\n\\nteaching. Machine learning, 8(3):293\\xe2\\x80\\x93321.\\n\\n25\\n\\n\\x0cLipton, Z. C., Gao, J., Li, L., Li, X., Ahmed, F., and Deng, L. (2016). Ef\\xef\\xac\\x81cient Exploration for\\n\\nDialogue Policy Learning with BBQ Networks & Replay Buffer Spiking. ArXiv e-prints.\\n\\nLittman, M. L. (2015). Reinforcement learning improves behaviour from evaluative feedback. Na-\\n\\nture, 521:445\\xe2\\x80\\x93451.\\n\\nLo, A. W. (2004). The Adaptive Markets Hypothesis: Market ef\\xef\\xac\\x81ciency from an evolutionary\\n\\nperspective. Journal of Portfolio Management, 30:15\\xe2\\x80\\x9329.\\n\\nLongstaff, F. A. and Schwartz, E. S. (2001). Valuing American options by simulation: a simple\\n\\nleast-squares approach. The Review of Financial Studies, 14(1):113\\xe2\\x80\\x93147.\\n\\nLuenberger, D. G. (1997). Investment Science. Oxford University Press.\\n\\nMaurer, A., Pontil, M., and Romera-Paredes, B. (2016). The bene\\xef\\xac\\x81t of multitask representation\\n\\nlearning. The Journal of Machine Learning Research, 17(81):1\\xe2\\x80\\x9332.\\n\\nMesnil, G., Dauphin, Y., Yao, K., Bengio, Y., Deng, L., He, X., Heck, L., Tur, G., Hakkani-T\\xc2\\xa8ur,\\nD., Yu, D., and Zweig, G. (2015). Using recurrent neural networks for slot \\xef\\xac\\x81lling in spoken\\nlanguage understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n23(3):530\\xe2\\x80\\x93539.\\n\\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M., Goroshin, R.,\\nSifre, L., Kavukcuoglu, K., Kumaran, D., and Hadsell, R. (2017). Learning to navigate in complex\\nenvironments. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and\\nIn the In-\\n\\nKavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning.\\nternational Conference on Machine Learning (ICML).\\n\\nMnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual attention.\\n\\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou,\\nI., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control\\nthrough deep reinforcement learning. Nature, 518(7540):529\\xe2\\x80\\x93533.\\n\\nMo, K., Li, S., Zhang, Y., Li, J., and Yang, Q. (2016). Personalizing a Dialogue System with Transfer\\n\\nLearning. ArXiv e-prints.\\n\\nMoody, J. and Saffell, M. (2001). Learning to trade via direct reinforcement. IEEE Transactions on\\n\\nNeural Networks, 12(4):875\\xe2\\x80\\x93889.\\n\\nMorav\\xcb\\x87c\\xc2\\xb4\\xc4\\xb1k, M., Schmid, M., Burch, N., Lis\\xc2\\xb4y, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johan-\\nson, M., and Bowling, M. (2017). DeepStack: Expert-Level Arti\\xef\\xac\\x81cial Intelligence in No-Limit\\nPoker. ArXiv e-prints.\\n\\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and ef\\xef\\xac\\x81cient off-\\npolicy reinforcement learning. In the Annual Conference on Neural Information Processing Sys-\\ntems (NIPS).\\n\\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. The MIT Press.\\n\\nNachum, O., Norouzi, M., and Schuurmans, D. (2017). Improving policy gradient by exploring\\n\\nunder-appreciated rewards. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nNarasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language understanding for text-based\\ngames using deep reinforcement learning. In Conference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP).\\n\\nNarasimhan, K., Yala, A., and Barzilay, R. (2016). Improving information extraction by acquiring\\nexternal evidence with reinforcement learning. In Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP).\\n\\n26\\n\\n\\x0cNg, A. and Russell, S. (2000). Algorithms for inverse reinforcement learning. In the International\\n\\nConference on Machine Learning (ICML).\\n\\nNogueira, R. and Cho, K. (2016). End-to-End Goal-Driven Web Navigation. ArXiv e-prints.\\n\\nO\\xe2\\x80\\x99Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2017). PGQ: Combining policy\\n\\ngradient and q-learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nOh, J., Guo, X., Lee, H., Lewis, R., and Singh, S. (2015). Action-conditional video prediction\\nusing deep networks in atari games. In the Annual Conference on Neural Information Processing\\nSystems (NIPS).\\n\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. (2015). Is object localization for free? ? weakly-\\nsupervised learning with convolutional neural networks. In the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR).\\n\\nOsband, I., Blundell, C., Pritzel, A., and Roy, B. V. (2016). Deep exploration via bootstrapped DQN.\\n\\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nOudeyer, P.-Y., Gottlieb, J., and Lopes, M. (2016). Intrinsic motivation, curiosity and learning: the-\\nory and applications in educational technologies. Progress in brain research, Elsevier, 229:257\\xe2\\x80\\x93\\n284.\\n\\nPan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge\\n\\nand Data Engineering, 22(10):1345 \\xe2\\x80\\x93 1359.\\n\\nParisotto, E., Ba, J. L., and Salakhutdinov, R. (2016). Actor-mimic: Deep multitask and transfer\\n\\nreinforcement learning. In the International Conference on Learning Representations (ICLR).\\n\\nPfau, D. and Vinyals, O. (2016). Connecting Generative Adversarial Networks and Actor-Critic\\n\\nMethods. ArXiv e-prints.\\n\\nPowell, W. B. (2011). Approximate Dynamic Programming: Solving the curses of dimensionality\\n\\n(2nd Edition). John Wiley and Sons.\\n\\nPuterman, M. L. (2005). Markov decision processes : discrete stochastic dynamic programming.\\n\\nWiley-Interscience.\\n\\nRanzato, M., Chopra, S., Auli, M., and Zaremba, W. (2016). Sequence level training with recurrent\\n\\nneural networks. In the International Conference on Learning Representations (ICLR).\\n\\nReed, S. and de Freitas, N. (2016). Neural programmer-interpreters. In the International Conference\\n\\non Learning Representations (ICLR).\\n\\nRussell, S. and Norvig, P. (2009). Arti\\xef\\xac\\x81cial Intelligence: A Modern Approach (3rd edition). Pearson.\\n\\nSandholm, T. (2015). Solving imperfect-information games. Science, 347(6218):122\\xe2\\x80\\x93123.\\n\\nSaria, S. (2014). A $3 trillion challenge to computational scientists: Transforming healthcare deliv-\\n\\nery. IEEE Intelligent Systems, 29(4):82\\xe2\\x80\\x9387.\\n\\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In the\\n\\nInternational Conference on Learning Representations (ICLR).\\n\\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61:85\\xe2\\x80\\x93\\n\\n117.\\n\\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy\\n\\noptimization. In the International Conference on Machine Learning (ICML).\\n\\nSchuurmans, D. and Zinkevich, M. (2016). Deep learning games. In the Annual Conference on\\n\\nNeural Information Processing Systems (NIPS).\\n\\nShah, P., Hakkani-T\\xc2\\xa8ur, D., and Heck, L. (2016). Interactive reinforcement learning for task-oriented\\n\\ndialogue management. In NIPS 2016 Deep Learning for Action and Interaction Workshop.\\n\\n27\\n\\n\\x0cShen, Y., Huang, P.-S., Gao, J., and Chen, W. (2016). ReasoNet: Learning to Stop Reading in\\n\\nMachine Comprehension. ArXiv e-prints.\\n\\nShoham, Y., Powers, R., and Grenager, T. (2003). Multi-Agent Reinforcement Learning: a critical\\n\\nsurvey. Web manuscript.\\n\\nShortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. (2011). In-\\nforming sequential clinical decision-making through reinforcement learning: an empirical study.\\nMachine Learning, 84:109\\xe2\\x80\\x93136.\\n\\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser,\\nJ., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with\\ndeep neural networks and tree search. Nature, 529(7587):484\\xe2\\x80\\x93489.\\n\\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic\\n\\npolicy gradient algorithms. In the International Conference on Machine Learning (ICML).\\n\\nSilver, D., Newnham, L., Barker, D., Weller, S., and McFall, J. (2013). Concurrent reinforce-\\nment learning from customer interactions. In the International Conference on Machine Learning\\n(ICML).\\n\\nSilver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert,\\nD., Rabinowitz, N., Barreto, A., and Degris, T. (2017). The predictron: End-to-end learning and\\nplanning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nStadie, B. C., Levine, S., and Abbeel, P. (2015). Incentivizing exploration in reinforcement learning\\n\\nwith deep predictive models. In NIPS 2015 Deep Reinforcement Learning Workshop.\\n\\nSu, P.-H., Gasic, M., Mrksic, N., Rojas-Barahona, L., Ultes, S., Vandyke, D., Wen, T.-H., and Young,\\n\\nS. (2016a). Continuously Learning Neural Dialogue Management. ArXiv e-prints.\\n\\nSu, P.-H., Gas\\xcb\\x87i\\xc2\\xb4c, M., Mrks\\xcb\\x87i\\xc2\\xb4c, N., Rojas-Barahona, L., Ultes, S., Vandyke, D., Wen, T.-H., and Young,\\nS. (2016b). On-line active reward learning for policy optimisation in spoken dialogue systems. In\\nthe Association for Computational Linguistics annual meeting (ACL).\\n\\nSukhbaatar, S., Weston, J., and Fergus, R. (2015). End-to-end memory networks. In the Annual\\n\\nConference on Neural Information Processing Systems (NIPS).\\n\\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks.\\n\\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nSutton, R. S. and Barto, A. G. (2017). Reinforcement Learning: An Introduction (2nd Edition, in\\n\\npreparation). MIT Press.\\n\\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011).\\nHorde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor\\ninteraction, , proc. of 10th. In International Conference on Autonomous Agents and Multiagent\\nSystems (AAMAS).\\n\\nSynnaeve, G., Nardelli, N., Auvolat, A., Chintala, S., Lacroix, T., Lin, Z., Richoux, F., and Usunier,\\nN. (2016). TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games.\\nArXiv e-prints.\\n\\nSzepesv\\xc2\\xb4ari, C. (2010). Algorithms for Reinforcement Learning. Morgan & Claypool.\\n\\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In the\\n\\nAnnual Conference on Neural Information Processing Systems (NIPS).\\n\\nTaylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\\n\\nJournal of Machine Learning Research, 10:1633\\xe2\\x80\\x931685.\\n\\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level\\n\\nplay. Neural Computation, 6(2):215\\xe2\\x80\\x93219.\\n\\n28\\n\\n\\x0cTessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2017). A deep hierarchical\\nIn the AAAI Conference on Arti\\xef\\xac\\x81cial Intelligence\\n\\napproach to lifelong learning in minecraft.\\n(AAAI).\\n\\nTheocharous, G., Thomas, P. S., and Ghavamzadeh, M. (2015). Personalized ad recommendation\\nsystems for life-time value optimization with guarantees. In the International Joint Conference\\non Arti\\xef\\xac\\x81cial Intelligence (IJCAI).\\n\\nTran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., and Blei, D. M. (2017). Deep\\n\\nProbabilistic Programming. ArXiv e-prints.\\n\\nTrischler, A., Ye, Z., Yuan, X., and Suleman, K. (2016). Natural language comprehension with the\\n\\nepireader. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nTsitsiklis, J. N. and Van Roy, B. (1997). An analysis of temporal-difference learning with function\\n\\napproximation. IEEE Transactions on Automatic Control, 42(5):674\\xe2\\x80\\x93690.\\n\\nTsitsiklis, J. N. and Van Roy, B. (2001). Regression methods for pricing complex American-style\\n\\noptions. IEEE Transactions on Neural Networks, 12(4):694\\xe2\\x80\\x93703.\\n\\nUsunier, N., Synnaeve, G., Lin, Z., and Chintala, S. (2016). Episodic Exploration for Deep Deter-\\n\\nministic Policies: An Application to StarCraft Micromanagement Tasks. ArXiv e-prints.\\n\\nvan Hasselt, H., Guez, A., , and Silver, D. (2016a). Deep reinforcement learning with double q-\\n\\nlearning. In the AAAI Conference on Arti\\xef\\xac\\x81cial Intelligence (AAAI).\\n\\nvan Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D. (2016b). Learning values across\\nmany orders of magnitude. In the Annual Conference on Neural Information Processing Systems\\n(NIPS).\\n\\nVezhnevets, A. S., Mnih, V., Agapiou, J., Osindero, S., Graves, A., Vinyals, O., and Kavukcuoglu,\\nK. (2016). Strategic attentive writer for learning macro-actions. In the Annual Conference on\\nNeural Information Processing Systems (NIPS).\\n\\nVinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016). Matching net-\\nworks for one shot learning. In the Annual Conference on Neural Information Processing Systems\\n(NIPS).\\n\\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C.,\\nKumaran, D., and Botvinick, M. (2016a). Learning to reinforcement learn. arXiv:1611.05763v1.\\n\\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016b). Du-\\neling network architectures for deep reinforcement learning. In the International Conference on\\nMachine Learning (ICML).\\n\\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big\\n\\nData, 3(9).\\n\\nWen, T.-H., Gasic, M., Mrksic, N., Su, P.-H., Vandyke, D., and Young, S. (2015a). Semantically con-\\nditioned LSTM-based natural language generation for spoken dialogue systems. In Conference\\non Empirical Methods in Natural Language Processing (EMNLP).\\n\\nWen, Z., O\\xe2\\x80\\x99Neill, D., and Maei, H. (2015b). Optimal demand response using device-based rein-\\n\\nforcement learning. IEEE Transactions on Smart Grid, 6(5):2312\\xe2\\x80\\x932324.\\n\\nWeston, J., Chopra, S., and Bordes, A. (2015). Memory networks. In the International Conference\\n\\non Learning Representations (ICLR).\\n\\nWilliams, J. D. and Zweig, G. (2016). End-to-end LSTM-based dialog control optimized with\\n\\nsupervised and reinforcement learning. ArXiv e-prints.\\n\\n29\\n\\n\\x0cWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao,\\nQ., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y.,\\nKudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa,\\nJ., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. (2016). Google\\xe2\\x80\\x99s neural\\nmachine translation system: Bridging the gap between human and machine translation. ArXiv\\ne-prints.\\n\\nWu, Y. and Tian, Y. (2017). Training agent for \\xef\\xac\\x81rst-person shooter game with actor-critic curriculum\\n\\nlearning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and Bengio,\\nY. (2015). Show, attend and tell: Neural image caption generation with visual attention. In the\\nInternational Conference on Machine Learning (ICML).\\n\\nYahya, A., Li, A., Kalakrishnan, M., Chebotar, Y., and Levine, S. (2016). Collective robot reinforce-\\n\\nment learning with distributed asynchronous guided policy search. ArXiv e-prints.\\n\\nYang, X., Chen, Y.-N., Hakkani-Tur, D., Crook, P., Li, X., Gao, J., and Deng, L. (2016). End-to-End\\n\\nJoint Learning of Natural Language Understanding and Dialogue Manager. ArXiv e-prints.\\n\\nYang, Z., He, X., Gao, J., Deng, L., and Smola, A. (2015). Stacked Attention Networks for Image\\n\\nQuestion Answering. ArXiv e-prints.\\n\\nYogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and Ling, W. (2017). Learning to compose\\nwords into sentences with reinforcement learning. Submitted to Int\\xe2\\x80\\x99l Conference on Learning\\nRepresentations.\\n\\nYoung, S., Ga\\xcb\\x87si\\xc2\\xb4c, M., Thomson, B., and Williams, J. D. (2013). POMDP-based statistical spoken\\n\\ndialogue systems: a review. PROC IEEE, 101(5):1160\\xe2\\x80\\x931179.\\n\\nYu, L., Zhang, W., Wang, J., and Yu, Y. (2017). Seqgan: Sequence generative adversarial nets with\\n\\npolicy gradient. In the AAAI Conference on Arti\\xef\\xac\\x81cial Intelligence (AAAI).\\n\\nYu, Y.-L., Li, Y., Szepesv\\xc2\\xb4ari, C., and Schuurmans, D. (2009). A general projection property for dis-\\ntribution families. In the Annual Conference on Neural Information Processing Systems (NIPS).\\n\\nZagoruyko, S. and Komodakis, N. (2017). Paying more attention to attention: Improving the per-\\nformance of convolutional neural networks via attention transfer. Submitted to Int\\xe2\\x80\\x99l Conference\\non Learning Representations.\\n\\nZaremba, W. and Sutskever, I. (2015). Reinforcement Learning Neural Turing Machines - Revised.\\n\\nArXiv e-prints.\\n\\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning\\n\\nrequires rethinking generalization. Submitted to Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\nZhao, T. and Eskenazi, M. (2016). Towards end-to-end learning for dialog state tracking and man-\\nagement using deep reinforcement learning. In the Annual SIGdial Meeting on Discourse and\\nDialogue (SIGDIAL).\\n\\nZhu, X. and Goldberg, A. B. (2009). Introduction to semi-supervised learning. Morgan & Claypool.\\n\\nZhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Li, F.-F., and Farhadi, A. (2016). Target-\\ndriven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. ArXiv e-prints.\\n\\nZinkevich, M. (2017).\\n\\nRules of Machine Learning: Best Practices for ML Engineering.\\n\\nhttp://martin.zinkevich.org/rules of ml/rules of ml.pdf.\\n\\nZoph, B. and Le, Q. V. (2017). Neural architecture search with reinforcement learning. Submitted\\n\\nto Int\\xe2\\x80\\x99l Conference on Learning Representations.\\n\\n30\\n\\n\\x0c',\n",
       " '7\\n1\\n0\\n2\\n\\n \\n\\nb\\ne\\nF\\n0\\n1\\n\\n \\n\\n \\n \\n]\\nL\\nM\\n\\n.\\nt\\na\\nt\\ns\\n[\\n \\n \\n\\n1\\nv\\n4\\n3\\n3\\n3\\n0\\n\\n.\\n\\n2\\n0\\n7\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nPublished as a conference paper at ICLR 2017\\n\\nBATCH POLICY GRADIENT METHODS FOR\\nIMPROVING NEURAL CONVERSATION MODELS\\n\\nKirthevasan Kandasamy \\xe2\\x88\\x97\\nCarnegie Mellon University, Pittsburgh, PA, USA\\nkandasamy@cs.cmu.edu\\n\\nYoram Bachrach (cid:91)\\nDigitalGenius Ltd., London, UK\\nyorambac@gmail.com\\n\\nRyota Tomioka, Daniel Tarlow, David Carter\\nMicrosoft Research, Cambridge, UK\\n{ryoto,dtarlow,dacart}@microsoft.com\\n\\xe2\\x88\\x97 (cid:91) This work was done when KK/YB was an intern/employee at Microsoft Research, Cambridge, UK.\\n\\nABSTRACT\\n\\nWe study reinforcement learning of chatbots with recurrent neural network archi-\\ntectures when the rewards are noisy and expensive to obtain. For instance, a chat-\\nbot used in automated customer service support can be scored by quality assurance\\nagents, but this process can be expensive, time consuming and noisy. Previous re-\\ninforcement learning work for natural language processing uses on-policy updates\\nand/or is designed for on-line learning settings. We demonstrate empirically that\\nsuch strategies are not appropriate for this setting and develop an off-policy batch\\npolicy gradient method (BPG). We demonstrate the ef\\xef\\xac\\x81cacy of our method via a\\nseries of synthetic experiments and an Amazon Mechanical Turk experiment on a\\nrestaurant recommendations dataset.\\n\\n1\\n\\nINTRODUCTION\\n\\nChatbots are one of the classical applications of arti\\xef\\xac\\x81cial intelligence and are now ubiquitous in\\ntechnology, business and everyday life. Many corporate entities are now increasingly using chatbots\\nto either replace or assist humans in customer service contexts. For example, Microsoft is currently\\nactively building a chat bot to optimise and streamline its technical support service.\\nIn these scenarios, there is usually an abundance of historical data since past conversations between\\ncustomers and human customer service agents are usually recorded by organisations. An apparently\\nstraightforward solution would be to train chatbots to reproduce the responses by human agents\\nusing standard techniques such as maximum likelihood. While this seems natural, it is far from\\ndesirable for several reasons. It has been observed that such procedures have a tendency to produce\\nvery generic responses (Sordoni et al., 2015). For instance, when we trained chatbots via maximum\\nlikelihood on a restaurant recommendations dataset, they repeatedly output responses to the effect\\nof How large is your group?, What is your budget? etc. Further, they also produce responses\\nsuch as Let me look that up. or Give me a second. which, although permissible for a human\\nagent to say, are not appropriate for a chatbot. Although there are ways to increase the diversity\\nof responses (Li et al., 2015), our focus is on encouraging the bot to meaningfully advance the\\nconversation. One way to address this problem is to provide some form of weak supervision for\\nresponses generated by a chatbot. For example, a human labeller, such as a quality assurance agent,\\ncould score each response generated by a chatbot in a conversation with a customer. This brings us\\nto the reinforcement learning (RL) paradigm where these rewards (scores) are to be used to train a\\ngood chatbot. In this paper we will use the terms score, label, and reward interchangeably. Labelled\\ndata will mean conversations which have been assigned a reward of some form as explained above.\\nNonetheless, there are some important differences in the above scenario when compared to the more\\npopular approaches for RL.\\n\\n\\xe2\\x80\\xa2 Noisy and expensive rewards: Obtaining labels for each conversation can be time consuming\\nand economically expensive. As a result, there is a limited amount of labelled data available.\\nMoreover, labels produced by humans are invariably noisy due to human error and subjectivity.\\n\\n1\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\n\\xe2\\x80\\xa2 Off-line evaluations: Unlike conventional RL settings, such as games, where we try to \\xef\\xac\\x81nd\\nthe optimal policy while interacting with the system, the rewards here are not immediately\\navailable. Previous conversations are collected, labelled by human experts, and then given to\\nan algorithm which has to manage with the data it has.\\n\\n\\xe2\\x80\\xa2 Unlabelled Data: While labelled data is limited, a large amount of unlabelled data is available.\\n\\nIf labelled data is in short supply, reinforcement learning could be hopeless. However, if unlabelled\\ndata can be used to train a decent initial bot, say via maximum likelihood, we can use policy iteration\\ntechniques to re\\xef\\xac\\x81ne this bot by making local improvements using the labelled data (Bellman, 1956).\\nBesides chatbots, this framework also \\xef\\xac\\x81nds applications in tasks such as question answering (Fer-\\nrucci et al., 2010; Hermann et al., 2015; Sachan et al., 2016), generating image descriptions (Karpa-\\nthy & Fei-Fei, 2015) and machine translation (Bahdanau et al., 2014) where a human labeller can\\nprovide weak supervision in the form of a score to a sentence generated by a bot.\\nTo contextualise the work in this paper, we make two important distinctions in policy iteration\\nmethods in reinforcement learning. The \\xef\\xac\\x81rst is on-policy vs off-policy. In on-policy settings, the goal\\nis to improve the current policy on the \\xef\\xac\\x82y while exploring the space. On-policy methods are used\\nin applications where it is necessary to be competitive (achieve high rewards) while simultaneously\\nexploring the environment. In off-policy, the environment is explored using a behaviour policy, but\\nthe goal is to improve a different target policy. The second distinction is on-line vs batch (off-line).\\nIn on-line settings one can interact with the environment. In batch methods, which is the setting for\\nthis work, one is given past exploration data from possibly several behaviour policies and the goal\\nis to improve a target policy using this data. On-line methods can be either on-policy or off-policy\\nwhereas batch methods are necessarily off-policy.\\nIn this paper, we study reinforcement learning in batch settings, for improving chat bots with\\nSeq2Seq recurrent neural network (RNN) architectures. One of the challenges when compared\\nto on-line learning is that we do not have interactive control over the environment. We can only\\nhope to do as well as our data permits us to. On the other hand, the batch setting affords us some\\nluxuries. We can reuse existing data and use standard techniques for hyper-parameter tuning based\\non cross validation. Further, in on-line policy updates, we have to be able to \\xe2\\x80\\x9cguess\\xe2\\x80\\x9d how an episode\\nwill play out, i.e. actions the behaviour/target policies would take in the future and corresponding\\nrewards. However, in batch learning, the future actions and rewards are directly available in the data.\\nThis enables us to make more informed choices when updating our policy.\\n\\nRELATED WORK\\n\\nRecently there has been a surge of interest in deep learning approaches to reinforcement learning,\\nmany of them adopting Q-learning, e.g. (He et al., 2015; Mnih et al., 2013; Narasimhan et al., 2015).\\nIn Q-learning, the goal is to estimate the optimal action value function Q\\xe2\\x88\\x97. Then, when an agent\\nis at a given state, it chooses the best greedy action according to Q\\xe2\\x88\\x97. While Q-learning has been\\nsuccessful in several applications, it is challenging in the settings we consider since estimating Q\\xe2\\x88\\x97\\nover large action and state spaces will require a vast number of samples. In this context, policy\\niteration methods are more promising since we can start with an initial policy and make incremental\\nlocal improvements using the data we have. This is especially true given that we can use maximum\\nlikelihood techniques to estimate a good initial bot using unlabelled data.\\nPolicy gradient methods, which fall within the paradigm of policy iteration, make changes to the\\nparameters of a policy along the gradient of a desired objective (Sutton et al., 1999). Recently, the\\nnatural language processing (NLP) literature has turned its attention to policy gradient methods for\\nimproving language models. Ranzato et al. (2015) present a method based on the classical REIN-\\nFORCE algorithm (Williams, 1992) for improving machine translation after preliminary training\\nwith maximum likelihood objectives. Bahdanau et al. (2016) present an actor-critic method also for\\nmachine translation. In both cases, as the reward, the authors use the BLEU (bilingual evaluation\\nunderstudy) score of the output and the translation in the training dataset. This setting, where the\\nrewards are deterministic and cheaply computable, does not re\\xef\\xac\\x82ect dif\\xef\\xac\\x81culties inherent to training\\nchatbots where labels are noisy and expensive. Li et al. (2016) develop a policy gradient method\\nbot for chatbots. However, they use user de\\xef\\xac\\x81ned rewards (based on some simple rules) which, once\\nagain, are cheaply obtained and deterministic. Perhaps the closest to our work is that of Williams &\\nZweig (2016) who use a REINFORCE based method for chat bots. We discuss the differences of\\n\\n2\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nthis and other methods in greater detail in Section 3. The crucial difference between all of the above\\nefforts and ours is that they use on-policy and/or on-line updates in their methods.\\nThe remainder of this manuscript is organised as follows. In Section 2 we review Seq2Seq models\\nand Markov decision processes (MDP) and describe our framework for batch reinforcement learn-\\ning. Section 3 presents our method BPG and compares it with prior work in the RL and NLP\\nliterature. Section 4 presents experiments on a synthetic task and a customer service dataset for\\nrestaurant recommendations.\\n\\n2 PRELIMINARIES\\n\\n2.1 A REVIEW OF SEQ2SEQ MODELS\\n\\nThe goal of a Seq2Seq model in natural language processing is to produce an output sequence\\ny = [a1, a2, . . . , aT ] given an input sequence x (Cho et al., 2014; Kalchbrenner & Blunsom, 2013;\\nSutskever et al., 2014). Here ai \\xe2\\x88\\x88 A where A is a vocabulary of words. For example, in machine\\ntranslation from French to English, x is the input sequence in French, and y is its translation in En-\\nglish. In customer service chatbots, x is the conversation history until the customer\\xe2\\x80\\x99s last query and\\ny is the response by an agent/chatbot. In a Seq2Seq model, we use an encoder network to represent\\nthe input sequence as a euclidean vector and then a decoder network to convert this vector to an\\noutput sequence. Typically, both the encoder and decoder networks are recurrent neural networks\\n(RNN) (Mikolov et al., 2010) where the recurrent unit processes each word in the input/output se-\\nquences one at a time. In this work, we will use the LSTM (long short term memory) (Hochreiter &\\nSchmidhuber, 1997) as our recurrent unit due to its empirical success in several applications.\\nIn its most basic form, the decoder RNN can be interpreted as assigning a probability distribution\\nover A given the current \\xe2\\x80\\x9cstate\\xe2\\x80\\x9d. At time t, the state st is the input sequence x and the words\\nyt\\xe2\\x88\\x921 = [a1, . . . , at\\xe2\\x88\\x921] produced by the decoder thus far, i.e. st = (x, yt\\xe2\\x88\\x921). We sample the\\nnext word at from this probability distribution \\xcf\\x80(\\xc2\\xb7|st), then update our state st+1 = (x, yt) where\\nyt = [yt\\xe2\\x88\\x921, at], and proceed in a similar fashion. The vocabulary A contains an end-of-statement\\ntoken <EOS>. If we sample <EOS> at time T + 1, we terminate the sequence and output yT .\\n\\n2.2 A REVIEW OF MARKOV DECISION PROCESSES (MDP)\\n\\nWe present a formalism for MDPs simpli\\xef\\xac\\x81ed to our setting. In an MDP, an agent takes an action a\\nin a state s and transitions to a state s(cid:48). An episode refers to a sequence of transitions s1 \\xe2\\x86\\x92 a1 \\xe2\\x86\\x92\\ns2 \\xe2\\x86\\x92 a2 \\xe2\\x86\\x92 \\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 \\xe2\\x86\\x92 aT \\xe2\\x86\\x92 sT +1 until the agent reaches a terminal state sT +1. At a terminal state, the\\nagent receives a reward. Formally, an MDP is the triplet (S,A, R). Here, S is a set of states and A is\\na set of actions. When we take an action a at state s we transition to a new state s(cid:48) = s(cid:48)(s, a) which,\\nin this work, will be deterministic. A will be a \\xef\\xac\\x81nite but large discrete set and S will be discrete\\nbut potentially in\\xef\\xac\\x81nite. R : S \\xe2\\x86\\x92 R is the expected reward function such that when we receive a\\nreward r at state s \\xe2\\x88\\x88 S, E[r] = R(s). Let S0 \\xe2\\x8a\\x82 S be a set of terminal states. When we transition to\\nany s \\xe2\\x88\\x88 S0, the episode ends. In this work, we will assume that the rewards are received only at a\\nterminal state, i.e R(s) is nonzero only on S0.\\nA policy \\xcf\\x80 is a rule to select an action at a given state. We will be focusing on stochastic policies\\n\\xcf\\x80 : A \\xc3\\x97 S \\xe2\\x86\\x92 R+ where \\xcf\\x80(a|s) denotes the probability an agent will execute action a at state s. We\\nde\\xef\\xac\\x81ne the value function V \\xcf\\x80 : S \\xe2\\x86\\x92 R of policy \\xcf\\x80, where V (s) is the expected reward at the end of\\nthe episode when we follow policy \\xcf\\x80 from state s. For any terminal state s \\xe2\\x88\\x88 S0, V \\xcf\\x80(s) = R(s)\\nregardless of \\xcf\\x80. We will also \\xef\\xac\\x81nd it useful to de\\xef\\xac\\x81ne the action-value function Q\\xcf\\x80 : S \\xc3\\x97 A :\\xe2\\x86\\x92 R,\\nwhere Q\\xcf\\x80(s, a) is the expected reward of taking action a at state s and then following policy \\xcf\\x80.\\nWith deterministic state transitions this is simply Q\\xcf\\x80(s, a) = V \\xcf\\x80(s(cid:48)(s, a)). It can be veri\\xef\\xac\\x81ed that\\nV \\xcf\\x80(s) = Ea\\xe2\\x88\\xbc\\xcf\\x80(\\xc2\\xb7|s) [Q\\xcf\\x80(s, a)] (Sutton & Barto, 1998).\\n\\n2.3 SET UP\\n\\nWe now frame our learning from labels scenario for RNN chatbots as an MDP. The treatment has\\nsimilarities to some recent RL work in the NLP literature discussed above.\\n\\n3\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nLet x be the input and yt\\xe2\\x88\\x921 = [a1, . . . , at\\xe2\\x88\\x921] be the words output by the decoder until time t. The\\nstate of our MDP at time t of the current episode will be st = (x, yt\\xe2\\x88\\x921). Therefore, the set of\\nstates S will be all possible pairs of inputs and partial output sequences. The actions A will be the\\nvocabulary. The terminal states S0 will be (x, y) such that the last literal of y is <EOS>. The stochastic\\npolicy \\xcf\\x80 will be a Seq2Seq RNN which produces a distribution over A given state st. When we wish\\nto make the dependence of the policy on the RNN parameters \\xce\\xb8 explicit, we will write \\xcf\\x80\\xce\\xb8. When we\\nsample an action at \\xe2\\x88\\xbc \\xcf\\x80(\\xc2\\xb7|st), we deterministically transition to state (x, [yt\\xe2\\x88\\x921, at]). If we sample\\naT +1 = <EOS> at time T + 1, the episode terminates and we observe a stochastic reward.\\nWe are given a dataset of input-output-reward triples {(x(i), y(i), r(i))}n\\ni=1 where y(i) =\\n(a(i)\\n, <EOS>) is the sequence of output words. This data was collected from possibly mul-\\ntiple behaviour policies which output y(i) for the given input x(i). In the above customer service\\nexample, the behaviour policies could be chatbots, or even humans, which were used for conversa-\\ntions with a customer. The rewards ri are scores assigned by a human quality assurance agent to\\neach response of the chatbot. Our goal is to use this data to improve a given target policy \\xcf\\x80\\xce\\xb8. We\\nwill use q to denote the distribution of the data. q(s) is the distribution of the states in the dataset,\\nq(a|s) is the conditional distribution of an action given a state, and q(s, a) = q(s)q(a|s) is the joint\\ndistribution over states and actions. q will be determined by the initial distribution of the inputs x(i)\\nand the behaviour policies used to collect the training data. Our aim is to \\xef\\xac\\x81nd a policy that does well\\nwith respect to q. Speci\\xef\\xac\\x81cally, we wish to maximise the following objective,\\n\\n1 , . . . , a(i)\\nTi\\n\\nJ(\\xce\\xb8) =\\n\\nq(s)V \\xcf\\x80\\xce\\xb8 (s).\\n\\n(1)\\n\\n(cid:88)\\n\\ns\\xe2\\x88\\x88S\\n\\nHere, the value function V \\xcf\\x80\\xce\\xb8 is not available to us but has to be estimated from the data. This is\\nsimilar to objectives used in on-line off-policy policy gradient literature where q is replaced by the\\nlimiting distribution of the behaviour policy (Degris et al., 2012). In the derivation of our algorithm,\\nwe will need to know q(a|s) to compute the gradient of our objective. In off-policy reinforcement\\nlearning settings this is given by the behaviour policy which is readily available. If the behaviour\\npolicy if available to us, then we can use this too. Otherwise, a simple alternative is to \\xe2\\x80\\x9clearn\\xe2\\x80\\x9d a be-\\nhaviour policy. For example, in our experiments we used an RNN trained using the unlabelled data\\nto obtain values for q(a|s). As long as this learned policy can capture the semantics of natural lan-\\nguage (for example, the word apple is more likely than car when the current state is (x, I ate an)),\\nthen it can be expected to do reasonably well. In the following section, we will derive a stochastic\\ngradient descent (SGD) procedure that will approximately minimise (1).\\nBefore we proceed, we note that it is customary in the RL literature to assume stochastic transitions\\nbetween states and use rewards at all time steps instead of the terminal step. Further, the future\\nrewards are usually discounted by a discount factor \\xce\\xb3 < 1. While we use the above formalism to\\nsimplify the exposition, the ideas presented here extend naturally to more conventional settings.\\n\\n3 BATCH POLICY GRADIENT\\n\\n(cid:21)\\n\\nOur derivation follows the blueprint in Degris et al. (2012) who derive an off-policy on-line actor\\ncritic algorithm. Following standard policy gradient methods, we will aim to update the policy by\\ntaking steps along the gradient of the objective \\xe2\\x88\\x87J(\\xce\\xb8).\\n\\xe2\\x88\\x87J(\\xce\\xb8) = \\xe2\\x88\\x87Es\\xe2\\x88\\xbcq\\n\\n\\xe2\\x88\\x87\\xcf\\x80\\xce\\xb8(a|s)Q\\xcf\\x80\\xce\\xb8 (s, a) + \\xcf\\x80\\xce\\xb8(a|s)\\xe2\\x88\\x87Q\\xcf\\x80\\xce\\xb8 (s, a)\\n\\n\\xcf\\x80\\xce\\xb8(a|s)Q\\xcf\\x80\\xce\\xb8 (s, a)\\n\\n(cid:20)(cid:88)\\n\\n(cid:20)(cid:88)\\n\\nit and work with the approximate gradient g(\\xce\\xb8) = Es\\xe2\\x88\\xbcq[(cid:80)\\n\\nThe latter term inside the above summation is dif\\xef\\xac\\x81cult to work with, so the \\xef\\xac\\x81rst step is to ignore\\na\\xe2\\x88\\x88A \\xe2\\x88\\x87\\xcf\\x80\\xce\\xb8(a|s)Q\\xcf\\x80\\xce\\xb8 (s, a)] \\xe2\\x89\\x88 \\xe2\\x88\\x87J(\\xce\\xb8).\\nDegris et al. (2012) provide theoretical justi\\xef\\xac\\x81cation for this approximation in off policy settings by\\nestablishing that J(\\xce\\xb8) \\xe2\\x89\\xa4 J(\\xce\\xb8 + \\xce\\xb1g(\\xce\\xb8)) for all small enough \\xce\\xb1. Expanding on g(\\xce\\xb8), we obtain:\\n\\n= Es\\xe2\\x88\\xbcq\\n\\na\\xe2\\x88\\x88A\\n\\na\\xe2\\x88\\x88A\\n\\n(cid:21)\\n\\n.\\n\\n(cid:34)(cid:88)\\n\\n(cid:104)\\n= E(st,at)\\xe2\\x88\\xbcq(\\xc2\\xb7,\\xc2\\xb7) [\\xcf\\x81(st, at)\\xcf\\x88(at, st)(Q\\xcf\\x80\\xce\\xb8 (st, at) \\xe2\\x88\\x92 V \\xcf\\x80\\xce\\xb8 (st))] .\\n\\n\\xe2\\x88\\x87\\xcf\\x80\\xce\\xb8(a|s)\\n\\xcf\\x80\\xce\\xb8(a|s)\\n\\n= E s\\xe2\\x88\\xbcq\\n\\n\\xcf\\x80\\xce\\xb8(a|s)\\n\\nQ\\xcf\\x80\\xce\\xb8 (s, a)\\n\\na\\xe2\\x88\\xbcq(\\xc2\\xb7|s)\\n\\na\\xe2\\x88\\x88A\\n\\n(cid:35)\\n\\ng(\\xce\\xb8) = Es\\xe2\\x88\\xbcq\\n\\n\\xcf\\x81(s, a)\\xcf\\x88(a, s)Q\\xcf\\x80\\xce\\xb8 (s, a)\\n\\n(cid:105)\\n\\n(2)\\n\\n4\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\n\\xe2\\x88\\x87\\xcf\\x80\\xce\\xb8(a|s)\\n\\xcf\\x80\\xce\\xb8(a|s) = \\xe2\\x88\\x87 log \\xcf\\x80\\xce\\xb8(a|s) is the score function of the policy and \\xcf\\x81(s, a) =\\nHere \\xcf\\x88(a, s) =\\n\\xcf\\x80\\xce\\xb8(a|s)/q(a|s) is the importance sampling coef\\xef\\xac\\x81cient. In the last step, we have used the fact that\\nE[\\xcf\\x80(a|s)\\xcf\\x88(a|s)h(s)] = 0 for any function h : S \\xe2\\x86\\x92 R of the current state (Szepesv\\xc2\\xb4ari, 2010). The\\npurpose of introducing the value function V \\xcf\\x80\\xce\\xb8 is to reduce the variance of the SGD updates \\xe2\\x80\\x93 we\\nwant to assess how good/bad action at is relative to how well \\xcf\\x80\\xce\\xb8 will do at state st in expectation. If\\nat is a good action (Q\\xcf\\x80\\xce\\xb8 (st, at) is large relative to V \\xcf\\x80\\xce\\xb8 (st)), the coef\\xef\\xac\\x81cient of the score function is\\npositive and it will change \\xce\\xb8 so as to assign a higher probability to action at at state st.\\nThe Q\\xcf\\x80\\xce\\xb8 , V \\xcf\\x80\\xce\\xb8 functions are not available to us so we will replace them with estimates. For V \\xcf\\x80\\xce\\xb8 (st)\\n\\nwe will use an estimate (cid:98)V (st) \\xe2\\x80\\x93 we will discuss choices for this shortly. However, the action value\\nfrom \\xcf\\x80\\xce\\xb8(at|st) we will need to re-weight future rewards via importance sampling r(cid:81)T\\n\\nfunction is usually not estimated in RL policy gradient settings to avoid the high sample complexity.\\nA sensible stochastic approximation for Q\\xcf\\x80\\xce\\xb8 (st, at) is to use the sum of future rewards from the\\ncurrent state (Sutton & Barto, 1998)1. If we receive reward r at the end of the episode, we can\\nthen use Q\\xcf\\x80\\xce\\xb8 (st, at) \\xe2\\x89\\x88 r for all time steps t in the episode. However, since q(at|st) is different\\ni=t \\xcf\\x81(si, ai).\\nThis is to account for the fact that an action a given s may have been more likely under the policy\\n\\xcf\\x80\\xce\\xb8(\\xc2\\xb7|s) than it was under q(\\xc2\\xb7|s) or vice versa. Instead of directly using the re-weighted rewards, we\\nwill use the so called \\xce\\xbb\\xe2\\x80\\x93return which is a convex combination of the re-weighted rewards and the\\nvalue function (Sutton, 1988; 1984). In our setting, they are de\\xef\\xac\\x81ned recursively from the end of the\\nepisode t = T + 1 to t = 1 as follows. For \\xce\\xbb \\xe2\\x88\\x88 (0, 1],\\n\\n\\xcb\\x9cr\\xce\\xbb\\nT +1 = r,\\n\\nt = (1 \\xe2\\x88\\x92 \\xce\\xbb)V \\xcf\\x80\\xce\\xb8 (st+1) + \\xce\\xbb\\xcf\\x81(st, at)\\xcb\\x9cr\\xce\\xbb\\n\\xcb\\x9cr\\xce\\xbb\\n\\n(3)\\nThe purpose of introducing \\xce\\xbb is to reduce the variance of using the future rewards alone as an esti-\\nmate for Q\\xcf\\x80\\xce\\xb8 (st, at). This is primarily useful when rewards are noisy. If the rewards are determin-\\nistic, \\xce\\xbb = 1 which ignores the value function is the best choice. In noisy settings, it is recommended\\nto use \\xce\\xbb < 1 (see Sec 3.1 of (Szepesv\\xc2\\xb4ari, 2010)). In our algorithm, we will replace \\xcb\\x9cr\\xce\\xbb\\nt where\\n\\nV \\xcf\\x80\\xce\\xb8 is replaced with the estimate (cid:98)V . Putting it all together, and letting \\xce\\xb1 denote the step size, we\\n\\nfor t = T, . . . , 1.\\n\\nt with r\\xce\\xbb\\n\\nt+1\\n\\nhave the following update rule for the parameters \\xce\\xb8 of our policy:\\n\\n\\xce\\xb8 \\xe2\\x86\\x90 \\xce\\xb8 + \\xce\\xb1\\xcf\\x81(st, at)\\xcf\\x88(st, at)(r\\xce\\xbb\\n\\nt \\xe2\\x88\\x92(cid:98)V (st)).\\n\\nmators for V \\xcf\\x80\\xce\\xb8. We will study two options. The \\xef\\xac\\x81rst is a simple heuristic used previously in the\\n\\nIn Algorithm 1, we have summarised the procedure where the updates are performed after an entire\\npass through the dataset. In practice, we perform the updates in mini-batches.\\n\\nfunction. We \\xef\\xac\\x81rst need to acknowledge that this is a dif\\xef\\xac\\x81cult problem: S is quite large and for typical\\napplications for this work there might not be enough data since labels are expensive. That said, the\\n\\nso it is not critical that this be precise \\xe2\\x80\\x93 even a bad estimator will converge eventually. Secondly,\\nstandard methods for estimating the value function based on minimising the projected Bellman er-\\nror require the second derivatives, which might be intractable for highly nonlinear parametrisations\\n\\nAn Estimator for the Value Function: All that is left to do is to specify an estimator(cid:98)V for the value\\npurpose of (cid:98)V in (2), (3) is to reduce the variance of our SGD updates and speed up convergence\\nof (cid:98)V (Maei, 2011). For these two statistical and computational reasons, we resort to simple esti-\\nRL literature, namely a constant estimator for (cid:98)V which is equal to the mean of all rewards in the\\ndataset (Williams, 1992). The second uses the parametrisation (cid:98)V (s) = \\xcf\\x83(\\xce\\xbe(cid:62)\\xcf\\x86(s)) where \\xcf\\x83 is the\\nlogistic function and \\xcf\\x86(s) \\xe2\\x88\\x88 Rd is a Euclidean representation of the state. For (cid:98)V (s) of the above\\n\\xce\\xbe(cid:98)V (s) can be computed in O(d) time. To estimate this value function, we use\\n\\nform, the Hessian \\xe2\\x88\\x872\\nthe GTD(\\xce\\xbb) estimator from Maei (2011). As \\xcf\\x86(s) we will be using the hidden state of the LSTM.\\nThe rationale for this is as follows. In an LSTM trained using maximum likelihood, the hidden\\nstate contains useful information about the objective. If there is overlap between the maximum like-\\nlihood and reinforcement learning objectives, we can expect the hidden state to also carry useful\\ninformation about the RL objective. Therefore, we can use the hidden state to estimate the value\\nfunction whose expectation is the RL objective. We have described our implementation of GTD(\\xce\\xbb)\\nin Appendix A and speci\\xef\\xac\\x81ed some implementation details in Section 4.\\n\\n1 Note Q\\xcf\\x80\\xce\\xb8 (st, at) = V \\xcf\\x80\\xce\\xb8 (st+1) for deterministic transitions. However, it is important not to interpret the\\nterm in (2) as the difference in the value function between successive states. Conditioned on the current time\\nstep, V \\xcf\\x80\\xce\\xb8 (st) is deterministic, while V \\xcf\\x80\\xce\\xb8 (st+1) is stochastic. In particular, while a crude estimate suf\\xef\\xac\\x81ces for\\nthe former, the latter is critical and should re\\xef\\xac\\x82ect the rewards received during the remainder of the episode.\\n\\n5\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nBatch Policy Gradient (BPG)\\n\\nAlgorithm 1\\nGiven: Data {(xi, yi, ri)}n\\n\\n\\xe2\\x80\\x93 Set \\xce\\xb8 \\xe2\\x86\\x90 \\xce\\xb80.\\n\\xe2\\x80\\x93 For each epoch k = 1, 2, . . .\\n\\ni=1, step size \\xce\\xb1, return coef\\xef\\xac\\x81cient \\xce\\xbb, initial \\xce\\xb80.\\n\\n(cid:73) Set \\xe2\\x88\\x86\\xce\\xb8 \\xe2\\x86\\x90 0\\n(cid:73) For each episode i = 1, . . . , n\\nT +1 \\xe2\\x86\\x90 ri\\n\\xe2\\x80\\xa2 r\\xce\\xbb\\n\\xe2\\x80\\xa2 \\xcf\\x81t \\xe2\\x86\\x90 \\xcf\\x80\\xce\\xb8(a(i)\\n\\xe2\\x80\\xa2 For each time step in reverse t = T (i), . . . , 1\\n\\n|s(i)\\nt )/q(a(i)\\n\\n|s(i)\\nt ) for t = 1, . . . , T (i).\\n\\nt\\n\\nt\\n\\nt \\xe2\\x86\\x90 (1 \\xe2\\x88\\x92 \\xce\\xbb)(cid:98)V (s(i)\\n\\n(i) r\\xce\\xbb\\n(ii) \\xe2\\x88\\x86\\xce\\xb8 \\xe2\\x86\\x90 \\xe2\\x88\\x86\\xce\\xb8 + 1\\n\\n(iii) Compute updates for the value function estimate (cid:98)V .\\n\\nT (i) \\xcf\\x81t\\xcf\\x88(s(i)\\n\\nt+1\\nt )(r\\xce\\xbb\\n\\nt ))\\n\\nt+1) + \\xce\\xbb\\xcf\\x81tr\\xce\\xbb\\n, a(i)\\n\\nt\\n\\nt \\xe2\\x88\\x92(cid:98)V (s(i)\\n\\n(cid:73) Update the policy\\n\\n(cid:73) Update the value function estimate (cid:98)V .\\n\\n\\xce\\xb8 \\xe2\\x86\\x90 \\xce\\xb8 + \\xce\\xb1\\xe2\\x88\\x86\\xce\\xb8\\n\\nCOMPARISON WITH OTHER RL APPROACHES IN NLP\\n\\nPolicy gradient methods have been studied extensively in on policy settings where the goal is to\\nimprove the current policy on the \\xef\\xac\\x82y (Amari, 1998; Williams, 1992). To our knowledge, all RL\\napproaches in Seq2Seq models have also adopted on-policy policy gradient updates (Bahdanau et al.,\\n2016; Li et al., 2016; Ranzato et al., 2015; Williams & Zweig, 2016). However, on policy methods\\nbreak down in off-policy settings, because any update must account for the probability of the action\\nunder the target policy. For example, suppose the behaviour policy took action a at state s and\\nreceived a low reward. Then we should modify the target policy \\xce\\xb8 so as to reduce \\xcf\\x80\\xce\\xb8(a|s). However,\\nif the target policy is already assigning low probability to a|s then we should not be as aggressive\\nwhen making the updates. The re-weighting \\xcf\\x81(s, a) via importance sampling does precisely this.\\nA second difference is that we study batch RL. Standard on-line methods are designed for settings\\nwhere we have to continually improve the target while exploring using the behaviour policy. Critical\\nto such methods are the estimation of future rewards at the current state and the future actions that\\nwill be taken by both the behaviour and target policies. In order to tackle this, previous research\\neither ignore future rewards altogether (Williams, 1992), resort to heuristics to distribute a delayed\\nreward to previous time steps (Bahdanau et al., 2016; Williams & Zweig, 2016), or make additional\\nassumptions about the distribution of the states such as stationarity of the Markov process (Degris\\net al., 2012; Maei, 2011). However, in batch settings, the \\xce\\xbb-return from a given time step can be\\ncomputed directly (3) since the future action and rewards are available in the dataset. Access to this\\ninformation provides a crucial advantage over techniques designed for on-line settings.\\n\\n4 EXPERIMENTS\\n\\nImplementation Details: We implement our methods using Chainer (Tokui et al., 2015), and group\\nsentences of the same length together in the same batch to make use of GPU parallelisation. Since\\ndifferent batches could be of different length, we do not normalise the gradients by the batch size\\nas we should take larger steps after seeing more data. However, we normalise by the length of the\\noutput sequence to allocate equal weight to all sentences. We truncate all output sequences to length\\n64 and use a maximum batch size of 32. We found it necessary to use a very small step size (10\\xe2\\x88\\x925),\\notherwise the algorithm has a tendency to get stuck at bad parameter values. While importance re-\\nweighting is necessary in off-policy settings, it can increase the variance of the updates, especially\\nwhen q(at|st) is very small. A common technique to alleviate this problem is to clip the \\xcf\\x81(st, at)\\nvalue (Swaminathan & Joachims, 2015). In addition to single \\xcf\\x81(st, at) values, our procedure has a\\nproduct of \\xcf\\x81(st, at) values when computing the future rewards (3). The effect of large \\xcf\\x81 values is a\\nlarge weight \\xcf\\x81t(r\\xce\\xbb\\n\\nt \\xe2\\x88\\x92(cid:98)V (st)) for the score function in step (ii) of Algorithm 1. In our implementation,\\n\\n6\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nFigure 1: Illustration of the encoder\\nand decoder RNNs used in our exper-\\niments. In this example, the input to\\nthe encoder is x = (..., A, B, <EOS>)\\nand the output of the decoder is y =\\n(U, V, W, . . . ). We use four different\\nLSTMs for the bottom and top layers\\nof the encoder and decoder networks.\\nIn our RL algorithms, we only change\\nthe top LSTM and the softmax layer\\nof the decoder RNN as shown in red\\ndashed lines.\\n\\nwe clip this weight at 5 which controls the variance of the updates and ensures that a single example\\ndoes not disproportionately affect the gradient.\\nRNN Design: In both experiments we use deep LSTMs with two layers for the encoder and decoder\\nRNNs. The output of the bottom layer is fed to the top layer and in the decoder RNN, the output of\\nthe top layer is fed to a softmax layer of size |A|. When we implement GTD(\\xce\\xbb) to estimate V \\xcf\\x80\\xce\\xb8\\nwe use the hidden state of the bottom LSTM as \\xcf\\x86(s). When performing our policy updates, we only\\nchange the parameters of the top LSTM and the softmax layer in our decoder RNN. If we were to\\nchange the bottom LSTM too, then the state representation \\xcf\\x86(s) would also change as the policy\\nchanges. This violates the MDP framework. In other words, we treat the bottom layer as part of\\nthe environment in our MDP. To facilitate a fair comparison, we only modify the top LSTM and\\nsoftmax layers in all methods. We have illustrated this set up in Fig. 1. We note that if one is content\\nwith using the constant estimator, then one can change all parameters of the RNN.\\n\\n4.1 SOME SYNTHETIC EXPERIMENTS ON THE EUROPARL DATASET\\n\\nTo convey the main intuitions of our method, we compare our methods against other baselines on\\na synthetic task on the European parliament proceedings corpus (Koehn, 2005). We describe the\\nexperimental set up brie\\xef\\xac\\x82y, deferring details to Appendix B.1. The input sequence to the RNN was\\neach sentence in the dataset. Given an input, the goal was to reproduce the words in the input without\\nrepeating words in a list of forbidden words. The RL algorithm does not explicitly know either goal\\nof the objective but has to infer it from the stochastic rewards assigned to input output sequences in\\nthe dataset. We used a training set of 500 input-output-reward triplets for the RL methods.\\nWe initialised all methods by maximum likelihood training on 6000 input output sequences where\\nthe output sequence was the reverse of the input sequence. The maximum likelihood objective\\ncaptures part of the RL objective. This set up re\\xef\\xac\\x82ects naturally occurring practical scenarios for the\\nalgorithm where a large amount unlabelled data can be used to bootstrap a policy if the maximum\\nlikelihood and reinforcement learning objectives are at least partially aligned. We trained the RL\\nalgorithms for 200 epochs on the training set. At the end of each epoch, we generated outputs from\\nthe policy on test set of 500 inputs and scored them according to our criterion. We plot the test set\\nerror against the number of epochs for various methods in Fig. 2.\\nFig. 2(a) compares 3 methods: BPG with and without maximum likelihood initialisation and a\\nversion of BPG which does not use importance sampling. Clearly, bootstrapping an RL algorithm\\nwith ML can be advantageous especially if data is abundantly available for ML training. Further,\\nwithout importance sampling, the algorithm is not as competitive for reasons described in Section 3.\\n\\nIn all 3 cases, we used a constant estimator for (cid:98)V and \\xce\\xbb = 0.5. The dashed line indicates the\\nGTD(\\xce\\xbb) estimators for(cid:98)V . The on-line versions of the algorithms are a direct implementation of the\\n\\nperformance of ML training alone. BPG-NIS is similar to the algorithms of Ranzato et al. (2015);\\nWilliams & Zweig (2016) except that there, their methods implicitly use \\xce\\xbb = 1.\\nFig. 2(b) compares 4 methods: BPG and its on-line version OPG with constant (CONST) and\\n\\nmethod in Degris et al. (2012) which do not use the future rewards as we do. The \\xef\\xac\\x81rst observation\\nis that while GTD(\\xce\\xbb) is slightly better in the early iterations, it performs roughly the same as us-\\ning a constant estimator in the long run. Next, BPG performs signi\\xef\\xac\\x81cantly better than OPG. We\\nbelieve this is due to the following two reasons. First, the online updates assume stationarity of the\\nMDP. When this does not hold, such as in limited data instances like ours, the SGD updates can be\\n\\n7\\n\\nLSTMLSTMLSTMLSTMLSTMLSTMsoftmaxsoftmaxsoftmaxLSTMLSTMLSTMLSTMEncoderDecoder\\x0cPublished as a conference paper at ICLR 2017\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 2: Results for synthetic experiments. (a): Comparison of BPG with and without maximum likelihood\\n(ML) initialisation and BPG without importance sampling (BPG-NIS). The dotted line indicates performance\\nof ML alone.\\n(b): Comparison of BPG with its online counterparts OPG. We compare both methods using\\na constant estimator (CONST) for the value function and GTD(\\xce\\xbb).\\n(c): Comparison of BPG with different\\nvalues of \\xce\\xbb. All curves were averaged over 10 experiments where the training set was picked randomly from a\\npool. The test set was the same in all 10 experiments. The error bars indicate one standard error.\\n\\nvery noisy. Secondly, the value function estimate plays a critical role in the online version. While\\n\\nobtaining a reliable estimate (cid:98)V is reasonable in on-line settings where we can explore inde\\xef\\xac\\x81nitely\\n\\nto collect a large number of samples, it is dif\\xef\\xac\\x81cult when one only has a limited number of labelled\\nsamples. Finally, we compare BPG with different choices for \\xce\\xbb in Fig. 2(c). As noted previously,\\n\\xce\\xbb < 1 is useful with stochastic rewards, but choosing too small a value is detrimental. The optimal\\n\\xce\\xbb value may depend on the problem.\\n\\n4.2 RESTAURANT RECOMMENDATIONS\\n\\nWe use data from an on-line restaurant recommendation service. Customers log into the service\\nand chat with a human agent asking recommendations for restaurants. The agents ask a series of\\nquestions such as food preferences, group size etc. before recommending a restaurant. The goal is\\nto train a chatbot (policy) which can replace or assist the agent. For reasons explained in Section 1,\\nmaximum likelihood training alone will not be adequate. By obtaining reward labels for responses\\nproduced by various other bots, we hope to improve on a bot initialised using maximum likelihood.\\nData Collection: We collected data for RL as follows. We trained \\xef\\xac\\x81ve different RNN chatbots with\\ndifferent LSTM parameters via maximum likelihood on a dataset of 6000 conversations from this\\ndataset. The bots were trained to reproduce what the human agent said (output y) given the past\\nconversation history (input x). While the dataset is relatively small, we can still expect our bots to\\ndo reasonably well since we work in a restricted domain. Next, we generated responses from these\\nbots on 1216 separate conversations and had them scored by workers on Amazon Mechanical Turk\\n(AMT). For each response by the bots in each conversation, the workers were shown the history\\nbefore the particular response and asked to score (label) each response on a scale of 0 \\xe2\\x88\\x92 1 \\xe2\\x88\\x92 2. We\\ncollected scores from three different workers for each response and used the mean as the reward.\\nPolicies and RL Application: Next, we initialised 2 bots via maximum likelihood and then used\\nBPG to improve them using the labels collected from AMT. For the 2 bots we used the following\\nLSTM hidden state size H, word embedding size E and BPG parameters. These parameters were\\nchosen arbitrarily and are different from those of the bots used in data collection described above.\\n\\n\\xe2\\x80\\xa2 Bot-1: H = 512, E = 256.\\n\\xe2\\x80\\xa2 Bot-2: H = 400, E = 400.\\n\\nBPG: \\xce\\xbb = 0.5, GTD(\\xce\\xbb) estimator for (cid:98)V .\\nBPG: \\xce\\xbb = 0.5, constant estimator for (cid:98)V .\\n\\nTesting: We used a separate test set of 500 conversations which had a total of more than 3500 input-\\noutput (conversation history - response) pairs. For each Bot-1 and Bot-2 we generated responses\\nbefore and after applying BPG, totalling 4 responses per input. We then had them scored by workers\\non AMT using the same set up described above. The same worker labels the before-BPG and after-\\nBPG responses from the same bot. This controls spurious noise effects and allows us to conduct a\\npaired test. We collected 16, 808 before and after label pairs each for Bot-1 and Bot-2 and compare\\nthem using a paired t-test and a Wilcoxon signed rank test.\\n\\n8\\n\\nNumberofEpochs50100150200AverageReward0.30.350.40.450.50.550.6ML(NoRL)BPG+MLBPG-NIS+MLBPGNumberofEpochs50100150200AverageReward0.350.40.450.50.550.6BPG-CONSTBPG-GTD(\\xce\\xbb)OPG-CONSTOPG-GTD(\\xce\\xbb)NumberofEpochs50100150200AverageReward0.250.30.350.40.450.50.550.6\\xce\\xbb=0.1\\xce\\xbb=0.5\\xce\\xbb=0.8\\xce\\xbb=1.0\\x0cPublished as a conference paper at ICLR 2017\\n\\nMean (ML)\\n\\n0.8951 \\xc2\\xb1 0.0070\\n0.7009 \\xc2\\xb1 0.0066\\n\\nMean (BPG+ML)\\n0.9052 \\xc2\\xb1 0.0069\\n0.7317 \\xc2\\xb1 0.0066\\n\\nPaired t-test Wilcoxon\\n0.07930\\n0.00017\\n\\n0.10296\\n0.00007\\n\\nBot-1\\nBot-2\\n\\nTable 1: The results on the Mechanical Turk experiments using the restaurant dataset. The \\xef\\xac\\x81rst two columns\\nare the mean labels of all responses before and after applying BPG on the bots initialised via maximum like-\\nlihood. The last two columns are the p-values using a paired t-test and a paired Wilcoxon signed rank test.\\nFor both Bot-1 and Bot-2, we obtained 16,808 before and after responses scored by the same worker. Bot-2 is\\nstatistically signi\\xef\\xac\\x81cant at the 10% level on both tests while Bot-1 is signi\\xef\\xac\\x81cant on the Wilcoxon test.\\n\\nResults: The results are shown in Table 1. The improvements on Bot-2 are statistically signi\\xef\\xac\\x81cant\\nat the 10% level on both tests, while Bot-1 is signi\\xef\\xac\\x81cant on the Wilcoxon test. The large p-values for\\nBot-1 are due to the noisy nature of AMT experiments and we believe that we can attain signi\\xef\\xac\\x81cance\\nif we collect more labels which will reduce the standard error in both tests. In Appendix B.2 we\\npresent some examples of conversation histories and the responses generated by the bots before and\\nafter applying BPG. We qualitatively discuss speci\\xef\\xac\\x81c kinds of issues that we were able to overcome\\nvia reinforcement learning.\\n\\n5 CONCLUSION\\n\\nWe presented a policy gradient method for batch reinforcement learning to train chatbots. The data\\nto this algorithm are input-output sequences generated using other chatbots/humans and stochastic\\nrewards for each output in the dataset. This setting arises in many applications, such as customer\\nservice systems, where there is usually an abundance of unlabelled data, but labels (rewards) are\\nexpensive to obtain and can be noisy. Our algorithm is able to ef\\xef\\xac\\x81ciently use minimal labelled data\\nto improve chatbots previously trained through maximum likelihood on unlabelled data. While our\\nmethod draws its ideas from previous policy gradient work in the RL and NLP literature, there are\\nsome important distinctions that contribute to its success in the settings of interest for this work.\\nVia importance sampling we ensure that the probability of an action is properly accounted for in\\noff-policy updates. By explicitly working in the batch setting, we are able to use knowledge of\\nfuture actions and rewards to converge faster to the optimum. Further, we use the unlabelled data\\nto initialise our method and also learn a reasonable behaviour policy. Our method outperforms\\nbaselines on a series of synthetic and real experiments.\\nThe ideas presented in this work extend beyond chatbots. They can be used in applications such\\nas question answering, generating image descriptions and machine translation where an output sen-\\ntence generated by a policy is scored by a human labeller to provide a weak supervision signal.\\n\\nACKNOWLEDGEMENTS\\nWe would like to thank Christoph Dann for the helpful conversations and Michael Armstrong for\\nhelping us with the Amazon Mechanical Turk experiments.\\n\\nREFERENCES\\nShun-Ichi Amari. Natural gradient works ef\\xef\\xac\\x81ciently in learning. Neural computation, 10(2):251\\xe2\\x80\\x93276, 1998.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to\\n\\nalign and translate. arXiv preprint arXiv:1409.0473, 2014.\\n\\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville,\\nand Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086,\\n2016.\\n\\nRichard Bellman. Dynamic programming and Lagrange multipliers. Proceedings of the National Academy of\\n\\nSciences, 42(10):767\\xe2\\x80\\x93769, 1956.\\n\\nVivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291\\xe2\\x80\\x93294,\\n\\n1997.\\n\\nKyunghyun Cho, Bart Van Merri\\xc2\\xa8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. arXiv preprint arXiv:1406.1078, 2014.\\n\\n9\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nThomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839,\\n\\n2012.\\n\\nDavid Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam\\nLally, J William Murdock, Eric Nyberg, John Prager, et al. Building watson: An overview of the deepqa\\nproject. AI magazine, 31(3):59\\xe2\\x80\\x9379, 2010.\\n\\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement\\n\\nlearning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and\\nPhil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing\\nSystems, pp. 1693\\xe2\\x80\\x931701, 2015.\\n\\nSepp Hochreiter and J\\xc2\\xa8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\\xe2\\x80\\x931780,\\n\\n1997.\\n\\nNal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, volume 3, pp. 413,\\n\\n2013.\\n\\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\\n\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128\\xe2\\x80\\x933137, 2015.\\n\\nIn\\n\\nPhilipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pp.\\n\\n79\\xe2\\x80\\x9386, 2005.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function\\n\\nfor neural conversation models. arXiv preprint arXiv:1510.03055, 2015.\\n\\nJiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. Deep reinforcement learning for dialogue generation.\\n\\narXiv preprint arXiv:1606.01541, 2016.\\n\\nHamid Reza Maei. Gradient temporal-difference learning algorithms. University of Alberta, 2011.\\nTomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc2\\xb4at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural\\n\\nnetwork based language model. In Interspeech, volume 2, pp. 3, 2010.\\n\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using\\n\\ndeep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.\\n\\nMarc\\xe2\\x80\\x99Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with\\n\\nrecurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\\n\\nMrinmaya Sachan, Avinava Dubey, and Eric P Xing. Science question answering using instructional materials.\\n\\narXiv preprint arXiv:1602.04375, 2016.\\n\\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun\\nNie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of conversa-\\ntional responses. arXiv preprint arXiv:1506.06714, 2015.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\\n\\nAdvances in neural information processing systems, pp. 3104\\xe2\\x80\\x933112, 2014.\\n\\nIn\\n\\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9\\xe2\\x80\\x9344,\\n\\n1988.\\n\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press\\n\\nCambridge, 1998.\\n\\nRichard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for\\n\\nreinforcement learning with function approximation. In NIPS, volume 99, pp. 1057\\xe2\\x80\\x931063, 1999.\\n\\nRichard Stuart Sutton. Temporal credit assignment in reinforcement learning. University of Massachusetts,\\n\\nAmherst, 1984.\\n\\nAdith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit\\nfeedback. In Proceedings of the 32nd International Conference on Machine Learning, pp. 814\\xe2\\x80\\x93823, 2015.\\nCsaba Szepesv\\xc2\\xb4ari. Algorithms for reinforcement learning. Synthesis lectures on arti\\xef\\xac\\x81cial intelligence and\\n\\nmachine learning, 4(1):1\\xe2\\x80\\x93103, 2010.\\n\\nSeiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework\\nfor deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-\\nninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.\\n\\nJason D Williams and Geoffrey Zweig. End-to-end lstm-based dialog control optimized with supervised and\\n\\nreinforcement learning. arXiv preprint arXiv:1606.01269, 2016.\\n\\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\\n\\nMachine learning, 8(3-4):229\\xe2\\x80\\x93256, 1992.\\n\\n10\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nAPPENDIX\\nA IMPLEMENTATION OF GTD(\\xce\\xbb)\\nWe present the details of the GTD(\\xce\\xbb) algorithm (Maei, 2011) to estimate a value function in Al-\\ngorithm 2. However, while Maei (2011) give an on-line version we present the batch version\\nhere where the future rewards of an episode are known. We use a parametrisation of the form\\n\\n(cid:98)V (s) = (cid:98)V\\xce\\xbe(s) = \\xcf\\x83(\\xce\\xbe(cid:62)\\xcf\\x86(s)) where \\xce\\xbe \\xe2\\x88\\x88 Rd is the parameter to be estimated. \\xcf\\x83(z) = 1/(1 + e\\xe2\\x88\\x92z) is\\n\\nthe logistic function.\\nThe algorithm requires two step sizes \\xce\\xb1(cid:48), \\xce\\xb1(cid:48)(cid:48) below for the updates to \\xce\\xbe and the ancillary parameter\\nw. Following the recommendations in Borkar (1997), we use \\xce\\xb1(cid:48)(cid:48) (cid:28) \\xce\\xb1. In our implementations,\\nwe used \\xce\\xb1(cid:48) = 10\\xe2\\x88\\x925 and \\xce\\xb1(cid:48)(cid:48) = 10\\xe2\\x88\\x926. When we run BPG, we perform steps (a)-(f) of Algorithm 2\\nin step (iii) of Algorithm 1 and the last two update steps of Algorithm 2 in the last update step of\\nAlgorithm 1.\\n\\nThe gradient and Hessian of (cid:98)V\\xce\\xbe have the following forms,\\n\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s) = (cid:98)V\\xce\\xbe(s)(1 \\xe2\\x88\\x92(cid:98)V\\xce\\xbe(s))\\xcf\\x86(s),\\n\\xce\\xbe(cid:98)V\\xce\\xbe(s) \\xc2\\xb7 w =\\n\\n(cid:104)(cid:98)V\\xce\\xbe(s)(1 \\xe2\\x88\\x92(cid:98)V\\xce\\xbe(s))(1 \\xe2\\x88\\x92 2(cid:98)V\\xce\\xbe(s))(\\xcf\\x86(s)(cid:62)w)\\n\\nThe Hessian product in step (d) of Algorithm 2 can be computed in O(d) time via,\\n\\n\\xce\\xbe(cid:98)V\\xce\\xbe(s) = (cid:98)V\\xce\\xbe(s)(1 \\xe2\\x88\\x92(cid:98)V\\xce\\xbe(s))(1 \\xe2\\x88\\x92 2(cid:98)V\\xce\\xbe(s))\\xcf\\x86(s)\\xcf\\x86(s)(cid:62).\\n\\n\\xe2\\x88\\x872\\n\\n\\xe2\\x88\\x872\\n\\n\\xcf\\x86(s).\\n\\n(cid:105)\\n\\ni=1, step sizes \\xce\\xb1(cid:48), \\xce\\xb1(cid:48)(cid:48), return coef\\xef\\xac\\x81cient \\xce\\xbb, initial \\xce\\xbe0.\\n\\nAlgorithm 2 GTD(\\xce\\xbb)\\nGiven: Data {(xi, yi, ri)}n\\n\\n\\xe2\\x80\\x93 Set \\xce\\xbe \\xe2\\x86\\x90 \\xce\\xbe0, w \\xe2\\x86\\x90 0.\\n\\xe2\\x80\\x93 For each epoch k = 1, 2, . . .\\n(cid:73) Set \\xe2\\x88\\x86\\xce\\xbe \\xe2\\x86\\x90 0, \\xe2\\x88\\x86w \\xe2\\x86\\x90 0.\\n(cid:73) For each episode i = 1, . . . , n\\nT +1 \\xe2\\x86\\x90 ri, g\\xce\\xbb\\n\\nt\\n\\nt\\n\\n(a) g\\xce\\xbb\\n\\nT +1 \\xe2\\x86\\x90 0\\n\\n|s(i)\\nt )/q(a(i)\\n\\n|s(i)\\nt ) for t = 1, . . . , T (i).\\n\\nt \\xe2\\x86\\x90 \\xcf\\x81t\\nt \\xe2\\x86\\x90 \\xcf\\x81t\\n(b) q\\xce\\xbb\\n(c) \\xce\\xb4t \\xe2\\x86\\x90 g\\xce\\xbb\\n\\n\\xe2\\x80\\xa2 Set r\\xce\\xbb\\n\\xe2\\x80\\xa2 \\xcf\\x81t \\xe2\\x86\\x90 \\xcf\\x80\\xce\\xb8(a(i)\\n\\xe2\\x80\\xa2 For each time step in reverse t = T (i), . . . , 1:\\n\\nT +1 \\xe2\\x86\\x90 0, q\\xce\\xbb\\n(cid:16)\\n(cid:17)\\n(1 \\xe2\\x88\\x92 \\xce\\xbb)(cid:98)V\\xce\\xbe(s(i)\\n(cid:16)\\n(cid:17)\\n(1 \\xe2\\x88\\x92 \\xce\\xbb)\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\nt \\xe2\\x88\\x92(cid:98)V\\xce\\xbe(s(i)\\nt )(cid:1)\\xe2\\x88\\x872\\n(d) ht \\xe2\\x86\\x90(cid:0)\\xce\\xb4t \\xe2\\x88\\x92 w(cid:62)\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\n\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\n(cid:0)\\xce\\xb4t \\xe2\\x88\\x92 w(cid:62)\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\nt )(cid:1)\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\nt ) \\xc2\\xb7 w\\n(cid:16)\\n\\xce\\xb4t\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\nt w(cid:62)\\xe2\\x88\\x87\\xce\\xbe(cid:98)V\\xce\\xbe(s(i)\\nt ) \\xe2\\x88\\x92 q\\xce\\xbb\\n\\n(e) \\xe2\\x88\\x86w \\xe2\\x86\\x90 \\xe2\\x88\\x86w + 1\\n(f) \\xe2\\x88\\x86\\xce\\xbe \\xe2\\x86\\x90 \\xe2\\x88\\x86\\xce\\xbe + 1\\n\\nt+1) + \\xce\\xbb\\xcf\\x81tr\\xce\\xbb\\n\\nt+1) + \\xce\\xbbq\\xce\\xbb\\n\\nt )\\n\\nT (i)\\n\\nT (i)\\n\\nt+1\\n\\nt+1\\n\\n(cid:17)\\n\\nt )\\nt ) \\xe2\\x88\\x92 ht\\n\\n(cid:73) w \\xe2\\x86\\x90 w + \\xce\\xb1(cid:48)(cid:48)\\xe2\\x88\\x86w.\\n(cid:73) \\xce\\xbe \\xe2\\x86\\x90 \\xce\\xbe + \\xce\\xb1(cid:48)\\xe2\\x88\\x86\\xce\\xbe.\\n\\nB ADDENDUM TO EXPERIMENTS\\n\\nB.1 DETAILS OF THE SYNTHETIC EXPERIMENT SET UP\\n\\nGiven an input and output sequence, we used the average of \\xef\\xac\\x81ve Bernoulli rewards Bern(r), where\\nthe parameter r was r = 0.75 \\xc3\\x97 rr + 0.25 \\xc3\\x97 rf. Here rr was the fraction of common words in the\\ninput and output sequences while rf = 0.01pf where pf is the fraction of forbidden words in the\\ndataset. As the forbidden words, we used the 50 most common words in the dataset. So if an input\\n\\n11\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nhad 10 words of which 2 were forbidden, an output sequence repeating 7 of the allowed words and\\n1 forbidden word would receive an expected score of 0.75 \\xc3\\x97 (8/10) + 0.25 \\xc3\\x97 0.01(1/8) = 0.7406.\\nThe training and testing set for reinforcement learning were obtained as follows. We trained\\n4 bots using maximum likelihood on 6000 input output sequences as indicated in Section 4.1.\\nThe LSTM hidden state size H and word embedding size E for the 4 bots were, (H, E) =\\n(256, 128), (128, 64), (64, 32), (32, 16). The vocabulary size was |A| = 12000. We used these bots\\nto generate outputs for 500 different input sequences each. This collection of input and output pairs\\nwas scored stochastically as described above to produce a pool of 2000 input-output-score triplets.\\nFrom this pool we use a \\xef\\xac\\x81xed set of 500 triplets for testing across all our experiments. From the\\nremaining 1500 data points, we randomly select 500 for training for each execution of an algorithm.\\nFor all RL algorithms, we used an LSTM with 16 layers and 16 dimensional word embeddings.\\n\\nB.2 ADDENDUM TO THE AMT RESTAURANT RECOMMENDATIONS EXPERIMENT\\n\\nMORE DETAILS ON THE EXPERIMENTAL SET UP\\n\\nWe collected the initial batch of training data for RL as follows: We trained, via maximum likelihood\\non 6000 conversations, \\xef\\xac\\x81ve RNN bots whose LSTM hidden size H and word embedding size E were\\n(H, E) = (512, 512), (256, 256), (128, 128), (512, 256), (256, 64). The inputs x were all words\\nfrom the history of the conversation truncated at length 64, i.e.\\nthe most recent 64 words in the\\nconversation history. The outputs were the actual responses of the agent which were truncated to\\nlength 64. As the vocabulary we use the |A| = 4000 most commonly occurring words in the dataset\\nand replace the rest with an <UNK> token.\\nUsing the bots trained this way we generate responses on 1216 separate conversations. This data\\nwas sent to AMT workers who were asked to label the conversations on the following scale.\\n\\nforward.\\n\\n\\xe2\\x80\\xa2 2: The response is coherent and appropriate given the history and advances the conversation\\n\\xe2\\x80\\xa2 1: The response has some minor \\xef\\xac\\x82aws but is discernible and appropriate.\\n\\xe2\\x80\\xa2 0: The response is either completely incoherent or inappropriate and fails to advance the\\n\\nconversation forward.\\n\\nSOME QUALITATIVE RESULTS\\n\\nIn Tables 2 and 3 we have presented some examples. The text in black/grey shows the conversation\\nhistory, the response in blue is by the bot trained via maximum likelihood (ML) alone and in red is\\nthe bot after improvement using our BPG reinforcement learning algorithm.\\nThe \\xef\\xac\\x81rst two examples of Table 2 present examples where the ML algorithm repeated generic ques-\\ntions (on budget, group size etc.) even though they had already been answered previously. After\\napplying BPG, we are able to correct such issues, even though there are some grammatical errors.\\nIn the second, third and fourth example, we see that the ML+BPG bot is able to take context into\\nconsideration well when responding. For example, the customer asks for oriental/Mexican/Italian\\nfood. While the ML bot doesn\\xe2\\x80\\x99t take this into consideration, the ML+BPG bot is able to provide\\nrelevant answers. However, in the third example, the name of the restaurant suggests that the food\\nmight be Indian and not Mexican. In the \\xef\\xac\\x81nal example of Table 2 the customer asks a direct ques-\\ntion about smoking. The ML bot provides an irrelevant answer where as the ML+BPG bot directly\\nresponds to the question.\\nIn some examples, the ML bot had a tendency to produce sentences that were grammatically correct\\nbut nonsensical, sensible but grammatically incorrect, or just complete gibberish. We were able to\\ncorrect such issues via RL. The \\xef\\xac\\x81rst three examples of Table 3 present such cases. Occasionally the\\nopposite happened. The last example of Table 3 is one such instance.\\n\\n12\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nCustomer:\\nAgent:\\nCustomer:\\nML:\\nML+BPG:\\n\\nHi there, I\\xe2\\x80\\x99m looking for a restaurant recommendation.\\nHow many in your party?\\n5, including children.\\n\\nHow many people will be joining you?\\nThat is a fun size, and what is your budget joining you?\\n\\n[0]\\n[1.5]\\n\\nI always do\\n\\n...\\nCustomer:\\noriental tonight.\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nfriendly ear.\\nML:\\nML+BPG:\\non site such?\\n\\nCustomer:\\nAgent:\\nCustomer:\\n...\\nCustomer:\\nAgent:\\nCustomer:\\nML:\\nAmerican food.\\nML+BPG:\\nbar?\\n\\nHey... Sorry, it\\xe2\\x80\\x99s the anniversary of my wife\\xe2\\x80\\x99s death.\\n\\nI will do my best to find just what you need.\\n\\nI\\xe2\\x80\\x99m sorry to hear that.\\nThank you very much.\\nWhat kind of budget are you looking at?\\nDoesn\\xe2\\x80\\x99t matter...\\nWill it be just you, or are others going to join you?\\nIt\\xe2\\x80\\x99s just me tonight, but if you are off soon I wouldn\\xe2\\x80\\x99t mind a\\n\\n$100 sounds like a good number.\\n\\nNo pressure.\\n\\nWhat type of budget are you looking to stay within?\\nHow about:\\n\\nCaf\\xc2\\xb4e Hungry <UNK> ?\\n\\nThey serve sea American to oriental\\n\\n[0.33]\\n\\n[1.33]\\n\\nI would love to find a great Mexican restaurant!\\nGreat, and will anybody be joining you?\\nThere will be a total of 3 of us.\\n\\nCould you find me a place with outdoor seating?\\nHow about:\\nEh, I\\xe2\\x80\\x99ve heard that one isn\\xe2\\x80\\x99t great.\\n\\nThe Painted Burrito.\\n\\nThey serve Mexican food.\\n\\nCan you please suggest another?\\n\\nHow about:\\n\\nThe Hungry .w seafood restaurant?\\n\\nthey serve sea and\\n\\n[0.33]\\n\\nI am happy to, one moment.\\n\\nThey serve Mexican food.\\n\\nHow about:\\n\\nDiva Indian Bistro Boston\\n\\n[1.67]\\n\\nHello, I\\xe2\\x80\\x99m looking for a restaurant.\\nI can help!\\n\\nPlease tell me what kind of foods you most enjoy.\\n\\nI see\\n\\nI would like a restaurant that serves Italian food.\\nAre you looking for something a little less expensive or are you\\n\\nI am within a budget, 20 to 50 dollars.\\nI highly recommend The Redhouse Restaurant.\\nIs there another restaurant?\\n\\nthe restaurant would also need to host a\\n\\nIt serves ...\\n\\nCustomer:\\nAgent:\\nyou are in the Cambridge area.\\nCustomer:\\nAgent:\\nlooking for a high end restaurant?\\nCustomer:\\nAgent:\\nCustomer:\\nparty size of 5 people.\\nML:\\nML+BPG:\\nand Italian food.\\n\\nHow about:\\nHow about:\\n\\nAmuse?\\nOcean prime - Boston Seaport?\\n\\nThey serve French and American food.\\n\\nThey serve sea, American\\n\\n[0]\\n\\n[2]\\n\\n...\\nAgent:\\nCustomer:\\nAgent:\\nwith today?\\nCustomer:\\nML:\\nML+BPG:\\n\\nHow about Cafe at Taj Boston?\\nOh, I just ate there last night.\\nTry Mela Restaurant at 578 Tremont St.\\n\\nAnything else?\\n\\nAnything else I can help you\\n\\nThat sounds nice.\\n\\nDo they allow smoking?\\n\\nThis sure is within your budget for all.\\nNo.\\n\\n[0.5]\\n[1.5]\\n\\nTable 2: Some qualitative results on the AMT experiment using restaurant data. In each cell, the text in\\nblack/grey show the history of a conversation between the customer and a human agent. The dots . . . mean\\nthat the part of the conversation history is hidden. The blue response is by the bot trained using maximum\\nlikelihood (ML) alone. The red response is by the bot improved with BPG after ML. The numbers on the right\\nof the blue/red responses are the average scores each response received from 2-3 AMT workers.\\n\\n13\\n\\n\\x0cPublished as a conference paper at ICLR 2017\\n\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nML:\\nML+BPG:\\n\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nML:\\nML+BPG:\\n\\nCustomer:\\nAgent:\\nCustomer:\\n...\\nAgent:\\nCustomer:\\nML:\\ndining?\\nML+BPG:\\n\\nHow may I help you?\\nI\\xe2\\x80\\x99d like to find a French restaurant.\\nDo you have a budget in mind?\\nBetween $50 and $75.\\nHow many will be in your party?\\nJust me.\\n\\nAnd how will you be dining?\\nHow will you be traveling to the restaurant?\\n\\nHello, how may I help you?\\nWe are kinda hungry, any suggestion in mind?\\nWhat type of cuisine do you like?\\nI want them Americano food.\\n\\nVery evening.\\nExcellent choice.\\n\\nWill you be dining alone?\\n\\nHow many people will be in your party?\\n\\nI\\xe2\\x80\\x99d like to find a good local restaurant.\\nWhat type of cuisine would you like?\\nI have a hankering for Indian curry.\\n\\nWill you be needing parking?\\nWe\\xe2\\x80\\x99ll be driving altogether, so yes.\\n\\nThank you for 1.\\n\\nHow about:\\n\\nT. w your curry this place should be\\n\\nHow about:\\n\\nThe Maharaja?\\n\\nThey serve Indian and American food.\\n\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nAgent:\\nCustomer:\\nML:\\nML+BPG:\\nand a place.\\n\\nHow can I help you today?\\nI would like a restaurant recommendation.\\nOkay what type of resturant?\\nChinese would be great.\\nWho will be dining, how will you be travelling?\\nJust me and by bus.\\n\\nOkay, thanks.\\nDo you want a wonderful place evening, or whats your party adults,\\n\\nDo you have a budget in mind?\\n\\n[0]\\n[2]\\n\\n[1]\\n[2]\\n\\n[0]\\n[2]\\n\\n[2]\\n\\n[0]\\n\\nTable 3: Qualitative results on the AMT experiment using restaurant data, continued. See caption under\\nTable 2 for more details.\\n\\n14\\n\\n\\x0c']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hmmm. One off, but what the heck. Save it. \n",
    "pkl.dump(all_texts, open('all_texts_chatbot_litreview.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_texts,columns=['orig_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import LineTokenizer\n",
    "    from nltk.tokenize import WhitespaceTokenizer\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    \n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    sw = list(stopwords.words())\n",
    "    extra_stops = ['R', '', ' ', 'abstract', 'keywords', 'introduction', 'figure','morgan', 'harvey',\n",
    "                   'david','elsweiler','northumbria','university','newcastle','united','kingdom','university',\n",
    "                   'regensburg','germany', 'h', 'k', 'f', 'b', 'user', 'g', 'use']\n",
    "    for word in extra_stops:\n",
    "        sw.append(word)\n",
    "    \n",
    "    # Step 1 - Clean up unicode\n",
    "    clean_string = ''\n",
    "    doc = []\n",
    "    #for x in text:\n",
    "    #    if ord(x) <= 128:\n",
    "    #        clean_string += x\n",
    "    #clean_string = unicodedata.normalize('NFKD', clean_string.encode('utf-8', 'replace')).encode('ascii','replace')\n",
    "            \n",
    "    # Tokenize each line to get rid of the line carriages\n",
    "    lines = LineTokenizer().tokenize(text.lower())\n",
    "    \n",
    "    clean_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('e-mail') or line.startswith('doi') or line.startswith('For all other uses, contact') or line.find(' acm. isbn ') > 0:\n",
    "            pass\n",
    "        else:\n",
    "            line_str = ''\n",
    "            for char in line:\n",
    "                #if ord(char) <= 127:\n",
    "                if (char in string.ascii_letters) or char == ' ':\n",
    "                    line_str += char\n",
    "        \n",
    "        # Clean up other undesirable characters\n",
    "            if line_str != ' ' and line_str.rstrip().lstrip() not in sw:\n",
    "                clean_lines.append(line_str)\n",
    "    \n",
    "    # Tokenize the lines\n",
    "    for clean_line in clean_lines:\n",
    "        tokens = WhitespaceTokenizer().tokenize(clean_line)\n",
    "        stopped_tokens = [i for i in tokens if not i in sw]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        [doc.append(i) for i in stemmed_tokens]\n",
    "        \n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(df.shape[1], 'clean_text', df.orig_text.apply(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in df.clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel4 = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel3 = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.015*\"chatbot\" + 0.014*\"question\" + 0.013*\"answer\" + 0.008*\"iot\"'), (1, u'0.000*\"learn\" + 0.000*\"model\" + 0.000*\"system\" + 0.000*\"use\"'), (2, u'0.000*\"messag\" + 0.000*\"learn\" + 0.000*\"respons\" + 0.000*\"use\"'), (3, u'0.022*\"model\" + 0.018*\"gener\" + 0.017*\"dialogu\" + 0.013*\"learn\"'), (4, u'0.018*\"system\" + 0.015*\"use\" + 0.010*\"question\" + 0.010*\"answer\"'), (5, u'0.037*\"learn\" + 0.014*\"polici\" + 0.013*\"deep\" + 0.012*\"network\"'), (6, u'0.022*\"model\" + 0.022*\"respons\" + 0.015*\"word\" + 0.014*\"topic\"'), (7, u'0.039*\"messag\" + 0.022*\"context\" + 0.019*\"respons\" + 0.015*\"data\"'), (8, u'0.049*\"bot\" + 0.018*\"revert\" + 0.015*\"interact\" + 0.014*\"edit\"'), (9, u'0.000*\"learn\" + 0.000*\"gener\" + 0.000*\"model\" + 0.000*\"system\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=10, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[nikolao, polatidi, supervisor, dr, peter, han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...</td>\n",
       "      <td>[r, v, v, x, r, review, confer, paper, iclr, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Response Selection with Topic Clues for Retrie...</td>\n",
       "      <td>[respons, select, topic, clue, retrievalbas, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep Reinforcement Learning for Dialogue Gener...</td>\n",
       "      <td>[deep, reinforc, learn, dialogu, gener, jiwei,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic Aware Neural Response Generation\\n\\nChen...</td>\n",
       "      <td>[topic, awar, neural, respons, gener, chen, xi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nEven Good Bots Fight: The Case of Wikiped...</td>\n",
       "      <td>[even, good, bot, fight, case, wikipedia, mile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n1\\n2\\n\\n \\n\\n \\n \\...</td>\n",
       "      <td>[p, v, v, x, r, chatbot, greet, humancomput, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Detecting Context Dependent Messages in a Conv...</td>\n",
       "      <td>[detect, context, depend, messag, convers, env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Applying Chatbots to the Internet of Things: \\...</td>\n",
       "      <td>[appli, chatbot, internet, thing, opportun, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Visual Dialog\\n\\nAbhishek Das1, Satwik Kottur2...</td>\n",
       "      <td>[visual, dialog, abhishek, satwik, kottur, khu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sequential Match Network: A New Architecture f...</td>\n",
       "      <td>[sequenti, match, network, new, architectur, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hierarchical Recurrent Attention Network for R...</td>\n",
       "      <td>[hierarch, recurr, attent, network, respons, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n6\\n2\\n\\n \\n \\...</td>\n",
       "      <td>[v, v, x, r, deep, reinforc, learn, overview, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7\\n1\\n0\\n2\\n\\n \\n\\nb\\ne\\nF\\n0\\n1\\n\\n \\n\\n \\n \\...</td>\n",
       "      <td>[v, v, x, r, publish, confer, paper, iclr, bat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            orig_text  \\\n",
       "0    \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...   \n",
       "1   6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...   \n",
       "2   Response Selection with Topic Clues for Retrie...   \n",
       "3   Deep Reinforcement Learning for Dialogue Gener...   \n",
       "4   Topic Aware Neural Response Generation\\n\\nChen...   \n",
       "5    \\n\\nEven Good Bots Fight: The Case of Wikiped...   \n",
       "6   6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n1\\n2\\n\\n \\n\\n \\n \\...   \n",
       "7   Detecting Context Dependent Messages in a Conv...   \n",
       "8   Applying Chatbots to the Internet of Things: \\...   \n",
       "9   Visual Dialog\\n\\nAbhishek Das1, Satwik Kottur2...   \n",
       "10  Sequential Match Network: A New Architecture f...   \n",
       "11  Hierarchical Recurrent Attention Network for R...   \n",
       "12  7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n6\\n2\\n\\n \\n \\...   \n",
       "13  7\\n1\\n0\\n2\\n\\n \\n\\nb\\ne\\nF\\n0\\n1\\n\\n \\n\\n \\n \\...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   [nikolao, polatidi, supervisor, dr, peter, han...  \n",
       "1   [r, v, v, x, r, review, confer, paper, iclr, l...  \n",
       "2   [respons, select, topic, clue, retrievalbas, c...  \n",
       "3   [deep, reinforc, learn, dialogu, gener, jiwei,...  \n",
       "4   [topic, awar, neural, respons, gener, chen, xi...  \n",
       "5   [even, good, bot, fight, case, wikipedia, mile...  \n",
       "6   [p, v, v, x, r, chatbot, greet, humancomput, c...  \n",
       "7   [detect, context, depend, messag, convers, env...  \n",
       "8   [appli, chatbot, internet, thing, opportun, ar...  \n",
       "9   [visual, dialog, abhishek, satwik, kottur, khu...  \n",
       "10  [sequenti, match, network, new, architectur, m...  \n",
       "11  [hierarch, recurr, attent, network, respons, g...  \n",
       "12  [v, v, x, r, deep, reinforc, learn, overview, ...  \n",
       "13  [v, v, x, r, publish, confer, paper, iclr, bat...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tab_filter_regex(text):\n",
    "    \"\"\"\n",
    "    Clean tabs out of a text field\n",
    "    \"\"\"\n",
    "    # Get rid of digits and extraneous characters\n",
    "    p = re.sub(r'(&(nbsp|amp);)+',' ',str(text))\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned = re.sub(r'\\s+', ' ', p)\n",
    "  \n",
    "    return cleaned.lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_abstract(text):\n",
    "    abstract_start = text.lower().find('abstract')\n",
    "    introduction_start = text[abstract_start:].lower().find('introduction')\n",
    "    return tab_filter_regex(text[(abstract_start + 9):(introduction_start - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now get the rest of the text\n",
    "def get_rest_of_text(text):\n",
    "    abstract_start = text.lower().find('abstract')\n",
    "    introduction_start = text[abstract_start:].lower().find('introduction')\n",
    "    return tab_filter_regex(text[introduction_start + 12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(df.shape[1],'abstract',df.orig_text.apply(get_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.insert(df.shape[1],'body',df.orig_text.apply(get_rest_of_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[nikolao, polatidi, supervisor, dr, peter, han...</td>\n",
       "      <td>The communication of potential students with a...</td>\n",
       "      <td>ered by my parents. 3 Table of Contents Chapte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...</td>\n",
       "      <td>[r, v, v, x, r, review, confer, paper, iclr, l...</td>\n",
       "      <td>In this paper, we apply a general deep learnin...</td>\n",
       "      <td>��cient attention mechanism in order to genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Response Selection with Topic Clues for Retrie...</td>\n",
       "      <td>[respons, select, topic, clue, retrievalbas, c...</td>\n",
       "      <td>We consider incorporating topic information in...</td>\n",
       "      <td>are determined by themselves as well as the me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep Reinforcement Learning for Dialogue Gener...</td>\n",
       "      <td>[deep, reinforc, learn, dialogu, gener, jiwei,...</td>\n",
       "      <td>Recent neural models of dialogue generation of...</td>\n",
       "      <td>erence, and ease of answering (re- lated to fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic Aware Neural Response Generation\\n\\nChen...</td>\n",
       "      <td>[topic, awar, neural, respons, gener, chen, xi...</td>\n",
       "      <td>We consider incorporating topic information in...</td>\n",
       "      <td>the generation of words in decoding. To increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nEven Good Bots Fight: The Case of Wikiped...</td>\n",
       "      <td>[even, good, bot, fight, case, wikipedia, mile...</td>\n",
       "      <td>In recent years, there has been a huge increas...</td>\n",
       "      <td>umb” bots may give rise to complex interaction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n1\\n2\\n\\n \\n\\n \\n \\...</td>\n",
       "      <td>[p, v, v, x, r, chatbot, greet, humancomput, c...</td>\n",
       "      <td>Both dialogue systems and chatbots aim at putt...</td>\n",
       "      <td>ed, as they might be put to use in the human-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Detecting Context Dependent Messages in a Conv...</td>\n",
       "      <td>[detect, context, depend, messag, convers, env...</td>\n",
       "      <td>While automatic response generation for buildi...</td>\n",
       "      <td>context dependent messages. With the character...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Applying Chatbots to the Internet of Things: \\...</td>\n",
       "      <td>[appli, chatbot, internet, thing, opportun, ar...</td>\n",
       "      <td>��Internet of Things (IoT) is emerging as a si...</td>\n",
       "      <td>ssed. Keywords—Internet of Things, Chatbots, H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Visual Dialog\\n\\nAbhishek Das1, Satwik Kottur2...</td>\n",
       "      <td>[visual, dialog, abhishek, satwik, kottur, khu...</td>\n",
       "      <td>We introduce the task of Visual Dialog, which ...</td>\n",
       "      <td>baselines. We propose a retrieval-based evalua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sequential Match Network: A New Architecture f...</td>\n",
       "      <td>[sequenti, match, network, new, architectur, m...</td>\n",
       "      <td>We study response selection for multi-turn con...</td>\n",
       "      <td>ng opera- tions. The vectors are then accumula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hierarchical Recurrent Attention Network for R...</td>\n",
       "      <td>[hierarch, recurr, attent, network, respons, g...</td>\n",
       "      <td>We study multi-turn response generation in cha...</td>\n",
       "      <td>terance level at- tention respectively. With t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n6\\n2\\n\\n \\n \\...</td>\n",
       "      <td>[v, v, x, r, deep, reinforc, learn, overview, ...</td>\n",
       "      <td>We give an o</td>\n",
       "      <td>cent exciting achievements of deep reinforceme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7\\n1\\n0\\n2\\n\\n \\n\\nb\\ne\\nF\\n0\\n1\\n\\n \\n\\n \\n \\...</td>\n",
       "      <td>[v, v, x, r, publish, confer, paper, iclr, bat...</td>\n",
       "      <td>We study reinforcement learning of chatbots wi...</td>\n",
       "      <td>d customer service support can be scored by qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            orig_text  \\\n",
       "0    \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...   \n",
       "1   6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...   \n",
       "2   Response Selection with Topic Clues for Retrie...   \n",
       "3   Deep Reinforcement Learning for Dialogue Gener...   \n",
       "4   Topic Aware Neural Response Generation\\n\\nChen...   \n",
       "5    \\n\\nEven Good Bots Fight: The Case of Wikiped...   \n",
       "6   6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n1\\n2\\n\\n \\n\\n \\n \\...   \n",
       "7   Detecting Context Dependent Messages in a Conv...   \n",
       "8   Applying Chatbots to the Internet of Things: \\...   \n",
       "9   Visual Dialog\\n\\nAbhishek Das1, Satwik Kottur2...   \n",
       "10  Sequential Match Network: A New Architecture f...   \n",
       "11  Hierarchical Recurrent Attention Network for R...   \n",
       "12  7\\n1\\n0\\n2\\n\\n \\n\\nn\\na\\nJ\\n \\n\\n6\\n2\\n\\n \\n \\...   \n",
       "13  7\\n1\\n0\\n2\\n\\n \\n\\nb\\ne\\nF\\n0\\n1\\n\\n \\n\\n \\n \\...   \n",
       "\n",
       "                                           clean_text  \\\n",
       "0   [nikolao, polatidi, supervisor, dr, peter, han...   \n",
       "1   [r, v, v, x, r, review, confer, paper, iclr, l...   \n",
       "2   [respons, select, topic, clue, retrievalbas, c...   \n",
       "3   [deep, reinforc, learn, dialogu, gener, jiwei,...   \n",
       "4   [topic, awar, neural, respons, gener, chen, xi...   \n",
       "5   [even, good, bot, fight, case, wikipedia, mile...   \n",
       "6   [p, v, v, x, r, chatbot, greet, humancomput, c...   \n",
       "7   [detect, context, depend, messag, convers, env...   \n",
       "8   [appli, chatbot, internet, thing, opportun, ar...   \n",
       "9   [visual, dialog, abhishek, satwik, kottur, khu...   \n",
       "10  [sequenti, match, network, new, architectur, m...   \n",
       "11  [hierarch, recurr, attent, network, respons, g...   \n",
       "12  [v, v, x, r, deep, reinforc, learn, overview, ...   \n",
       "13  [v, v, x, r, publish, confer, paper, iclr, bat...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   The communication of potential students with a...   \n",
       "1   In this paper, we apply a general deep learnin...   \n",
       "2   We consider incorporating topic information in...   \n",
       "3   Recent neural models of dialogue generation of...   \n",
       "4   We consider incorporating topic information in...   \n",
       "5   In recent years, there has been a huge increas...   \n",
       "6   Both dialogue systems and chatbots aim at putt...   \n",
       "7   While automatic response generation for buildi...   \n",
       "8   ��Internet of Things (IoT) is emerging as a si...   \n",
       "9   We introduce the task of Visual Dialog, which ...   \n",
       "10  We study response selection for multi-turn con...   \n",
       "11  We study multi-turn response generation in cha...   \n",
       "12                                       We give an o   \n",
       "13  We study reinforcement learning of chatbots wi...   \n",
       "\n",
       "                                                 body  \n",
       "0   ered by my parents. 3 Table of Contents Chapte...  \n",
       "1   ��cient attention mechanism in order to genera...  \n",
       "2   are determined by themselves as well as the me...  \n",
       "3   erence, and ease of answering (re- lated to fo...  \n",
       "4   the generation of words in decoding. To increa...  \n",
       "5   umb” bots may give rise to complex interaction...  \n",
       "6   ed, as they might be put to use in the human-c...  \n",
       "7   context dependent messages. With the character...  \n",
       "8   ssed. Keywords—Internet of Things, Chatbots, H...  \n",
       "9   baselines. We propose a retrieval-based evalua...  \n",
       "10  ng opera- tions. The vectors are then accumula...  \n",
       "11  terance level at- tention respectively. With t...  \n",
       "12  cent exciting achievements of deep reinforceme...  \n",
       "13  d customer service support can be scored by qu...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean those\n",
    "df.insert(df.shape[1],'abstract_pp', df.abstract.apply(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.insert(df.shape[1],'body_pp', df.body.apply(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>abstract_pp</th>\n",
       "      <th>body_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...</td>\n",
       "      <td>[nikolao, polatidi, supervisor, dr, peter, han...</td>\n",
       "      <td>The communication of potential students with a...</td>\n",
       "      <td>ered by my parents. 3 Table of Contents Chapte...</td>\n",
       "      <td>[commun, potenti, student, depart, perform, ma...</td>\n",
       "      <td>[ere, parent, tabl, content, chapter, context,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...</td>\n",
       "      <td>[r, v, v, x, r, review, confer, paper, iclr, l...</td>\n",
       "      <td>In this paper, we apply a general deep learnin...</td>\n",
       "      <td>��cient attention mechanism in order to genera...</td>\n",
       "      <td>[paper, appli, gener, deep, learn, dl, framewo...</td>\n",
       "      <td>[cient, attent, mechan, order, gener, answer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Response Selection with Topic Clues for Retrie...</td>\n",
       "      <td>[respons, select, topic, clue, retrievalbas, c...</td>\n",
       "      <td>We consider incorporating topic information in...</td>\n",
       "      <td>are determined by themselves as well as the me...</td>\n",
       "      <td>[consid, incorpor, topic, inform, messag, resp...</td>\n",
       "      <td>[determin, well, messag, vector, respons, vect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep Reinforcement Learning for Dialogue Gener...</td>\n",
       "      <td>[deep, reinforc, learn, dialogu, gener, jiwei,...</td>\n",
       "      <td>Recent neural models of dialogue generation of...</td>\n",
       "      <td>erence, and ease of answering (re- lated to fo...</td>\n",
       "      <td>[recent, neural, model, dialogu, gener, offer,...</td>\n",
       "      <td>[erenc, eas, answer, late, forwardlook, functi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic Aware Neural Response Generation\\n\\nChen...</td>\n",
       "      <td>[topic, awar, neural, respons, gener, chen, xi...</td>\n",
       "      <td>We consider incorporating topic information in...</td>\n",
       "      <td>the generation of words in decoding. To increa...</td>\n",
       "      <td>[consid, incorpor, topic, inform, sequencetose...</td>\n",
       "      <td>[gener, word, decod, increas, possibl, topic, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           orig_text  \\\n",
       "0   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\...   \n",
       "1  6\\n1\\n0\\n2\\n\\n \\nr\\na\\n\\n \\n\\nM\\n8\\n2\\n\\n \\n \\...   \n",
       "2  Response Selection with Topic Clues for Retrie...   \n",
       "3  Deep Reinforcement Learning for Dialogue Gener...   \n",
       "4  Topic Aware Neural Response Generation\\n\\nChen...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [nikolao, polatidi, supervisor, dr, peter, han...   \n",
       "1  [r, v, v, x, r, review, confer, paper, iclr, l...   \n",
       "2  [respons, select, topic, clue, retrievalbas, c...   \n",
       "3  [deep, reinforc, learn, dialogu, gener, jiwei,...   \n",
       "4  [topic, awar, neural, respons, gener, chen, xi...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The communication of potential students with a...   \n",
       "1  In this paper, we apply a general deep learnin...   \n",
       "2  We consider incorporating topic information in...   \n",
       "3  Recent neural models of dialogue generation of...   \n",
       "4  We consider incorporating topic information in...   \n",
       "\n",
       "                                                body  \\\n",
       "0  ered by my parents. 3 Table of Contents Chapte...   \n",
       "1  ��cient attention mechanism in order to genera...   \n",
       "2  are determined by themselves as well as the me...   \n",
       "3  erence, and ease of answering (re- lated to fo...   \n",
       "4  the generation of words in decoding. To increa...   \n",
       "\n",
       "                                         abstract_pp  \\\n",
       "0  [commun, potenti, student, depart, perform, ma...   \n",
       "1  [paper, appli, gener, deep, learn, dl, framewo...   \n",
       "2  [consid, incorpor, topic, inform, messag, resp...   \n",
       "3  [recent, neural, model, dialogu, gener, offer,...   \n",
       "4  [consid, incorpor, topic, inform, sequencetose...   \n",
       "\n",
       "                                             body_pp  \n",
       "0  [ere, parent, tabl, content, chapter, context,...  \n",
       "1  [cient, attent, mechan, order, gener, answer, ...  \n",
       "2  [determin, well, messag, vector, respons, vect...  \n",
       "3  [erenc, eas, answer, late, forwardlook, functi...  \n",
       "4  [gener, word, decod, increas, possibl, topic, ...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, try the text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, try text summarization given the rest of the text. \n",
    "FN = 'vocabulary-embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 19722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = False # dont lower case the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rename tokenized columns\n",
    "heads = df['abstract']\n",
    "desc = df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     The communication of potential students with a...\n",
       "1     In this paper, we apply a general deep learnin...\n",
       "2     We consider incorporating topic information in...\n",
       "3     Recent neural models of dialogue generation of...\n",
       "4     We consider incorporating topic information in...\n",
       "5     In recent years, there has been a huge increas...\n",
       "6     Both dialogue systems and chatbots aim at putt...\n",
       "7     While automatic response generation for buildi...\n",
       "8     ��Internet of Things (IoT) is emerging as a si...\n",
       "9     We introduce the task of Visual Dialog, which ...\n",
       "10    We study response selection for multi-turn con...\n",
       "11    We study multi-turn response generation in cha...\n",
       "12                                         We give an o\n",
       "13    We study reinforcement learning of chatbots wi...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "def get_vocab(lst):\n",
    "    vocabcount = Counter(w for txt in lst for w in txt.split())\n",
    "    vocab = map(lambda x: x[0], sorted(vocabcount.items(), key=lambda x: -x[1]))\n",
    "    return vocab, vocabcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, vocabcount = get_vocab(heads+desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'of', 'to', 'a', 'in', 'is', 'for', 'with', 'on', 'that', 'The', 'as', 'are', 'et', 'we', 'by', 'In', 'be', 'from', 'an', 'can', 'We', 'al.,', 'this', 'al.', 'which', 'not', 'it', '=', 'model', '.', 'or', 'A', 'learning', 'response', 'data', 'have', 'more', 'will', 'our', 'information', 'using', 'neural', 'at', 'each', 'two', 'topic', 'system', 'has']\n",
      "... 19720\n"
     ]
    }
   ],
   "source": [
    "# See the most popular tokens\n",
    "print vocab[:50]\n",
    "print '...',len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e9b3950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot([vocabcount[w] for w in vocab]);\n",
    "plt.gca().set_xscale(\"log\", nonposx='clip')\n",
    "plt.gca().set_yscale(\"log\", nonposy='clip')\n",
    "plt.title('word distribution in headlines and discription')\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('total appearances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0 # RNN mask of no data\n",
    "eos = 1  # end of sentence\n",
    "start_idx = eos+1 # first real word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx(vocab, vocabcount):\n",
    "    word2idx = dict((word, idx+start_idx) for idx,word in enumerate(vocab))\n",
    "    word2idx['<empty>'] = empty\n",
    "    word2idx['<eos>'] = eos\n",
    "    \n",
    "    idx2word = dict((idx,word) for word,idx in word2idx.iteritems())\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_idx(vocab, vocabcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'glove.6B.%dd.txt'%embedding_dim\n",
    "import os\n",
    "datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "datadir = os.path.join(datadir_base, 'datasets')\n",
    "glove_name = os.path.join(datadir, fname)\n",
    "#if not os.path.exists(glove_name):\n",
    "#    path = 'glove.6B.zip'\n",
    "#    path = get_file(path, origin=\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "#    !unzip {datadir}/{path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fm61/.keras/datasets/glove.6B.100d.txt'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  400000 /Users/fm61/.keras/datasets/glove.6B.100d.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {glove_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_n_symbols = !wc -l {glove_name}\n",
    "glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, embedding_dim))\n",
    "globale_scale=.1\n",
    "with open(glove_name, 'r') as fp:\n",
    "    i = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        w = l[0]\n",
    "        glove_index_dict[w] = i\n",
    "        glove_embedding_weights[i,:] = map(float,l[1:])\n",
    "        i += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040815727600190289"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_weights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w,i in glove_index_dict.iteritems():\n",
    "    w = w.lower()\n",
    "    if w not in glove_index_dict:\n",
    "        glove_index_dict[w] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random-embedding/glove scale 0.0706949139514 std 0.0408294661504\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate random embedding with same scale as glove\n",
    "np.random.seed(seed)\n",
    "shape = (vocab_size, embedding_dim)\n",
    "scale = glove_embedding_weights.std()*np.sqrt(12)/2 # uniform and not normal\n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "print 'random-embedding/glove scale', scale, 'std', embedding.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19722"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vocab wasn't big enough from sample pdf's to be 40,000. Changing it to vocab instead\n",
    "vocab_size = 19722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens, in small vocab, found in glove and copied to embedding 7966 0.403914410303\n"
     ]
    }
   ],
   "source": [
    "# copy from glove weights of words that appear in our short vocabulary (idx2word)\n",
    "c = 0\n",
    "for i in range(vocab_size):\n",
    "    w = idx2word[i]\n",
    "    g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is None and w.startswith('#'): # glove has no hastags (I think...)\n",
    "        w = w[1:]\n",
    "        g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is not None:\n",
    "        embedding[i,:] = glove_embedding_weights[g,:]\n",
    "        c+=1\n",
    "print 'number of tokens, in small vocab, found in glove and copied to embedding', c,c/float(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lots of word in the full vocabulary (word2idx) are outside `vocab_size`.\n",
    "Build an alterantive which will map them to their closest match in glove but only if the match\n",
    "is good enough (cos distance above `glove_thr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_thr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2glove = {}\n",
    "for w in word2idx:\n",
    "    if w in glove_index_dict:\n",
    "        g = w\n",
    "    elif w.lower() in glove_index_dict:\n",
    "        g = w.lower()\n",
    "    elif w.startswith('#') and w[1:] in glove_index_dict:\n",
    "        g = w[1:]\n",
    "    elif w.startswith('#') and w[1:].lower() in glove_index_dict:\n",
    "        g = w[1:].lower()\n",
    "    else:\n",
    "        continue\n",
    "    word2glove[w] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of glove substitutes found 18\n"
     ]
    }
   ],
   "source": [
    "normed_embedding = embedding/np.array([np.sqrt(np.dot(gweight,gweight)) for gweight in embedding])[:,None]\n",
    "\n",
    "nb_unknown_words = 100\n",
    "\n",
    "glove_match = []\n",
    "for w,idx in word2idx.iteritems():\n",
    "    if idx >= vocab_size-nb_unknown_words and w.isalpha() and w in word2glove:\n",
    "        gidx = glove_index_dict[word2glove[w]]\n",
    "        gweight = glove_embedding_weights[gidx,:].copy()\n",
    "        # find row in embedding that has the highest cos score with gweight\n",
    "        gweight /= np.sqrt(np.dot(gweight,gweight))\n",
    "        score = np.dot(normed_embedding[:vocab_size-nb_unknown_words], gweight)\n",
    "        while True:\n",
    "            embedding_idx = score.argmax()\n",
    "            s = score[embedding_idx]\n",
    "            if s < glove_thr:\n",
    "                break\n",
    "            if idx2word[embedding_idx] in word2glove :\n",
    "                glove_match.append((w, embedding_idx, s)) \n",
    "                break\n",
    "            score[embedding_idx] = -1\n",
    "glove_match.sort(key = lambda x: -x[2])\n",
    "print '# of glove substitutes found', len(glove_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699985542931 clicks => clicking\n",
      "0.689336379355 secret => Intelligence\n",
      "0.680689053196 rare => most\n",
      "0.631476127374 Mustafa => Ahmed\n",
      "0.609102761318 leveraged => derivatives\n",
      "0.586243656511 slot => spot\n",
      "0.577361925372 kinda => okay\n",
      "0.57303534233 snapshots => images\n",
      "0.569999467123 incapability => humanness\n",
      "0.551016340239 coupling => equivalence\n"
     ]
    }
   ],
   "source": [
    "for orig, sub, score in glove_match[-10:]:\n",
    "    print score, orig,'=>', idx2word[sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_idx2idx = dict((word2idx[w],embedding_idx) for  w, embedding_idx, _ in glove_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'communication',\n",
       " 'of',\n",
       " 'potential',\n",
       " 'students',\n",
       " 'with',\n",
       " 'a',\n",
       " 'university',\n",
       " 'department',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'manually',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'time',\n",
       " 'consuming',\n",
       " 'procedure.',\n",
       " 'The',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'communicate',\n",
       " 'with',\n",
       " 'on',\n",
       " 'a',\n",
       " 'one-to-one',\n",
       " 'basis',\n",
       " 'is',\n",
       " 'highly',\n",
       " 'valued.',\n",
       " 'However',\n",
       " 'with',\n",
       " 'many',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'applications',\n",
       " 'each',\n",
       " 'year,',\n",
       " 'one-to-one',\n",
       " 'conversations',\n",
       " 'are',\n",
       " 'not',\n",
       " 'feasible',\n",
       " 'in',\n",
       " 'most',\n",
       " 'cases.',\n",
       " 'The',\n",
       " 'communication',\n",
       " 'will',\n",
       " 'require',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'academic',\n",
       " 'staff',\n",
       " 'to',\n",
       " 'expend',\n",
       " 'several',\n",
       " 'hours',\n",
       " 'to',\n",
       " 'find',\n",
       " 'suitable',\n",
       " 'answers',\n",
       " 'and',\n",
       " 'contact',\n",
       " 'each',\n",
       " 'student.',\n",
       " 'It',\n",
       " 'would',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'his',\n",
       " 'costs',\n",
       " 'and',\n",
       " 'time.',\n",
       " 'The',\n",
       " 'project',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'burden',\n",
       " 'on',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'admissions,',\n",
       " 'and',\n",
       " 'potentially',\n",
       " 'other',\n",
       " 'users,',\n",
       " 'by',\n",
       " 'developing',\n",
       " 'a',\n",
       " 'convincing',\n",
       " 'chatbot.',\n",
       " 'A',\n",
       " 'suitable',\n",
       " 'algorithm',\n",
       " 'must',\n",
       " 'be',\n",
       " 'devised',\n",
       " 'to',\n",
       " 'search',\n",
       " 'through',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'data',\n",
       " 'and',\n",
       " 'find',\n",
       " 'a',\n",
       " 'potential',\n",
       " 'answer.',\n",
       " 'The',\n",
       " 'program',\n",
       " 'then',\n",
       " 'replies',\n",
       " 'to',\n",
       " 'the',\n",
       " 'user',\n",
       " 'and',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'relevant',\n",
       " 'web',\n",
       " 'link',\n",
       " 'if',\n",
       " 'the',\n",
       " 'user',\n",
       " 'is',\n",
       " 'not',\n",
       " 'satisfied',\n",
       " 'by',\n",
       " 'the',\n",
       " 'answer.',\n",
       " 'Furthermore',\n",
       " 'a',\n",
       " 'web',\n",
       " 'interface',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'both',\n",
       " 'users',\n",
       " 'and',\n",
       " 'an',\n",
       " 'administrator.',\n",
       " 'The',\n",
       " 'achievements',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project',\n",
       " 'can',\n",
       " 'be',\n",
       " 'summarised',\n",
       " 'as',\n",
       " 'follows.',\n",
       " 'To',\n",
       " 'prepare',\n",
       " 'the',\n",
       " 'background',\n",
       " 'of',\n",
       " 'the',\n",
       " 'project',\n",
       " 'a',\n",
       " 'literature',\n",
       " 'review',\n",
       " 'was',\n",
       " 'undertaken,',\n",
       " 'together',\n",
       " 'with',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'existing',\n",
       " 'tools,',\n",
       " 'and',\n",
       " 'consultation',\n",
       " 'with',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'admissions.',\n",
       " 'The',\n",
       " 'requirements',\n",
       " 'of',\n",
       " 'the',\n",
       " 'system',\n",
       " 'were',\n",
       " 'established',\n",
       " 'and',\n",
       " 'a',\n",
       " 'range',\n",
       " 'of',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'tools',\n",
       " 'were',\n",
       " 'investigated,',\n",
       " 'including',\n",
       " 'keyword',\n",
       " 'and',\n",
       " 'template',\n",
       " 'matching.',\n",
       " 'An',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'combines',\n",
       " 'keyword',\n",
       " 'matching',\n",
       " 'with',\n",
       " 'string',\n",
       " 'similarity',\n",
       " 'has',\n",
       " 'been',\n",
       " 'developed.',\n",
       " 'A',\n",
       " 'usable',\n",
       " 'system',\n",
       " 'using',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'algorithm',\n",
       " 'has',\n",
       " 'been',\n",
       " 'implemented.',\n",
       " 'The',\n",
       " 'system',\n",
       " 'was',\n",
       " 'evaluated',\n",
       " 'by',\n",
       " 'keeping',\n",
       " 'logs',\n",
       " 'of',\n",
       " 'questions',\n",
       " 'and',\n",
       " 'answers',\n",
       " 'and',\n",
       " 'by',\n",
       " 'feedback',\n",
       " 'received',\n",
       " 'by',\n",
       " 'potential',\n",
       " 'students',\n",
       " 'that',\n",
       " 'used',\n",
       " 'it.',\n",
       " '2',\n",
       " 'Acknowledgements',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'Dr',\n",
       " 'Peter',\n",
       " 'Hancox',\n",
       " 'for',\n",
       " 'his',\n",
       " 'immeasurable',\n",
       " 'help',\n",
       " 'and',\n",
       " 'support',\n",
       " 'throughout',\n",
       " 'this',\n",
       " 'project.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'need',\n",
       " 'to',\n",
       " 'express',\n",
       " 'my',\n",
       " 'thanks',\n",
       " 'to',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'support',\n",
       " 'team',\n",
       " 'for',\n",
       " 'their',\n",
       " 'excellent',\n",
       " 'help',\n",
       " 'and',\n",
       " 'instructions.',\n",
       " 'Finally,',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'need',\n",
       " 'to',\n",
       " 'acknowledge',\n",
       " 'the',\n",
       " 'constan']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token not found: constan\n",
      "Token not found: fo\n",
      "Token not found: al\n",
      "Token not found: convolut\n",
      "Token not found: att\n"
     ]
    }
   ],
   "source": [
    "# Um, just remove the last token in the first one. \n",
    "Y = []\n",
    "for headline in heads:\n",
    "    item_list = []\n",
    "    for token in headline.split():\n",
    "        try:\n",
    "            item_list.append(word2idx[token])\n",
    "        except:\n",
    "            print('Token not found: ' + token )\n",
    "    Y.append(item_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y = [[word2idx[token] for token in headline.split()] for headline in heads]\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_df = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x11fe79850>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x120023ed0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1200b56d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1200c4d50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12017a450>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1201f32d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12025a990>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1202e19d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1203551d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1203da390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x120441d10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1204c5fd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12045e8d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1205bfdd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1205fcdd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b47afd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b50f050>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12b573f90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b5f7fd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b66d890>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b6f3a50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b767410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b7ec6d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b775ad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b8e33d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b915f50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12b9d1f10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ba59d90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bab7410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bb3f350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bba6dd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bc2be10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bca0950>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bd24cd0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12bce55d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12be1da10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bea5890>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bf16590>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12bf9e410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c003c90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c08bcd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c0ff3d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c185590>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c1eaf10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c27f210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c209bd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c36aed0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c3a6f50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c463a50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c4ea8d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c55e390>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12c5e42d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c638250>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c6c20d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c726d90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c7b8150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c76c850>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c8a5e50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c92dcd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12c99d9d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ca27850>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ca9a110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cb22150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cb87810>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cc0d9d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cc81390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cd08650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cc93a50>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12ce02350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ce32190>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ceeee90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cf75d10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12cfe97d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d070710>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d0e6190>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d16a290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d1d2c10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d256d10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d2cc790>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d352b10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d312410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d44c850>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d4d46d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d525310>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d5ad4d0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12d621610>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d6a57d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d71a190>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d79f450>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d72b9d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d898150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d8c6110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12d987c90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12da0eb10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12da815d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12db0a510>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12db70f50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dc03090>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dc69a10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dceeb10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dd63590>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dde8910>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12dda8210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dee2650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12df694d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12dfdd1d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e066050>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e0ca8d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e151910>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e1b8fd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e26b1d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e2d2b50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e358e10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e2ef650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e450b10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e48bb90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e54d690>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e5d5510>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e639f90>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12e6c2ed0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e6ff6d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e791c10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e807250>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e88b410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e819c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e985210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12e9b3310>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ea71d50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12eafabd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12eb6b690>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ebf45d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ec68050>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12eced150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ed54ad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12eddbbd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ee4f650>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12eed39d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ee922d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12efcd710>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f056590>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f0c7290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f151110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f1b6990>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f23d9d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f2b20d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f33a290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f39fc10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f424ed0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f3bb090>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f51cbd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f559c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f618750>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f6a05d0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12f711090>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f78cf90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f800a10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f886b10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f8fb4d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f9845d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12f9f7050>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fa7c3d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fa2fad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fb75110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fbeff50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fc60c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fceaad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fd5d390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fde33d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ec18f90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12fea7890>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x12ff02e90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ffa2350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x12ff56c90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13008ef50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130115dd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130188ad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130210950>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130282210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13030b250>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130371910>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1303f8ad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13046a490>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1304f2750>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13047cb50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1305ec450>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13061b550>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1306dbf90>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x13075fe10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1307d58d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13085a810>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1308cf290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130955390>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1309bad10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130a41e10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130ab4890>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130b3ac10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130afb510>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130c96950>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130d1c7d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130d8f4d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130e17350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130e7dbd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130f03c10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130f78310>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x130ffd4d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131066e50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1310f7150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131083e50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1311e4e10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131220e90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1312de990>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131366810>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1313da2d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131460210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1314c6c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13154cd50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1315c1710>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131648810>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1316bb290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131741610>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1316f5f50>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x13183c350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1318c41d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131929e90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1319aed10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131a235d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131aa9610>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x130881e90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131b64d50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131bda7d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131c5db50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131c1c450>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131d56890>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131dde710>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131e51410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131ed9290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131f3eb10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x131fc7b50>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x13203b250>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1320c2410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132126d90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1321bb090>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132144d10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1322a6d50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1322e3d50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1323a08d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132428750>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13249a210>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132524150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13258bb90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132610c90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132684650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13270b750>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13277e1d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132803550>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x1327b9c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1328fd290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132984110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1329eadd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132a71c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132ae5510>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132b6c550>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132bd3c10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132c5bdd0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132cce790>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132d53a50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132ce93d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132e4d750>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132e7f350>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132f472d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x132fcf150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133034bd0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x1330bdb10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133130590>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1331b6690>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13322c050>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1332b0150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133316b90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13339df10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13335d650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133495c50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13351fad0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1335917d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133617650>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x13367fed0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133705f10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133779610>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1338027d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133874190>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x1338fb450>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1338879d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1339f5150>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133a24110>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133ae0c90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133b66b10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133bdb5d0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133cd9510>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133d41f50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133dd5090>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133e3ba10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133ec2a50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133b76910>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133f7f410>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x133fb8290>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x134077310>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x1341000d0>]], dtype=object)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEVCAYAAADgh5I1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvXd4VGX6//96JpNJJ0ACCQmQkADSqwVXlKCgICsoKBYU\n9aMiLGAFwQaIIiq6KlJExQUWxIaChVB0E1FXFgFpUiW0hFBDek/u3x/POcNkMi1lPx+/1y/3dc2V\nzMw577nr85zzlPdRIkKDNEiDNEiDNIjl/1qBBmmQBmmQBvlzSEOH0CAN0iAN0iBAQ4fQIA3SIA3S\nIIY0dAgN0iAN0iANAjR0CA3SIA3SIA1iSEOH0CAN0iAN0iBAQ4fQIA3SIA3SIIb86ToEpVRTpdSX\nSqkCpdQxpdRddcCaoJTaqpQqUUotqQfdApRSiw298pRSO5RSg+uIuVwpdUoplauUOqiUerAe9Gyn\nlCpWSi2vB6xUAyvfeB2oB8w7lFL7jBgfVkpdXUucfKdXhVLqnTrqFq+UWquUumDEZZ5SyloHvI5K\nqX8ppXKUUn8opW6pwblu81cpdZ1Sar9SqlAplaKUiqstnlLKppT6XCl1VCklSqmkuuinlOqjlNqo\nlMpSSp1VSn2mlGpRB7xOxucXjNd3SqlOtcVzOmaaYfOAOugXb2A45uLzddFPKRWslFqglDpn5M4m\nb3j1IX+6DgGYD5QCUcAoYKFSqnMtsU4CLwEf1pNuVuAE0A8IB54DPlVKxdcB8xUgQUQaAUOBl5RS\nveuo53zg1zpiOMoEEQk1XpfUBUgpNRB4FbgfCAOuAdJqg+WgUygQDRQBn9VFP2ABcBZoAfRAx/pv\ntQEyOpI1wDdAU2AMsFwp1d5HCJf5q5SKBL4AnjdwtwKf1BbPkJ+Au4FTPurmCa8J8B4QD8QBecA/\n6oB3ErgdiDReXwEf1wEPAKVUInAbkOkDllc8oLFDTr5YR7z30LHtaPx93Ecd6yS1vvL5b4hSKgQY\nAXQRkXzgJ6XUGuAeYGpN8UTkCwP3UqBlXfUTkQJghsNH3yiljgC9gaO1xNzj+NZ4JQLbaoOnlLoD\nyAb+DbStDcZ/WV4AZorIZuN9Rj3hjgDOAD/WEacNME9EioFTSql1QG0vSDoAMcCboikB/qWU+hmd\nz16vID3k73DgdxH5zPh+BnBOKdVBRPbXFE9ESoG3jO8qfDXOA16y43FKqXnAD3XAy0bntNnJVuBD\nbvtQ//OBKeiLAK/yX2hPXOIppTqgLw5bikiu8XGt2oOayp/tDqE9UC4iBx0+20ntC/K/KkqpKLTO\nv9cRZ4FSqhDYj75aWVtLnEbATOCJuujjQmYbt64/+zqc4EqUUn7ApUAzY/gk3RiSCaoHHe8Flknd\nuVjeAm43btljgcHAujprd1EU0KWOGJ3RdQHYL1T+4E9aJ+i7wDrVCIBSKhsoBt4BXq4j1m1AiYjU\nqtbcyDEjp/9h3MXVVi4HjgEvGHW3Wyk1op509Ch/tg4hFMh1+iwXPbTwpxKllD+wAljq6arMFxGR\nv6FtvBo9FFBSS6gXgcUikl4XfZxkCpAAxKJvY782brVrI1GAP3Ar2tYeQE/00FutxRg/7wcsrQuO\nIZvQDXYukI4ejlldS6wD6LuWyUopf6XU9YaewXXUMRTIcfrsz1on3YBpwOS6YolIY/RQ7QTgtzro\nFIbuUB6tq06GnAMuQw+P9UbHYUUd8FqiczAHfYc5AViqlOpYRz29yp+tQ8gHGjl9Fo4eg/zTiFLK\nAvwTPdcxoT4wRaRCRH5CJ8O4WujUAxgAvFkf+jjo9R8RyROREhFZCvwM3FhLuCLj7zsikiki54C/\n1wHPlHuAn0TkSF1AjLiuQ3fKIejx6iboOY8ai4iUATcDQ9Bj808Cn6I7mrrI/yt10hZIBh4VkboO\n5QH2u6F3gWVKqea1hJkB/FNEjtaTTvkislVEykXkNLpNuN7oeGojRUAZ8JKIlIrID0AKcH196OtJ\n/mwdwkHAqpRq5/BZd+rhdrO+RCmlgMXoq90RRtHXp1jRcwg1lST0JN5xpdQpYBIwQim1vf5UA/Qc\nh6rViSIX0I2h47BOfdDtjqZ+7g6aAq3RcwglInIePRla6w5LRHaJSD8RiRCRG9B3W1vqqOfv6LoA\n7HNvify56iQO+A54UUT+Wc/wFvRdVmwtz78OeETpVWSngFboxSFT6kk/M6dr277u8oD5X5U/VYdg\n9P5fADOVUiFKqb7oyZVaJZRSyqqUCgT8AD+lVKCqwxJCQxaiZ/5vEpEibwd70a+50kswQ5VSfkqp\nG4A7ge9rAfceulHoYbzeBb4FbqiDfo2VUjeYflNKjUKPB9dlTP0fwETD9ibo1RPf1EHHv6Abhrqu\nLsK4YzkCjDXsbYyem3BVoL7q183wX7BSahJ69dISH891l79fAl2UUiOM76cDO70NXXqqB6WXVAca\nh9qM7zx2/O7wjLmXf6E71nd9sdUL3kClVE+jRhqh7yovAPtqae916CEZs1ZOAg+jJ5lro98VSqlL\nlFIWpVQEMBdIFRHnYT1f9dsEHAeeNo65CugPrPeEVy8iIn+qF/oqbTVQYDjlrjpgzeDiyh3zNaMO\neHEGRjH6tt18jaolXjP06ots9BjwbuChevLjDGB5HTGaoZev5hk6bgYG1hHTH72qIxs9jDIXCKwD\n3iL07X995V8PIBXd4JxDD/FE1QFvjoGVjx4+aVvDGLrMX/Tw4H708EIqEF9HvKMuvvOI6Q4P3UGJ\nU43k11Y/9NLQ/QbOWfSFTre62Ot03FFgQB30uxN9IVGAXhSyDIiuYzw6A78YmHuBW+orxz29lPHj\nDdIgDdIgDfL/c/lTDRk1SIM0SIM0yP+dNHQIDdIgDdIgDaLlf2Ncyss42gT0Wu8SYInD56OoOgZZ\niB5j610bPOO7keiJqDz0uNzNtdXP+O5B9IagfPREa4wXrCXoMd9SQ4cd6PHRjcAh9G7Eg4atKUBc\nHfFOo+dhBEjywdYA9AqqYw54g43v+hi4Wehx3M+AFnXA62T49YLx+g7oVEd7N6KXiU4zbPY4LuwF\n7wjVx8Gfr629xvfB6PmTc+g15pvq4L8a14cP/tuF3jvhU334YG+N6sM4Zzl6bikXXQsPOnx3HXou\nwaf68IQH2IDPuTh34rU+vOBlGj4oRy8Z9VofXvBqXB/18ar5CZp34wywx+GzpjgVpYfzWxnB3Ite\nJrcYvVZ7IUaDi15CmWMk2A50gd8HHAY97+EBf7gznvF5rFEIg9HLJocYidXci605RsIvNArKtDXd\nwLvSSK6FwA9edLsePQl6AH139lcD41X0mvdi9IR6IHoycnMd8GzA10bCZfqS8Oi19zPQy1dNvDzj\n/WB049EI3bB9CKyrA15j9KooP+P1CLCrtvYa3081vt+NXjnirUPw5L94dEPxWg1qw629DsX/MXqy\n3g/vFzce8ZyOvQ8v9eHF3lh0Y/YpvteHp/ieMvD2oy90vNaHgdkFCDb+72Dg9EbXR46Rgz7Vhxc8\nG/AY0Bcf68ML3mngf6hBfXjBq3F91MerxpPKSqlr0D3+MhHpYnz2GpAlIq8opaaiOwSXa3qVZj1s\nERERsa1p06YcP36cxMREsrKyKCsrIz4+nry8PDIyMujQoYP9vAMHDhAWFkZMTAwA27ZtOycizczv\nIyMjpVmzZoSEhACQkZFhxwMoKCjg0KFD9OjRw465c+dOEhMTCQ0NdYkXEhJCSEgIR44coXHjxpSV\nlWG1WrFarRQVFVFSUkJoaCgtW7aktLSU3bt306VLF/bs2VMFy1E/q9XKH3/8QefOmmVg+/bttG7d\nmpKSEnJzc6moqKBLly5UVFSwc+dOOnXqxO+//15jvMjISHJycjhx4gSVlZW0adOGsLAwl75z5T9T\n9u7dS4sWLWjSpEmVz8+fP8/x48fp2bNnnfAKCgoIDg7m7NmzpKen06tXL4943uw9fPgwiYmJHD9+\nnLi4OBo1alQrvLCwMPbs2YPNZqNr165VbKiNvceOHaOiooLAwEAsFgsdO3asE156ejpKKSwWi/2v\nWR+1sTcoKIhDhw5htVrp0kUza5j1ceDAgWp4jRs3lrZtq9MJmfodPXqUJk2aYNrgrT5c4RUXF3Pg\nwAFatWpFUVEReXl59jbBW314w8vIyLD7rbi4mHbt2nmsD1/wWrduTXh4OACFhYUcOHDAbX14w2va\ntKn9cxHxWh/1JrXpRdBXAI53CAcwbo/Q66wPeMPo3bu3zF2+WoYOHSobNmyQZ599Vu69914REUlJ\nSZE+ffqIKUePHhWLxSJpaWn2z4CtrvBMaXTl7XY8EZHWk9dIt27d5KuvvpLy8nL58ssvJTY2VvLz\n8z3iHTlyRDp37mzXr3379nLy5EkZOXKk/b2ISHp6ugCyevXqaliu8ERETp06JYDs27dPhg8fLg8/\n/LCEh4fbde7SpYt8/vnndjw0W+ZWYGtUVJR8smq1rFy5UuLj4yUlJUVWrVolgCxdulRSUlLkq6++\nkpCQEImMjJQ333xTUlJSJCUlxaN+jnLq1CkJCAiQffv2ibOMHz9errjiCpe+qwleSEiI+Pn5iVJK\nXnzxRa947vwnIjJt2jSxWq0iIhIXFycbN26sglcT/61cuVIAUUpJZGSkDBo0SFavXl3Ff67wTB+b\neP7+/rJ06VIJCwuT1q1by6233iqNGjWSNm3ayIwZM2odj7i4OFm9Wn/vXB+18V95eblcc801Ehwc\nXK0+XNkbGdlM5i53b29wcLAMGjRIvv76a0lJSZFPP/1UAHnxxRdd6teqTaLETflGRETGjRsnQUFB\nAkjPnj0lLy9Phg8fLmPHjq3iD+f6qAmeo/9iY2MlJSXFjlsbvPj4eElMTJRevXrJokWL5M0336xW\nH+78N3ToUAkICBBA2rVrJ2vXrrX7NCQkRCwWiyil5O6773abL/X1qi+20ygRMSlkT6F38VYTpdQY\nwylERUWh8k6zefNmxowZw7Fjxzh79iypqans2LGDPXv2kJiYSGRkJDExMXTt2pVjx45x7Ngxt3hR\nQZCamgpAn+aVnDp1yv5+Unfh2XePMGzYMH1rpBSzZs3i119rxhJ9+vRpWrRoweWXX84rr7xCcXEx\nRUVFzJw5E6UUhYWFHvXbvHkzBQUFfPfdd0yZMgWr1cqpU6fIy8sjICCAiooKu84iUkU/EXkPvQGN\nSy+9VE4XwU1X9SEkJISrrrqKwYMHY7PZGD16NKB94e/vT0BAAD169CApKcljPBz9V15ezpQpUxg4\ncCCnTp0iKSmJ4OBgLBYL5eXlnD9/nlmzZtmP9yZlZWWMGjWKe++9lw4dOhAfH09YWBjR0dGEhYXx\n0EMPERcX53McTDybzUaHDh3Iy8tj8eLFBAW558mrif/y8/P59ddfGTBgAHv27GH8+PEsWLCA9evX\ne8Qbafi4rKyMwYMHc//99zN69GjGjx/P8ePHuf/++1mxYgW//PILQ4YMYeTIkbXyn6MsW7aMq6++\nmjZt2tTafwCjR49m06ZNBAQEYLPZ+Oyzz6rcoTja2zqhrbyx28rRUa7tfeqpp9i4cSN79+5l8uTJ\n7N69G6UUiYkXN+E75l9kZDOmdS0nNTWVkSNHMmLECPbu3cuOHTvYvHkza9euJSAggI0bN+Ln58ei\nRYuq1Yc7WbBgAe+88w6//PILqampBAQE+OwnX/F++uknDh06RKdOnejbty+nTp3i22+/rXKeO/8l\nJSVRUVFhxxswYAD+/v4A5OfnU1BQwNKlS8nJyalWw/Ut9U5/LSKilHI5DuXokJ49e8qrr77KwoUL\nGTJkCL/88gt+fn4kJSXRq1cvLrnkEgYPHszatWu5+eabWbRoUTVneCrIYc+8yy3to+3n3H7HLPLz\n89mwYQPXXnst27ZtY+jQoQwePLjKMJKv0rt3b2bMmMEjjzxCfHw8jz32GGFhYbRseZEV110DFBwc\nzAcffEBUVBRt2rThkksuISwsDJvNRkxMTBU7L7vsMq+6iAj33HMPNpuNuLg4MjMzadGiBefPn6d5\n8+YUFBS4O8+l/yorK7nrrruIioriiy++wN/fn8DAQLZs2UJ2djb9+vVj4sSJPPLIIz75qrKy0q7f\nvHnz7J+npKSwZ88ekozfbNasGfv2edx86tbe119/nb59+/LbbzXnPHPnv9jYWN1RRkUxb948WrRo\nQV6ed7ogV/YqpVBK8dVXXxETE8OYMWPo378/GzZssJ/nroOurKzkpZdeorCwkNtuu43U1FRKSkp4\n/PHHmTx5MmfPnmXMmDHVOmd3FyT5+fnccMMNFBYW0rx5c1atWsXRo0eZMWMGUVFRfPTRRxw8eJDR\no0fz6qveaZxc2btt2zZWr17NrFmzePzxx7n//vs91odzBwNw3XXXMXbsWPbu3YtSiuHDh/Phh1Uf\nHeBLfQD4+fnRt29fli9fzsKFC1FK8eSTTzJz5ky39eGuw3KULVu28MQTTzBixAjy8/PZuHEj6enp\nXHnllVUu7mqqn2NthYSEMHbsWJo0acIDDzzgE15tpb6WnZ425gbMOYIzng5WSvmnpaVx6V/6MXz4\n8GrfN2rUyH6lFx4eTnl5Of3796+TgqWn07DZbPTo0QOLxcJll13GFVdcwXfffVcjnKioKDIz9c3Q\n8OHDadu2LadPn2bEiBGUl5fbx1/diYhw8uRJTp8+zapVqxg2bBhLly4lPj6e1NRUhg0bBuix9cOH\nD9vHemuKB7B+/Xo7nq8iIjzwwAN2PPNKBeDEiRMMGDCA559/nuuv941nyxOeo1RWVlJYWEhGhufH\nI7iz9/vvv2f16tWkp6cTHR3NiRMnGDlypNcGzZP/li5davefyeJQWVlZK3sfe+wxrFYr33zzDfPn\nz2fTpk12TIdz3xORS0Xk0pYtW3K6CPr168c///lPRISUlBQGDBhAUlISW7du5cMPP+Tvf/87OTk5\n9O/fn6SkpCoXE67wrrjiCnJycux4d9xxB4cOHUJEaN26NaNHj+baa69l7NixXH311WRnZ9fK3tjY\nWMaPH89HH33Es88+S5MmTXyqD2cpLy/n8OHDWK1W9uy5+OgQX+vDHd5PP/3EBx98QHJyMgUFBezc\nubPasY7+CwprxBu7rXYfm6/mzZtTUVFB7969yc/P55lnniE6OprJkydXi4criZ9a9S7C1M9ZKisr\nKS4u9lofdZX66hC+QnO+YPxd4+5AkxwuMDCQa28cRnl5OcXFxVRUVFBRUUFxcTHp6enm3ARz5swh\nKCjI5+EEEw+5iFdeXk5Ai3aUlpZy1VVX0bt3b5577jl+/PFHunXr5qjbGKUfabc1PT2dqCD4+eef\nycvL48iRI5w8eZJu3brx/PPPk5WVxcSJE+nRoweffPIJI0eO5Oabb3aZWKYsmfcGnTt3Jjs7mwMH\nDvDRRx8xdepUNm7cyEcffUR6ejqdOnWiuLiYF154ge7du1cbHvAVr127dvz666889thjAJSWllJc\nXGz3qzsZN24c+/bt4+uvv64y/FJRUUGfPn0oLy/HYnGdNq78N2zYMP7zn//w1FNP8Z///IfU1FRS\nU1PJzc2lZ8+ePPDAA0yYMIGRI0cSEhLC2bNn3eJ9vPANOnbsSHZ2Nrt372batGn07duXTz/9lNOn\nT5OYmMiCBQtYsGABERERPPLII1XiWxP/tWrVijVr1vDUU09x/vx5HnnkEZKSkuyThjX13zXXXEPr\n1q1ZvHgxw4YN45NPPiElJYUbbvBMNeUOLzZW87p99dVXdO3atUpjWRt733rrLQ4dOsSQIUMA+O23\n36rVh6/6FRQUcPbsWfbs2UNhYSFff/01ycnJPProo9UWJzhKRUE2zYY+RX5+PhUVFaxfv56VK1dy\n3XXXERQUxLZt20hISGD+/Pku68MxX4rychnT+hzTpk0jOTmZ77//ntdee40VK1bQvHlzDh06xIUL\nF9ixYwf+/v5s3LiRDRs2kJKS4lG/jz/+uJp+PXr04IorruCee+6huLiYBx54gEGDBnmNR0VBNgV7\nf3Bp78aNG/ntt9+oqKggNzeXJ554grCwsCqLEf4rUtNJB2AleplWGXrp5QNABJqQ7RB6vWxTD+f3\nBSQoKEhiW8dLVFSUM5eHDB48WOLi4qRLly7i5+cnb7/9tjgLbiaBp0+fXg1v+vTpEjflG/mf//kf\nSUxMlJCQELHZbPK3v/3NI16vPldLSEhINbz4+Hhp0aKFhIaGSnBwsERFRcnUqVOlvLzc7aRU7969\nZcZb7wkgAQEBEhISYn8tX75cUlJSZOPGjXLJJZdIYGCg9OvXT44cOVJrPBFx6dsjR47UCu+JJ54Q\nQIKDg8VisYjNZpOQkJBa6/fuu+/abfXz85M+ffrIzp0762Sv48Sgq0nlmuB99NFHEh8fL8HBwRId\nHS333HOPZGZm1gpv8eLF8u2338qePXvk8ssvF4vFIq1atZIvvvii1ni5ubmyfv16CQ8Pl06dOkly\ncrLbXPbVf++8844kJiZKaGiotGnTRl5//XW3eK3aJErs2A9d4v3973+Xzp07S2BgoCilJDQ01Gt9\ntGqTKC0nrpCAVl0kPDxcwsLCpEuXLvLee++JiMinn34qGzdulLZt24pSSnr06OGxPky8a665phpe\nfn6+5Obm1qg+POGJiMyYMUOAKn7wVB++2HvJJZfYF4XceOON8sEHH7jFq69XjecQROROV58rpeYA\nb6NXII1BPyvY1fk/KaUGV1ZWJpcUF/Pk448xdepU52MYMWIEu3btsk/S+CpLii8jboomzzz6yhDi\np37LkmIoStvGxs0bsdls+F96K9byUj7aW8y3U7/l6CtDXGLdN+FJJo5yPeTy2muvsXjxYioqKnjw\nwQer2eBKmkY2d3uF7og3ffr0OuOtW7cOm81GYmJivegXERHBN998Q0VFBfHx8bRr146FCxfWST8R\noWnTpnTs2JFBgwZ5vRptGtmcuCnfuIzXunXrGDNmDDabjQcffJCjR496xPJFP5vNRosWLersv7S0\nNPr27cv58+cREQYMGFBlctoTnpnLgN3utLQ0unfvTnq6fqxCy5Ytfboi9ea/d955B4Bnn33WJ3ut\n4e7917FjRx599FHy8/OZOHGiT3h+weFE3/WKS/2OHDnCc889h4hw7bXXMmjQIPuSck94P/xQ/cmd\naWlpDBgwgKysLPz9/enfv79P8XCHB3oorlWrVvb8mzp1arXhIFd47uy97bbbuO2226p85ut8RF2k\nXoaMlH404nz05qVOwJ1KqU6ejm3Xrh3PvPYOK1euZO/evVWOSU5OJiMjg0OHDvHee+8xblyNnxdT\nRaSygvMb5jN9+nT27t1L/u8pFB7ajK2Z76taHKWiooK3336b/P5PUXbL6y5tqA1ecnIye/furRe8\n8ePH88orr9QLXm5uLuPGjSM5OZlff/2VX375heDg2j/0yxFv4cKF/PLLL/Y14LWRutjrqmhNvPqK\nR1xcHEop9u3bR35+PqdOnaoXvCVLltDi0U9J+e0ge/fuJX7qt14bIWeJn/qt3d78/k9VsbemWKY4\n+m/JkiU19p+jHfFTv6XVY5/y6pzXq+RfWFhYrfXr/+4eTlwoYsmSJTWOh+Nvmv/HPfUVN915P/c/\n/myt8sW011X8HD/bneGRTbtepL5WGV0O/CEiaQBKqY+BYejdyC6PDQgISLBa/bnjjjtYs2YNNy47\nYu8p16xZw/XXX49Sij59+pCdnU1mZiZXvq2f9eLuit6dlGYexC80kpdffpm5c+dSWZiDrXkbghJ6\nez3XMUDm727ZsoWYmBjONo4GqGJDbcTES0hIAOB44x4+45n6Ofpky5YttG3blpiYGGw2W53xNmzY\nwLlz57jlllsoLy/nqquuIjc316eCjHdxB7ZhwwaOZZzilltu4cz5C1Q2b09WVlatC9zZ3vqIR9u2\nbe3xqAmeo73m/87xrS/9YmJiULsKCOl4DWvWrAE832F5wzvQOLqK/3zBcxXflve9SXZlOAkJCRw/\nftxljddEio/+RkF+XpX8y8rKorbPuS/NPIi1cYsq+XLVuFcJv3JkrfQz8SKb19x/fzapF/prpdSt\nwCARedB4fw9whYhMcDpuDPoZvY3Q28/PA5XoZ8Qedzi0LZpzxZxSb4+eryh0OOYS9CMJxzi8P4/m\niXGWJsZvFhjfN3Xxm3HAszXAa47ekIcLvDgRaea4bO3/AM+Tve7wQMelnItP3/IzvrMZeGnoxzVG\norlX6qLfMQPHOQdqgxcPVBi6+6FpDurqP3PTi7P/LhGRsP8H41uf9nYB3M1k1ya+3vA8+a+2+tXE\nf/+beO4k0sCPkz/TTmXnF/qh6R84vL8H/bQkj8eid+39jHakndwJ/QSt/Q7nHMaJrAwXEysGXjWy\nKOM3PzYwKtAJWol3sjJPeGe5SFaWhya/80ZW5gmv0tDN1K8M72RlnvBMnYoNvCy8k5XtRz98pMwB\nb53xGx8Aq9BEW8UGtjcyv60O/7czzltu5gCarKzIwErHO5mfJzzzIT5HDb996UPeesJbw0Uyv1w0\nSVsL5/NqgJfFRbKyAuAE3sn8tjpjOdaQgWcS+a321V4XeOOoSuRXjANPkCt73dnqoN8/0LVRhr6Q\ny6hNLIz3bzvpV4JDbdRSv++N3MtDX3h+Wwf9bgV+5OKDs3YBH/oYi1SqPnDrgMMxzmR+/3Uuo/pa\ndpoBtFJKHVVK7UaTZd3s6ViH97+iyckaoR+X+RI64DYApVQi+t7wtI+6vAIkOOGFoik1AGLQdwLP\nisiLdcCzoTfVNEUn7Avox0HWFg/0jm8/Q7/V6Ktxb89EdodnRT+d7BYD7yvgI+X5weT5hi0HHfAG\noGPUDeiHbnRfQhf6Sh/sNWW+gQM6BxLQj0s9aeClAZ/UAc+KbmjvRhf5WTfn+Yo3iIt3pmZn8486\n4PkBt6Ov8mahSRs/rgWWiTcK/VStp9Edvqsrf1/xThl/G4tIKDqX3S4d91G/e4C70HmShY51bfE+\nBnJFJNTQ7ysD09fnhTvjlaAfSXkCfWW/ERjopTY84TXHGApHtweFaIJOX2WCaZuIXAKglIpE++x5\nA3Mruma3yPQUAAAgAElEQVT+q1JfQ0ZW9NWkDbgMfVV5l4hUe+i3eWxERESbwLDGREc0tn/nTO60\ne/duysvLsVqtlJWV0bZtWxo1akRBQQH79+8vFxH7LqfIyEjxhBcSEsKePXvo1q2by81RrsjtPOEF\nBwezb98+unXrhp+fn/0YQzeX5GKe8E6cOEHnzp2xWvW0jiOZnzuyMm94IuKSzM8VWZmJ1yQ0yE5+\nVlxczO+//06bNm0oLCykoqKCCxcu0KNHDztZWUJCAmlpaefE6ZY4KCiod3SLGPytfuTm5pKfn4/N\nZqOsrIwWLVqQnZ1Nbm4urVu3tm/2Onz4MHFxcRw9etSjvVlZWWRnZxMYGEhJSQlt2rRh9+7d+Pn5\nUVlZSXl5OYmJidXIymqiX1paGnFxcXbM0tJSTpw4Qbt27Th48GCV3PNFv507d2K1Wu2b0SIjI72S\n+QWGNcamKqthAezevZuAgADCw8M5c+aMncjPVS57wyspKbET+fn5+aGUcibfq2ZvSFgjKQ+NIjao\nohqemTddunSpRhPhrj484TmeGxIS4oroskb6GToQGBho30FeWlrqlujSG156ejpnz57FZrMhomlx\niouL7fZ7wrPlnyIiIoLIyMgqfsrIyKgRmV+9SX3dagA3oq8GjqCvvj0e60je5YosSkTspGIiVdeV\nGwRPRx0xveEZ64vFarWKv7+/XHnllXL27FkxBQ9kea7wli5dKv7+/hIRESF+fn4SGBgon3/+uU9k\nZa7woqOjpXv37tKrVy95+eWXfSYrc4cXFxcngYGBkpCQIAsXLnRJVuYK78iRI9KkSRM7nsVikby8\nPHnyySdl3Lhx0rhxYxG5SObnjqzMxMvJyZF27drJiRMnZPr06TJq1CgREQkLC5OmTZtK9+7d7QSB\nnsjKvOGlp6eLiMgXX3wh/v7+8tZbb7mNrS94cXFx9vxISUlxJis7WlO8qKgoOXv2rISHh/tM5vfq\nex+5xHKsjZSUlCq1URs8szaaNm0qsbGxct999znXRjV7W7VJlFaPfeoSz6yNhx9+WCIiIuxxNX3p\nSj9PeGY82rRpIx07dhTAmeiyRvqVl5eLv7+/LF++3CeiS294Tz75pISGhsrZs2clJSWlCtGlJ7y4\nKd9Iv379JDIyUiIiIuQvf/mLfS9NTcn86utVb09ME5G16NvrHGC4cTXm6Vi7LFiwgLy8PH788UeG\nDx9OQEAAeXl5ZGdnM2vWLHcwbm+RXeFFRkYye/ZsioqK2L59O3v37uXGG2+scp6rnbYm2dbXX3/N\n3Llz6dmzJ5s3b2bTpk2UlZUxYMAAkpOTmT17NnfffbdPXDyu9Js7dy47duwgOTmZt99+m27dulUj\nK6uJfnPmzOGhhx7i5MmTjBs3jpEjRzJ+/PgqZGCu8DZv3kx4eLgdLzQ0lG+//ZYWLVqwYsUKAgIC\nWL9+PWPHjkUpRU6O56Vwzz//PA888EAVDhvQW/FHjRrFjh07WLRoEaApS7xxBbnDM3fuNmnShMDA\nQPbv3+85CF7wlFIMGDCA3r178+GHHzJz5kzmzJljfu0299zhmZKdnU1OTg7z5s2jZ8+eHnVb+/lH\nLrHM2jh27BhjxowhPz/fu6Ee8CIjI4mOjub9999n27Zt5OXlMWrUKMdDXNqb/eM/XeKlp6dTVlbG\n6tWradWqFTfccAP33nuv19pwh2fKm2++yZ133km/fv2ca6NG+vn5+REUFMSYMWMICAjgrrvuYtGi\nRdWoxn3FGzRoEIWFhfz++++UlJS4JLp0J6+++ippaWlkZGQwZswYbrrpJg4fPkxRUVG1XfG+1Edd\npb7J7fqKSIYxFrdRKbVfRDaZX3pi1zTFJIs6ffo0gYGBjBw5EovFwoULF+y33WYB1ARvxIgRdOnS\nhZ9++gmAG264gU8//ZS1a9fa19SLB7I8gAc2FjPI/3f27t1r51B56623iI6OZuDAgXz//ff8/nu1\nUTKX4kxmZW7Kat68OWVlZbRv377aOd70cyQD69SpE0uXLuXHH3/k66+/Jjs7m7lz5zJ48GCPeCb7\n53XXXcd1113H4sWLWb58OV9//TWbNm1i06ZNjB49mscee4xNmzbRqlUr3En6sTS+++47l4RzFoul\nWkOWk5PjcT+CO7yCggIqKysJCwuzP6fCF+ZPT/r99NNPxMbG8p///IerrrqKKVOmcPXVV1c5xjn/\nik6lsWbNGt5//31SU1M5evQop0+fJjU1FRGhT58+WCwWbrrpJoYMGcItt9zCkiVLXOI1bdqU4NNn\n6DXpkWpYCxYsYPjw4YwbN4709HTuv/9+vvjiC/twY03xTJk0aRJWq5Vrr72WVatWVakNZ7zGjZsQ\nUrqDXr0mVsPLyMjAz8+PFStWkJuby6RJk4iPj2f+/PlVNtD5igd6ODQ/P59FixYxatQor5u0SktK\nKD66k8cf/7zad9999x15eXm0bduW0NBQBg0axIMPPkhycnKVIVZf9bNarYSEhDBw4EAqKyu58sor\n7c/4cNTTFVleUVER27ZtA/T+ko4dO/L2229jtVrZt29flfNPnjxZhe35vyH12iGISIbx94xS6kv0\nRMsmh+89NmgAy5cvx2KxcPDgQfz9/cnPz6eyspLc3FxeeOEFnn32Wa644ooa41122WX88MMPJCUl\nUVBQwKRJkwC46qqrvPLTOIpJPjV06FBAdyxWq5WHH364GlmZLx3WiRMnyMzMJDo6mtTUVLZu3cqF\nCxfo0qVLrXYmlpeXs3//fvLy8ujWrRsdO3bkkUceYdq0aRw7dswjmd+SeW/w8qRDnDt3jpYtW/LC\nCy/QpUsXfvjhB9q1a0dcXByHDx+madOmHDx4kJdeesljw3to3x6OHj1K69atAeycLXv37sVms/HJ\nJ5+wc+dOkpKSuOyyy6qRlTn779Qfe0hLSyM6Wu//KCoqorKykp9//tnOr2Q+xMhms3n1nyf9tm/f\nzrFjx7j99tu57LLLXHLwOOff5h17OHv2LPfcc08VvCeeeIJ58+Zx2223cebMGQYOHMiQIUMoLS2t\nstvWEa9Vq1Zy5uwZl1iVlZWkp6fzww8/UFpaSkVFBUuWLCEuLo4pU6bUGG/79u1s3brVTt9sErI5\n14YjXpOISDl9xjXenDlzeP/99+nXrx9Wq5WdO3fy2Wef0a5dO0JDQ2uMt337doKCgnjqqac4deoU\nsbGxLknjHPMlJCQEVV5eLVe2bNnCddddR8+ePXnjjTe4cOECkyZNIjIykkWLFnH77bfXSr99+/YR\nGxvLl19+ydNPP01FRQX33HNPlbwRL+yuoBkB2rZti7+/P5s3b7bbWVBQwKlTp7j99tuZMWNGNdvr\nTepr7An9OL0wh///jV6p4eqRm7f27t1bZs1fIitXrpS8vDwpLy+XdevWSXBwsKxZs0bOnTsnmZmZ\n9lejRo3k7rvvlry8PHMcshi4Af1c1x+84a1atUpatmwpXbt2lfbt20uXLl0kKSnJcRwyjYuP9jzm\nCq/5bS/Y8UpLSyUuLk5mzpwpGRkZkpiYKEFBQbJ06VIxMHaj51TOAYc96bdo0SJJSEiQbt26SePG\njaVHjx5Vxg4d8HLRKxiOmniRN02WVo9/5hLPYrFIQkKCvPTSS7J9+3Zp2rSprF+/3hHvD2AuMMgT\n3po1a6SoqEh2794tlZWVcuzYMenXr588/fTTjrE4YMSjFbCzd+/e8vriT6rE8Mknn5QRI0bImTNn\nZOfOndKoUSNZvHixxMfHyx133OE4Rl+GXqPd29Az0xueiEhxcbGsX79eYmNjZf369VJUVCSVlZWC\nXqFmLqCw58vriz+RluP/WQ3vyJEjsn//fklISJBZs2ZJ586dnbmCzNw7auiX7k2/OXPmyPbt26W8\nvFymTp0qV111lbRo0UKKiooc43EUPex6rGfPnvLSvH+4xDp27JgcOnRIMjMzZfny5WKz2eSZZ56x\nz72hV6SYvvsDWO0Jb/PmzbJ//375/vvv5dy5c9K5c2dJTEx0tPegQ+32Bo7ExrVx6bszZ87IhQsX\npE2bNjJz5kzJzs6Wzp07S1BQkOzbt88xX3LQK7f2AINMPBPTObbp6ekyZMgQue2226Rbt27yww8/\nOOpXiN5jkWP8f8aTfqmpqRIeHi6//fabiIiMGTNGgoODZf369a78VwbkeMJzrI2PP/5Y4uLi5Npr\nr3XUr1o8WrVJlFaPfizr1q2ToqIiKSsrk+XLl0twcLAcOHBAvvzyS2nUqJF8/vnnUlRUJJMnT3as\nD3u9OcXFsaY9PmrY3as+7xBuA9412Eyzgbkisk4pVQjMA5aBnbriffQb7ntqFqX3PQhSSef2ibz1\n1lsMHTqU1atXM3nyZESE0aNHU1JSQqdOnRyvMDLR65zfBHqYeAsXLmTs2LFUVlYSFxdnx5s6dSqZ\nmZlkZGQQGhrKzTffzGuvveaofywwEvgNTdJXDa84oCnzDbx169YBMGvWLF588UXCw8O57bbbzKvN\nOPQa4o+NAHXypN9DDz1EdnY277//Prm5ufTuXW0HtYlnA2aj77xAKfJ2JHN+wwKafPA/VfBat27N\nnXfeyfHjx5k5cyaLFy9Gut/MmH+VOeL9B1iL7rSr4PnPH10lHp9//jmjRo2irKyMkJAQJkyYwIsv\nvsiPP/4IehPbrWhiwyT0MjlsAQH2KzSA0NBQAgMDadasGdu2bSMsLIyxY8faGVS///5789BDxt+F\nwMPovREe8datW8dNN91EeXk5gJ1F9MiRI6A32w0CktH7NHqYeH6hIXZME6+ystK+Q/7ZZ5/FYrFw\n6623ApjDXAeN3MPw43ETr89b2+z63WfgBQcH8+uvv/LMM89QXl6OUorevXuzbt06AgMDHeNxDP3g\n9zYWi4VGjZtU061Zs2akpaVx/fXXk5GRQWVlJVarlf79+1e5+jZ89xB6eWSeO7zL3tjC9EvOMHXq\nVM6ePYvFYiEgIID333/fEau1UspPRCoMXH+LxYIl1L1+paWlTJ8+nWnTphESEsKKFSvo0KEDp06d\nMvPlNeB1A3++iQcQHR1dBQ/08xXWrl1LdHQ0nTp1YsuWLVxzzTWmfpnoJdNZwA/As570u/TSS+nb\nty+XX365Pffuv/9+Z0p3038fA8094WVkZDBkyBBOnDiBUorQ0FCeeOIJnKRKPEBT6jz33HPs378f\nPz8/OnTowOrVq2nfvj0nT55k1apVTJgwgbvvvpsrrriCjz+2r1K215tSqr1DXB7iYk2b+V4zqae7\nAz/05rEEdKO1E4dNNzg8chP9UPoCc1VG3JRv7C9TysvLpVWrVtKhQwfp2rWrBAQEyCOPPGL/3rjK\nOAusR19hbHf1yEFHvISEBJn293el9aQvpVu3bvL7779XOQZ9dXGl8VrvCs/Usby8XOLj42Xnzp1S\nUlIiXbp0kR49ekhycrKpW5Fh61H0PoET3vSLiYmRw4cPS0lJSTX9TDwD8+9AmbP/XNm7YsWKKnjm\nsU540xz1cxePhIQEl/o5PVJyvZGUKZ7szcnJkfj4eDl8+LBkZWVJQECAvPvuu472/oHekLPfVb64\ni+/hw4dlw4YNrvyXBiwydNwMbHZlr2N8rY2jJebhD6T1pC8lpnW8s/+2GrZmotfa53vy38GDB8Vq\ntUqHDh2kY8eOEhUV5Sm+09zli6N+XuzdibGx0/DfTnfxiJvyjRw8eFBsNpu0bt3anX5mbbQwcnq9\nuUrGWzy85MsP6D0LHvHMfFmxYoVkZWVJcHCwc76ko/dj3Al8CZxzfOSls/gQD0f/ZQJ73OE5+q9D\nhw4SFxfnDe9KYKcn/Uw/uROnejPj4riR904z32v6+r/gMorFy9zFli1b6NixIwd6PkIRMP3OXcBF\nnp0lg0JAb7pKB3oB9ssyd9w+JyvDiWwejTptrcYlYxxbaugGejdpFTxn/Vq2bGkfTzxz5gzdu3dn\nbGoFT3bNAX2bCXo35b1oevBq+jlz3Vz73j5gHw9X57ox8UBvQnK7eSR+6reUZOwjuzK8GrePA7eK\nz3imvScNbhqoysXzZNdyx0PT0RQJ9qVH3riMzIemOHEZmWvKj+IlX0x7uxncQ87cOYY4xjYG+Am4\nwpO91sYt8De4qnr16euKmyYdfafxOsYmSndy7tw5evToYV/hNXv2bDt3jiEe4+Gcg45cS27sNWsD\nw+7jpvKuuLnOnTtHUlISTz/9NElJScyePduZe8j0n7mD/YQrOx3z2R0XlFG7ppxCb67ziGdyaT3/\n/PPMmjXLzmXkkFs2AyMdPWR5HqPmnPMvfuq3rBzWtFo8nOx19J+Fqhtpq7Uxpv/Wr19Pamoqv/zy\niyc8r/GIn/otT3Ytr7azzUUtpXMxLukuPq+x1HhjmlLqQ+CvwBkR6WJ8dh9692Uhuog/B7qKwWWk\nlIpH01HMRXMZtUHvKjW5jOLR28jNXY1xxl/QCdgGzVGTjb41DEbzh/yKLuxC9LxFJhBtvA9FDz1c\ngr4VDzF+Mw4dkGbohDR/pwuwBLgenQDlGGPPQJBxTiw6uDnohiXTOL8pupc+aegWISI2pdRTwKPG\nb/mh72qaGXabPCc9DP1aoQPpjFcCtDXwxgAz0TsjTf+1coMXY/jqmBe86ejds6UGjgU9xHISvVP3\nOHqrfhYXuW56Gsdi+AfRG79+QvNQBQBhaBqObHTBWrg4FJRo+HQP0Nr4vWLjt+KM96fR+WBDx7/Y\nOPcUOl+2G+fmoxvQRsb35hxEMzSFAIZOP6F3uL5jnGMzbDAL55ihq+k/k4unB/oCIYSL+RJnYNnQ\nQ2RRXIxvc+P3d6MbO38uxlehb/cbeYmHmS/nDV3DHHwfa+hi0jco9BBEqPF/Ljre24BP0fUWYXx/\nytAv2/B1D/SOaZOLpyl6yLSd8VtmvLqgd4D3RO/GLzB0Omr4Is94b/qxnaHbHnQducuXXw0dso3z\nbVx8NsEO47sSdD6AzqEd6PwRw6YCw8cPoRvZ8YbvbOi7j0gu1kEoOndKDT2KjO/w4L9ngMaGfiWG\nrtFU3SXdE50XNYlHmOHrpsb/wej2JBt9AdQYPf9otjlHuZjvzURzNy1GDwsdBV4RkQEASqmrgSki\n8ldqKLXZh7AEPT7lKDejuUraoTlCbnJ1ouhZ9rvRgWiPNtiKdpwV3UiEoRP3O3SDYjq2BD1f8D66\nwV+CToYD6OGqP9CNiIWLVBBt0EFaiZ7kNq/Umxg6fCzGI/LQt6/L0RPgh0UkyNCvwji+A7powtBB\n/QOddInoBkEBzxnnpxn2vgYsMvR6Ed1QVKCToAPQEd0hTjG+e9/Q5Td0EbyFHls28d4DPgIqRCTM\nQT9XeAfRxdAB3TkEGLFxxluE7txNvHLD3rZGTJoa8dgAXIMucoUmuCsASuTirskC9Fb7negG/Izx\nWSN0wncw/PeM8TsdDJww4zd7G34dbXyvuJgvTxpYfsbnjudOQVMtZBifRRjHvWT4cxw6P98zfHvI\nsPe8YaMFXeQdDP/NNvBWGu8j0Pln5kszNJ3KAgO71CG++eiGPoSLDdw/jO/KjbjGGjZMchGP942Y\nTkY32u+jayHEiG+p4Y+v0XnVBJ3nP6MbqbvRc1YtHertGLAY3WhlGXkQYtgdaehn3mUnoHOlHLjJ\noTYy0HOBZ4zf2YPOC6thZ4Xxf3PjXIuhVzsu1u8KquZLoRNeOrqu8404mrE+bBxTbthZiM6pU4av\nZ6I73N/QNBQWdOd8ObpxNv0Xb8ShyPBjMyMWMegGONuF/+40/JeFnvfY4aBfR8N/Weh8mYduw5o5\n6O8uHlmGbxcbduWjLxKaABPReWe2OU0MjGbo+TST5K+lEZcMqlK/mp/XXGozzoTDnIDx/hiQYvzf\nAh2Mp10dj06abDTf0TZ0ImUY33VGNyYr0UVyDngXnUxn0MHZib5Cv9845hF0L7wL3cOWoxuVveiO\nayM62bahk+Mgepx2j3FeADpx04B/orl80ozPtqEboWmGfo+jJ5muBH4B9qETYI5x3C70+PIW9JVj\nGJrG45SBdQidiBsMvKnoJLvS0G0nOsnOohPieuO4LUAfdHH8gE5qUz93eL+iG6Ng47MswM8F3jrD\nb23QBZVv+MdxpdgKw2/LDR3/jW7Ub0MXp6MPbVzkEtpv4B0ERjnkw1tApkPMS9HEeccMjAQjPluA\nq9D5kmb4rwT40SlfrkHPDYjxW08bv1+MbqCSgRuNcyZy8a4zz/DzNAfd5qAb+i2G7ovRDdVO471p\n6xF00VuN39nvEN9Cw7+r0XcEN6EbIUHn8BvG+80u4pGBziUzvofR+dMH3dDNQC/l3oCenyo27DBr\nxjnGVkOf+9F3LSXoRrSPYevLhn7p6DuhRui7qEL0GHUbF7gZ6Fz/FV2jZUZMT6E7qhOGTq+hydt+\nMD4/QNV8OW74J83Ae8PQoczw51bD/qeNmJ0xvi837P7E0MdsF9ahc7PCsNVmHHfesDcZncObDDxz\nfP85Q1dnO5PQHYU58vELFyew5xvxeBmdu+sNG8uNz+YYPnUXjyOGfrkGnhnfWPQdx1anNuc+w187\n0XnhKi5mDtnzvaavWnEZmUNADkNG2egG5zp0shQB3UTkd6XUSsOx5q1bDlARFBSUEN0ihlOZJ2nU\nqJHLvQBpaWlYLBZKS0vx9/e3r3mvrKzkjz/+qEAncgqwMSIiom9gWGNyzp12yQ0Cmv9FKYXFYkFE\nKC0tte8W3bZtWwG6EdwNhERERCR4wwO9PtjchOLATZONTt54dGEUREREtPaEZ+pWVlaGv78/ImLn\npzHwMtC35xYgICIiws8TnslP06tXr2r7IxzwgtDJszYiIuJbT3i7du2irKwMm82G1Wp15rqpRHfC\njxn2/hgREYE3ey0WC0opPZlltTpyN5lstFkYdNYRERFRnvAKCgo4dOiQO+6mYiBYREQpFYgP+WJu\nVgsICEApRWJiIrt376ZLly7s2bOnAj1E8ZhxePOIiIjowLDGZJ89RWRkZDW8HTt2UFlZaefPad++\nvSM3jRmPJuir1aMRERHxnvCcY9OmTRvCwsLsXEHoh1UtMWK8NyIi4q++4gGcP3+e48eP07NnT7Zt\n25YrIuEASqlLgc+DQ8PiykOj8MvJoKysjO7duwOwZ88eLrnkEkpLS7HZbBw4cIDOnTs710clutMr\nR1805QSHhkW4wtuxY4c9piLC9u3badu2LcePH3esjwp0uxKCcXcTEBBobR4Ty9nMDMLDw6u1L4cP\nH6aiogKbzYZSqsoz2w8ePGj6bwW6houDQ8MalYdG4Z+X6dJ/xcXFLrnNQHNLOccjODTsr57wTN4m\nU5y4zUBf+DwmIskOcTHxk4GJUpvGvZ7uELLRXEYH0T16kTcMc9WDOy4PkYt8MrGxsVU+d8WH4iue\nyS/iyE1jOK5WeBaLxRzz9MpN4wkvLi5OJk+eLEOHDrW/9/ZMYE94sbGxggN3kyM/TW3wIiIipGPH\njnLrrbdW46eprb2mPitXrvSZu8kdXuvWrSUwMFDi4+OlV69ePnM3ucNr1KiRnUdLRKrw07jDe2vZ\nKvH395fZs2dLYmKixMbGyvjx46WwsFDCwsLkvvvuE0fxxt3kCc985nj37t3F39/frrcnLi1f8Xr3\n7i3jx493XPfukoun9eQ14u/vL82bN7fj2Ww2KSwslJSUFHMPiFvuJvQmsq3A1sjIZvLWslXi5+cn\njRs3lpiYGImMjBSr1Srr1q2TlJQUmT59uvj5+UlKSopERUXJ66+/7pEbydTPlb2RkZECSExMTDX+\nptrgRUREiL+/v0RGRoqfn5/X+vCG59jOHD161Lk+jjrj1dervriMTgO/iUh7oC9VHzzjUdxxedRW\nfMHbtWuXMzdNrfG++OIL8vPzmT9/vlduGk94lZWVLF++nLfffrte7LVYLGzcuJGioiJOnDjhip+m\nRngVFRXs27ePkJAQTp48ybx583zip/ElHhs2bODqq6/2iW7CHZ5SigceeICzZ8+yc+dOn/lp3OEF\nBgby73//m127dlFUVOQTP01eTg5lZWV8/vnn/Pjjj+zYsYPffvuNl156yU6t4SjeuGk84QGkpKSw\nY8cOmjf3jbXZV7wPP/yQZcuWea2PioJsysrKyM3NteNVVFTY8ZRSNG7c2C13k4i8J8YcXlBYI+Zs\n1rt/i4uL2bp1K/v27cNqtbJ27Vp69+7NsmXLaN26NUlJSQQGBtK9e3eXO5ad9XNlr1KKjRs3cuzY\nMXf8TTXCq6iooKysjPHjx1NYWOhTfXjCc5Rly5Y510dNqM5rJPW17PQr9PLKV4y/PnOpmzQUAPfe\ney8rV65k7dq1TJw40U4wdubMGb7++utqwXdFDeGOG2T48OGUlJQwYcIEMjMzGThwIBUVFV7pDTzp\nZ0pISAhjx46lWbNmXhtId3g5OTn4+fkxYsQIHn74YZfn1sTe0tJSCgoK7NxNd911FyNGjGDt2rUu\nsb3pZ9IE//zzzyxZsoQxY8bQv39/NmzYUCv9SkpK7Nw+Z86c4eGHH64Wi5rg5eTksHTpUt544w2T\noprRo0fz6quv1sre4OBgKisrufzyy7HZbDz99NOEhYVVITZz1i+mkV59OmDAAA4c0PN+119/PcuX\nL0dEWLp0qX0D3U033VSNm6YmeMXFxfz888+Eh4dTUlLCjh07ADwS3fnbNN7EiRNp0UI/HuSJJ56o\n0gD98ccfDB48mAkTJlTjbnIW5a+prSMiIux4zZs3Z82aNQwcOJDMzEyaN2/uc324wrv++uv57LPP\nsFqtJCYm+nTR5Yznyl4/Pz8mTZqEn58fDz/8MPPmzaNFixYeO2hPeEoprFYrzz33HFarlX79+lWr\nj5rgDRw40H7csmXLeOaZZ3y2uy5S4w7BcU5AKZWOXib3CvCpUuoB9MTgSPcIXvHNYSg7wVhMTAyr\nV69m2LBhjrsTzVUZHrmMTG6QpKQkO8bs2bPtHEeOeL5wD124cIGDBw+SmppKSUkJjz/+OJMnT2bI\nkBN3wNoAACAASURBVCHk5+cb69VrjtekSROysrJIS0vjb3/7G5WVlQwfPpw777yzVvYGBQUxffp0\ne8IPGzYM0Pw0tdFvwoQJzJ49m+eee46ZM2dSVFREVlYWhw4dsh9bE/22bt1KbGws33zzDUOHDqV/\n//7VOvya4Jl589577/Hwww8zduxY1q1bR3Z2dq3sfe211wgKCqKsrIxJkyaRn59PaWlpFTxn/Qr8\nQmnZsmUVPqCsrCxCQ0OZNm0aX331FV9++SUDBw7k+uuvr8ZNUxO88vJye3zLy8vp0aMHSUlJbsnU\noqKiaBMZSrNmzdi/f7/9uN9//538/HxKSkro3bs3J0+epF+/fvzlL3/x2EFHRjZj2mWBzGjalJKS\nEvuxCQkJ7N+/n/z8fKZPn07Pnj1JTU2loqKiWn04S9aGBVgsFjIzM+1cWsOHD+df//oX8+bNQ0TY\ntWsXy5Yt4+zZs4wcObIKd5Oz+AVq/znOoZn/z507twq3lHmXZT6bw529L7rxH2gesY4dOzJ06FBu\nuummavVRE7z8/HxSU1PZvXs36enpREVF1YrbrMby3xqL8vTC4M55ZdFyt1weIpqbpqioSGJjY+Xu\nu++Wl19+WSorK81xw3T0zH8hcMYbXnp6uiQkJNg5xqdPny5z5syxj9NRld/nmDe8DRs2yLp16+S7\n776TP/74QyIiIiQiIsLkpnHJxeMJ79y5c7Jq1So7T0p4eLh8+umnkpeXJ+hVE+YqrQDgZ294X331\nlezfv18yMzOlU6dO0r9/fzt3E264fTzhlZaWSmJiotx///3y/PPPy9/+9jcJDQ2Vffv2mbH4g4t8\nRnf7El8RkYceekgSEhKcY+HMnVPmDe+zzz6T8PBw+f7776Vbt27y/vvvO3I3bXXA2g2c9IZXVFQk\nH374oVRWVspjjz0mCQkJ8vTTT1fRz8i/Q0CJOSfx/PPPy6WXXiqnT5+WrKws6du3rzz33HNy5swZ\nOzfNs88+K/369XMcozdXzpTgwC01d/lqCf/LHS7x0tPTpbi4WI4dOyb+/v7y+uuvS1FRkfzrX/8y\n57TOO8T4Tm/6/frrr5KQkCDTp0+Xbt26OT9Polq+tGqTKMEdr5GQkBABJDo6Wt5++23p06ePxMfH\nS0REhFx++eVy5swZycnJkYkTJzpyN4lh6w0G5q3mzl13+jlzm7Vs2dJjfXjDmz9/vuzfv18qKipk\n8uTJ0r17d8f6KMLgX3O01xPekSNHJCYmRqZMmSJdu3aVefPmOdZHpRFXhbHXwxueOYfw0EMPyT33\n3COOQlUet0/Qq7rMlVB2HiOH7/9AU1rEe22b67ORNxqEP4CpHo7zwyR7W7BULr30UgkNDZXw8HC5\n4oorZMOGDSIi8uWXX4rVarVP2JqvI0eOmB1CIXrVzVCzwfCEd8cddwggSimx2WxisVgkMDDQ0ckF\n6GVbfkCxN7xnnnlG/P39RSklISEh0q5dO3n88ccdOxdz2eRfgAue8PLz82XVqlXSqlUradOmjbRu\n3VqaN2/uOKmcjzGRhN54U+JNvylTpojVarXb26tXL8nMzDTxcoDBDng5vugXFxdnt7lVq1byxRdf\nmHiOSwkPY5LvecFr27atWCwWady4sSvyOMfldOXe7E1OTpbGjRuL1WoVPz8/adq0qbz++usm3lYH\nrL94y7/8/HxZtmyZ3VaLxSIjR46U8vLyKvoZBbgdg9xu7vLVUlpaKuPGjZPw8HCJioqSiRMn2jv7\n2NhYO2bXrl3lyJEjjg3QvUbOJJsd1tzlq6X1pNXV8IqKiiQ5OdllfaxcudLMvzz0yhY/oNCTfkVF\nRXL33Xfb68PPz0+sVquEhIS4zRezQXOHN2rUqCr1ceONN8rOnTtNvH1c3D/kD1zwhpecnCzt27eX\nxMREmT17tvOii2r14Qnv3Llzcuutt9rrw2KxyHXXXedYH/nABCMePtmbnJws0dHREhAQIBaLRZo3\nb+5YH0VGXAejl6Pme8NLSUmRoqIiCQ8Pl++++865Qzhi+G48eln+FvTS2U/M3zF88TfgXeP/O4BP\n/lc6BLxwGTkd65YryBQfuYzSHfDOOeO540ZKSEiohmcGzQGvGveLM57JZbRhw4YqXEYGllsuHlfi\nA5fMcfRGFtDr7Ys86efMFeSC+8Utt48n/TxwtaTLxfhW4wryhNexY0dp0qSJN+6cavY62uwD180+\nA8uMx3pf9POB2+eEERuP3FfO9rriMsJYuYcLbiln8RRfozZ2GbotMvQsryneE0884ahftXzxxMXj\nAzfXLsNWk/uq0BueO24kA69afdSRyygNveR0jy/2+pB/RRj8Qugh9fN15DIyedw2o1kj9qOH/8/h\nwGNkHHOl8b/5vUcW1PpaZWTnMhKRUvRGimFujo3FDXeJKSaX0b59+9i1axdBV9xOdHQ08VO/deT+\nKHXAM7fFVznG/OuIt3jxYqZPn+4Kr8wBz+MqKUcuo7Fjx3LmzBn2Z8PY1AoTzx8dAJNTxCMXzzUz\n12CJ6czSpUvZu3cvjz76KGvWrHHUr8wBIwbdAbu11+R+ueWWW+jbt28V7hfjGGdunzR3uoHmarHE\ndGbfvn0sWbLElX6lDqcVcJEOwC1eUlISRcNeZ+/evYwYMcIVXqzxykUPp7j1X+Lo2XauG9PeqR/9\n2xHPZmCZ8fCYf6Z+HuJh6mdB7x6tRk/rjGf6z8S7atyrzvH9/6g77/ioqq3v/06mZtKZ9J5QQ0JA\nEQRFwfIqNgRUigVRr4iicqUoXlCUBwQfQBBBwAtXULAAIlGKUgx6L1dFWiB0AgmkUAIppGcy6/1j\n7X3mzGRKgtznfd79+cwnmZkz66y6zzn77P09srllGWmZN97ie6hIxUg1anzotc7dyausrHStNbf5\n4on1JdlcWpaWm20l+6r5Q85d5Ek2kgd5XuvDtUm2lDYeLvIawCvXfdor86+g6LxT/rmwuaStceD1\nWIo3eZ6aC+urUOhmB5+Q2cAnKlfhiJWa65rvrfDWvB0tWvoCo1iXad4/BWChm+1Gga8kLgnF88HO\nvgi+qtgDXhoumSE28ZncRr6vFC8prw6OhUy1Qv4e8JGzTmx7Sci4pJF3Sfz+CDgJpDy57wY3+5ar\nJ+0aXS6K1x6NbafAZ6Zy2xrxOg3HCkmJuZAcoati+yLx2R40t7deyJP+a9ToIuVJH1/yIK8GzJaS\n8uS+rwrdbOChkGohrwY8TlsBPliWCz/LfVZq4it1rRHbyFgeE7+X/qsBn7HsEdvJ7/aImD0KBx9L\n6ndR2CrH3F3jka+JhTYelQBu0sTjkpBr09i7R/zd7/LdfqGbO/0+1OjXJP6SZt9SZp6w8TQc+VwL\nR+7ZwFcIEkMgOUX5aJ5/F8Fj+jLHL4n3V4Sd+eJ/ufo3T8iq1sgjcJ15yxetf8vRPF/qxHeSP1ar\n2b5JxEv7/qJmf1fAZ9//Aq9utottpG/sYp8tzedaNK8PrX7u6uOqF3k1YLyHjIWMq5R3yUM88t3E\n46DY9jbxfxMc94ukb0hse1ojV/YFTXD0MafBdWQDr6A/C17/tV3UXx74imGjeJ8LRmZA83241778\nf/KAQJohI/H/HvCy9C/BxVotlP4JfBOkGHzTrhbcCUjERSEEGgO87qEBXJyyk5Xy5LL+CwCWgZeY\ny2XvBeBl9U74WPE+B3zJKG9+ySRrAi/r/ws4Ye+Cg1FTCL6n8SOAQXAZMgIzb0joKsfd7eDl8JXg\nZxI0gpfGE4CrGp+9ApdLYqGzTCY57mkHL91/FDz1t1TIvir2GyNkNBsCEP/Ls1O7sMkOXia/Cgws\nLITj4NYA5qu8CWdMyW/ilQBHZ1sPx0H7I6HfV3DcCKwR296tvdSFBres0U+rmzYeEkVQL/zYAOYq\nOSGCxf8/gi+tpe/scEDyDgp7Vws5+4RMG4BfPOkHRkpIeQ1CXhM4bx8F53W9+Fz6gsA3B5sNGYHH\n2JvEbxo09v5b6LYMPJuvSmxTJuKzR8hyGjICdxRNcOSfa74sg4MnJPM9VtirHYaQ8d0DB4ZBdlzn\nwTyfInAnehe4fknIfR+cL9JWdcgIQJ2Qv0j4vR6ct78KO/8h5Lurj2ZDRuAa/lQT3zIhb7f4Tp7w\neaoPpyEjEbN4oespjb2/gvli68B1LllJRUKOtNVpyAjcPyWI/TaJ31wVesaDT0AeFPu6JOxYJuT8\nrx8yKoIzItYbXOkPAO0VRUkBXzr9BTyNdQx4Gfu94Ms1Mxga908wvOo0gEjxvOZyAMMURTGBoWc6\n8N11G5jFcg+Y3ZIN7hiPg0mIA8GXz2XgS/2bAewmohIAlYqi9JL6gc8MJaPmRo38teJ9BYD14AS9\nBL7Rs0H8ViZcpdhfIxhWdgqcDDKJ6sA3YvfAAQEbCk6MWkVReokHDj0iZAEMtzKBZ8s0Cdn/ED6T\n7J5YcKKEgDus3eDEWy/kWeFYK7IaQGcRj3yx7ypw8tYIP30lYvGokPOTsPkr8BqUYYqimISMSPH6\nUPjgjPDXL+K3a8G50Q1ArqIobcGXwHYABiGjvSYupQC6iM9l4b7kIR4A0EP8Zhq4Y9fKkvFoD37w\nTgG448kT/msEEyW/EnKawJC/T4W9r2v0yxV+ugigCxijUiP0WCjk2cA3+YrAeXJM+H4BHFeLW4Xt\ncoL9LeDi/Vnzm4XCn7Xgm4g9hQ6rhE/eFfGMgYP6Gio++wMO+N4BcKd1Bs75clHYmwa+8flf4A5o\no8iXEXDJF/Aw3FARuzyhZzcAfYXeAeD62A2u4dPgiSfDxG8NwoaVQj8/RVH6AHhOvB8C4AEh81ax\nrR7u6yMMjvpYLWIcCT4o2kVcY4S8G+HIo3pw7cxG8/rY5mLvVTDq3CB+N1/oZgVwN3j20Dnxmg4+\nkYjWxFX6cC24b4sE144OfGC5BXxgs4ERFGZwXv0kfFkO4A7w5IMqOOL/oLB9kthWGyu5Pgzg2v1J\n3IPw2K6JZdRMiKLowdgKyTL6A8DjROT2ifOKotxvtVo3RURE4Ny5cx5ZHp7YNIGBgdi7d28N2Emx\nAC5YrdYob/IKCwtx6dIldOnSBXq9Hg0NDTh06BBSU1MRFhbmyvcxW63W2IiICHWl6+XLl1FcXIyM\njAwoioJLly7h8uXLSEhIQEBAAJqamrRsGrcsHm/ypM0BAQFObBoA2Lt3L4E7pwvgjuEpq9XapU2b\nNjh79izatm2LmpoaVV5VVRWKiorQqVMn1f6amhocP35csmkIPIvHie0TEREBvV6PU6dOITo6GsXF\nxbDZbOpiICJCTk4Ounbt6sqmqQEntQ3M90m3Wq2zfdkLACdPnkRkZKQrm0bLznkYwFqr1WrwJk+y\nmzp27Oj65DDs3bu3lBi3LHkvVqvVGu3NXsmmAYAOHTo4rXjeu3dvJbjQvxX5kuxLPy1HS/7VsGlk\nfPXgDnuZ1Wod601edXU1Tp06hXbt2qnbaNhNEPlXAl4nVG+1Wj/3Jm/fvn3w8/NT2T4u7KZmLKiQ\nkJA+7dq1w+HDhxEfH6+yggoLC9HU1KTyvTp35ocFutSHdGUpuCOrCgkJ+cqbvPDwcFV3N/UBOHD1\n/wVgZEhISK927dohNzcXdrsdmZn8LIsDBw4gJCQEKSkpaGxsxPHjx5GRkeHKboLI53AhsyYkJKS9\nN/20LCQicq0PiLiOBZ80/hQSEpLZEnnHjh1DTEyMa30APFSUDQZy9gIfmC6BAXuvaGL1ORx47mEk\nnlnjsV2PISNxULkfjku9Erjhd2hfvtgqNpuNDAYDJSYmUmZmpvp9VVUViaNcM/aQN3njx4+nwMBA\nlVci2TSe+EOus0buuOMOmjp1qvr+1VdfpcDAQEpJSVH5L77YNN7kJSUlqbK0bBp3umnlDRgwgLZu\n3eokLzs7m3r16kXapmU3wQ0LRcqLG72cDOGJqryQkBAnOWghmyYqKoq+/mYDZWdnU3Z2Nt1www30\n9NNPq++joqIoMjKSLBYLdejQwSebxpf/JLtJp9O1it105swZSk9Pb2bvypUrKSMjg0wmEwUHB7tl\n07TW3lWrVlF2drbKbvriiy+kvR7j4cnexMREslgsFB0d3Sp2kyd5FouFnnnmGfW9L3aTnCWzZMkS\nGjFiBFVXV1NhYSGlp6fT+vXrKSgoiIKCgtTaIPLObvIlLyoqyi27yVM+S3kzZsygkJAQVZ6fn586\nHdRut6vxdmE35bvGNzw8ghas2kCvvfYa3XPPPbRlyxZas2YNJScn07Rp0ygqKopSUlJUtpmiKPTs\ns896ZS15s5eIaM2aNXTLLbcQUTO2WTN7r9fr+grjIQevNy20CfpfH/2DAFD37t2puLiYLl26RLfc\ncgv97W9/IyIGjFksFtLpdOTv708bNzqmabnrwL3J27ZtG/n5+VGHDh2oW7dudNttt5GiKDRlyhRV\nnqcCdy3g7Oxsuu+++8hisahFnp2dTenp6fTGG2/4LEhXWJUMuATvucL8PMmbOm8pJSQk0KFDh5zk\nZWdnU1BQEHXp0oX69+9P33zzDYWFhdEvv/ziJM+dvVPnLaWI6FjV3oSEBFq3bh1lZ2fTunXrKCEh\ngTZv3kxjx46l9957r0UduDt75ZoLORe/pTA/b/K2bdtG27dvp/Pnz9MjjzxC99xzj095Z86cofbt\n26vyOnToQMXFxTRjxgwCQG3atKGtW7fSzp07KSAggI4cOfKn4ztt2jTq27evz/j6kjdv3jwym83N\n6uNa5IWGhpLVaqWcnByqqamhUaNGkaIo9MUXX3jt0I4cOUI33ngj6XQ6AkBPP/002e12CggIoMGD\nB5O23XLLLfTpp59ek7yoqCj1AN+S+pDytm3bpvoIABkMBrLb7U525+TkUFBQULP6aI1+WlhjVVUV\nLVq0yGs8fMmrrKykdu3a0ZdffqnG21t9XK/X9bqHcE3Nla0SHh6OcePGYfPmzdi+fTuuXr2KrKws\nNDQ04Oeff8Zf/vIXldnSWnl33303pk2bBrvdjsLCQpw8eRIWi0V9iDfgDNuKj4/HhVqgX79+OHXq\nFPr06YPhw4ejX79+6NevH9q3bw8/Pz8EBgaqnwFAjx49fNr9+eefo0+fPi2CuXlqTU1NWP7h+5g/\nfz42bNjgJO/GG2/E119/jYMHD+KRRx7BkCFD8OGHHzZj07izd9lxPS5XVqv2Dh06FCdPnkS/fv1w\n8uRJDBs2DPfddx8++OADzJkzRx0SaK295eXlGDJkCJKTk1ttuzt5fn5+6NatG3Q6HaKiorBw4UJs\n3brVK5tGtoqKClXegAEDsHLlSvj7+0On0+GZZ56BwWBoEZvGm36KomD8+PHo3r27Cj5zbYqijFIU\nZY+iKHsKCwtVtMa0adOQkZGBgoIC7Ny5Ezt37kRFRQXefvttvPfee9i6dSs++OADjBgxAsuWLbsm\n/cxmM8xmM3r27ImoqCgkJye7ZTdJ/WqvVuK19AbccccduOGGG7B582Zs2LABJ0+exPDhw2G321FR\nUaHqu3PnzmbsJtdGZEf//v0xePBgVFdXo7S0FGVlZV7xFN4akR1PP/00goODVXlGoxFjxowBAJSU\nlCA0NLTF7KaW6ifZTSNGjMDFixevSd4777yDp556CtHR0R5//59o1wtuJxsB2K4oShP4Tvcn2i9b\nw1b55ptvYDAYMHr0aPj5+eGhhx5C27ZtsXTpUgwdOrTV8nbu3Ilbb71VZfl8+OGH2LhxIyIjI30y\nQj777DNMmjTJ6bP09HQ0NjZi/PjxmDZtGkaOHIm8vDykp6f7dJI7ebLDmDZtGqqrq93+TmOvYjQa\nMXT4E2jTpg2WLl2K4cOHO9nR1NSEr776ClOmTIHFYkFwcLBPO1csnIvzh3Jhr6nAP3/fi+XLl2PS\npEkYMmQIli9fjqSkJKxZswYAM1/KKqvQ7Y2vrsneuro6zJkzB3PnzoXFYkFVVVUzNo0n9pA7eyUs\nDwAefvhhNc4///yzV3vfm3ASFy9eRENDg5O9x44dg91ux+uvv44jR45IfXza6snef/3rXzh58iSq\nqqowYMAAFWambeSB3TRq1ChMmjTJifUkYYOLFy/GhAkTWsVucue/WbNmISkpCWVlZS1iNyWmtqM5\nf9TiwoULmDt3rjoGTkRqzu3atQvjx4/HCy+8gCeeeKIZu8m12WuvovDsWbz88sswmUwwmUx45pln\nMGXKFBV0qdPpWlIfCA+PwLiUcvytuBiJiYn49ddfAQDdunXDV199hSFDhmDp0qUqWNEdu+la9Xvh\nhRfw7LPPoqamBkVFjrk1bvU7exZdu3ZV9evRoweWL18OnU6HS5cuYd68eVAUBRUVFc3YZv+Rdj0v\nNwDEib+REE+y8rCdymrxxPLYuXMnBQUF0f79++nChQvUvn17Cg4Oph9//FFeNml5N5d9ySstLaXf\nfvuN7HY7HT16lIKDg2no0KHqpSd46KQ7eLaEyqbZtWsXWSwWqqysdLr8vXjxIgUFBdE777xDBQUF\nFBERQZ07d3a6pAPPNrADsMshmY8++ojMZjNt2rRJHWrKzs6mNWvW0Pr16+mrr74inU5Ho0ePph9/\n/FGyaaRukrV0NjIy0qN+JSUl9PXXX1Nqaiq98sorlJCQ4HSZDMd0X8kfKpP2Rj0xmxSDiRL+usbJ\n3q1bt9K+ffvIZrOpbBpdYBtKHL9ea68TK8ib/w4dOkQlJSV06NAhSktLo4iICC2bRrKCEiBmyfiS\nJ9lN69atc8dukv4rAd+8bfIVj23btlFsbCw988wztGHDBlqwYAH5+/vTypUr3cVjgy/9iHgo7/nn\nn6fMzExXdpNrPCb7krd27VoKDw+nuXPnumM3ubKl+vuS9+OPP9KhQ4c8sZvscHCH2gA4Joc8UlJS\naNasWdTY2EhlZWU0cOBAGj58OOXk5FBAQAAtX76cMjIyaNiwYc2erwCeLXYFQIMco4+JiaFRo0bR\n9u3b6fvvv6c+ffrQnXfeSStWrKAff/zRU3241c+SdjvpdDry8/Oj2NhYWrBgAd1///0UFRVFycnJ\n5O/vT++++64aG008tKygagA2X/Z+8cUXtG/fPiouLqb09HQaPHiwlt1Uq8mX0wAu+pIn2U2Sb+bC\nbvr/j2UEnpc9wc12KsvIG1tFy6YxGo0UGhpKDz74oGvQdOJFvuQtXLiQFEUhAOTn50e333472Ww2\n1wPCbvD0QRUdMGrUKHryySfV/Sa9sZGS3thIW7Zsobi4ONLr9aTX6ykpKUlbQFp2TgmAM57kybZl\nyxZ1HFH7OnPmjFa3XuDHRTJzRq8ni8VCd9xxBy1evJgWL15MREQLFiwgo9GoyjCZTBQQEKBl0+SC\nD9gmiAeWS/0Cu/angPQ7VDvly5XddP/991PMMx9R0hsbXe1VWUG+7JVsmrvuuovCwsK0Y6QytjFC\n10Jf8nywm6T/HgAvUKtpiX5JSUlkMplIr9dTWlqalk2jjYfOlzzJboqPj1c5Ny7sJtd4NGrj4Um/\niIgI0ul07thNrmwpr/GoqqqiN998k4xGozd2U56wdTaARtmh7d+/n/r27aveg3jsscfo/Pnzqn4G\ng4H8/PwoNTVVy26SB/yXwNOb83zJe//99z2yza5FPy27yWg0ktls1tbHGThYQVcB1PmSp60Pg8FA\nnTp10rKbajX5MhtAgy95smkfKKWpD61+/2tZRqfB82aN4CPhfgD93WzbIpZRcnIyLVu2jOrr65ux\ngoRTJE/mQQB2X/I8sVA0B4QcuGHduLakNzZS4sQsXywjLTunBMBP3vRrAQslB46Fc7cCqM/IyKAP\nVqxVbdFyUTZt2kQ9e/Yku91Ov/76K/Xs2dNpf3Be2NcbQJOWdeP6SpyY5dZ/8nt4YQV5s9cLa8mV\nZdRieVlZWb7855ZVpW2u+ZKamurKummVvBawkVzjUaqNxzXY65EtpW1a1pIPtk8tHIvxCgFUyw7N\nm35e8vmS0E0uTPvRFysoOjraW7541M9dc9cffPrpp+r2cF74dQUaGJ0n/Xzkc60mXwoAHPbFMpLN\n6QmN7vW7rgvTrtc9hJ4iEKvE+0gAB4joBzfbemTJSFZHfdFRXGm04L333sOCBQtw8eJFdO3aFaN3\nNgE7NyF/1gOAg6+SCr5k1GlliG0AOFgod35yFMBRvDBsGG598X2E9B6C8V1s6MebST6IT/0aSk7g\nSqMFTz31lMour7TEqvrBmZ0TBqCrqwytflu3blVZKDNmzHDHQpG6Abzwp8JkMkXo9QacDe2m2iJb\nVlYW7rnnHiiKgl69eqG8vBwlJSXo/eE+uYlRY2MceBjF4wSDhpIT6CRYMgBUlgyv9VPltZgVpGXn\n2Gw21V7tLoWcRvCiGyd5Wt5L/qwHnORdvFwGe2QHb/6TrKpMKUsbi+RJm1BfdBSZGnvvvPNOZGVl\n4f7PzviU565JNtKbb76Jfv36YebMmcjKytLekHeNx2VomDOuOaO1t6Kiwl2+uLKlyJNuUr9u3brh\n0l3vIH/WA6p+9392RusbGVsruJO0uMqRvpT6vfXWWxj56pvQaeIh5El2URx4MabXeptzUy0qKiqa\n5YtL7HzqJ2V++XAbtT/In5WKYcOGYdeuXYB1oNxMxjcNXBdGb/5rQT4rcORLOHg1fGetz9zZ7fq5\npmn1U1lGiqJ4ZRmJ763w9sQ1b0eLlr7w51hG+eDCPwVn9otkg9jg4OlIFozkySwRBjcIeefgwBbY\n4FjCLlk3dXAwjSRvRjJDyuDMuqmFYzHYRfAqT7nvMjhYKe5YRlp2Tr5Gnh2M47BrZEmeifzdaTRn\nLZUB2C78dx4OlEK+2K4GziwoydXRsp+OaN7XAngSziyoWmGrlhV0DLz4T8tucscKqkVzVlCt0KtA\nxOSYZv9yG/nelRXUCGeWUSkcGIgaODg+e8BXYL5YVVr/SVYVwcGH8SbvtItsT/JkPLyxubS8Hi2f\nyFM8SsRfyXw6KWQUaeIh96mNjye2VJ3GnjrwildZb5I95FpvksWzHFxvdnB9yfy3C/ukr2R8Vdob\n+wAAIABJREFU5TMFjmj+l3yxeqFfERz1USdeZ+G+P/CmX5MP/SQbqUEjr1bsR2IjJLfsmLBnObiG\nLsCxqlkuNpXDhrJ/kfVrA+e7Nv8KxO9kvjTBORckbaAcziwjmWt2NGcZSf2uO8vof3TaKfEshScB\n7COiILDDZQdkBSMvroKZJzvAbCA/8GpkE5ioehMR3QRe2r8SzAhqEPIugM+IDOArBj9wEX0EdrxB\nvELES/KEngQfsRXx/37wyuta8MrWK0KXePBl2hRw4DoBCARfBYSBbxydAt/wKRLyhoOXsT8r5B0H\nJ0A7Ie8guJPPEp8HgodKJC1V6hYv/LcAXMjLwR3FbnCCJAp5/wIH3ia+t4LPlqrBC1puAmMdEjTx\nqAJwp9CjTvhKPszjdwAzwUkppwC1ARfJI8K2fI29T2rsrQbzo64K/9kBfCzkbBf+CAfjFgo1sa0R\n8haCCwFg9EeZiGcleOl+KHglbjmYySPPgMLFNrlu/Pc0OC9+BRfUBg/yfhY2xgh760XsXOU9CS56\nGY8Nwt5kIU9iJWTcHwLnVwSAEh/xkAf7CqFfEHj8eLvYpjv4xrs8084CME/ES9bbEfFqC+7IpL3y\n/tsFsX0dGBdxVehmFDIHgDuheHC9FcPxACiJ66gTMQwBH8TfEJ+Hi7ib4KiPz8A1BSFjBjhfcsVn\nsWK/EcJ/F4V8f3Ae20VMAGCX0K/Bg36D4DhAmMVvrgh7L8NxQDCBc+IrEa8VQl6l+K1B+EUy0OrA\nfVQTmNZqA/dX9eCD0lTwyWkIOHeahDwjHHidRqFfExhfsg5c+35CzhHwyuYGcD5eFb6xavQrFtvH\nC1pECDhH5NQmFSmk+V4lQ7tt1+kKQQXWifdOwDOXbfXgI12KUK4enKRmMPvjEph98jM4gWcLefPg\nuPGWImTohHPrhHP3gxNAngVtEfsoBBdQATi5q8Dgs4fE/rUPY7lVOL8Qjs5nA7jAo8H3R3aCixng\nxCsBd3zy7EDK2wPg/4jPfoHjrPkk+KASDe6E/xDfkdCnCJwgdgDRGt16gWF+TWBW015h+1Fw8UUL\nfSqF3dUAPgB3RnsBnBWy0jW+7AJOzrbCfwTucOaAk7oODtCbPDttEDHaL2zb4+K/M+L7k8KeQvCZ\nzNdwcGpOgIuoj4jHZU1s60VsFXAnWyU+rxC/Xyi+v1Nsu0vY95rw77vgA22NS2x7gZfyN2r8V+9B\n3g7wRIkcYfN2NM+VXnAA3rTyPgAX350iBvKsrhHcATcKv37tJh7tNPHYC86BL4V+8ox3hoiNPFP9\nAcDPbmTJOjkNPohVC/99AD64/kO8Xy18ux+cS9PBrJ4TADqA80fW2xw4zuDfEvqsBedRntjHGOET\nmRfnhD/zwDV5v/h8DPim8mnwwfk8HGfbMh5NQq9Ksa9j4P5gg3jfC44rH1f9ZD5L/S4JeRvguFpY\nDkeeVwlfPiPkfSriLkcbSoV92WL7ZeATpsPi8x+E77PAB54CcN9oB9fEJLFvSYOVZF3Z5+SD0TS/\nC/lvCp8cBkP/zgk9pH4vw/mm8ho3/tXeVF7jqy+/3iwjI/ho1hZAHhFleNheZRnl5+e7ZXlER0d7\nZNMAUHk3iqKkAvinZA/l5+cjKioKFy5cQHp6usowateuHU6dOoW0tDScOnUKGRmsmuQH7d27txQM\n9loDIMVqtcKbflp2CQAcP35cy6aR7JyHwWcdeqvVqvcmT8739sCmkbqtAJ8pRVutVos3eTqdDna7\nHVarFQEBASq7SbBpSsEF/yy40/W3Wq3RwcHBKCgoQLdu3dQ59ydOnEBAQAAuXboEk8mEtLQ0VFdX\nw2w2a9k0NiIyaFhBkVarNcKbvCtXrsDPzw9EhIaGBrRv317LpmkiIr2Anf0TQKPVajV4kxcXJ4dN\nOaaKomjZTU6xBXDVarUGtVQeUTM2jWs8jlit1gd92ZuWloYzZ84gMjIS+fn5SElJkWwabTxsAFZZ\nrdZp3uRdvnwZjY2NMJvN8PPzQ2Jiopo3bthSitVq3eRNntVqxdGjR5GZmYm6ujpXdhPg4A5tAPDv\nkJCQDnFxcTh27JhbeaWlpdDr9ep6CRd2k6wPydq5NSQkJEbKMxgMan3u27cPERERaNOmDU6ePAm9\nXq9+58JuapV+RAS73Y7ExERUV1fDYDBo66MJfLJ4BjwiEGwwGBTJQ5IcM8kKa2pqwoEDB2A0GqHT\n6VBXV4eEhAR10avwn8RmNwCINBgMFm/yZG01NTWpsaisrERBQQEaGhqkftn4/4BldAbAZF/by1kP\nnlgeYjoZhYeHU1xcnBObhniHbtlDS5YsocGDB1NaWpoqz2KxqGyasWPHkqIoKlfFZdppM5SDJ3aJ\nds66GzaNR3SAJ3sTExOpd+/eNGPGDLLZbF7ZNADu9yUvODiY2rRpQ23btqXu3bv7ZNN0796dJs38\nkAICApzWLNx99900cOBACgoKopEjRxKRY+aDhk2T31p5cql/dna2OxTBNcmTrJsOHTq4sptaba9s\nISEhHtlNrdVv2bJlNGDAACIiioqK8onq8CZPsptiY2MpLi6O+vfv75Pd5E2e1Wolk8lE4eHh5Ofn\n55bdpH0lpLSlmGcXepSnRU24ojK8ybNYLOp6HiIivV5PAwcOJJvNRrfffjtZLBZ39ZHPYp3ZQ5Nm\nfkhms5l++ukntVa7d+9Offr0oSFDhtCAAQMoOzubvv/+e1qzZg1BsM3c1Nt0S2CQOsvn1VdfVZ/N\nLpvBYKB//OMfREQUEBBAv/32m/odeGjnamvkufZPRETbtm2jpKQkt/67Xq/rdg+BiDaDL617ENGM\nlv7u9ttvR25uLoKDgxEfH4+bbroJAwcORHh4OKKjoxEYGIiQkBBs2LABTzzxRIvknThxAkePHlXl\n6fV6FBYWIjc3F6GhoQgODlbxAdql9OQG5fD888+jtLQUDz74IIYMGYK+fftiypQp6NevH0aOHIlX\nXnkFr732GiwWi4q2uBZ7/fz8cO7cOUyZMgVGoxGPP/44li5d6nSmJtEBAKZJtIHZbMavv/6qYgbi\n4+MRGhoKPz8/2Gw2jB07FjNmzMDo0aOhKAr27NnjUbeomDhERkZi9uzZaGxsxNatW/Hzzz+jpqYG\nRIRNmzahe/fu+P777wEAwcHBEg1RqtVPohe6pMYhODgYo0ePxvbt2zF79mzs3LkThYWFqKurw65d\nu1BVVYX6+nocOHBARRzAwywIb/oBQHZ2Ng4cOIDXX38d06ZNw+zZs73Gwpu85ORkdOnSRf27cOFC\nlfp6rf576aWXkJeXh08++cTt71vjv4aGBsyZMwefffYZFixYgMrKStx3331eV9t606+pqQn19fUY\nM2YMfvjhB7U+JPHVXTO0ifcaD9k+++wz3HbbbT5RLYY28bBarerVz9atW2Gz2VBTUwOdTocRI0ag\npqYGJpPJtT5KAef69Q8KxheXkxATE4M//vgDt956KxoaGnDw4EFYLBbs3LkTmzZtwgsvvIDXXnsN\n27dvVymvvlpVVZV6RS6bn58fqqr4QYG1tbWu318GEKh4WOruTp6mtv5H2/W+qUxgdMVesUzbqbkm\nfITJ7pGFsmfPHhARpk+fjkWLFmH58uXYunUrNm/e7DHp7XZmg9x7771IS0tT2SA6nQ6NjY0wGAx4\n7rnnEBUVpbJp/vjjD4/GSHne2CXZ2dkIDAzEggULfDrHm7yZM2eipqYG77//PtLT0zFv3rxm7CbX\nA1ZJtR1vv/02nn76adTU1KC0tBQmkwlbtmxBSEgI/va3v+H999/HiBEjcMsttyAoKAj33HOPR/10\nej02bNiATZs2ITo6GnPnzsWQIUMQHx+PIUOG4NFHH8WWLVuwYcMG/PLLL6ioqFCHedzpd7lRjx9+\n+AHHjh3D0KFDsX37dgwdOhSZmZnw9/fH1KlTMW7cONhsNnTr1s2JCeUuX2KD9Jg8eTJWr16N8PBw\nTJ48GX379lUv03ft2oXVq1fj9ddfx+jRo9HU1OS1g/Rmr4ztgQMHsH///haxabzJ69+/PyZPnoyf\nfvoJixYtQkNDQ7Pft8Z/AQEB+Pzzz/H666+jtLQU48ePx549e9C9u+Npnq3xn81mg16vx2233Yb6\n+noQETIzM7Fo0SKP9io6z/ZKlENL2U21VysxoRswZswYlJeXq/pZLBbYbDbMnTsXf/3rXxEVFdVi\ndpM3/fz9/TFjxgzY7XZcvnzZLbvJUwsMDERlZaXTZ3a7HXPnzkX37t2h1+tdv9eB7088L07onm1q\nbMD4Ljbs3LkT5eXlOHr0qFvuk8Tu7Ny5Ezk5Oairq/Op359q1/NyAy1EV5C4hH1v8WcEgMrLy9XL\nom+//ZbS09OJyJkQef78eadt4WbISMrLyclRZXz77bdktVrpueeeI4PBQNOnT6eJEycSEdFDDz1E\nY8aM8SnPm34bN25shgJwldUSebNnz6aBAwdSdnY2TZ06lWbPnk0PP/wwzZ49+5rkJScnU9euXal9\n+/a0dOlSOn78OFksFrpy5YoqD17wzVqC62uvvUbjxo2j9PR0ys7OpuHDh9Nzzz1HRqNRRTm408/d\nQq3evXvTkiVLqLCwkIiI1q9fTwaDgebPn+/Tf97kJScnU1paGhmNRrrrrructrkWe6OiomjDBsf3\n27dvJ6PRSJ988sk16de1a1eyWq0UFRVFAQEBpCgKhYWF0axZs/6U/y5cuECZmZlyqIPKy8uvSV5U\nVBQpikI33HADjRs3joi4PubPn+/WfxI14cl/K1asoOzsbJoxYwYpikKzZs1St3Gnn1yoJXHkssXG\nxtLAgQNp9uzZ1LlzZ7V2ichrfXha+CXt1dJJs7Oz3daHfMFliGfp0qUqlpqIV3qbTCY6evQoXbhw\ngUwmE02YMEGbf/kQTyVsqTyz2UxHjx79/3fISBxcisTfi+CHh/TUfq8oSoKiKNmKohQAQGBQMFJS\nUrBkyRLYbDaUl5dj5cqVyMzMxO+//w6bzYZx48aha9eueOCBB9CvXz/tpZVFUZRCRVHqFEUpKyws\nRNvIYJjNZvTq1QtHjx6F1WrFxIkT0blzZ+zduxd2ux2LFi1C79698dFHH2H79u1IT0+XZ5EZiqIc\nVxTlCUVRcnzpJ+zBiBEjYDab8eWXXzbzh6Io0xVFsSuKYpf6xcTE4PXXX8eOHTuwceNGzJ07F+Hh\n4WhqakJ2djb++OMPrFu3DufOnUN2drZMICiK0l1RlAZFUerFAhOv+m3fvh2rVq3CrFmzMG/ePAwd\nOhRjx45FWFiY9N0hAK+Dp2tOkUNkbdq0Qa9evdCzZ0/s2bMH1dXVmDJlCl5++WWcO3cORUVFOHDg\nACoqKnDDDTdgxIgRUr83FUU5JXw4Wfrg4MGDSJrwLRLHf4M5c+agpKQEI0eORFxcHOrr6xEQEACz\n2Yzc3FzU1dVJe5MVRakStjaTV1dXh5qaGid5a9euRX19Pd58802cOXMGv/zyizYUGYqiHAbPnjED\neNebvTNnzgQATJw4EePGjUNOTg6ysrJgtVrx1FNPAfyEuVOKouwQOtbLITx5JfvDDz/gxRdfxJkz\nZxATE4NJkybhk08+wdy5cxEXF4egoCC8+uqr2lzqrijKIUVR8hVFueTL3sLCQhw/fhzh4eG49957\nsWLFCm19pGtkHlEUpc6XvN9++w2pqam455578O2332LRokXIzs7GvffeCwBdFUXJBU/rzAMQ5x8U\njLmH9B79l5SUhH79+iE/Px9dunSBTqfTXgF2k8MniqJEK4pSBgCXvvtv3HTTTTh27BhiY2MxZMgQ\n6PV6lJeXY/78+Th58iQeeIAXa+3fvx///Oc/pf8yFEW5V+Gn9n3dEv8REW699VbceOONWL16NUaN\nGqWtDylvtKIoewGMbWpswKsda7Bjxw5ERkYiJycH7777LrZu3YrnnnsOqampOH/+PI4cOYIuXbpg\n2bJlWLt2LdauXQvwNNkViqLoxUSTN0AEsjXAZrNh0KBByM3NxTfffIO6ujq8++676Nq1q3qT2W63\no66uDo2NjbI+FEVRjJqcOaUoygJPQ1KtatfryAJ+zFuQ5v9/wwVdIRxzEzRsFU8sD8niN5vNFBkZ\nSaGhoSpLRhx1q+GGJeONDZKbm0u9evUii8Wismk0N5X3gmeg5EPDMvIm79SpUxQSEkJr1zJC4uef\nf5ayPLKM3Mn7/fffKTMzk2JjY1V2U0pKipZNs0cjK7Il9paVlVGXLl1URssdd9yhZdNI3yngaWoq\njG7ChAkUGhpKAQEB1L9/fzp58iTl5eVRZmYmpaSkkNFoJJ1OR3379tWyaTyyeCZMmEB+pgBSDGZV\nXlVVFVVWVsqzHaeXmExwGjxr7bA7ea76ERG98847BIACAgJUH2rYNHvAs0VeFrpe8iVvyZIl1LFj\nR/V5HL169dKyaY5qfHdBG19v/svMzKTOnTvT9OnTm91UhmMaaxyAWm/yqqqqaPny5ZScnEwWi4UM\nBoMru6lWk3+roIE/erJXWx96vZ4iIyO17KYC4TfJHiqTZ+Ce9Nu0aRPV1tZSSEgIde7c2RU70wQH\nb2cbNKwgd/ki20cffURt27ZVH0ylqY8zcLB9lrrKc2fv4cOHqUuXLuTv7086nY6eeOIJbX0cFPLe\ndc1P+VChbdu2UceOHclsNlOfPn3o0KFDRMRn9507d6ZHH32UwsLCKCwsjCCemgjn5257lKetLc1V\nletrpyZnZC7e96f78T8rQBUEjATPna0XReJ2phFawLohcsDPYmNjaebMmeowiiapajXyvLJktPLa\ntm1LM2fOVD/XzjIS8jyyX7zJ0+qHVrKM3NmrbXBwlvIh5sa7soeIHKwTCVPr0KEDRUdHU2JiomtB\n1pIjHm9rD4Ce+DRae59//nl1f4Jl5JHFo91O/h/7wjL14KLX69UH2Wj0kw9kz/Ulz9XelJQUd/C4\nPeAFgqtFoXuV52rvXXfd1YxOKmwdDj64tji+Ml9cZlW1io2Ul5en+s9gMNA999zjjtYp868AQLY3\nNlIL/HdZxEKyh7yygvLy8igmJsZJP20DT72UvJ1aAHu18nzFw019aNk+E+CDPeQq78Ybb3RHn5Vs\npAd9yXONx0033eSqXzM2V0tZRu5ipu1fyFHHKsPoz7yuC8tIURQdeJFKZ1HMf4CHjNw1J9aNltsh\n/yd7E/zWjkXdHRPwet8o/GPOu9Dr9ShuPwgLHeyPRo08ryyZpqYmjBkzBtu2bUN8fDx69OiBAQMG\neHq4i0/2S1NTE1588UVkZWWhU6dO6o0kOdQALywjX/rl5eVhwoQJGDBggDt2TlsAi4VMj624uBjD\nhw9HamoqTCYT6urq8Oy6MzDuVFk3jZrN7S2xV+u/tLQ09RkBGns9snhcmz44Aoqi4OjRo6q9R44c\n0dprBB9Im8nTcowkB0Zrr9lsRnl5ORITE522BfsvGuxLr0OllZWVePHFF7Fjxw6EhYUhPj4ejz32\nmLtNC8EL01ocX5l/Lg8+8cpa0rb8WQ8gKSkJ58pqcVzMpOvRowfy8/O12ypw5F9v8Fl4P1e9WuI/\nD+whr/1GUlKSGl+p35EjR7T1RnDwdkzghW83tsZ/sj40+km2Txi4Aw7wJM81vjExMU6TI0STbKRU\nX/Jc7ZX1obHXI5vLHcvIU9Nsq+1fXPX9U+163UPoCcZKnCaiBvBirIe9bO81oRpKTiA+Ph6XN87B\nnLcn4uLFi4iIiIB/andvP2vWZIHs3r0bxfYQpKamwmg0qnC25EmbcKioolWypLz4+Hg89dRT6NGj\nh6rf6J1NzQoYfKD8VivHdRsJ30tNTYXBYNDA45q1PmCEw+/edJUwtaNHj2LFihUYO3Ysak/+1iI7\n3TWtfkajUYW9XWtrKDnRUntb1LT2HjlyBI888sg1y0uetAltR8xUYWV9+vRxgqm5ie8xuMTXtbn6\nT8LUPMjz2Xbv3g19aEwzedfa/oz/XPVPnrQJu3fvRmxsrNt6u5bmzn9/Jl+0MLo+ffogIyPDFQ7Y\noqbtX9zVx7Xa+/+yXa+Vyo+C7xf8Rbx/CsDNRPSyy3ajAIwDn+k2gVdTyoPDefHXCOaDVIKP9gp4\nDM4PvMy7GHzETQePab4G5qXoxXdSXpCQcUHISQKPSTeCx+FjweCxtuCrAhOAyeBl/QDffFTgOFNt\nEDILwJwVLSVUAZ/15ICZMwYhdx945XassMsPfIkcAGaVNIpt08BjjHYhqwl8VmISNkWCL4s/0Nhr\nEtsEgedhFwlZHcX/0XBA6uxC39PSd8QrgUeBuSsR4KG+y27iAfBZUqj4v15sUwFmr+QK/aeL378G\nB37hstDjgpDXRuir08izCd9ddZG3EMz1qRTydEJGhLA3TOMbCR4rEP6X47YB4JgnAXgVvGq3q5Al\nfV8qZOQK3+WC7yWZwZfmAF/51oHP7DoAsBFRkKIofwdD+PzEviQQMUDE8QCax1eOAdeLv57iawbn\njpzUr42HzL8Dwr8WEZMTwgYLmHWzDw4OkwEObs8FjazzwpcJ4OE0WVtXwFcqbcD5e0rYFCC2lTVZ\nCsam5AhdIPaXIuxrFL/TCX2NQv4y8H27JcJvOqF/F3BuVQlZ4ULHWHB8E8R+JeNHcoyy4ai9WLFf\nncbfh8T3ZnBNBwi7beDc8xfyzgv/7wOjJR4B538jHLlP4BOBDPDVTSA4nwPgyF+5DcBXe33BJ3TT\nwbkUKPZnEn+DwLkg+6IM4R/ZP8n4y/xOA5BNRJ0AQFGU4QD6EdEL+BPtf/qZyn8HF7giXoHgTuMJ\nYsjXGwCiwE5vBCdbPYDR4ITzBwc/GOy8g+AhFJOLvL1gDslI8Vk8OAjJYtsgcHG0EbKiwLyYn+AA\naSngAq4Fw+rywQUbCMfl30XwpXixsO0xsX8FPGwmGThmsV8duNA+F/t7AzwW6AcO8lBwgo8WMoPB\nBWACJ0K1i716cKf/pfDfbrFNvPgrmUYLhV1PgYtNURTlirBTXiIZNf4bCOAOoV8QONlJ6P0uuGOU\n8MBOQpcZ4E73azjAaf7CT1OEPBlPrbyLAP4C5h4dg6OzeQncKUh5BOYDNYJvqC2Ag3MkC/uvQv8r\nANaL71LFb6eBOxwSfpMHvhARIwkV6w7mTvmDOxcJVMsXftUDqBczOvqIfQWDDzxm8f7fYt/h4M5g\nN7hj7wc+yMwX2wYB8Cciq4hVBxE3f4jJCMKeB8AdQrjQ7wiAAhHze4X/LSIW/uDnDZQIe6rEfirA\nOVAm5B0BH4i6A/hOfJdBRF2FjyxCn1gwVwzgzi4Ajn5DnqavEv6RQ1WrhYxzYvu/CDv+KuRKEF+k\n8K1F+FqeZPXV6GcSftcJWXuETRZw7SSKmKSJ78xw1FOx+DsODnKyEZxrh8Q+FoAPCP8E52UnYVN3\nEYtoOBDYkiT8hpBXD45vrNhvhfClPEHoJPQygB89WgtnuN0f4Px/AHzwjRf7LQZwTsS3CJxT+8AH\nqkChqx+ASkVReolcHKGJx7W3P3sTgjQ3ioWTDgmDCt1s1wccoAtwnBmuBHcMR4Sz/wCD6s4IhxeD\nk6kKPEtEDz4LKgEnnITDSaTsJ0LuYfH+DHh6pcTQ5oHPdj4Cj0vvAR+1a8DBWSF+3wTHGU4JHOjh\nAjBR9KzQL1zYfhl8trACDjjew3CgqiWGd7LY7y9C3kFwQjQKXQrhANP9pPHdw3Age2vhOBudDgc4\nzAYGX/WFAwNuF/9/Iex7QsTriLBhM7izlP6bDO5Ec8X7c+CDXSO4IHbAAQ9ro9HvNDhxi8AdgpT3\nsfCLPIt3lVcJ7thLxT6jwAeBRjjO9H8DF5vEGEtQYT34IPx3sa2Mb4Pw5XZwHhSIz6qFvHqhm4xJ\nodDxPhHreWLbPPH5v8Fn8PFiG7t4vxOcX5UaeZfhgP6dE/uaAAfZdbKwxQYGmeUL/90k4nEZjvyz\nC30GgHM8T/jlIPjsUPpeklQr4UB7Xwazm85pZMl8MYI750+EHhKpfkjEREL4ZK1If5WC60DeZ5N5\nECXk9QHn/iXx/VCh35vgkxIJcJQTBz4Fz7yp13xeBgdQcIvYbxP4gLUdfBK3W8i6DMeB7j7wCdUu\nl/1Ua+RNEts2CJm7hLxicE6tETafAK+5eBZ8MDmokSdR80vhwPXIq+s2wt5CcJ8g8+cgePLGYOEH\nKatJE195oJNXyzcLWRJeWC98PQBMjF0PzplcsZ+F8PHwm5a8/hNwux7CgMeJ6LCn31gsFlL89Ejr\n1AGAZ2CUwWCAwWBAeHi4CosCgL1799ZCgKcAQK/Xdzeb/REby2Tc8vJyVFZWIjExEQDP5T116hQS\nEhJgMpng5+e4OGpqakJeXl4j+JkOL7VGP5PJBIAfYh4cHIxTp06hrq5ODodNg3ioti/98vLy0NTU\nBIPBgLCwMNhsNjQ2NiIqKgqnTp2SMLBRrZFnt9uRlJQEo9GI0tJSNDY2IiYmBidOnCglIoczW2Dv\nwYMH1dXeMTExrrG4Aj67mgK+xG61vfX19fDz80NERAROnDhRCr66Ww5x46y19p46dQqxsbGwWCxS\n3jzwtLzbFEWZbjKZJofHxKOqrBSRkZEIDg5W7YyKisLVq1fRrl07VFdX49y5c5CwQ2FvE3imUS+A\n15tIeZfqFHSJc2AIDh48iJSUFOTn50On00FRFAQHByMuLk4LK9PGdwz4zNJoMBgQGhoq58YDAE6f\nPo3Q0FBcuXIFBgPf6w0MDITVaoXdboefnx9OnDhRC2A5Eb3iql+0NRTaptUvLS0Nej1fOEnoo81m\nQ05OjlNtKIoyXafXT9aFJzvZ6sne8PBwnhRSXIz4+HicOnXKTkQ6TT7H+vn5xRjNZljDwpxwLXl5\neTAYDGqcATjlsqhrCVdMAHeWGQDCdUHh6NYhyUm/nJwcNDU1wWxmGnZoaCiIGLCYkpKC6upqHDt2\nzKk+pL12nQntEmPVXJHyTCYTEhISnPSWrb6+Hrm5uQCQQkT5LfWf9iZ3QUEB7Ha7iv5X7J7VAAAg\nAElEQVTYu3dvJYDzRNSx2Q7/bLseVwjioNIquF10dDT1vO0Oks0d4Kljx470zjvvqKsx5Tx/u90u\nj9SjWyPPYDBQUlKSunp39erVFBQUJI/WjeAOfA+APQEBAdS33x3qPODBgwerMCz5SkhIoHfeeYc+\n//xzSk1Npc6dO9O0adMIfDBwukLypV9hYSFlZGTQ8uXLKTMzk0aOHEnBwcFSv1oAL7ZGXlJSEiUm\nJqqAsjvuuINeffVV6t27t7T1ewCJrdVv4sSJTrEgDn4RgJ2ttTclJUW1V67WLCoqIvDl8RcA5rfG\n3rS0NNVeq9VKH3/8sdRvD/jMvIzfYrrVaqWp85ZSQkICVVRUqHLi4uJo3rx5FBYWRl26dKGePXtS\nYGCgW1gZHCt3i81mM339zYZmq3fDw8Np3rx5tGbNGsrOzqb169dTcnIyzZ8/n+bMmUNRUVHqNFah\nWwyAZVarlf77719S+/btnR5nKfV74IEHyLVlZ2e7rQ1pr7tprBIsqF25S0Q0efJk19roqpWnXWnr\nTp7rSurOnTur637Azy/R9h3T/S0BpAuKcIoFEVF4eDgFBgZSly5dqH///pSbm0tTp06l4OBgCgsL\nk2tZLgs5E8FXvsVGo4kiomPp008/dYpHSEgIpaWlOX12ww030ODBgyk9PZ1CQkIIfJWV2Br9UlNT\nVf207d133yUIsJ02X0wmk9uV3jJftJ8tWbKE0tLSaO3atbRlyxZ59afWxvV8Xa9HaIKINiuKUgi+\nhBusKMol4gd0tKhVVVU5nXUCgNVqRW1tLSIjIzFo0CDs3r0bt99+O9555x25yaetkde1a1eMGTMG\nISEheOedd/DRRx+hsrISJ0+eRIcOHS4B+JiI3gaAmJgYqm2Cytb5/PPPERER4cTaSUhIQFJSkgqp\ny8nJwVtvvYW3337b6zQ1d/rFxTHIzM/PD4MGDUJ1dTWOHj2Ko0eP4u67784H8LaiKOVE1HxJtBt5\niqLgwoULGD9+PC5fvozCwkLs27cP27ZtQ8+ePQ+CD9xfglnsrdZPxkI0K7jDbZW9FotFlXfhwgUk\nJCRI7PQN4HHmlz2Ic2tvQUGBam///v3x5Zdf4sknnwR4HPpZaB6raLfbsfzD9/Hh/PlOZ3wA0L59\ne5w9exaBgYGYNWsWdu/e7Q5WFgPg70T0iaIo0wMCAiZfqAXmHtIj/4l+6oYmk0nlNMmWlZWFxsZG\ndOvWTT1TlY2IShRFOQ8AZn9/pKWloaioyNMU6WatJbXhrkn2kE6nwwsvvIC7774b06dP19bGBZ9C\nNE3iwyMjI9GpUyf8+uuvGDRokCe2lKGurhbhA8Y2i4XRaMRnn32G++67D5s3b8bAgQORlZWFUaNG\nISoqCosXL8Yrr7wSpCjKcCKaDWC2oijT9Ubj5KpAziet7w0GA0JCQpw+q6qqwo4dO7Bt2zaUlZXh\n3nvvrYdzbfjUz9/fHzU1NRg4cCBOnjypfv/ZZ58BGvgegE8URZmuMxgnu+YK4D5fbrjhBnz99dd4\n7LHHoNPpAL5PNM2L+6+5XbcDgmh9iKhIUZRIANsURTlGRCpDQDvkERAQAH8d1AQpLy9HaWmp+r62\nthZFRUVQFAVbtmzBunXrMGLECIwdOxZr1qwBgJNEVO9JEXcAqtraWgQFBSEsLMypU2vfvj3AZ+Ef\ng8f5fMqrrq5GWVkZgoKCUFZWhu+++w4LFy5ssaO08qqrq2G321FRUQGDwYCtW7fi7bffRmxsLE6c\nOAHwOOhK8IwWtwcEV/3+9a9/oU+fPnjrrbfw4Ycfwm63Y9CgQejRowfAZ33vAihVFGU/gCZv8ait\nrQURobi4GDabTY3Fzp07cejQIYBvmq1rrb2SQ79161aYzWYEBwfj8uXLsFqt+8Bc+y0Abm6pvffe\ney/GjRuHDz/8EDNmzMD8+fNlbNsBmAtA0mLjKioq8OCAgWjTpo1TJ1VfX4+TJ0+ql/8ZGRlQFAXf\nf/89zp9XJ/roAFQRtWy8tba2FlevXkVQUBCqq6uxZ88eDBs2zOfvLl+6gP379+Pmm5u74N///jcy\nMzMRFxeHOXPmID09Hd9++y2+++47QNSGdkimuroaUf5o1iFL0ux///d/IyIiAmVlZZgwYQKef/55\nJxMAfKwoyg9SnhbO5irv999/R01NDSwWC8rKyrBhwwa88MIL2LlzpxOsEQAURTEAeFSvN8DS8ZZm\ndiqKAn9/fwDA/fffj5deegmRkZEIDw8HAPlshItwqQ2y29Fw4bRb3+Xm5jr5zt/fH4MGDcJj31zE\niv4BAF8B3qIoyljwzdp0nZ8f3nq0ZzN7GxoacPz4cbRr1w6BgYG4evUqsrKyEBISgkOHDqGoqAjg\n+yLX3MaMGYOGhgZcvnxZol7K4KU2/ky7rgcE0rCMFEWRLKNfNN9/Ar6R1ewM/MSJE1i5cqX6Pjc3\nF2fPnsWqVavw3Xff4fHHH0d4eDgWLFiAXbt2oW3bto3w0tLT07Fy5Ur1/cWLF5GXl4f09HScOXNG\n7XQ1TQFP8WqRvDNnzuDYsWP429/+hqtXr6KhoQGTJ0/G5MmTIeQo4kyvF4mxQ0/yLly4gIcffhhH\njhzB1KlT8cwzz6B///6uPyGhY4v0Cw0NRXFxMW655RacPXsW33//vfqQEI08gKeqVXiLx+nTp/Hw\nww+joKAAX375JV566SW8/vrrAIDVq1cDPBRTBS/Nnb0FBQWqvV9++SXGjh2LNm3aSN0+AjBNUZRw\nImqGw3Znb15enmrvsWPHVEy54Bj5gW+IPw4gx2g0Rvf4Pw83w5WbTCYkJiaib9++UBQFixcvdndW\naQHfrG5RKysrQ58+fQAANpsNvXv3Rv/+/bF9+3aPv2npFYw8a37zzTfxxRdf4Pfff1drQ3tGKq9g\nhvTr57QY1N0ZaU5ODgoKCjBp0iT5kQKgbUvPcBMSEvDmm28C4LNvm82GVatWYc2aNZLyapC1AT7T\nLTUYjame/CCPu7t371Yf+uSmaZO7VVccvXr1cq0N2VaAZ/3YjGb/nu7sNRgM6Nq1KxRFgcVigdFo\nxIABA6AoClavXo3HHnsMn3/+ud2d8Ja2AwcOYNiwYbI2AD4A9vRUG3+qXa+xJ3hhGYEf1XcRfEdc\nD+Ch6OhouunWvlRbW0uNjY108eJFCg4OpnXr1lFtbS1NnDiRbr75ZnVJ/qpVqygqKoqOHDlC4szs\nAhxTLwe7yot/ZTUpRosq7/nnnyeLxUKZmZmUlJREAwcOpPiXV1HSGxvp8OHDcobKFfBshRbrR0S0\nfft2KikpUV/gWQxyDn4qgJOtkUdEtGHDBrpy5Qr99NNPBDEuCl4zcAxAvTd5paWlNHbsWEpLS6Oq\nqirq3bs3zZw5k0JDQ2n//v0EnpY7Dzwb5hCA4tbGg4iopqaGgoODCTxFTubBgwBqtfISJ2ZR/Cur\nm8lLS0tTZY0cOZIGDx4saZ17AfwNjml6jVJe4vj1Hu29+eabVXuXL19OGzZsIJvNRuBZRjbwLJ5v\nAJBOp6PYhETKzMykTZs20YIFC2jBggUUFxdHL730EnXq1Im6dOlCaWlpNGHCBOrUqRMVFhbKBw3J\nqdB68Iyt8jZt2tDcf6yhxIlZRERUV1dHtbW1FBcXRz/++CPV1taqD5LZsWMH1dbW0ubNmykxMZHA\nM18eEPLMALYaDAZ6aOhT1NjY6FOe1WqliIgIWrFihayNOvB01E9F3V2V+jU2NlLSGxspcfy3qrys\nrCy6ePEi2e12qqqqotTUVHrrrbeIiGRtyNlep8CPclxvCQhUY+FNv8bGRiopKaFvvvlG/QueXRMN\nnopMABoUPz/SRyTRd999R4sXL6YFCxZQbW0thYaGUmJiImVkZFDPnj1p165dam3Y7Xb6+OOPZb2t\nBc+ysgGwGYzGFumXlJRE69evp9DQUIoZuYC2bdsmx+irwRMaCECFL/2Sk5NV/bS1sWPHDgJwQtSG\nzJemlvpP1sZtt91G5eXl1NDQIGdoFYEPVodEXBbgOswyup4HhJHwwDICcDt4aXouWgiM6tKlC6Wk\npKhsn+TkZNLr9RQQEEABAQHyt5s8yUt6YyNFDp3eTJ5k8YwcOZL8LKGkGEzyxtR58HTWK67yQm4d\n7lE/LdtHNjimgOrAQxUttleyWoYNG0Zt2rQhs9ksbyq/Cp5u94UveSaTicxmM+n1eieWzMcff0yx\nsbGyuL8HTzFcfi3xICL64osvtB2afNZwiTv/Jb2xsZm8mJgY1d7S0lJ6/PHHKSIiQur3L/B9jg0t\ntddgMKj2fv3112S1Wsnf3192QGPhBi4mUdnuQHuJiYmUmppKe/bsoYkTJ5KfOZD8zIFSP7ewMpkr\n7uTJ+D7wwAPNvhMx9lgb7uR17NiRVq5cSTqdjvR6vQoyhGNh4wpPtaELjmwmT/ovNjaWwsLCyGKx\nyP02gqfRNrPXm37afJYnEuKmsh184tUqeR07dqS9e/eqtREQEEAJCQkEPlnKAQ+rtjgeiYmJFBUV\npR5YdIFtKDBQje+Aa9VPWxviJn+dp3zxJi9u9HIiIiotLaW77rqL/Cwh8qb3VfDoy/9OuJ0wNg98\nNmwUwenssk0yHLAyr3A7m81GqamplJeXR1u3bqXMzEynmRai0z0NngvcDH5G5AzI0sqrr6+n1NRU\nOnz4sNM28AC3cwfa8iUPbuBYvuTpQ6Mp9oVllDjhW9XepDc20oJVGwjOsDwn/7nKS3pjIyVOzCJ9\naDStXr2a6uvrneQJ/TzKu5Z4wA28y1U/rY5SXgv1GwTgqic4m7RXGw8pT24PZ5jfVAC2jIwM+mDF\nWre5tWnTJurfvz/Z7XZatGgR9ezZ0ymnfNnrzn/60GinfIl57mN3+XIr+AYkH9ANBurYsSNFPjqV\n2tzzEi1evJg2bdpEaWlp1LlzZ9KHxlBAQIB6VqqxV5t/zwJoakk8tP779NNPtbVRByBX+O9tAI3e\n4Gye5GnyRdauTsjeH5+cSoaIZLexMKd0p8TXv6dff/1VjYWMhzjAFILXOtwuO1ODwUjQGShM+G3x\n4sVERDR69GgKDAykzMxMSk9Pd7pK1ciT4Ey5kLTYl34ffv6tW/2Eva75UuvOf97eZ2dna+X9x+B2\n/y9YRs3gdnJM0x17SMu6cWG/SGBUHHysuG4li6cajmXzf0aehE11hmM1sEd5+tAYGEKjoejcsn20\nMCsn/7lrDSUnoA+NQWxsrCf2i0d5rWEtabbVxsIJ3uXJ3nbt2rVUv2g4w/jc2uuDdaP9fSyARpPJ\nBL3ePUcpKysLI0aMgKIo6Ny5M8rLy1FSUqLdpNX2atlDd955pzu2VCF4QeFlAMXp6enoP2gonn76\nafi37YGgG+7H6NGjkZWVhbfeeguHDx9G3At/R1xcnKdHU8rYtgefkXvVzx1rSdNcYYi6a5HnpnZ7\ngjvwo4qiICDtdrexCMy4E4qioFevXu5iATjgio3gzrIhOj4BobcOB9VVYfTo0Rg9ejQrb7fjk08+\nQU5ODnJzc9HU1OROnrS3Jzgetj+pn2u+2Lz5rwXtPwa3u64sIwB3gy9ngsFL8iM12ySDVwbWie/D\n4Vg9HAheMXgAvBSdwJ18AXgYpwY8k0UugAN45shZ8P2KJPBQ1SUhLxE87t4WPIShCPk6IccEDspB\nsa1c3j8ZwBwhXy9+dwHs6AtCz0ChV7D42w08iyAAPOx0RejzHTixHgCv5GwQ+64W2x4Vtp0QOkic\ng+TdlAn7Oor3d4JXdBvFvgPF/6VCz2I4Eq0dHHgOyU+qFfKShP3u5JHwYSz4iq+diEm4+MwgbGoS\n9sj5dRlg7tANYBaPxC+YhD0VLvq1Fd+fh2YqqAf9LML/8pmTJnCRnhXvu4k4xIJjfQUc6xhwLpYC\naEdERjHrZqbwdQ0cvKoGcL50B1/dJgvd2oELLRY8Fi+nGbmz1yx8HCjeW8FnwslCRqLYj0l874fm\n+ZIA4BYRk3qxvRmci03C5g5CbqSIhxmcm5KpVQDn/HsYnH/Vwm+J4vNDLv5L0viiBLwqW660bg9e\ncbsAfIUVC8fqYrmCe5+QVQQHngHgDtAsdJBTjNqBhwQLwTjtcnBNFAs9roj/G8W+TcJ+iRG5JH7b\nTfyNEnL6gFfiBwgdZC5fEr4LBNfTcXCOSLxIIZgPVSn2YRX5sgSMe5H8Ij9wH3IVPG7fBdyfkbDX\nDs4rm7DpirC3K/gMXuaL5JmdA9eCZJJpp2B1g6M/rIKDG6Wtj1lEdDcAKIpyG4A3iOhB/In2n2AZ\n3QFesbrGzXdlYDKkEQ4n/h0MusoDX04ZwMm+m5jlYQcDsDaL3ySK13YAvwp5JH4XCB4TrwcHphAc\n+HBwsB8lIn9w0krYVgA4of4hdLkMR+AN4ALZCr7ErQIXTSQ4KZ4Tel8Fj3cTHMyhePCY5kYhywJO\nlB/FfkeL7VLgmO1VDgcd9SS4kALFfgeIfUv/yc52PvgG8Wjh31Qhzw4+6PyXkH1O+CJM+GeUizy9\nsHsJmF2zVWzXFty5XRa25go7zoj43CLefwwHnsAo7G0StrrTzw9cCPPACIh8cOGEie9/FnJ+FHpK\nbPUVcL68LPyniPjWAvhK6DQV3AlInpNeUZQLAO4R2/uBby7Xid/lCd8D3CnXAJglPi+FA4jYSbyM\n4Oc1VIvYWoW8KnBHu1bIlg/orRAyUkRcjWIfEcIfvwk/1wmfGYS8CnCungJ30olC/yvgaZapQk6D\n0LON2GaR2HeVkKkIeZL9lCf0aie+qwYfFD4W300VPgoHX6HJK2YjHK0GPIMwW+M7nfBTNTh++8AH\nznVCXnsh80Vhl8zTMLGPd8X7VcJf8sShXMg4ASb9moStfuADf7aIm5SnCP1mgmtyg9AvQfy9Csar\n/B2MBPkB3A9EC/2qBH1B1qVR6FcPPjH4VHxugOPE5iy4Dv4A3+f7O7g2AoRfiuDIF2nv2+AD3yHx\n23bC5ho4WEbzwCcAieDck/WhgyO/IP4vwp9s1+uAUASHswHvysmOrhF8pvayeP89+KzhFXBQ71UU\n5Qj4yHkL2BlNYHZNT/DK6GpwMdWCOxSAO7pycAFNAZ+hPQwO6jixTRC4iO4F8JD47QDwzBZ/cLDW\ngv0TAWYhSYbIQvB8ZyP4oJcFx30TA3gsUw9O/t3gqY4AJ0wt+IrhU3AxbxD7kTfDGsFndreBk3aS\n0PMuAE8LuVfg6FBqAQwTv0kFT+lNBt9MbBT/rxN6nQFjJc6CgVvtwJ32FXCncUns63H8X+rOOzyq\navv73zM1mUkmlfRKCC00ESnSRFERwYZXBS+CDfWqKBbkiopgQbzYrxRFVCy8oCBSr4IOivgDBOk1\nBEIJBJIA6W0y6/1j7X3mzJkzyeDlfX7Pe55nnmRmznzP2mvtfvb+HK7gzoIb3XeFjxzCH17xXbqi\nKHYAr4MzZwMxUK0C3Ig1CjtzDOxbJ2zqJNJ1g/D5SmFfJBE5wY3QBc01G4SfwsAF4yR4eerN4nqP\nCZs6ixinie+3gXtov8C3Jvw2oeMEV7i7wXnzXXDhnA/Onx3hGw3FCe16cF6QD6CpAUPy4sD55YCI\n3y/i+vHimmPAMZer444LfzSCkS+bwIW/CT7iaga44vWAK9dd4jpPCb16+EjA9eBe8r3g3ugmcMMF\n+BoTu7jGneK7t0U8ksD5TBHXMYu0dgaXiUShL5kK0SJmt4MXdywAN95F4F74rWCY33XwVdSzxG8l\niO0wuDKV5NQbxP/fg+99xIMbBxO4TCwHl6uz4PL2FriR6AbucJyD79kNkUKPwI3HneD8chrc8XgE\nXCF3BcPizoI7qHIl5FvCvibh20hwGXGBWVsLwPn1M+HnVPg6Ad+Ay+Uk+GCeW8D54gS43Jo0aVwi\n9GaI93LEBJHmTuB82xm+8rEZ/w/gdpeaZWQFByUXwAwimiq+XwjOHK3AGaPSYrHE2u1haGioR0pK\nisoEAoz5JYcPH0ZqairCw8NVfgn4wdX9FUW5OTw8fFk9WXBZXtsA+6qrq6EoCg4ePIjLLrsM9fX1\nPAebmorExETJL5GrC+S8dbiiKIiKiUVkVDTCrb5p05qaGhQVFYGIYLPZEBkZqa6Nrq6ulptRhhLR\nGkVR+gHYEB4eDi0rSH/s3LkTHo8HDodDZd3IY9u2baXgRqgPmK5os1gs0LJ99Ie0MSsrCydOnEBm\nZqbc5YhDhw6VgXvuMeAKz2I2my32cAfSDPS8Xi8aGhpw4sQJPz2v14uTJ0+isbGxBpxhp4FHOGkW\niwXB4ltTU4PS0lLU19fLjWOGrCVFUX4AD40tcp24jJn2qKysRH5+PogI4eHhAXyabdu2XQBXVOHg\nyvUOq9XqQEw6TOVFyMnJUTc/AcxxOn78OC6/PPD5G+fPn8eRI0cAwaZRFKUHgHV2uz3KiBUk06vP\ne61atUJ6erqMr4TYvQoG3jWYzeaIjMwsWC2B0/V1dXU4ceIEcnNz0djYiKNHjyI1NVXdTHfo0CE5\nOqwBV5ZWk8mE1Mxsv3wM8KKS8vJylJSUICsrC0VFRTCZTHC5XOrO9EOHDhH86b+NJpMp2mQyIz4x\nCS6nz3fnyitRcb4MjY2NyMnJ8WOGna+oxPnSEng8njpwB8MMMWIyWywpRmwfIkJJSQlOnjyJ7t27\no76+HgcOHECnTp3U/Cy4ZjvBjaAdAEwmE8xmM1JSUuA1cUc/3GpGdXU1SkpK0NjYiNzcXJX/JA/B\nNfPABwqMB7DbbLH0bck+q9WKzp07A+ANeocPH5bsJhCRIqYsJwLIkfGQdvnpnTuP8nNlAWUjOj4B\n4Vaztnz0ADdI4eBVRo/Tf1uh/7d3pTV3uYeCex4F4F7jTgADgp0v2TQ33XQT/fjjj3531+Pj46l3\n795+n8XFxdHmzZvVO+7givu8Vs/Z6RoyOtxuN73zzjvUq1cvqqyspO7du5PD4VD5NEJvH7hXlwyg\ne1JSEl3eZwBZYlIo+f5ZAXodO3YMWJdPRHIdc32w9AY7Xn75ZerVqxedOXOGWrdurWcFbQXvGQjK\nbtIfjz76KPXo0YO6d+9OS5YsISLSs5tKoOHTxMbGtug/vd6NN94o2U0HEYTdZBRfyc7R7rvQ8mmg\nYTfp46Fn+0i9jh07+q0YaY7dBGChy+WizOdWGtr36KOPUnR0tKEvxo4dSxBsGvLl/aCsICIKyHvj\nxo1TuU3V1dUEDZtGlKOylvQ6dOig6kVGRqplg1jEr2wAeFWmV3tERUWR2WwmRVHolVdeoaNHj1Je\nXh65XC7atGmT9vGy+7RphmAZhbfpRQl3vOKnB8VEAOiVV14JKBsJd74ql1dq2U1DAZQZsZH09sn0\nTp06VT1Hz26Sei6XS42tXFUVFRVFJpPPPiLfI3QNuGZdL8a+p556irKystTvbrzxRi27yUO6/CL1\ntJpSD1Aoqv/f1c9l2TCFRVDHjh0JjD6/ZHW39nVJWUbgaQEAgKIoDdDtVNYf9XV12L79cEjby4cM\nGYIZM2bgs88+kz3weABmgSKA0+lEXq7XkJWye/duvPTSS5g6dSoGDRqEXr16YceOHTh48CBqa2tR\nVVUF8NAwAkwRPJ2cnAyzxQxrXDqaKsv89AoKCpCfn4+ffvoJTU1NF+uqgGPXrl1477338P333yMh\nIQH9+/fXs4LkjbFPg6v46y1YsADt27fH3XffjdtuYxrHqFGjMGrUKCiKsgc8dRESn6agoADTpk1D\nmzZtVL3vvvsOTU1N6N+/f9DfBYtvQUEBjh07BrPZjBtuuAEzZ87EHXfcofJpLBZLIZjd1B3c60Nl\nZSUi7GbYW7XC6tWrcfbsWVVv9erVOHjwIF577TU1/vn5+diyZQtmzpyJ//mf/9GzmwoAwFMeiIaQ\nviOiADQEwE/bgmDThIKGKCgoCMh7AwcORH5+PlJTU2XvVGXTEDPBZiMIG2rXrl2YNm0aXnzxRYwY\nMQJ33303/vzzT7VsnDlzBuCyEXRXuzwuXLiA6upqfP7558jMzFQ/r6qqQlRUFGpra+VHTQAiFEVR\nRAOhoiHsKe389BLvmIrKlTMwb948rFixAvPnz1d9pz80/qs3QmEsW7YMtbW1+OGHH6Aoiuq/7t27\nq+d9+qlaJD7V+s/r9U5WY/vTBtW+NWvWYMSIEZg3bx42btyIO+64A1dddZVaNrTspouxz2w2w+vl\n+mfDhg0oKSlBTEyMRHWEtFNZxiPj1mdhjlLX46hlo8+7W/HxLfHo27dvsmA3GWJs/qvjUrUsaGan\nstErMTGRHM4ItbepPVJSUmj16tVExOt827RpQ2VlZTRq1ChKTEyUvYxTAAqkXrARQn5+PsXHx9Pn\nn39Oo0ePpieeeIKIiFwul37E4dcLSkpKoq5X9CFzZCtKf3JxgJ7D4VBpmFrC4cWOEPLz8yk5OZnm\nzp1LRPzA87y8PPUh5x988AGB527TQtVLSUmhvLw8Na36AzziuAvAUqkXbISQn59PcXFx1LdvX1Wv\nqqqK2rRpQ4cOHZK+CxghBIuvTO/TTz9NRL74Gtg3CcASo3hoiZPaeGiJk5IYq9GLA/f+ogC8GhkZ\nSbbEHD/7pO/mzp1LlZWVAfZt2LBBbhT8U5tWBBkhSD193hs8eDDdfPPNVFZWRnV1dQSeztp8MXrX\nXnutqqctG6IH6Vc2EGSEII+mpiaKjY2lP/74Qx0hbN68ubkRwgzFZKJWtzwfoJn+5GIqLy+n2NhY\nmjx5sl9sjUYI0r5g9FQi3tdgs9nUDaDSrg8++ICysrIIwE6d3gyTyaTGVtsTLy8vV+376quvKDU1\nNeB64A7DUgrRvqamJoqOjqb09HS/siFthQHd1WiEII+MicvJFBZJZ86c8ftcsw/hpLZsXMrXpWQZ\nJQL4Tsz1WgB8TUT/MTpRURSrzWZDYmqG2nvVfR8AtPJ6vZKbg/Xr12PQoEHyRk3Q49ixYxg8eDBG\njx6N1q1b44svvkDnzp3RrVs3la/Ss2dPebofn4aIkL9vN2KvHQ+T3eGn9/zzzwubiEIAACAASURB\nVOPBBx9UaZh6wqEmHUFhfgBQXFyMJ598EkOHDsWMGTMwY8YMtdcdFhaGiRMnyh7QQSI6GXCBIOkd\nOXIk3nrrLZhMJnTr1g0A8Prrr2Po0KHa0y1oht2k1bvmmmuwePFiVFRUoFu3bqitrcXRo0fRv39/\nyabxYzcBKDKKr9R76aWX1OcsyPiWlpaqwDIZAmh6udp4SD6NPh5bt25ViZMGfBo5t7oeQFJVVRVu\nHn67CreTsRg1ahTatm2LrVu3AgAcDocKLPv444/Rp08fuN3uFnt80rYXX3wxIO/t27cPzz333EWx\nafR6a9euRXFxsV98ZfkQNxmbLRvaQ4IVhwwZgvLycphMJnz44Ye499575Slq2WgJRmeyO+BwOFBT\nU4PMzEw0NjYaxfaijg0bNqChoQFutxvdunVDVVUVhg0bhm+//Ra//vqrH9dM2me1Wg3rFpfLBY/H\ng5qaGnTo0AEej8fIvma5Zvpj5MiR6vM5srOzce7cOXXkrGc3kQHXLOAgAnnqUVRUhISEhGBntTgC\n/CvHpZwyOgK+Y9/sIW5AV1gsFiSmpKKurg4WiwUWiwX19fUgIni9XtTX16Ourg47d+6E1+vFhQsX\noCgKoqOjsXnzZoCHxUsVRdkNoFVSUhJAXlXvzJkzuPrqq/HYY4+hR48e6Nevn2ydAQBz5szBe++9\nh6KiIpSUlAC8OiBWUZTDACw2mw0JKekw5fQAeZtQVFSk6k2YMAEAb+J6unMHNDY24uzZs7jizY2o\nO7YL4AxQAL7R0xvADZGRkcurG73o3bu3at/999+PZ555Bs8884yfj9avX4+ioiIsWLAAv/76Kzp2\n7Cjtkk9s65mUlASv1zi9zzzzDIYNG+YHLJs3bx7Onj0rM1gYeImfS1GUQgDOmJgYP/9lPfEV7P+Z\nhvLsa9DvppuwOfse7HrjRgBQC9EVr67D4x08eOGxewvBlcaz4FUaDhnfzGe+Q8Ebw3HmzBm06dYb\nEd2G4rU9EVjQjmMhgWW//fYbHl9fD5PdCaH1DIA6RVEqANi18fB4PH7plfEAfA3Mbbfdhvvuuw/j\nx48HuPB8AJ5rzgEQYbFY8GfycPTr1y8gFsXFxUhMTISiKNiyZQtsNhuuvfZa3HPPPZgyZQrcbjcU\nRbkcfEPPAbE4o7GhIcA2uSFKm/duuOEG7Nu3D+Xl5XA4HACv0DsF4IKiKH0APCb1Micux+9PXB6g\n53a71fgWFBQgOjoamROXo+7odoAXRbgVRfkUvKotnIhAHrbP7XYjPj4eXbp0QfYzS3FL0wYkJCTg\nyJEjCAsLU8tGmx4D5Y3NZJGnD0OygqxWkKcBMJmxdu1aVa/x3ClMmDABMTExqK2thdfrRUxMDOrq\n6oAmj/RDJ0VRbgSvSrIA6Ikg9lVXV2Pp0qVITk7G7t27ERYWhsmTJ+OTTz6B2+1G69atASBJUz6K\nAHjMFqua92qPbofZ4UJTUxOOHDmC999/HzExMaiqqgIR4bvvvsPNN9+Mnm//gdX3ZMt4VCmK0ij0\nopqzLzExEcnJyfjss89w9dVXo7TU16b//vvvGDFiBMAr1daJhRK5Uk/vv+rqapz/eR5MYRHo0KED\nAN78NmDAABARtmzZAvCKJrl0WJsXw8HL858gbYa7mONSDTXAG9MOgpdqTWrmvE+hY3aMGjWKZs+e\nTbGxsaT/zm630/z582nRokWUnJxM4eHhlJOTQ+BVTVtgwOKJ6juSXn75ZQJATqeTwsLCKCwsjBRF\nUdkqXq+XXD1vI1NYhLyZdA7MW2psSc/hcJDVaiVAofiEJEpMTJQQOv2rDL7ny6qvKVOm+OlJG6V9\nDz74oB+7SfyuSmhVhKJns9n80jt27FhKSEggh8Mhp6BOg7f6G7KWovqOIgCkWMPIZg8jxeyvJ4ew\nAq1xEDyM3Q6+ZxQQX8nvUaxhBLNF/S48PJzmz59Pd911F5nCIvl7vql8DNyYftZSemU8pH2STxN7\n3T/IHBFL4ErsPPgegp9WREQE3X333Wpek74DQDabTbVPsmlEnLeKvDcvlFg4nU5SzDZq27Yt5eTk\n0OjRo2nUqFFkckSRYncSeI65N4KwkYz0bDaf3siRIyk5OZkUi52sCdkE3+IOQ98tXryY2rVrR06n\nk0zhLho6dCjNmjVL1Xv99dfp2WefJYczQt7kbxD2zdfrOToMpAceeIASExM5nVZmSlksFrXsimkT\n/Ssou0lrX3x8fIB9ERERZLFYSLGGacvHR+Bl4AF5L6LLdWRyRKu+k/aFh4fTxIkT1bKh4Zo1gpeX\nGpa1YPalp6cHlA8Nu+mkUXqj+o4M0Atv3YOS7/1AnSqS7CbFGkbt2rWTU4wF8PHDLhnT6FI1Bi2y\njDTn9gXgDZUlo+WDyDk34eSd0LF49JwW+XfdunWGrBud3n7wWvBGWTmkpGc2y0OxtsqipNR0dXWL\nlk0j0joSvPErgBXUHEvGgI2kzs+D1zxvb4k9lJKSEjS90ndCrw+AnW3btg1gIsm/7y5Y4sfiaYG1\n1BdAjYyvlv8ir59w+xTq2bNn0Pjq7HtEHw8ZC308Wrdu7cenCaLXF0BDVlYWZTzzXVA+TbD8J9Ib\n4L+W4qFnGenicdFspBbiG8Ay0sdXG2cj9pBkQREnMijbR78CL+H2KYYcKG2eggHr62LYSFr/Cb2T\nGvsC2EjaOkEf2/bt2xvZJ/UKha2bQrHPiM0l/adJ75vQsKCCxSTY5wb+u6RMo/8NltFAAN5QWTLN\n8EGsCJHtc+DAgZZYNwA3ZEng1ryoU6dOmDT9vWZ5KCn3/RvPz/ggGA8Fwr70luwLgY3UoPm/Wrya\n1UtJSWkuvdJ3APvveICI5jhWkO/H4mmBPTQQwDkZXyP+S03+Zlx33XXNxVdrXwyAc9p4nD9/3jAe\nn3zySTA+jVZvIIBKm80GxWxsXwj576L8Z8QyMojvRbGRWogv8F+yjIKwoALYPnomU03+5pY4UHr7\nQmJ9hVg+ekIgOppjD2ljW11dHYw9BHAvvp2wsUX7mmFzNWrSG4n/nmUEjd6lZRr91ZZE+wLvVpyn\neT8awL8NzhsH3plYB17zL4FUdWAnbRWvC+K9BEF5xe+2gpdfngVPAewHj0xKwPPD5eDpkAvi3B3i\n71lwK90g/jaJv/J6leJa/4Zv802l+F9iMLZqbKwHZ7xGodkgArFP6JSI9L4pzisR1zwBHs6XijTv\nEdpF4lrSnhLN+7MAKjT+q9DYViv0vODeuUx3k7DNK2w7InRKhL+k78bBh21oEraeEedK320VNkqf\necT1z+p810NoyvhK30nfS63TwqYajX0Vwnda+9bBl1/kDuVCoUcava2a9FeItIaqJ2Oq1avT6J01\n0Dur05P+k89bMMp/BSK9peJ9LTivaq95O4DXxHklwq/FGp/Lcz3w4VLk+yPid0fgyz+fgDEkp0V6\n6jS+qtfpFcLHp6oVdtaKtO6DWCUj9KrF7+vE+ZXgvHZYfFYh/q8QmrLsSl/q7TshdKR9pzT27cDF\nlY858OUnmZZykZ4D4n95H076tFak0WOgNxH+5a1a/N4DX3kj+KaWPJp4eMBlaZ+wRab3rC4eMj6H\nhS17dbEp09gl/Vcp9G4Hl7t1mjq2P4CV/9sjhJAO4icubQFv27aBb4IcBGeYE+CeVw58cKp7wTtf\nj4IzyXTwsDBefB8LH4tHwu/WgbeA3yPOkSyeSgCniVlGEjfRAYzEiBC2DAEH1gwfb+k0eIrgKY2e\nBZxR7wJnkErw/OBWiB2YiqKsABe0w0JLAfcOPgbzSWrh48zEC52niPklteBMtlfomRRFeVn4rww+\n2qQVPv7Se+At9cPBGdci0nkOzKZZLOyMF+lqI34r/WcS13sXwDtE1I2Iegh7GsX3w+FjI+0SehLm\nViTS8ofGdwRe1bNDo9UHXABkHM6J36wF3+xOErpddPkF4lrrhd+GC73hwr524DxixKepB98fChMx\ntAIg8XuJ7BgOnoeVLJw5Ik5S7x7wPZIoEZ80kc4G8HzzEfHdOvAu/R5E1A0+bMdm8B6XHiItkfCx\nkSQ8Tc9GqgFXPKUa/10Qek3CZskeOizsbi/ScCX8WUZV4vdVAPZqfHcBDEw7IXQShJ/t4LySCb6h\nvBuBbJ+D4v2P4rpV4PKSA65M98KH1ChGILtJ2mfV2PcLfPmlm/BbitCRMa8TMc8Alw+5okoCKSX/\n6zy4Qv4EnD8c4vtkkW6ZdyaAG4tscJ7ZLlZq1YDznhO+TuAFcW47+PASYfB1VArAMyQExntEgvOG\nZEvJ/FUFLgsAN4I5wg/ZwpcrwHXAaXC59oJ7/nHCBokHKsIlZBq12CAoipKuKIpbUZR9iqLsFc8Z\n1R8JAMYoirJDUZQd4MITzKhtYGd9A3ZMWzAEawV4XixevD8MzqDLwY44Ar6p5QEXzNbgFvgcxHZ6\ncMWYJn5zCsxJigffgCZwgQW4h9AI5iJ1BhfqjeJ3cqfZN+JvK3AGvhrMG4kHsBRccF+Dr6LPBK+y\n2gXgFnBmfAl8kzUMPsrr40J3PfihQtvBNyfNAH4S/ksQ5zvAUxzHAYxWFOVZcEUqoW0EniNeAeBJ\ncAU7QaSBwPC/KGFbrvCN5NOYwQC3TeDC3wCufO4CsFxRlGQAUBQlBz7KJOBjI9WK63rA+y4kx6gt\nfL3CBnCPRZI1AeAB4bMmjX3p8MHuYgBcCyBKUZQXwKMOs9CMBvOn9oI5PhDphUivC8Z8mo7gyuI6\nMBtHJE3ZJzTLhd4/NLa9IT6TeiZwb6xB2CKZTe8IvdbgCkI2AlAUpTc4j5WDb+CniviawBWMZCMd\nB28UPADOtwRmLWWCWUeVmlgQOG9EgctSNBiy2AY+2Ns1wjYty8imKIqEIh7T+I7APehKcOwbwI1D\nFXi/hVN89z/wZ/uYxLXDwJX/7eDKOwFcYR0W38vecAwC2U35wg8AEKMoylPgVVFfi/R2gq/StAHY\npijKS+Al7hVC61YA/RVFeVDYboKvrDvA+VGyvO+Cj43lFf4LBz88SQHHeRj4WcUTIJ6dLV4WcJmW\njQ7Aizz+AJff4+L8leBy1gDOf00i/Z1EPE6Ja9vAC2xiAczWxONbIooAN7bHwLMICrguOwBuxBLB\n+WyLLHeXimnUIstIVAzJRPSnoiiRYCfeQkT7NOdcAx+EqQjspFFEFPDcWUVRcgHsCw8PtwRj+xAF\n8kv27t2L6OhoucxM8n0+AvfcLRaLxRKM7UPk47Xk5uaivp7X+Eq+j+TBgDN+JFgwKCtIq2e1WpGV\nlYUzZ87AZDKhVatWOHToUBO45xkFzvQxZrM53R4WjkYDtk91dTXOnD2LJo8HaVmtEW414/Dhw4hP\nSoYzLAxHjxTUgDNVOLg31MVut8c0KrYAdlN+fj5cLhfOnj2LlJQUFBcXIy8vDwcOHEBcXBwcDgcO\nHDggCZ5y8bXVbDZDzzKqbWxC2ZnTiI6OxtmzZ5GYmAibzYYTJ04gMTUdJpMJdbU1KCk+7SUis6Io\nn4MzZFD/nTx5UtWLj4/HuXPnkJWVBQAqV+bQoUOlYCjgMJFmk9lshj0sHA319UhN9fnv5MmTCA8P\nx7lz55CSkqLyfGobuV23KMDRIwWS1NoGXCE6rVYr9Cyj/Px8JCQkoLCwEHa7Xd0rUV1dDYvFomXT\neMGVxmfgijfMbrcneqNSVT2v14tTp04Z6hUWFoKI1OcYbNu2rYKIogBAUZTrAXxrt9sjYhNTcO7M\nqWbtq66uxoULF1Ruk9CTo4d7wAyxRKvVqihxmX4sHql3/PhxZGZmwuVyob6+HgcPHkRWVpb6fs+e\nPQB3qmIgsNEmkylKz0bSxjYxMRFOpxONjY0oLCyEy+VSGVSCtXQaPKq9F0CqyWRymUzmkGJ7+PBh\npKSkyCW7OHTokESy8xJR4H/MFsvwJlLQsV0uTCYT9uzZg+7du6OqqoqXQLdpo9q9b98+tGrVCq1a\ntZL+095DOAUg1myxREmWkfRJ9+7d1b0ukl2UlZWl2llRUYFjx46hoaHBC25kJerdbjKZXJJl5Gls\nxOmTx5Gbm+u3d0bPWaptbJIsIwBYQUQ3iTxz6ZhGf+F+wfcArtV9dhW4d3AIXPAmt6AxNNhO22D8\nkuzsbJVNIx6zeAGCTUMUfOeukV5ubq7fDtWpU6cS/HdihmTf3/72N8rLyyMioj/++EPl04BHOF9D\n8GmIfDuB9ewcLV8lqv/f1ZUEcXFxlDT6LbmqZSsYZXBer6c9Fi9eTDfddJPKCvriiy9U+7R8GqH3\nqjbNRnrxN0+im266iYj4EX/Lly9XWUbqiqE7XyXodmYH85+0T+pp7SMi7c5YP3YTgKHSvvA2vVT/\nadMbHh5Oa9euVbUyn1tJ8cOfIXtYuJy6UtlNMGAZaW1LTEz04yy53W49myZg52lcXJyf3pQpU4Lq\njRkzRuU2GbFpoNmp3JJ9brfbj9tkVDZgsFNZH4u1a9eqZW3JkiVqLIzKBgzYPs3pzZgxg/Ly8gzZ\nTdI+yUYKJbZxcXE0a5ZvdRM07CatfVLv6NGjBIBSUlIoPj6e0tLSqKSkRP290+lUuWZCz5DdJNOr\n1UtNTaWxY8fStm3bKC8vz4/ftHbt2hZ3Zmc+t5JSH/4kQK+kpCSABZX53ErJbqqHhmt2KV8XdQ9B\nPOTmMlH5649ccC/2EHz8cf3vxwn20LTKykp15672tWzZMqxYsQLjx4/345fU19fjk08+wT333IMv\nv/wSwiktsjwuXLiA8vJy/Pvf/0bnzp0xYsQIDB482G8X44IFCwDBpwEAYi5Ti3qyZwsAubm5SE9P\nl5TSy8D3J6Zpf0sNtQHsnAsXLmDlypVQLHZU7fwRZxZPwd69ezFkyBBUbF6COubJ2MFTQxGKomxV\nFGVrXV0d8mK8qt9Wr16NCRMmYOTIkaiqqkJdXR3279+P6upqrF+/HlVVVTh48KDkNgGM3I5QdNt5\n5VFZWYkLv36O9957T/1s6tSpfmykYIeR/yorK/H888/76TVzpICH/p9q9aihFg1njqBXr14BevX1\n9Rg3bhxuuOEG7N3LA1Nnx6vwr3kLAR52zwFwrch/A+vr6zE2qQibNm3ChQsXVN+tX78eDQ0N2LVr\nF3JyctCrVy98/fXXejYNAL/8fF91dbWf3kcffRRULy0tDZ988gm++eYbPPLIIwCQrCjKSL2eUnmm\nRfuefvpp7Nu3z7BsaPXq6+tVFo82r6xfvx51dXX4888/1bIWGxuLqqoqrF+/HnPnzgUCy8ZsaA6j\n2Ho8HpW19NBDDyEhIQGpqalyl7nKblJ1BRsplNgOGTIECxcuRGVlJQ4fPgzwSNchyADTANR7GuoR\ndr4ADQ0N2L9/P+bMmYMFCxZg/PjxKC4uRkZGBnr16oVPP/0UtbW1OHjwoFqW4GM3qf7Tsoy0eu+/\n/z6OHDmCe++9F9XV1arf1q9fj507d/KGvBYOU7gLSfe8g2PHjmHbtm2orKzE3XffbXjuyy+/LO37\ntEXhv3KE2nKAh8bbANxm8J0LQISmhc5vSa8lWqeeX9K6dWuVTSNa8e0Qc4UXq6dteTds2CA3t/jx\naVrSa2pqooiICGrXrh0REd19990qn0b46UVo+DQxMTFkCos0ZDeVl5dT2pOLyBQWSfHDnlHZTY6O\nAykyKlquY34VGj6Nvkf/1FNPqRTIYCMEyacB98B7oJkRwlNPPUVR/e4mIiZKOp1OuvXWW9Xvmxsh\nGPlPax9R8BFCMHaT9F+rW54P0CsvL6eMjAxau3atyh6S9mlGWCq7CTqWkd62jIwMWr58ORERLVmy\nhMxmc4tsmpiYGD89bV7V6q1atSqAnwMdm0bqpWW1btG+6dOnk81ma7ZsQDdCMIrF4MGD/dhXbrc7\naNmArsfckt7dd99Nffv2bY7dpLKRQoltWVkZXXPNNc2xm/z0tEd5eTkdPnyYANA333xDbdq0IafT\nqafFNjtC0B+nT58mANS+ffu/NELQ7zuQeqtWrfK7TszghwzZTZfyFWpjYAXf8H0qxPMLAcQ3d05L\nFa7c3dimTRvq2rUrRURE0LXXXqsN2kU1CFq9nJwc6tq1K61atYoeeOABGj16dEDQWtK74447CACZ\nzWZKTU2llJQUWrZM3cizFb5HYsYDsFosFrIl5QbVy3j2e1IsNkoe+x5lZmZSSUmJvkJ7HcBCaZ++\nAu/atSvFxcVRYmIixcTEkKIopCgKmUwmSk1NpZycHProo4+0DcL94OdJBNUzhbsoMTFR3UFuNpsp\nOTmZunbtSgm3T7moBkFrX2JiYoB98+bNo2effZZSU1MJwC7yz0+q/2TB0euZTCaKiYmhN954gzIz\nMynt8a/0/vu7LEgAXjebzRQz6P4WtbZv304AqFWrVqpvRVyLAWRJPavV6qfncrkM9Yh4ykc7ZSEa\nBC1M7XWr1Uq3jLq3Rfvcbjc5nU4aOXKkVq/ZBsFIDwAlJSVR165dqWvXrjR9+vSgZUNfQbakFxYW\nRqNHjzayL17ULQVWmz3k2OqnU8D3IxbKvKLX0x/FxcUEgC5cuECZmZnUrl07+uijj7R6hdqy0VKD\ncMstt6jlIz4+nubNm0dEf71BkPatWLFC/eyTTz4hc0QcFRQUBOhdylcolbsCXl3zbjPnJMF3g7on\n+I67EuRcC4DhSUlJ1KPvQKqtraXGxkb68ccf6c8//ySPx0Pl5eX0+OOPU3JyMtXW1hIR0VtvvUXR\n0dG0fft2amhoIPgQE/UAhkm9jKeXUsaz3zerJzNT+lNLSLE56KeffiL4noNaAOBoKHpxcXGqfWPH\njiVH2ysp/clFcoQg13zXAaix2+3kzBtkmN5Dhw5RRPdhZI6IpcRRb1B6ejrl5+dT2viv6d0FSwi+\nRysWi97LsZiYGD+90tJSOn36NJ0+fZqWLFlCaWlptHjxYpXaOXv2bGrfvj0tXryY4Nu70CReHiO9\ntEe/UDX1ehkTl1PG00sp4faXZQXZAN8a/GNa/+ntM9L78ssvKTIqmlLuny17aLvBK1X2AmiQ/jPS\n27lzp6rndrspPT2dYq9/jNIe+1I2CHvg269SAKDBarUGtS0lJYUWLVpElZWVtHHjRoqLi6NTp06p\nvhVpTQIvU90DoNFut/vpLVmyxFBv8+bNFBUVRWVlZeT1emnz5s0y740BTwOcA2/cpLfmL27Rvlmz\nZlGrVq30ZaMMvBLtMPhxmosiIyNDjgUR0X/+8x9yuVyybDSKdE4G98abHM4IVS9t/NfN5pWxY8dS\n//79Kf3JRdK+Svgeofk1gAqtnrQv7dEvDGObn59Py5Yto4xnv6fVq1cTfM+bPgh+6mKNVm/Tpk10\n4MABampqor1799Idd9xBV111FW3evJnS09PpySefpPbt29PJkyfp5MmTckT+sKjL9gCoDKZXWlqq\n6hFxx7OpqYlqa2tp9erVlJGRIcvvjSK9YQAWSb2MZ7+npNFvUcoDcwL0ZD315ZdfUmJiIqXcP1s2\nWHUArtfUqZdrysv7mrrYDl6yehg8zZ91KRqEfvAtdasXGWKocJi86feYyIRyre7YZvRC4pdcccUV\nlJmZqbJB3G63yqaJjo6WmWA4eH7TT8+ID6LVk9NQ8cOfJbOrlfYhG/8Rr5D0EhMTVftKS0vJ0XEg\nmRxRsoDvEpVPAA9l1KhRfvwXp9NJMJkJipkUi01lN5kjYsnKXJ068Jb0jQjCRtIebrebEhIS1JGB\nlt3kcEbI3/0C3l8RwG6Setpei14vceTrpP+dKOjVLdlnpJeVlUUms1myjGQ85EaeAP9p0RXvv/8+\nmUwmlU8zf/58cnYaTCZHNNnsdtlpOA0DjlZERAStWrXKTy82NpYiIyNVltHEiRNVf4h0y41aRXq9\nxMREWrVqFU2YMIFmz55NXq+X7HY7mc1mCgsLo06dOlH37t1VNo0lNk1WuGYj+6SetE+vl52dTRs3\nbqRZs2aROSKWTMxG8oAf+fpyKLHIzMyk119/XWUFTZ8+nV544QVt2dgHXi1XbKSn7+Hq9V544QW6\n5ppryOSI4ofosH1PwICNpI1t5nMrDWO7aNEiiouLI8Vip65du8ryYUcQltF9991HcXFx5HA4yOVy\nkc1mI6vVqur9/PPP9Oyzz5IpTGU3FYM7wgHxnTJlCn399deUlZVFDoeDkpKSaPTo0fTVV19R27Zt\nKSUlhR588EGjsmHIborqO5LroahEP73Tp0+rDYLkmmnYTU3gpczNsozAy6jniP/vArDoUjQILXKK\nwA3EGmFQb2jmBw30VPZQc+wXPVvl008/9ct44GF2FrgF92O/6Id2LbGCNHqSPVQaip4Ru0SwZCTb\np0C8aiQ7Jxi7KSz7csqYuIKS/j7TiJ0TwEYKxqYhap7dJPRqwc9+ni3iWWikJ98bxSP5/lkBrCVh\nXwmAmpZYPMHYOUKvVpNfpgDwXCx7SHcPQc8yqm8pHka8G4P4HhD55tfLLrvMzzZZmI1sk98ZsIxS\nRWVZddlll1FKRstp1bN4DPz3EoATzeWX5sobscguUUnOBk8dH9SyffRlJAS9IwDmIgh7SJueYFwp\nt9sdMsuopbyijYfQ08a3D4AfQmUt/fjjj0YsIy17yE/PaMpIHkarjAz0grKM5Dnkm5kpRZCZG/kK\nZZVRKJyimwEsEHZvAhAtNzYZHC2yh4zYKhs3btSfpuX7NPusgBBYKFJPsofKAkQM9Jphl0i2z2nw\nZqVzkp0TjN0U0elqKIoCe2r7lvgv6fA9KN7wCIHd1CjsawBX4IlGOvr0avV0DBsZi5Pg1UGXip0D\n8Iojz8WyhzxV57Sn6FlG5S3FowXejYxvJbiQHTKZTCHbVlYWkL0ky+g6CCSEyWRC9979/iqLR+s/\nL7g3GvQIsbxZ4du0dtH5Racn09tT2LY/VPZQkLLxl1lGQeKhZXNdVH1ltRrnKfizh5rVC/EIhWWk\nXouIPOCOR1yzqs21FqJlaZFTBN6d10/z/icAPQy0JPtFsloK4ePsbAWvF/Fk7AAAIABJREFU368G\nV6Ql8LFgjoCH/ZIFcxY8FfNPcG+3RPyVO0glC0RyR+TOXi2/RHJNKjR6kj1UBx+rRfJ4dsDHLzkv\nvi8U38mX5KxItk8+uHBKjpJXnGfERirS2K1lI9UikI0kOUU7wbsZzwp7SoSPSsX3p+Bjq9QK30t+\n0L/Bu6WlfyQzqlJ8L/1XpbmetK9CvJecljqNfR74drNKbtRZje8ki0dymGR8jdg548CNcxP82TWy\nl6RnGYXCHioW/m7SXLNGEwstS0ter0Ho6eN7Uvwv8592B/lZ+O5L1YPzj+RzVWnSrmUZLYGPh1MJ\nHyvnLIKnVbJ45P0Rvf+K4GMPSV9r9f4U30m20g6ND0+D89I5EYefxHWkjZJ7pfWVRG+XaGwqMvDv\nSvCUYJPQrhO/le+lLXr2kIxtk+aaRiwjWYZr4M/mugBfGZFl7bzGNm18taw0qXsEPnbaAfjycwl8\nZaY5tpQ8V8ajED4WlLZOkN/pfavVa5ZlBJ49SdN8V4AWFvv8b7CMJDvHBl7KKtk+d4HX71vAK3Si\nwUvxeoCnDSSjvC34xswu8JzheaFlBheeM+I3j4N7q4r4PhI+zgmB0QZHwZUmxGfbwYGRLKNGcEF5\nB4yjkPwSB3jOMlX8rggcvK7g3ZwW8By9DVxgzeAelge8fX0nGOtsBnNLrOChX3uRtkrwstUnhUa4\noihnxDmykVPAFdIc8I7PGvCqDbmbVAH3BprAhR7g3rYi7HsYPt5NjbBFQrsU+PYCOOHbrp8FXkNe\nI363VXxn07Cb5IN4ZZrPgbf2fwjeRWoS2mHCf2Hg+JaKtCbAn91UKPQUcAZfD8EyAuMg5NZ+yaxy\nwMceqhS+IHDesok0ytgWCL2d4MK2R6TRKTQz4Bvt1ID32kSA8RlF4n+vuO4W8I27vSLN8p6KQ1wv\nW8TCJHzmAOenOHCFVwQfd8gMbhwaxG/m6dKaBm58Phbp0HKbYsEPd7GI60L8lawgWd4GiPh1EjaF\nCZus4HtVdeKcCHCD0ghfuZWjwKXCtgJx7hHw6CFJ+PlPiNVd8OV1mccrhB+84DxbBZ5j9wKYKfR+\nF/Yni3S7xLV/Efo14DylCKSOlmVkEj7cKdJ7pYiRQ/gqRvxP8OdKxYDz+Bfi/wYRI7nv6WMAbwtN\nOQxNFvZINtIrIn1G7CapJ1lGDWAWVBf4mExt4YN1FgAYDO7pJ4Dz5IcIjWVUBJ5RkA8mi0ILsx+h\nNAiNAG6VLCMAdyCQU+QF8KOGZdTV4Bx55IMzRC240n8MjL2QzBUngKfF/3dq2EhecMvnAhfIrmAu\njpwTnw92bphgyHwsNG4Gr+CoAQc0DpypLeAM7QQ7KVyc9xs48x4SuhHgSqmV0LsVvKmrFJzZhoF5\nNF5wr+c4+NnSTnBjJW/aNoJ7XFeBM8jtQi8azAc6C2bQWMEZ9A8wd6cRwA1g7MJ18FEbq8Vnfwdn\nVrlZ8FtwwZTAuY/A93jqwZt2zoArAtnzDwewCr5VRxvADeM6oVcCzoCN4EJwJzhjlosYHBR+L4ev\nQHvBLCiCb2XFCKH3FpgLtRtcESrCfwnidxngiu8BRVHuha/SlZVpf/hYRh+BK/o9YPDhP+FjI40A\nF469Qt8h/PuhsKtJXLu/iEcjfBsK/xT2DQFX6hYRizfE/5IhI6mvVeDK5CS44q8BVybF4rNt4CdZ\nvQfugFjAeSsRvPy3Nbgh2g3f9Mdt4AbeK/ynTes7wl8vCf8kI5Db9Ja41hlhj0mwjO4Cl7ePwOXo\nU3BePEBEHcHlZYi47rvgig0irVnClkZwPr1JvP9e6J0T1yMRh6vAedYm9AaD87Zb+POw8JFHxPYG\n8f86odcKXM7mCv/NF2m6U1x/LhghUig0y8R15Gh3v7B5OXilZCy4DlgpYtgRXCZN4HtLkiu1UZxX\nK65D4HrCLvy3Fjz6agDzvMwiXduEb+4Qn+nZTZvAK7Wq4Nt4GQNmQX0k7JsDzssTwHXV9+B4l4LL\ndT8w7uMytMwyWg4f8+t2AD+LexBBj1BYRmkiIf3Bhf40gNuJaIXmnElgplAiOEDvE1FPAznJRro9\nPDz8fYvFgoaGBuTk5LTIGwGYKeN0OuHxeLBz505puAKgNDw8PN5qtcpH/hnyRs6fP49Tp04hLy9P\nPUfDpwE485cDSA8PD4fD4VB3I0u9du3aISIiws+uw4cPw2xmpkt2drbkLK0DQ9riACA8PBwt2Sft\nycvL8+PdCDbNGXDPejcAp91ubx0RERFgn1ZPa19CQgLMZjOcTie2bdtWDc7gslJUwsPDFaP0BtPz\neDyIiIhAWloatm3bBqFlBjeCLrvdbm7Jvvr6ehw6dAidO3fGjh07kJubK+2T0yxR4J5Rjt1uN2n1\n5E7sLl26qL7X+u/AgQOIjIxEamqqjMds8GM5TwPIttvtSnZ2NpxOp6GWUX7RsGlKiaiVoii9wA2o\nx2q1hnfp0iWobXr/uVwu9Xm5gp0jK8C/ATBZrVbk5eVh//79SEtLw9GjR1W9yspKw/IhbczPz5dT\nUxfAiz3irFbrXc3ZJ8uWtC8mJgbFxcXasuEBV5AKAIfVajWZTCbk5ORAURRVr6amJsC2U6dOob6+\nHtnZ2aiursaBAwckuNEM7iDarFZrSqj+27NnD2w2G9q2bSv9VwNfwxIPINZqteKvxEPDbsoGj+A+\nBLOgIkJNb0FBAex2O9LS0qR9XjBpOQ0AFEVZ2FI8tIcsGwBk+fAC2E9EnYSeIctIUZQw8EjnMnBj\nfRfxo46DHy3dQyDfKiLJKdoHrui0y06vArfSBeAKK+D+gf4lNy5JVktLvBEiosmTJ8vHXcoed1e9\nnjyMeCNz58712xmr59OAW/2tALY6nU7q2X8Qud1ucrvdNOWduQSAYmNjKT4+noYMGULLli2jhQsX\nUkJCAg0edhuFhTvkss5a8JSQ1Mt3Op008Cqf3sKFCwkAxcfHq3pz586lrKwscrvd1KVLF7rtttso\nLy9PpnUFgEnSPofD4WefXq9X/6v97Fu5ciUtWrJMrropAk+V+KX3+uuvp/e/5HOM7NPqvf7665SX\nl0fffPMNgXv6X4MbwZDsk+nNyMggt9tNLpeLZs2apd04Nxk8ytkKIF/qSfv+/tATlJ2dTbfffju5\nXC7Kzs6mJ598UvWf0+mkV6e/qd2Ydjl887bnwsLCVH9MmjQpQOu+8RNp3rx5qp7b7aZHJ02l2PhW\n2lVf48AVmsdut9P7Xy4jt9tNf3/oCUpOywjQk35ISEigRYsW0ftfLqP3vviOwD34h430rrzySrrz\nzjv97EtOTqbc3FzVLrfbTZMnT5aPRw0oG9BtTPv888+pU6dO9OSTT1JcXBx16tSJXn75ZbXcpKen\n0/XXXx/AbtLkl1N2u506de9J//rXv/z853Q6KTw8nF5++WXVtjFjxpDT6aTIyEhKT08naNhNMGAj\nGdn37bffqvbZ7XZ6+OGHA9hNweJhFF9pnzYebrebxo4dSxC7lP9qet9++221bKxZs4YQhN0USnqJ\nmN0kKQOih6+ymy716+JO5uHPcQAu3edXgVugXeAWKi/I7/0r3N591Mpq9erVNGfOHFq3bh1NmzZN\nXXfcs2dP9bmsK1asILfbTV988QWBh47PaPW0Fa5Wb8mSJTRgwADKy8tTC/i0adPUJWfvvPOOmum1\nDYx25276hG/ojz/+oHXr1lFxcTGNGDGCrrnmGhUGlnz/LEr9x+eU8ez3ctnaaQAjgzVYlZWV9Mcf\nf1BjY6Oq179/f7XBSktLo6ioKNqyZQuBh6LvA9go9fQ7iysrKynpnndUPUfbK/3sIyLtss466PaK\nJCUl0ZgxY9QlcJWVlTRnzhw/+7R6K1asoDvvvFO7pno7gNjm7DNKb1ZWFhEx3uC2226jiooKEp2K\nAmh2QEs9aV/0gHsIYl14fX09rV+/nhwOB7Vp04aIiEwmE01+89/aR0rmCjsVaOBxmc+tpNdeey1A\nS7GG0cyZM/06EAl3vkpmV4LaIGgLuLbCNbJNsYbRH3/84QePy3xuJUVdOVI2CHa9XurDn1B6ejq9\n+OKLfnoiv1Jubi4NGTKE9uzZo9qoKRtJ+gpNVpD3338/AaAxY8bQjz/+SO+88w6FhYXRnDlzKDc3\nl+66665mywaAV8MdTjJHtqLy8nI///34448UERFBJpOJ+vXrR3v27KG9e/dSUVEReTweiSZpAC+P\n/Ev2xcXFkdPppNmzZ9MPP/xA4NHzRq192grXKL5OpzMgHkQkn9d+9L9Jr7ZsmM1mGd/Yi03vZ599\nRm63mwYPHkz9+/enRYsW0ZdffinLbwAd4FK8LqYxuKQso8TERHI4I4KyffS8ESL/dbmiwlC3+7eE\nmtDyRqqqqqhNmzbN8mn0DYLMXNKG48ePEwB67bXX1O91z7SdBA2f5mLsI/JnNwm9OFGhRZFBhau1\nkYgo9ZFP/ewj4gYh8e4Zcv48Qp9ebYOg97c+vYMHD26W3WRkn1F6MzIyiIiorKyMRo0aRYmJiXKE\nZchukvbFXP0AWa1WamxsVDWvvvpqSkpKIiJmNz099U1tPFR2k75BePvttwO0wnOuoHvuuecvNQgx\nVz9AMFn89MJa96B27drRW2+9pfo2ZvBDZI5KJOjYNNCxlvT2lZeX05AhQ+jdd99V+T7yEHnZr2zo\n7TNKb+/evaldu3b0+uuvt1g2oGMFafXKy8upsrKShg0bRuPGjfOzTaMXwG7SVuBG9g0dOlT1X5cu\nXVpkN12MnrSrGXbTRaVXWzaCsJtatG/YsGH07rvvEpGvbGjouH7spkv5CmmVkaIoVvAd+6+IaKn+\neyKqIKIq8f9q8BOW4vXnafXOnz+PuIQkQ3Kmy+VS5+mvvfZaNDY2orS0NEAGfOc+pEM7D56fn4/C\nwkL0798fSUmqDVZFUYoF0bXZg4jw1FNPAQAeffTRoKfB9yCNi7IPgDpXqdML6SAinP95nqF91bt/\nAni4WRX4y+B6+vQePnwY9957L2JjY6VtHwDo2VzctYc+vbGxsfjqq69QXFwM8I1gE3yrLgIOa6us\nZjXz8vJQdLxQ+3VXoRtwyLlcnViwS7d46G0jIjSWHEViYqLqx9WrV6Ni07dIvOs1wH/fAACYampq\n4Ox4FW677bYA+1wuF6xWKwBg6NChRuWj2bKh1yMiHD58GImJibjhhhuaLRuiLrjdYrHC0e7KAD1Z\nduUzlYOUXWljyPbt2rVL9V+XLl0C8k9zR0t68vj8889letV9NH8lvYcOHVLLhni2w1k0UzaM8p82\nfbJsLF26VJJ8FTRTNv6bI5QnpingNa/7iejtIOckSYyyoig9ha7h8iax/OkHi8WCxJRU1NXVwePx\nYPPmzTh48CC8Xi/27duH8ePH46qrrlI/++6773D+PO/H2rdvH8ArSTIURTmoKMpbAD9QwkivrKwM\nV1xxhfqQkxtvvBEzZszAjh07sGPHDsybN0+a1w1AkqIo/EQp8unVnzqo6q1evRrffvstnE4nBg4c\niG7duuHChi/QVFclewDZ4K353yuK4lAU5VdpX+Yz3yFz4nJD+2w2m3qTOyYmBp9/vRgp934A8M23\nF8EroN5VFIXndMk/vY1lJ1X7ag5u9LNv9erVaGioR/WB3wBeiZWvKMpaRVHiFUUZDgDfbj0O8jSA\nvE3YvHkzjh8/HjS9cXFxGDXxTWRMWAxwBl0EHnkUKIpSJe3LfOa7oPGw2WwoKipCWloapk+fjvQn\nFiJz4nKAR5yPALhcUZTDiqIsl3rSvrD0TsjIyMD06dPh8XgwePBg/PzzzygpKUFaWhpycnLgXrMc\nnspSgJdWzgLwh0jrI0SExgbWGjBggJ/Wxo0bUXtkG77//nscPHgQqampmDNnDtAkH7+MJGFXo6Io\nhQAeJCKQpwEejwdh6Z1gcbVS9WbPno2myjKcPn0a3bp1Q2ZmJmbPno3EO1+FNTpJpheKolgURekD\n4J8mkwmR3W9E5sTlAfYtX74cbrcb119/PbZs2YKKigo0NTUha9Iq/LhlD8Crjn5SFKWnoih7ADyh\ntU+vN3v2bJSWluL06dMYM2YM2rZti5kzZ+rLxpPgnukPANKsVquh3smTJ/Hbb7/B7Xbj1Z9Pwev1\n4rfffsP58+eR+dxK7N+/H+DlxgUivWEAeqIF+06ePKn67/fff8fixYuxY8cONDY2ArzY4jcAjymK\ncgrA81LPyH96vW7dumHDhg34ZMHXGDt2rLau6gmuzFsHS2/aPz5X05ucnAyv14sOHTpgwYIFyJiw\nWNrXSvhuvqIoHgD/bC69GzduVOML8E3qsrIyNDU1Yc2aNVJPlo33gyHs/9LR0hACPpbRLvDmjh0w\nZhntBS/D3ATgymb0PkUIvJGoqCjq0KED9erVizZu3Ehjx46lmJgYcjgckiAo9yW8p9cLxhsxmp6S\nw0V2BQFB+CXxw5+lrKwsCgsL8+ONyMPRYQCZwiIli6cJwGyht0qvF9V3pKF9Wj0tuwncY1kBXrbn\nCZZeLQ/FmTfIT4+IaMw/niKzqxXBt9V/EniJnaF9SUlJQe1btmyZlt0kN7D1BS97DSkeWl6LZDcp\nFruccz0AxqAY8l8yn1tJe/bsod69e5PD4aAOHTrQ0qVLVfu8Xi9dc+MtZAqLIOGzufDtD/DTIqIA\nrVa3Pq/aJvOH7vULDDhVku2TfN+HAXryyMrK4ocsWcO07Ka5MGAPmRzRtGrVKnrxxRcpOztb9V9a\nWhp16dKFevXqRUOHDqWEhARSrHZ507sYvBzUMC8bpXfatGnNlY0G8DSUYdmdPXs2vfjii9S7d2+y\n2Wxks9koMzOTbMntaOPGjXTXXXep7CZxU7lI6BlyzYzs08aWiGjWrFmUkpIiuWYXwEuZA1hLweKr\n19Oxm+SigZKW0guTRU2vrKuWLVtGo0aN0rKbKsG7qAP0QknvokWLKDk5mex2P3ZTALvoUrxCvX8w\nBLze/DCASQbfK+CbnofBDUf3ZrT6ANhqt9spPiGJpk+fHpAJvV4vPf7445STk0OdO3embdu2ERH5\nsXE0lVofaNhDwY41a9ZQenq6Cu8i8mewCCcng1dLVV9++eX+wC7dvPqaNWv8YGC6ewhH4OOJbAKw\nqTk2kpGevOkobwILrdkADhERmmMPEREl/G2qn57Wf/BhoJPBK4R+kOkNdg+hBftOAvinJh7npH3B\n+C9SLyUlJSAe8GcP9QGwU6tnpKu3T5deWcA3iVdyc6wl6b9g+UWT9woB/AygjzYeRvbp46Hzn2QZ\nJYNvkHqtVitFDxwToBOsbMjrilVVASye5lhGREQzZswI8J8BO+dBEZt6i8VC0QPHBOho7bO2ygqw\nT7OKTLJ4hgGoao4VRGQcX519ks31AxFBywoKRU/GQ+hp/fckAG+w9GZMXBEQD1lugujdCqCypfQa\nHbJ+gz+rSmUXXYpXKFNGZvBa3BvAm15GKorSUXfaDeBVHLngO+mzEfxIB9A2NzcXz7/5ARYuXCin\ngNRjzZo1yM/PR35+Pj766CP5VCn9ITfwpKKF3XdNTU149NFH8cYbb2Dfvn2G1wQ3aqnw7aBsUW/N\nmjWqXkPpcb1tkieSAm4gLkqvsLBQe4q0JwvBN/z56Z1bO9tPT5deOWddDF633SxbJQT7bBqNVLTA\nWtLqffbZZ0b2adlDqeCVbRdln0F8AV7GWgVfbILqnVs7u7n8on3mblfwJqsW9Zrxn8wv6eCNijVZ\nWVmo3veLPl+FWjYuisXT1NSE9957ryX/nQRvvmsDYGtSWgaq9/3SbNmNu/6xYPZJvVRwnVLekn0h\nxFeyuVrkBIWYn6X/JgGoCZbeuiNbLzYeSQi8Z3Sxh/b3WnbRf338b8Dt2gCosNvtsFhCg4s1A3sL\n6QgBRveX9JqBvf3XegZwsYvSs0QnNwePA6DOkbV4s/r/hX0hwMD+kt6liq8lOjmU/NIP/LjY10LR\nC8F/eeBKbbmEs+nz1aUuG9K+lJSUUPyXBG5Qa0OBx7UAarwo+y51fEPMz1HgTtPyYOmtyd8cCizv\n/5+jpSEEQofb3QbePr4PnGlmGmiNA7doDfDB7c7Bx8XRwr5KwHPT8uEwZ8Et41ahUS309ovPJUzt\ngvheq1cgriOhbfprVoprPAsf+EwCyKp11z0v7JacmXrxmRZKVQueEx4ndKrENY6J32jt2wYfXK1W\nvCTcbCt4iO4R/lsP3+YqCe86KzTKwb0jCcWSlb3EPpCwow5AmdCT6S3XXO8kfLiFWqEp0ysf+FMq\nzpe2VIOH7DIeEh0gn7fQCB8vqExoNYjPm4TeWWHHfnHOAY2eFj5XJ76vFe8bhVYd/EGCEmp3Ej44\n4BHhg53iuvL7IzrNQmG/1NPG9ySAak1+ljZ7wHtPZLq1MSrUxEBC8JrE7yRgrQ8YryBZ9zK2TeA8\nXSd0L2j0jICOsgwkC3v2Cz1Jtj0NX36WNhZq4uTV+Enm0xLwFM96oVMrfncOvrKiBdcdEP/vEt/v\ngy9fFWr03gBvJJPlfKuIg74+kPdqGsV5x4Vvdor3leD8tx4+SJ9EWMg0SP/J8i5jcVKkQ/qwXOO/\n8+Il/Vch/urjUaHRq9PFQ6t3Gjyt3qSJh0TayLIu/SD9pdU5DU198L8yZXQRRxOAp4n5IFsB/E0/\ntUQMK/sBPnhcBDj4q8DTTBIwZQcPnU1gJtEG8DRCI3gFTwS4UH0OvqliAW/btovPrwfzPzoIvTQw\nM0RygiSVsAoMkgoHr6L6F9jpAN+Us4MDtEDoxQq7kkQaTPAhi+vFNdqJ31lEetcIfZv4faXGvvZg\nqJVNc02pJ89vB6BOUZR3wHwTs9C3ifRcA76h/7u4fnthn+z5DxS2VYIL6ocAmhRFOSm0Nojr2sAF\nt0zYVyOuEyP0JCjwUxEHu7h+e/H5qwB+Ba82kufKBuE/4IopAz5AmYS8NYAL7u/CxhKhnSO0Z4v3\nZmFfqTi3FLx6CEJL8qmqwbCwWnGtZDAcsEFcX9p8XsTlenABdgqN9vDB1M6BmUEWcU46OP94RN7O\nFL5rLfw9SfhvP3zQQgkuNIlrpok0m8B7S7KEzVvga3Rd8FE6Ccy6+VHoRIDz4C9EFA7OyxL2mAMf\n90hSXSH0FDA3SXZo3hRpNgm9BuHrMvAsQLW4Xjx4liAXfH/DKq5hFb+vBbN7pF4kuEzFiVcNmP1T\nK/SSwWU6Fzz9WS5saAAD6KzgDafvCa128OXfIvDy5jJwnmgHH+RuFXj2IQxcpmQDcj0x1E/6r53w\nxSlwR/F6+DpKo8B5ooqYEdQIH1RPAdc3ikE8AO4MF4AX3TjAU4kdRBpk/STjIcuHjMdC4T+ZTxxg\ntlsZeKHG30T6I8AxJkVR3jZgF/3XRygsoz4AXiai68X7fwIAEU3XnDMXwHoiWijeHwSTRN8iorU6\nvUkAHrDb7Tl2u11ldOgPuQyuU6dOqKmpQWFhIZqamtC+fXsAwO7duwnsMA+AQrvd3ttms6lspPDw\ncFWrsrISJ06cQFNTE7KzsxEREYHq6mrU1dXB6/WiuLgYDQ0NAGe43wEMstvt8c3ZV1RUhHPnzqFT\np04oKCiAxWJBQkICwsPD8eeffxK4Z/0cOGNOsdlsHSIjI1UWj1F6T506hc6dO6OgoADR0dGIjo6G\nxWLBtm3bzoMz2kowUbGf3W63hOq/Q4cOITk5GS6XC6dPn8apU6fkyqBfwI1dG7vd/uLF6Nls3IZp\n2E3vArgPojK12+1hoehlZ2ejuLgYcXFxOH78ONLS0lBYWFgKhtN9Ai5QJ2w2W49Q/GcymSAZRfIQ\n7Jd6cKX7K4DxVqs1ynD/gS4WiqJg79698Hq9aN++PcxmM7Zv3+4BVxYWcKNhs1qtMc3pSd8pioJd\nu3ZB+qaxsRF79uzxgkGNieAbt1lWq9Uk9RoaGrBnzx7k5OTA5XKhoqICBQUF6Ny5MxoaGozKRhN4\n5V+k+Btvt9sd0i/B9Nq1a4fCwkJkZWWpZePUqVPweDzy6YT9wBVQtJbtYzabVT1FUVBUVITa2lp0\n7twZVqsVxcXFsFqtiI2NRX19vVxL/wYY4OgF8IzVau3bUnptNhs6deqE+vp6mM1mWCwWLdfsGTCe\n5AZhnxKqHsAsp4qKClRUVKCqqspLRGZFUT4Q8bCGEg+pp2U3ifwn2VfbwQA9s5a1FEwvJycHUVFR\n8Hg82Lt3r7ocffv27RLhbYKGXWSY+S7yaPbmqTj+AJCrKEo2uMK8C9ySao/l4DXA/wcMt6sF3yza\nrD1JUZRx4CmodIvFgsu6X46Xp7xkeNEJEyZg0KBBGDNmDAoKCvDwww/j4YcfxogRDM0cNGjQMXBl\new78EHZ0v7wHGhvqceutt6JHjx6q1o4dOzBlyhTceeedGDFihB+cbtu2bfjXv/6FM2fObBM9CQBA\ncnIyZeR2wOZffza077LLLsPDDz+MMWPGYMCAAdizZw9cLpdMZy241R4H0duzWCzI6dgFM6YFT2/f\nvn0xfPhwPPHEE/jwww/VSk2k1QumS0YA2GmxWC4PxX/XX389Hn/8cbzzzjuoqKjApEmTAB62OsD4\n8PEAEGo8pN61116LkpISTJ48GYMGDTpGRK8pilIi0oxQ7evXrx+ef/55XH755cjJycHzzz8v9X5V\nFOVfQk8JxX89evRQ46mN8aBBg/aC5+d/ADc0NSaTKWrMhBf9NDqnRvnF4sEHH0RxcTHuu+8+DBs2\nDFfddg8AYPvfbykU8XhL2JdiMpkwZsKLqoZRWseMGYPi4mKMGjUKb7zxBrp37y7tOw1uTL8CjyBK\nTSZTgtTbt28fJk+ejI8//ljVvOWWWzBu3DiEhYUZlY0TAMaC6aAl4I1ljqmvvYGE2OB6I0eOxKef\nforZs3lNyLZt2/Dmm2/i7NmzO4hojaIomeAR0wWTyRSRm9cVY0b+DQ6HQ9XbsWMH5n3+FQoLDmHc\nuHEqGPDsuXKcqWUfDxo06Cx4p/xSgOFsVntY39LBU1H4xo3YvHlVZru2AAAgAElEQVQzhg8fjvz8\nfNU+l8uF+Ph4bN261c+3ZWVliI+PJ/Do4BfwCCDFZDIly/iay4swefJkjH95phqfm266CU6nEzNn\nzgTAGy1fmDIVL7zwAiZNmtQk5PdCwDODxWN3Ed8Pn/TQ3+GKjMDMmTPx2WefoaioCDeNfUym95jQ\nm63JL8l6Pa19t9xyCx566CHYk3OxYtEXSEhIwPAxj0q9Am1ddUmPUOaV4A+3myw+0+5DUMBTEQXC\nifthgLjQ6jWHcigsLCSTyURHjhyhyspKys3NJYfDQZs2bVLPAU8fVErNpKQk6npFH0pPT6fy8nI/\nPQlWa9euHfXs2dOP/bJ27Vq5r2ErdKwlLRtJ+1q4cCGZTCaaP38+5ebm0tSpU/2+Bw/Jt0v/EDWP\ncpDpnT9/vh976KuvvtLC/ErQDMzPSG/37t3UvXt3mjp1KhH5YH7gJcQntTG5WL0pU6aQy+WSz6Ct\nhdjsdbF6ubm5tGTJEho0aBCNHz+e+vTpI4f6KwBkXIz/fv31V8rLy/NbLktEct37evLlPxXloF/G\nKrW+/vprqqyspO7du9OMGTPoyiuvpNR/fE7pT30rR6bvGukFs03m5e7du1NkZCRt3rxZa58KKxNl\nrUyr5/F4aMCAAbR8+XLyeDz03XffUWpqKhUXF4dUNqBBdTSnt3fvXj9Ux9q1ayVKZKtGayiAMi3b\nR6u3bt063lthMtPgwYPVsiaX+GqeXy5hfgFsn3Xr1lGXLl3otddeo3Xr1tErr7xCMTExKgjRAObX\nBB3MT6IhMp9bqdrXasSLanqTkpKoQ4cOalp79+6t7j2BBtUBA/ieNr0Zz35PrW6dTCZnjKony4Yp\nLEKiJlSYn94+rZ7WvtTUVFq9ejVlPreS7BldaPz48WRPae8H89NqXqrXpRXj+b8fADzV0rnNVRjT\npk2jgQMHUkNDA1133XX0yCOPkMlkov3792sz/W6RGRSi5tlIkydPpn79+hER0fTp0/34KtoG4WLs\n69KlC1133XUqC0V7gOcnd0IDLGuuQps2bRoNGDCAevToEUxvN/ihG0mh2jdgwADVPrfbTUuXLqUh\nQ4ZIvYtqEIz0tMAyhADzC6b3yCOPEBFRbm7uRcH89HoD/y91bx4fZXX2/3/umUlmsofsG2SBgIIS\nBRGxtqKiVRF3lEUBi6ItLs8DiGgronVr3aHiBkUUQWStgLRCBSs+LoCEJQhhSYAkBLIA2SbLZK7f\nH9d15r5nMmuSftvfeb3mlWXu+cx1zrmuc859lvd95ZVUUlLiq0Nwg/n56xCU1qZNm1z5PXv2rA7y\n00yqQUvwpufLNuXLr732mhvI79ChQ8q+Fn96CxYsoKioKDKbzRQREUFr164NOjY8OwRveuvXr3eV\nn0reOgTRc2P7eOpplnBKvvMZN86S6hBmz56tys8N5mdsIL3Z9/jjj7vZplJxcbGKtzRvekpzwYIF\npIXZXHoLFy506a1evdr1HG4f7Caf9kEzkWaxUuINj7r0VGz0evxv9O233xIE5hdIz2jf+vXrXWcj\nLD0yKC4ujtImvE52u53gAfPrzle3LSoHg7gINn300UeYMGECJk+ejPPPPx933XUXoqOjUVdXZ7zM\nDF78oUBspOXLl+M3v/kNAOCyyy7zx1cJ2j6Hw4Hzzz/fjYUCAH/5y18AXkwbSUQt3j7vTa+trQ3Z\n2dkd9CS1gO+85oeip+yz2+2YOXMm5s6dG8zHA+oBQP/+/ZGRkaH47Y3gRcA7/ch41bvrrrsA8PMi\nbrvtNgwZMgTgxuxZAJdrmtZxDsaL3sSJE72+t23bNoAHKiuDtW3ChAn485//7Mrv1KlT0dzcjKxH\nl6HXtFUALwRuDEVP+fK0adMwd+5c2Gw25Ofn45ZbbgF42tPnc8E3b96MmTNnYuvWrWhtbcXWrVsx\nduxYJCUlBYyNYPS+/vpr3H///b7Obrglb2wfT73UcS+jduNcZGRkuMVa3c51+OijjwCGX/qMDW/2\nffDBB7Db7R2ulTUqO/zEhtJLHfuSS2/WrFmw2+1obGzEzJkz8cgjjwTMuzf7ej2+FqnjXsKZrxe7\n7FOxoZnMuPzyywHeHOAzNrzZd//996vnUECzhOO2226DNb0vbDYbwB1gULERagpmDSHY9AvwltS9\n8pQzAHiKGHbnM+XM2gAAKH15JADg22+/RUVFBbKysvDxxx/jwgsvxPr165lX89s/IuHah9S1kQCK\nVEdksViQntWzg/63336L8vJy1/zqzz//DKfTicGvfA9N02AvLUREh0/5Tt9++y3KysrQ3NwMs9mM\niy66CADw4osvorKyEi+//DIAHCQinwHuTe/w4cPIy8tz6Z3MvxURvYcYL7XAD7AsZ9YGlL480k2v\noaEBF110EWpqmKNz3sWuZxb1BvfhlQAuI6JSow7gXh+eeg0NDZg7dy5+9y+39oYQBMwv7Z5XcKrk\nmEtv/fr1iI6ORmJiIlbsLMMW+X4ECfNT/rJhwwY88Mh0aC31GD16NF5++WX88VAaajbOBYKE+Rl9\nb9OmTaisrMRFF12EgwcPYsaMGShsi1GXumBlRORzZPHtt9/iyLETmPWPclR9xr5s9Jdve45DIwDs\nv8kvrKywsBC/+tWvcMkllyBn1gY0lxXBbrfjq6++wg8//ACHw4ElS5bgrtVV6iOR8AHyM+rdufIU\nSi8xYciQITCbzXyexm5HVlYWnn32WTy1qdKtQg2Dvuqw8PA8b/ZVVlYiPC0f4Rn98Ne//hVOpxOJ\niYn47uvNqPt+JXbs/hG9e/f2ezDLqAcAr7/+umvRV9k2efJk40f8wvyU3q70fJhMJrz++uuoq6tD\nTU0NcnNzUVtbi0cffRTnWgm3v68BAvODR2x4s09buQF129fC2dKE0tJ6X/YpG4Oyb8iQIRg6dCg/\neCruJoSn5IYE8+tK6rY7BCLaRkQaEQ0koovk1aEzEKDVKIBhbwpWptLixYtxxx134LrrrgMRUwkX\nLFiA1157DfbSQjjqq1FeXg7wlr988LNX721tbcXenT9i4MCBWLduHd599128++67WLx4MS644AIM\nGzYMBQUFmDdvHj799FMABHK0Au0OdRvXU9O0cLEvVtmn4HFG+0aPHo0tW7Zgz549LkDemTNn8NRT\nT2HTpk0Ab+vcJAC5zZqmjSUiNxidpx4RYeHChS49R0Mt2hvPqssuBG/r/KfY9z+B7FNlV1hYiA8/\n/BBlZWXImDQPGZPmAbwgShBgmQDGfNaHp96CBQvQ1tZmhPmdJ1p/E/tO+bKvcd8/EXXeFW51W1hY\niD/84Q+wF3+H1lNHAQ6ep8E7q7Zpmlbhr/zuuOMOrFy5ElkPf4y2tjasWLECkydPhrPNBfOLFQji\nDZqmLQYwg4j8+p6xfseMGYP9+/fD2dIIancADBerArBF07QjAO5Sep62Rfa9HJG5g9zKb9WqVRg6\ndCjI2Q77kR0AL9Q+b4DbPWzUGzJkCLZt24bCQh5nmcJsSEhIwOLFi12xsWnTJjjqqxXMLxXAh6L3\nNARu19bqridljV27dsFut2PlypVoa2vD8ePHMX78eIH5AQDiNH785q/Ag76+9qZGlP91KtatW4eK\nigps3rwZhYWFWLlyJSreewD2wz/iyy+/xKeffoqlS5di/WdLkHr388jLywN0OOBBTdNuApAPP/md\nOXMmIiIisHHjRpSVlYGIcPr0aQAu0GU6+DGShzVNKwbwSxjq1zO/M2fORFRUFDZu3IiKigqUlZVh\nwYIFyJg0T8H82sCgy3ukfh/1Z1/c0DtgCrO57EtKSsKZM2dARPjxxx8B3ratYmMdDPA9X/XxzTff\nqLJC1IUjsGbNGrSeOmqE+anYcAPcaZpm1TRtufz/By0IerNb6q65JwTgHRmu8worIyKy2+0UFxdH\nmzdvdmONPPDAA+R0Oin20tvJZItWC5m10J8T7KangFFKb9OmTS7eSF5eHu3cuZNSx75Inp8D7+We\nE0hv8+bNHdgvOTk5BJNZwcrUNsd3wU8RC6jnyVqKumAEmSLjSQuzqjnIs+CRn1cgWPYT6/2WH5Eb\ni+cgeM7fK2AsmPoYM2aMEebXBmCzr/o15lezRlHK3c97ZRkZYH4O8KJyIXhEGlT5WRIy3fKbNOpx\nBfPbCT6/4hVul/3Eeuo1fbVLi8id7fOHP/zBBSvTrFEE3hNeBH7AvF/bUu5+nrKfWO9WfmPHjqX0\n9HTSLFYKS8lV9Wv253vz5s2j3r17kxYeQZa4VLr//vtd5ffiiy/S448/TiZbtIL5NUDf6OFTzxKf\nTtHR0ZSbm+vSU+XnJTa2BmNfdHQ0aeGRZLLFuNg+OTk5ZBKYnzxvgMBnJd4MRi83N5dGjhzpsm/w\n4MEumJ+s/1VJ7P7Rly/7y6+RZWSA+fWHF1ier/qIHnidS2/gwIEumF+/fv1UfASsX6N9r776qhvv\nav78+WSOTjDC/ArhBXAH4HcA3pXfxwBYHlI73k2dgVkalzzw4Z7dAPr7uHYYBKbmC/bmcDgoLy+P\njhw5Qi0tLZSXl0dFRUWeMLo9EphTANgvuOACev3DFTRw4EAqKipy09uwYQNdf/315HQ66e2333Yt\nIHnoGYF0//AHy3M4HJSRkeGyT32nEUYHIF20bgLQ6A8upvL7ySefuPTSJ8/3fOCOGwzM0z6jdqDy\ngw57U5rpXakPqe+Dhvq1+yq/7CfWU6/H/+bS+/LLL72VX0hwNmVfxoMLgsnvAwC+8wfLC6J+fcL3\nAuXXh54RblfqL79Blp+CIbqAb97qw7jLxZd90kAq+8zgtY5/ZeXkUVhyjtdYs+UOpl4z19F3331H\nl156KRF5B1OKjd8D+N4f7M1ffSg9uMMVq73B7fzld9GiRd5geU8q//Nln6oPS3yaP/s86/drb3qe\nfxs7BOP7Rv+TPLtOK6uYlt8t4PMKGnlpi729umvKKBjekUoBYVuerJGrr77aF7ukDXwSN2g2Uv/+\n/XH27Fk4Gmo9tYxAOr+wqCDYLxbik44An5z0u1bjjbXkhY0UNAwshPJTmgHzG0CvDTxNAdFydBAx\npNaTxYFYRiHB2ZR9YfFpweS3H3gg4VcvQP2GBN8z5teHnvI9BVf0m98gyk/5W1DAtyBYQcq+S8EN\nTLE/llH0BVcHYi21Gn5XwEG/9gWoD0+4ol+YUJAsI+V3Af2v9WRxIHaYZ/12De7k7n+AO+DOZS8R\nOcBtRWKwwgFPKgcloml3gg/8pIEbhlgAR4noVx7XTQGf3o0FN5Q14OmVGPB0yEnw7VCiaCnHbgcX\nJMDOUwu+G7GCCzsS7BTH5PdY+fsnMK4gSrRT5HeFJQgHn68A+GDLcdEbIHZVQUckHJDPtYlOT7jz\nfWLAt3EAMAi8zxqS31y59qBcFyN2HJfvPQaeFwwHr+uUSP4rxc5s8OG/NgAjpYxbpPzCpbwUS+cs\ngIulzNQEuVXyocovhvjk5DbwqKUFjBuoFK1j8v3RkqceYNBXIvhOUP3+k2hmg+vqCXnFi11npaya\nPMonST6jEB2KQ3Vc/m8F8+0/lvdj5TubpDxipfzUwmko+e0DxgjEgqeSEsD4iDrwdKfSU4GhWEFO\ncP3XyWd2gg/2PSFlESP2nCf5Pg6eh1bllwH2+WboDaI6eVUAHuVdDMaTNIpeKdh/G6B3OmFSFpng\nEXu7/Kw2XDOIiEyapm0BDyAUDkH5ajy4HvsEYV8S2BfuAT+PYxzYX2LAu10SwX5ZCvaBC8F1GQWd\nL6U64l2SHxsRxUp78Kp8l/ruarDf1YJ90yLlbpOfqs6i5WchOF6Xgg/CPiFlYwLXZy54AHBc8p0k\n5RUNfaruuPy/WOrsDBjBEgF+XnysfJfKY4ZH+fUVPWWfOp7vq35VHRSr8oAeS4DuN32lPNToNRsG\n/yOiEQCgadovATxBRDfJA5GuJ9nUImsgQ8nP5gdj6k6WkRM6y+hZABf6YBmtAQe5YhktBFfA0wCe\nAjtSGriBewI8lWMCO5kKhPPkszYA+6Cv4MeC5zrLwQX3Z/m/YvXMAQfXLnClANyw54EreDeAD8GO\nEibfcZl85mIw1TIK3MgR2AlTwE7fCG4QeklZPCf5/VKuDZfv/QDA62LbHrk2EzrsrxXAM2LLCbBD\nmwBcBWaprIc7C+pfUjZPg3EA8VIe58A7Ly6Uv9UUW4bYCvm5RuxXjYZaV1gOdsZEsS8aDMXrAx1s\nFw0OdDuANkP9WsS+CDAr6TvRXSb2pYMbst7gTgiS1wRw49QCOaEM3V/CwPuvPwD7zBHwGo2v/G4C\n86L+CqBa2E0J0DtvEJ/2/EDq92fROl/Ko0HK9AVwB5MktqWLPbnysxXsd23gk+S18r3vSt2eD/bF\n7eCO9gj4tv4YuIM7T+rzF1IfZinzBqmHvQa9eMljA3ht6jkpE0jezpPv0zRN2yvfq4neCrFnJfhE\nr/I9b/Yp9lBP8MnpOqkTi3xG2fcM+BDca2KfahQVGO+Y6P5BbOwF9jG1o2qx1IdiVYWLbW+AN1G0\niJYFOhBulHzvenCd54pNo0VvP9hXzNDX8t6QuubdJPx9TfL5I+B4VKBKTeqhHDww2y/5Msk1X4mN\nC8ADsDhw+3IGfH7hEqnfs/L/fmLLleB1k3YpA9XJHQcP7EaB27s4+a56seMD0fwEetvYAmCQxk+f\nhNSRwuKXS72pp1PGIcAdkzF1V4dQDiCJiFQvlwQuLG9TEd+DnW2F/D1d/rcczJWfBq7cYnAwxoM7\nh3fBI+8DYIcsAlfu36GTBiMA/BIcuNtE72lw5XwmejbocLxw8BH/DHBB54FHPZ9C793vBDtBb4N9\nN8h3jpXvPCe6keDR5kIASZqm3QdufJrBDhMOXujZJLrXieaHco0KviyxbyiAIeC7gizwyOGfYKfa\nIvbdDh6V/ATgRfBU3XJwUCyT73GI3gfgux4IiiRf8mkCB48a+U8Ad8Qx4AZ5NziIYmVLcSK4If9S\nyvozAFZN08ZA72xXgJ1+GBgRvQvcwN0CBuQ1iX1nxNYS8F3mcXAj1UfKs1bqVo00x4Abw6gA+W0G\n31VNBXdsZnCjOUPKApqmXSF6LVK2t4jmcbCPfC7lqwibx8F3G1HgIIsG8Hupjzboo/eTorNEfpYA\nuATsq5DPDgD7yWD57AKwb6uOwgRgInhHk9IbA25Ui8EDleug3zXtIqJo8MLqCSn/r6BPyag7gCvA\nI03le97sS5U6vgTcUUYAuA884i4RPZPoqMV6Zd9pcAyZwbGbBW7A28CxcRuAX2qa9gA4plRHXyTl\nOxE8h/8K2LceBnAt2D9VmWSBp6808Oj7JqmLqWKfE9yQ/0Hqo0HTtF7gBvcduV41lp/L361g/9os\nZfA5gLvBHecZsf8JcGyki60p4M7tWuiwS4Dbwx5geN3FYH+8GXy3UQdu7NUdfl9wLP8WwGrRe0PK\nIA7AGE3TrOD4Pg0efPQX+1/zArj7XMoIkLaLQpgG6q4pIwvYSa8BF4a6De1HRHWG66aAR35xFoul\nj80WgYwMfmyC3W5HRUUFevfWtxMfPnwYJpMJeXl5cDqdMJm4vNva2lBSUgLoaGeT2Ww2WyMikSV6\nZ+sbUXO60k3vyJEjyMjIgIM0VJafQFRMLHokJiEizIzi4uJW6DhkzWKxDApk35EjR5CSkoKqqio4\nnU5kZmYiIiICxcXFtdBJks0AqiwWS55RT30mNZWn3h0OB44ePYqwsDDk5ubi+PHjiImJQY8ePVBc\nXNwOpjzeAiFwWiyWnjZbBHokpwAAGs7WotXRjsz0NDe9jIwMnD59GtnZ2Wh1As32JlRVnoTU19/B\nThpmsVjOt1ptaG1tQUZGBurq6lz2NTU1oaamBna7HTk5OQgPD4fT6URLO/vOiZIjbeBGvhXszJut\nVmtumxaOiwf0BQCUlfHznnv16gWAgV579+5Ffn4+SktLMWDAAHXIDTt37qwx+FB/AGaz2RxmrF/P\n8lOQt6ysLFRWViI7OxtOpxPl5eVobW11iq/MB0999DCbzVqv7ByEWcw+6yInJweapqG0tBRRUVHI\nyMgAABQXF1eDBwV/g8zPmkwma58+ffzWbUZGBqqrqxEdHY22tjbEJ6Wo8msnIovEx/8A6Gs2m82e\n9sUmsK8a7SMilJeXu+rX0daGk2XHAW6orwMwG0C6yWQyZWbnIiLMd36zsrJw+vRpREREwGQyITk5\nWeW3SeLiDfABRLvJZIoLpKd8+fDhw8jMzAQs4SrWFPb8HOQuxmq19UnJyMSp8hNITExEe3s76urq\nXP7S0tKCY8eOufzv+PHjiIiKQWKPOBw+fFh1+DGiWxNutWWk+tBzOp1oa2tz0wMAe1s72lvsqKio\nUHC7NVLPJqvVFhaKfUrPatZw+PDhdvG9C8ADgFir1QZ/eg6HAyUlJcjOznbTk/KrgT5ldRYGwJ1s\nIf8Y3BHVAhhDRH4f0OWWgl19DvSCzjs6Cu4UfLKMiDqiDbzxVYy8ESOO4NlnnyW4s1pu9EQb9Hr8\nbz75L+GpvWnKlClkzTyfMn+3mBobGwkefJpg7MvIyKCCggJatWoVxcbGuvg04N59DYRP401v+vTp\nLmwDEVFZWRkBoJ49exIRUWxsLL399ttGvcHwYDdd+surXLsQpk+fTtEX39hBLz093YXzyH5iPaXc\n/TzBgEnw1Lv55pvpyy+/dLNvy5YtdM011xAAWrt2ret/Hrug/LKbfOU3Ly8vKHZTIL3PPvusg54v\ndhM8UA6+bFu2bBkNGjSI7rrrLhe3yRebJjY2NmBep0+fTgMGDHBxm6wZ56lnUjfBwKbxZZ/aZWK0\nT7GgVP3GXTHeLTaM9qnP+7Lvvffeo5ycHNq+fTtdfvnlVF5e3iE24IXtE4wv//DDD567gs4YbeyZ\n25uyHv6YrFYr/fzzz/Too4/SQw895NIsKSkhq9VKK1eudGmm3fua8ZGczxvzHZ7Wx6feli1bOuip\n8nv11Ve9xkeo9ik9X2yzQHpbtmyhCy64wE0vadQMn/HRXa/uPJj2Bfg2+BAYe706lM+bzWZMmDAB\nY8eOhdVqxbhx4/DCCy+47gqMSY6/uxZJyNsBOFNHvbfffhsTJkxAVP/hGDVqFMwxSSifP1FRSiPA\nc7JB25eamooJEybg9ttvx6WXXoo//elPqK+vB3h6ZSB4ysNruv7667FixQrs2bMHdrsdzz33HDRN\nUwGMhoYGN2IneCQVrQ6geNNrOrDNpTdnzhwAwIABA7ziPLylluZm7Nq1C0OHDnWzr6WlBd988w0A\nYPbs2QpfbEwZkGccaJq2Q9O0Hc3NzRjQw4mtW7di69atSE9Px9KlS7Fw4UL84x//wIMPPggAyMvL\nQ0JCArZu3YqMjAx8/vnnAK8LvQt9fjxg+S1evBgAMGzYMNx+++1Ys2YN2tvbcdtttwXMt6+6ePHF\nFzF+/Hg888wz+Pnnn1FVVaUIoemapo0NVa+lhWkNZWVlWLx4MXqMmIKs3y4CeAphWSC91tMlHewb\nMWKEW/027vsKMMRGKPY1N/OyWn5+Pnr27InMzMwOsSGx9k4wesqXVWw4W5oUjiEJHrFBRKhe9yom\nTpyI8847Dw0NDYiLcyczmEwmFV9oaGiAyeomsRiG+AhVL1Dqop7aLLIoFL3Y2Fg3vaj+wxWiJGB8\ndDb917CMvPFLFG/EmLZt24bKykogwHN77aWFPvkvsZfehrfeegvU3oasR5ehsbFR6fnk0xjta2lp\nwfDhw3Hw4EFcffXVAIBHHnnExacB38otgx8+zYgRIzBnzhzccccdyMnJQU5ODqKjo2Gx8DqRemaD\nIcXBD59mxIgRiL9inEtvx44dCA8Px6hRo/wVkysREQ7t34s333wTsbGxbvbde++9ePLJJxEbG4vx\n48fj1ltvdX2ubuc6QGc3zSeiS4joEpvNhqIzJgwfPhzDhw/H9OnT8cILL+Dll1/GhAkTUF5ejvDw\ncMyePdt1jXohCHaTZ/kdPHgQ4eHhePDBB118mmDZTZ5a2dnZMJvN6NevH6ZNm+bGbQqGTeOtbmNi\nYpCYyLv/FLfJmt4XmiUcCMCmUXpVa1902WexWNCvXz8XCwoAmsuK0N54BggQG77sS0vj6UbFbqqp\nqQkqNgL5soqN8g+meGU3aZpmqqk6DZgtigXmjc8Ep9OJmJgY1/vOlibj26740DTN1H7uVEh6/lJn\n7FNpzZo1gAfbLFi9c+fO+bIvJLZZSKm7bjXAi1XqwFihvG70db3nFMorr7xCt956q+vvMWPGkNVq\nJZPJRJmZmTRjxgwiIrr//vvp3nvvdR02Ui/PKYX44b9x0/vmm28IAKWlpVFYSi6FhYVR3OV3G29j\nd4n9SYHsU1qxsbGUnp5OBQUFrtO2orUDvMi2zFd+PdONN95ImqaR2WymzEw+cTt9+nSj3mQA33vq\neTu4ouzTNI1ycnKooKCANmzY4HPKCEBYeHg49czt4800IiI6ePAgRUZGUm1tLWVnZ9PatWsp8YZH\nyRydSAD2eNavPzqp0b4BAwa47PMov3sA7O6MXt++fclsNlNqaqqidTrAc8uV4IXNDvRPY/rkk08I\nAPXv358KCgq82VcGYLWh/NymjLyVndlspvj4eLJYLBQREUG/+MUvjFNuyvfilJ43+1T9Gu3r3bu3\ny77ogddR1ICrOsQG/NBYjfalpKSQ2Wwmi8VCU6dONebXLTbghdZpTJ6+rGLXEGsnVWxAnsJntdmo\n5zSdVPzee+/R5ZdfTkTcFqSkpBAASklJoQULFtCwYcMo4dcPG6eMJoM3p2gAFmnhET71rr766g56\nyj7PKaPO2kdElHjDoyTPatgTih4R0RdffEE2m82NYutxcM4tPrrr9Z9iGd0HcI+aPWMNsmd+7pVf\nongjZWVlGDlyJHpNX42FHy3FpEmTjHqXappWSkRurBtrer6bXlRUlIv/knHfPFxzzTVoqz4BZ0uj\n4oNcAG44/qVp2jFln9Iz2nfFFVfgp59+QkMrof2Xv0VhYS43kx8AACAASURBVCF69uyJno8tQ/bM\nzwHeGTITwA2app3VNO2Ep15zczP27dsHIsLx48fR2NiIWbNmweFwwHLPe5gxYwaWLFuOrKmLlW2z\nofNpXOWneC3Nzc1orSoFEaFXr1648sorMWvWLCxatAiFhYW4/vrrXewm8NSOTdhNlwKotVgsSM3I\n9Grfjh07MGXKFDz22GM4dOgQnE4nfvzxR5z510dIvft5AEgwsGlu0DTtlGd9GPWM9u3btw+FhYWo\nqKhw8WnAu2GeBIMS93pjGXnqFRQUuPSKiopQVlaGsDtfRdidrwI6u8kE3q44lUhn+3jWxfvvv49x\n48ahqKgIhYWFePbZZzFs2DBkP7Ee6RNeB3gXzhDJ89sA8okI2TPWeNWbMmUKZs6ciTVr1qCtrQ3r\n169HUVERWk8dVWykAnCH9a0sCl4ayL4nn3wSRUVFLhbUVVddhcYD2xB14QgAuED4NhbR60dELnaO\nL/tOnTqFzZs345577sHJkyfR638/U7GRZ7CvA3vIny+XlZW5AIvkbMfGjRsB3tG3S85J1AAYn5Sa\nDlOY1RXXt912G/bt24dVq1Zh0aJFmDhxIoYOHYpTp05h8uTJmDBhAuq2r0VVVRXAO2/eljhpBjDO\nEu9b74knnuigp2JJWFThmqbdJPymdwAMD9W+Tz75BGf+9ZF6AE+rpmlPapp2GHy3dY0/vebmZixe\nvBgFBQWup+AtWLDAK9sM3Z26q2dBN7CMjPyStLQ0SkpKcmONKDaNPGRD4QjKPfUU28eXXvyVE2nt\n2rUU2f9KMkXGUVyca2HvFPywkTz1TBGxZIlPp5deeolmz55N5ugE0ixWAgfPi+Dtmoe96Z05c4Yu\nvPBCioyMpNTUVLrrrrsoPz/fZZ/T6aRrRt6q2DQOsCOZfZXfmTNnKCw5p4OeYgXJSMrztRW8OOX2\n/3HjxtFrr71GmZmZFBkZSTExMWSz2SgsLIwiIiLor3/9K6WlpRnZTQTeivgJvPCCAuX3pZdeokmT\nJhnZTS3gbYfb4YNl5Kk3fPhwNz01ovJgN7UHozVr1ix66aWXvLJpLAlZyr7L4INN401v/fr1bqwq\nxaYxMRupETxtdNqfnhZmddMzsqCWLl1K5thk6jVznRpBbgRvKw3aPqVnZDdJbLSDz0l4ZQ/50+vd\nuzddffXVLnZTQUEBgTeelIK3e7ru7hTraMmSJUTEz2Po168f2Ww2uvDCCyk3N9dVfoptZlhk/Ry8\n1ZT/9qMXHh7eQc9HfHzXWfsU28xms6nyawefWwpKr6CggEpKSlx3Byo+5KFALrZZd98hdFdn8G9l\nGRlZI4bbJsW7KYUX9os/tk9Yco43PcULegBBsIwUO6fXjDUd7APvm1cslNkATgSjZ7SvqKjIVT5w\nZxn5LD9vrBYfrBtPtkpTsCwoxafx2GXkyaZpCYY95IftEzLLKAB7SA0e7OCTol1lVdkN+Z0N4ESo\nrCpP+8CHnY7AB6sqxPrdAeHbSPlVB1sfnnrEmQyZPeSLfSV6Hdhcoeh5YS11iLfwtD4+9TZv3hxI\nr7vt88la8pU8H/JE1GHKyMUs6s7X/y9YRj5YI4p3E5D94qkXdf6vvOkB+hF7v4tyRnaOZg7zZl+b\nIZ9quiIk+7yweIzcoaDz64N1Y2SrOADUBsuCUnyampoaTz2VGiXPQee3O1hGAVg3KjnAjcbgLuoZ\nef5B16+RVeXFvkrwIa2QWFU+6hdw9xe/cR6kv6gUFHsoCJaWkc0Vkp6P+gg63g4cOBBIz9O+kNhh\nXWUtBZmM/KJuS93JMnoBfJDoNIA/gfkZD3tc58kyOgOej7WDp1Ys4KDtAT4FaQLfnreCC1Xtd8wG\nb+2ch47sl2zwNr4K8CnA4+ATua0G/XD5TDN0NlI2eGohH3yi1QGeQtLA2+6Og4+Em0U/B/rpyRLw\n6ckT8v928DH1CPBUWpJcVyffmSz6yrZmed8irxLwPGsj+ORqNnSW0bUhlp9D3gf009cXgBelLgaf\n3HVCP83cQ/4uEr2+8nek5MshdkTBN5tGAzduB8Cnjj1ZRj3Ad5MO6FwpQGcZRYNPnIfCMlKdkKpf\nFXQNAJKJKEbTtB/F3jDwKdoqqYsa6Bygi6TMc+TvVvB0UzrYp1rAu8iUbz8Drl8CbwdMAgfqMfBt\nvdILxKr6CswKagLXaZh8TrGqzNARKe3QfcAJ3mYaBq6vYtG7HTyvHiV/t4MxGAlSBvvBJ4VPSh2o\n+lAj2hzoLDB/7KFaye8pcL1FSx6ipazCwH4aBe9srlFStg7whpQssH/shR5vVtFT/nLcUB+58n9v\n8XZc/pcGnqY6C/ajPNExHo5T9WESWz3ZYU6p3wypj71ybT/4Zy2VyecfAMeQN9aSReraIflSDCmr\nlF8pdKaVsfw2ElFQTwIMNnUny+gQuDL8JcUaURvsM8HTS38i5nVMl/cUwuE4dPZLu1xzObiSm6Gz\nQaLBBWcDF+wbRHQR+Ph+Arii7OC7lmPgTksxYxQbST04LVx+OsBH07+CzkL5X3DDlgF2lp/ADcbr\n4ECaC73jmQjdycxiQxSY7XMWzDLaCnY0GzjQ74POMmoR+5Rt94E7g1DLr13K72fwraYKsv8Bn7R1\nStkBHIg1AAoNek1gR3aIDYr55AA3yJHgReo50Nk04QbN/dBZRpVgR8+T+hgKbsy2gvEX/wQHbBgY\nKWIy5Fd1dl+D1xXUPLQK/hbRU/X7LXR2k6rTs+AgVPlJkGsWgP1K8e9V4zhf7H4GOvuql/x8WvKr\n6lc1yBYw0qO/2Kf0CkVL1e9pcKfUF+xLvxD7fhYNAjckxVLOR8CNVzjYLwvleoD3pK+FbF+VMl0L\nhr3tB/uesu1fUlaV0AcRqj5Ggv3ldbmmQOrDqmlag3zWkz2k8jsL+vpRjJS/YiPVg31mA9ifIsAD\nkjbR0kTPAh5EFEo+doI7vmjJazN09hDAdavy9pDYb4w3BSRsNNRHPNhfvgX7UKXUxx7wAEFBA0dB\njxsL3DvcQvDDgjKknCJEz8ha+lK+N1PKdBI6spZIfn4p5dEq5ZEt+b4ZQKXE4hvoyDZT/KJuS93J\nMgqDPtIzwpaMyQIeVVaBeT514Nv3HZqmJYFP9z4NPnJPYGdU+uHC9HgRXIitYMevAwdKEXgk0lf0\negEYD+b3LBcdxUYqBlfSF9DZSBeCA6AYPGoqAAfYBIPeX8GN6iixbyDY2UeLTS+AWSg7ATwKHknG\ngZ35pOiNFjtHg58+VQt2vHrwnZViGX0IDgBlWwv4bENny68/OMhuAo9aSsAcngYpiwLoI5Ri0VsN\n7iD+Ae5c/iR650G/a7hS3rsX3IGEgRu7IgAvgUfI34B39xDY8RVYrFby/UtwRzVayqkITDu1ilYV\nOLjmS33sFTu2iZ7qhGoN9VsBd3ZTf+jcmGrwaK8KHHgpokfgjqlGyuQacINyodhyt+gfAHf2Y6SM\na6H7XwZ48VXZt0Lyq9AKqn6/EhuTwad/k8ADhu/BDeXnkucrxc51orddbEsB+4IG9o1Doj1Xro8C\n+/cuKeuz4EXwCVLmB8AHpe6R/J6T/ALsI6fAcWPkh5ngzh5qATd4b4rOJkN+Y6Ss/iZ1UwhutBW/\nSY3uD4NjuUTsu1Lyru5I/yXla5XXBrCf2KHD/XaA61yDHm8V0Efn5eBp7W1gX1WE5ViwH2RJmQ8E\nN9jDwI38JEN9eNr3PnhG4WYpP1W3E0VvONiX90L4TQDuhztr6bSU59/Bg7F28J1ntlzzkdgHybs3\ntlm3pu5mGU0AL2Q5AIwjoiKP6xTLKDUiIiIrKS3DJ2sEYFaQxWJBbm4unE4nGptbUXXqJFJTUlBe\nXu4EA6H+B0DviIiI8LT0DJSXeWeDOJ1ON73q6mo0t7QiNTUVYRYziouL1fSPHYApIiLC6s2+rKws\nmEymDnpVVVVoaW1DYkoqTpQcOQt2whzIbX1ERERYsPZlZ2fDZDLB3sY05xMlR9rBzv4HMAAu1Waz\nZSWn+2aheOpVVVXBCQ3xCYk4UXKkDowxVqTZfhEREVoo9XH48GEkpaXDarXhRMmRJugU0XIAGZ71\nYTxgc/ToUaSlpSEsLAzl5eXIyclxsWmiY2NxouSIA8CDYFBbAgBzREREmLIvISFBnaD1qQcw+6XN\n3oRKZjeVQMeOmwPZl5KSAqvV2kFP6uOsaLWCO9RzNputj7E+vOXXbDbj5MmT3vTUThSzxE59RERE\nQqj21Tc1o+rUSTja2pzix9Mk3uIjIiJ6B6oPo30qPrIyMxTLqFLKrg6AFm619cnMzPSpl5SUhNra\nWpdtHnrt4KmTV8VOiycryJ+e4poZ2D514Ma9CdxwOsOttrBUP3qJiYk4c+aMy75Tp07BCQ2pyUmK\nPfQc+OljZQBSwq22nkrPm/955lfFR2xkhGIt7QJ3Ek4ANVarLcNXfgGgubkZJ06cUAddXfGRnOjG\nNhtGRJehO1N3rU6DRzlHwaOG3we6fvDgwZT18McUFhbmlTVCRNSvXz8XD2XLli0U0XsIJd/2lOsx\nd556z7/9oU82iKfeM888Q1p4JEVGRXvl0yj7PPWMq/9GvYsvvphiBo8ia8Z5BB5RrIOBTxOKfeo7\nDLtGygFsNdrXt29fvywUT73t27e72E3gqa6lMPBpfOXXV30kJiZS2r2vGXe1/B4GPo1nfo0pMzPT\nxZPJyckhIp1NI3ql8GA3Ge1bvHixT70BAwa4/p/9xHqaOutZAuDw5n/+7HvjjTe86hnyexwGNo1n\nfXizb9myZW56SaNmkBYeobY4urFpOmOfig94sJuCrQ+jfSo+5HG1dgC/NeqFp/Xxq+eZ16uuuopi\nBo+iYcOGeY0PT7aPPz1jfBCR1/hQLCPVvnjqPf300272qfhYsWJFh/gw6vnyv2XLlrl8mUiPD8Ou\nJbf48JdfIqI33njDxXEj0uND8tuBbdZdr+5mGV0N4BARvRDE9ahe9yp+/etf+2R5REdHo72dR1Hf\nfPMNyOlEZN/Lfep9NP91v2wQo95dd92FjPvfwYvvLPbKp1H2BatXVVWFhn1foceIKQDPR5bAwKcJ\n1T6PlAieQupQfsHq5efnu9hN4EXY82Hg04Sqd/3116Puh1UKH2AF8BsY+DSe+Q2UPNg0airQK5tG\n3QWFkDoUaqj2eaQM8J3VIqOesfyCSVH9h6PX/64AvLBpQrVvzZo1IcVHoKTiQw5+lQKY7RkfoeiV\nlZWhYd9XeOuttwAf8RFq+RmSz/hQ7UugpOJj9OjRgEd8eNoXjP+p+Ghq8h0fvvK7Z88ePPfcc3jo\noYdc//PCbvLLNuts6s5F5aCTpmmmkpISwGzBY489BsA7y6O+vh5msxmNjY147733kMCNrU89i8U/\nG0TpAUD//v1hiUmEydSRT2O0L1i98PBwROYPgzW9L8AjvmchfJrO2KdSc1kRwPOMrt0EmqaZKisr\nQ7Jv6tSpLnYTeAS0GsKn6Ux+586dC1jCUP7BFMCD3eQtv4FSIDaNp31dSZ2xTyUPdpOLTeNZHyEm\nNzZNqPYpdlMo8REoqfiQ+m4EY69d8dF+7lRIehEREYjMH4YhQ4YAXuLDk+0TbNq2bRvgJT4Uy0i1\nL4GSig/ZLtohPjzZSIGSio977rkH8BIfvvJ7+PBh3HDDDXjrrbcwcOBA1//9xUdQBgWZuhNutwy8\nm6SfpmllmqZN9nGdBmBhW1sbkm99ygXAGjBgAHbv3u26bvTo0SguLkZFRQVyc3Nx8uRJVC59Aif+\nco+iO4ZpmlapaVqO0vvNY08gLCwsoF5WVhYWLlzoyzw3+4LVs9ls+nPbOBkrKiT7Ro8e7bKvce8/\nAb7VbDCWn8PhCNq+0aNHY8uWLYi+cATMETHKtnkALpXF45Dzu2bNGiSPehw9H14CcGNmAvCjsfyM\n+fVMzz33HIYNG4YTJ04gKysLycnJaD1dYrykAECRr/rwpXfw4EF/9Ytg7fvoo4866DXs+RJ1368E\ngIOkP6LQa314s2/q1Kn+7LMA6N0Z+3Jzc1FSUuKKD/B20nR/8dEJ+wiG+CBnu0+92tpa/O53v3PT\nGjhwoN/4cLb7Lj9PvQ0bNrjeE8pth/ggZ7tb++Kp9+abb7rZV1hYiOgLR6i1gQ7xofT82ad8eeHC\nhUhISEDyqMexevVqwEt8eMvvsWPHMGLECDz99NO499573b5jwIABXuOjgzFdTN05ZTSWiNKJKIyI\nsojIe0QyG+SyPn36+GR5tLa2Ijc3F0OHDoXD4UBFRQVWrFiBjEnzkDFpHhYsWADwPORF4Mf3je3T\npw/Cw32zQYx6ZWVlSEpKQntzA4gIP/74I8C7Rv4m9g33Zd/XX3/tVW/y5MmwF3+H1lNHAd7nvwC8\nq+FPAO4Mxb4VK1YwX6WtBY0HtgGyp17TtMHg6ZQJmZmZflkonnrXXXcdGvd9BWcLPywMvGDWDL4z\nmhhKfZSVlWH48OFot9eBnO2Azm76BXg64DLP/AL8EBGFWJ41axZKSkqwadMmlJWVYcaMGajbvhaO\n+mqAR3wvgndUOACM97TPl15rayvKyspw3333gRytaG93sWlKhSWzR9WvP/vGjh3rpmez2YzsJpuB\nTdMOYKJnfXizb9myZS49IjKyaWzgEXMGeIfMhFDsq6ioQFlZmSs+wFM8p8CbPHaq8gtUH0b7jPEB\n3mX1EoBfg33xMkt8uk+9hIQEfPrpp6ivr8eJEycwefJk3HfffbAXf6fYYv3B640V4PgY58n28ac3\ncuRIAICzrQWfffaZqo9DmqZtAm+UON+TZeSp9/TTT7vZN2TIEDTu+woNDQ0AL+6r3Vy7APwikN6n\nn36KL774wqV35MgRtNvr1BRrNngR/VbwVE+H/JaXl+Pqq6/Gww8/7DZVpJJiN5WXlwM6uylPmFXd\nN23UXYsRCIJlJAVDAJwmk4m0MBvZbLagWSNqUU8tKhv0yGQyUbg1eNbImDFjyGSLoXCrjfr166cW\nunKNet5YIz179vSqt2XLFkq47ndkjk4g6Ie8Lu+MfQ888AARkYvdBB29UKj0QmGrPPDAA1RdXe1i\nN4l9ague3/z6qo/ly5d7spseC1Qf2dnZagHV9UpJSXFj0xjYTafB+6472PfUU0/51OvVq1dQbJpg\n7VN6ik1jYDe1g6cBfNaHPz0v7KaT4O2UnbKPqAO7qczgL85Q9VR8REVFEXjgsAjcgBMAJzQtZPsS\nrvsdZWRkqLJbB+D/OlN+xvjo1asXQRbQwc8UZ3tFz9i++LNPxUd8fDznjzvVm13XGuwLxv9UfFit\nVqV3L/gcDwFweuZ3zpw5BICioqJcL5vNRiqp+JBFfgIf2tTA01o3dFs73k2dQadYRqpx90zeWCOB\nWEahsIKMrBH1Geh8HxdLxhtrZMuWLQFZS/DCQgnFPiP7pbNsnxD0bgNQ7yu/vuojENsn2Pz6YPGU\ngRETpeCBRrXRPk+fCVS/4F0yqj7+DKCtiyyjZvCd5DDR/qqLLKPdAA6QHh+tXbSvg78EUx/e7CM2\nSg1Ivgbv6PlHeFqfTsWbh57iLZV2kY20W/TSwdN5UCwjb+2Lv/KTgUQzgHTRvAlAo5GN5Mv/vJWf\nYhmRHh8BWVAqeX6PIb/GeBsL4D3y0z6H8vqvZBl5Y410J8vIB7tE8X06xS7xYl+nWShe2C8hs31C\n0EuDO5unQwqC/dIpto8fdo5ivxC44Y32puMrvz7sU2UXA74D8asXgGWk7lAVC6ogGPv8sIzCoD8w\nRml2xb5O+UsA1hLAZxGsweoFwZZS9gXtLz78WflfJfggmt8URPlZiOik/G4Dr+8EtM9P+XmyoPzW\nbxDJGG/dyzTqjl4FvPvgIPg2fx/49ugvXq6bAr6TqAKfltwDfXpgh/y+Q/62gx3FDm5Q7fLeDvl8\nHfhkcJNBr1L+bgNPXe2AjgywQ2fxHJXrFP5gB3hU8C7Y2VtFbwf4Vt5p0DvhQ69Vfir7FnroNcpP\nxcNRed0hemrayiF5Ow33vF4CHvmp8muV7z8qnzHqtYagpxgqTeD563bDtapOTkN/yIxdvtMutuyH\n7POX+i2X/NWLHccNei2Gz6n6VeWmvmOHvHcPeF3iiNSNuo1XUw47wCNgkmuUfWQok1NSf62G+jgt\nWo2iVyl/Ky1lXxvc/e+cwT4HGLPxLnhOvUquVy913QFDmdrle4z5rZLvOgM+4a7iQ33XCSmzM/Dv\nL6f91K/ylyboPl8N93hS9hl/Ks2jAKqkfrfLd6p4U/7nGb+tklf1HYomsF9+rxe9P0v57IbOz2qS\ncmsGx676vNLz1mZUGfxP2aDit8wjrw4PPQfYD1SbUQrA6dFeOQ16R8GHEVXb0gT2JeXL6hqnwfY6\ng54qtzZ5T6FWVJ6robeBVQa7la/sh+EMFvgE9Pr/tjsEoHMsI9XzvgE+st0C3iGRCC6sMugsI4AL\nM18+/2u4s4wAHcym2CBnwOyZJPl9ItgBngEXcCQYmZAIrpil0Nklyj41IlJ66T70CO5smmvAR/+N\nbBojC6Uc7Eh5YAyBA3wkvVp+/4yYYbILfHT/c3Rk+xD4Nv4odLxDHnTuj1GvFDyS7SHvfw0eSR4S\nXU2uU2yfh0VPk/JpAu/1VmyfKvA+7ToAZk3TFNxMsWQgP/eA6/cx6OgDVX5lUn/PgIPJBMZimOS7\nJkBnvzigw9ycYp8aWaaIfcuhs4LqJe9JcGfTVELn3KSAA7EC7DNOKZMk6OmYfO6olIfahH4Z2F/s\nkm/FU2qTMpomWhHyfQpMViv2fSY2KjDexdDjI0JsSYTOgjoh5dAbfFdHYHSGYkEpdlO82FkOd3/R\nxB7FglJ3JL3lvSbw2QDVQB0SGzIBNGia9rjktRV6vGkAfgTX7zSpI6Nemfhwqbyi5GXVNO0NcAdQ\nKTYrIOJqMCLGbtALk3yWgef1ndDj4wiAs5qmlUk9NYA3OQDs56o+JhhsDgf73Ul5r05+zwa3B9A0\nrRGMzTgCPam4OSt6Dg89p9j0OrgDnAJBbkh8xMq1JHUTDmGlgWNOwSV7i03tJI+kle9PFxstmqa1\napq2F74xQZ1K/0mWUQGYgWIG94A9wKOkN8HOaIMe8OXy2Y/AC0cWsCMaWUYF0EmFb4re+2D+xy1g\nJ5oD7oWzwCPXOnDjPBnckOZBZ5dMBDdaSkf9/NmHXh3c2TSp4EZD6U2Xn1eCG/ft0KmSy0Tn9+Cg\njgZQqWlaLnikp/gr9dBHDor8OQEcCE2iNx0c2J56DWCe0HEAMUQUJfmrATv2D+B98Irt00P05kl9\n2MEsH8X2iQE/xGaefPdYcMOk2D4Fon2j1O9N4M6h2FB+kLrMAt9ZvkpE0WB+TjOY8VIi1+4Fzzer\nzquH2HpYys8OxnrUi302qc/9oh0NZtNESj08KPV1JfQRcTG4Ub8FOik2Hux/A8CN15VybQR4F1SM\n5HeP5MUm33m1lPUyeVWLDXHgoM6XfF8or0jwTp59Ys9D4EZKsaDWST0ngePACX5qlmJBVco1Z8GP\ngDwJd3/5AfrOo71SBneL3qfynZnQWVB14E73ALgzfQw8/dkIPX7PAbhL6vdqsStJytkCoKf4cBaY\nczQIvOupRD7/JORRu1JWJWA/ygH7krJvhNjTE5w06O3LaegDuAngTuZKKec/QKfbVojeG+AF8nbR\nUyP1YvCAYhd45+IfwZ2wGRw7BdBZUHbRmyTXz5SyVtuGbgL7wN/FvoNSdjeA27QKsfM4uJ3ZAR7g\nfgk+T5EEHjCkaJpmlTKshx63O8B+skLs8Tof15n0/5RlZLj+xri4uA19+vTB/v37kZ6ejh49erhd\n09TUhAMHDsBqtWLAgAGu/9fV1eHYsWNobW2tJqJkTdMuAbAyMTExOzk5GceOHfOqV1NTg+PHjyM8\nPNxND+CDPQcOHGgF8D4RPaJp2qOJiYlvdVZv586dCjX8HRH9TtO0nYmJiYNC1WtpacHhw4fR3Nx8\nDuyUt4JHzr0TExNtoehVVFSgpaUFubm52Llzpyq7f4AD2ZKYmIhQ9MrLy13bUSW/B8CN5EYAXyQm\nJm7oQvnVght8NW1zLjExMSUUvWPHjsFsNiMrKws7d+5UiGjVqdckJibODkWvsLAQ+fn5iIqKws6d\nO9WJPStkUJWYmBjmS4+IUF5ejtOnT7v8+cCBA1BlvnPnzmrwWskMcGMbn5iYGBuKnkp1dXU4dOiQ\nk4jMEhsfsnmJab70Ghoa0NTUhLKysg56EhsKj07gBvJcXFxcoq/4bWhoQF1dHU6ePInBg90fPSF6\ngD41dwuANXFxcQND1WtpaUFxcTFaW1vVnPzXAO4iotr4+HgKRU/FR0pKCg4cOFAL7qh6ge+Y3o+L\ni1seip6KD9FT07EZkCmpuLi4PH96qj4GDRrkrfwUu+k4eBDwCHVHQ45u6hAAbuQB/AU8yniOvOAr\nDHA7JCcnD35szit49n8ewIIFCzocB1+5ciU2btwIp9OJRYsWAQD2lp/DwX27sXLRfJw6dWonOLin\nAEBqaurgV155BZMnT/aqt+ijj/F/3/wLDofDpadSfX09br75Zjv4dvW8zupt3rwZb7zxhjquroL8\nxs7qPfnkk7jxxhsxe/bsYvCdz3Odze+HH36IlStXwmQyob6+vhl8i9/eWb1p06YhNzcXBw8eRFFR\nkRNM1vwWcpq1K/Vx1VVXVYDvSpZ21r6DBw9i3rx5mDNnDkaPHr0L3GGlgEfAIevdcsstePnll3H+\n+efjqquu2gkePV4HvmP0qXfTTTfBbreDiHDnnXdi+/btWLRoEW666Sb8+c9/Rv/+/ZXeg1J++zqj\np9LOnTsxY8aMViKyGuOts3oNDQ0YNWrUTnBHUASJj+Tk5MHvvvsuxowZ41XPSYT4uDisWbPGpbV5\n82a8/vrrsNvtgEd8dEbvySefxNVXX40XX3yxQ3yEqqfiQ9M0NDQ0dIiPUPVUfOzfvx8HDhxwgO8W\nXPERSI+IcN9993U4oCb1UQFGBA1Hd6fuWoyQjiUHBc97yAAAIABJREFUwL5grs3PzydbdgFNmTKF\nPNPu3bupR48e9Mwzz3SAi6Xc/bza/7vDqDdo0CAaNGiQT73IqGhavny5m55KEyZMIAgpsTv0wHcH\nfwSQ1lm91atX0/XXX6/0OsDKQtUrKiqi8vJycjgcalviSQBjO6uXn59PcXFx9OOPPxJ4GmAugG+7\nqfyaAUzqSn7Pnj1Ld999t3GP+C4ACZ3VGz9+PN1+++1UV1en6vcIeGomoF5DQwONnjiFFi5c6NIz\nmUwuqBl4CiBf7NQ6o6fSpk2byGhXMPa9suBTSrj2t171ZNtkBXjh1xUf+fn5dM011/jUi7tiPPXq\n1avDe6LXIT5C1VPxIXod4iNUPRUfmzdv9hofoeqp+HjnnXe8xkcgvYRrf0vr13vf9u4tPrrr9R/p\nEACYYmJiyJY3mFpbW90yfOjQIcrIyKCPPvqoAzHRV4cAwNSjRw8aOnSoT717HnqsAx2SiGjevHn8\nwHjZy9xVPSJXgI8BsLozeg0NDdSnTx8qLi5Wem4O3032zQKwqrN6AwcOpEmTJhn1EqVBi+uKfd98\n843aRRTdlfyOHz+ebrnlFqqpqVEB+TSAHzqrV1NTQ+PGjaPU1FS18+h5AEcC6an05kerKT4+nvr1\n60dETK/84YcfjOV3CfTdNyHrqeStQwikN3fJWuo183Oveo8++iiB11WyyCN+b7jhBp96GQ9+QGaz\nmU6dOuX2noH+6RYfoegZ48Nbh9BN9rnFR6h6Kj4Mem7xEUiv18zPKSEhoYN9c+fO7RAf3fnqzs5g\nmfSqas/3ZB/XaQAWRUREUM9pq9wyW1paStnZ2fTOO+/QmDFjKCEhgSwWC2VmZtKCBQu8dghKLzo6\nmv7+97/71Bt02S8pLS3NTW/hwoWUmZlJn3zySbfoqSQOcI+MqkLWS05OJrPZTKmpqaoBUlsqK6XT\n7Q77ngBPkXUqv5dddhndd999Rr0Eg8N32r7777+fAFR7+kuoehkZGbR27VqjffFiX1I3ld+L4vM+\n7TOmi4deQQDIbDZTZmYm9e7dm95//32j3mTwBoRO6Sn7PDuEYPTmLllLEef9soPewoULKSkpiQDs\n8dSLiIigpqYmr3qDLvslmSLjCQAlJye7lZ2hgXSLj1D0Zs+eTRaLhVJTU9XJ3Q7x0Q32ucVHqHr3\n3HMP3XfffUY9t/jwpzd3yVrq9fjfyGaz0U8//eT23siRI93io7tf/x5RXjjZC9lHLP97GfpWMmef\nPn3cTuqVlZVRXl4evfLKK26Vo1J7ezv1mr6aUu6co46rt4LXLN4FUHLRRRe5Xe+p53mqcsmSJZSa\nmkr79+9XlbZL7KwH0Bqq3gcffODqzSX/ReDtZysBtIei19bWRidPnnS9wDtpKsBz1jWdsW/t2rVU\nW1tLTqdTjagqwDsc3gXwfah6//znPyk+Pp527dpFkL3R0PfZO0LVIyJqamqi2NhYFeC/Bu/qaATQ\nGKrepEmT6Pbbb6ezZ88SeIByBvpOrZZQ9Q4fPkzV1dVqyk2d3TgLHgA5PfW+/PJL+umnn8jhcNC5\nc+foV9eNpPT0dLLb7URE9M4779B5551HZWVlarpSnVU5A6ApVL329nay2+30xRdfqBGkAzyy3wvg\nZ396f3p/KcUMHuWmp+Ljww8/JCm3r6Vx3AGgtU+fPm7lE0hPxYfEWj14WuZ18b/ToegZ42PVqlVK\nT00NtgJoCtU+FR9fffUVSYwpPzwB4Fyoeio+PvjgA1Uf7ZCHKXWm/Ig4PgQlUgt9/dcK3h11GLyT\nLIf+GzoEuLOMzgBI8ni/EPpjFJsDsTxsNhtpmubGCjJUunqdUb+bTCay2XzrWcLC3PRycnLIYrG4\nvkucfgkM7JdQ9CZNmkQpKSkUGRmpbHsVfCahU/YZE3ywaULRU3dc4lAO8CM+s1V9dMa++fPnKzaN\nUwL8sq7kd+nSpaqzbwHvoHizs/ZVV1fTuHHjKDk5WdXHdvCiY6fKb/ny5ZSenk4REREqwF+UjpAA\ntHnqffbZZ9SvXz+KioqipKQk6pXXh7Kzs116TqeTHn/8cSObZj54O6TX/AbS8xEfTb7qw6gXFRNL\n4Wn5bnoqPiQ21IG/7+Vnu6ZprrIKRk/Fh+ipbc6u+AhVTyXJt3pGdLGq31D1VHyIfSS+d4Wq387Y\nN3/+fHWH5QTP+yu2mSOQXkTeJbR79263dmDp0qVqxuAchF8EhlS+K7+PAbD8P94hoCPLqBXA5Yb3\n46TxnqH+1xX2izSS58DcnI0QVpA3bonSS0xJ7cAu8XCqZrH1DIA/dkUPOhfJjbXUBT2vbJou6DWT\nRx12VQ86+2UMAHsX9c6AR6ILlM90Ua8DS6aLenbwHvp3wNMe73VRr87gLwe7Ib92yHoeDGwpX3pv\nfrSKLPFpXvUMUx47wAO6OQAO9u3b16tWkHq7VT6lDIu7qKfgdlvBzzI40kW9NtL9ZTeANV3UUyfm\nh4HPkvgtv7lL1vpsG0XvKIRfJHrD5HcLePeWRtS5tvzfxTJqALBS07SdsvUtFzxamaNpml3TtIP+\nxIJkq6gpihwEwQpKTk0PxFZRh1vM0E81dlZPcW6CZi0F0AuZTRNAzy+bpTN6pLNfrkMQbJoAepHg\n0XcsgEc0TdvTRb2QWDJB6LWB60GhOfyyZILQUw+rrwSQKs8w6Kp9KgVkSx07cgiW+ID+1xO63+R2\nUa8NOnMoBwFO2gahZ6zfavBBtK7oGf3jBPQDcZ3VA3hR+VNwftP86QWRVPsCGNoDInKA28LEzgp3\nV4fg2UjNAS/I3ABgKvhEZyqAa8H7hf024OXl5ejZU6+DrKwsxQHvVCovL0d8gk4jCKC3CnzCt7v0\nutu+/1o9TdPCwWiB1g5vhqZ3Atxo/AN813lRd9gnyQxgoK83O6EXMAWrRzzMI7AP/j+z7+yZWlhi\nk11/e9HrDd426YCM6LuoBwTopDqhF3T6D+jVgMtsBnhgbOv0l/27U2dvLYwv+IHbgTuHZ8FTJ1Pg\nDudygm+lysBrD+1wB4mpa5rAo6hG6KCqVvCR7XPyuXr5jhro8CkjDMwIH1O4ht3QgXUOsa9MXvXQ\nIVgtcm0zeLG4Ffop2jbwIo9aMPpZtIYBeFw+ew76yLRB9JsN319q0FJ6Ct51TvKfLmX1M3TYX73B\nXm/5VRCtVujQrN1wh3epz6jy2yn6J3yUX7PonQYvTJ+QMkoHP7SjXq5RenugQ8ns8l6pQc8JHRjW\nLHr14M7gZYMNCg6ogGUqzxXQYX4O0TsOHTrYANl1A+C30BdbG+RzSuso2C/UCVqn4XVcPnNIbGwH\nTxlthQ5XVHXbBt1Hm8EL0Oeg+0uz4dodhr+HgXe2tMMdaqf85Jzh+1ugL1S2gv20Bewbu+X/OeCR\ndzn0u+kTkteToqPKskFsVj5TDh2AZ5f3x4LZPsonWw11qnYWqjpR4MJmqR9jvJXK3wfB/ndarlP2\nNEt5Gn1QgQFVuarnMuyQMmqU+j0I9hOlp2wqkf+3Sxk1Q4+3HZJXNTV7Fu7x0STf0W64XgHsFHzO\n2FbtBk8RtUL39Vpwu/gy2LdVm+KtLsqgAzdVe7LbkPdScH3/V08ZlYvx14N3IuQBKNc0LQo8hfAd\n+Hb4/yRzGfI5B3gXhCJGtkAHt2nQg0s5Vru810/eiwUvTpnk/yng3ncG9Gmf3vJ/Dbyrpg3ckfwg\n399L7PkBzAbZKddDbHGCeT3L5Odx6MC6/xONJdAX8NLAo9DbwKOqOvBdkSb/rwfvoJkHYBkxuOq0\nfN8j4ApvAR9JbwOzcswyJaM6zkjwbbFar/m16FwJ4CowidMiWuXgefNqKZvzwItuj4Nharuhw8BS\nIA+PJ6KepIO1vhO96eCA+VnKKQH6drrP5PPVYj/Ad4WvgRvYRVLu86DzZ9aD6/4DKdd68MPNw8Bw\nwPcM9pnB9fy8lG0N+I5TbXWGlFmz5FlxoizgJ6Yp9IcT+p2xWV6/lnL9XGz8l9RXDTiYVYPyd7Cf\nmsCNd77UgUX+Fyl52wvmAM0Te/5PrtkE7iDOyt/nGWz4Gjx4qhUbHeC1ty/ke2vA0Le54FgzSd5b\nwAwcBWNLk/dWgBtygs5lSgX7yzIAc6Vufw3eqWKT728Hd+xvSxlYwb79HHijRLFoWcBTEyUA1oAf\n/pIpeVJMnzqwD/5NyuAMmGtkkzJYDH5QkxVc51Fi63fgu6Qsgx6BqaGK4qqIxLHgx+laRVODDlFs\nkLJtBseWCTz9Ey72aeCn/EHKvB0cb9A0bT50AKRNrm0F8Al0nlWSoQ4BbquWgdu6avnenqKbBqYz\nh8nfYVJ+30md3inlly6fOV+uu4CICqD7VI3k4w75zs/B7RpE4yu50+xU+newjP4KztAxcIaWEtEL\nmqatB3NzwgCcSUhISI6MjERFRQX69+8Ps9mM9vZ2hIdzXZWVleHcuXPQNA0DBgxAZWUlamtrkZ+f\nj+bmZhQXFxPY2T4CgISEhGdC0SstLUV7eztyc3Nx7tw5HD161AlegHsEgMnTPquVfVzxhRwOB7Ky\nslBZWYkBAwagqqoKp06dQt++fbF37952KY93wY/THJ+QkBAZil5jYyOam5vhdDpx/PhxEJGmadoV\n4Kk4hGpfZWUlYmNjERERgZ9++kmNAmeD54M/TEhIKAlFr6WlBWazGRaLRbGM6sEN2DEAtyYkJBSG\noqfSyZMnUVFRAXBjsgHcMdQmJCScF4qeB7vJCQ5S9YjKuoSEhF6h6DkcDhQVFSErKwulpaWqfjPA\nnQASEhLCOpNfgHET4IbdCu5wqhMSEs4PtX5VfOzZs0cFtRpULUlISLjPl96hQ4fgdDqRmZnp0lPx\nkZKSguLiYoAbzLfBjaEtNja2b3JyMkpKStC/f38AwL59+zBo0CA0NTXBYrHg8OHDrryq+MjKysKR\nI0fawCPs+eABxQWxsbF3hKKn4qO5uRmVlZUAtzWqYabY2FgtFD0VH06nEwcPHlR10Q4eiPaMjY1F\nKHoqPlpaWhS7SdlYB6AxNjY2y59eVFQUvKVjx46hurraCeAkEWVpmmYDPz3tYvBgYgwRHfX64WBS\nd0wZSadyo1RyC4Df+7jGNUWRmppK69ato7y8PJozZw5t2bLF9dq8eTP95v4pFB4eTjk5ObRlyxa6\n+eab6e6776YtW7bQunXrCHz7XNlZvdjYWJo/f75Rr0w0/eoNGTKEXnrpJUpOTqannnrKpTdgwACa\nNm2a2gXQCOA30J+PELKeer366qsEL1M8ndUz2DcXjN7tLr3N4CmUTusZtp2q2/1O53fixIk0YsQI\nl31dLb9x48bRtdde2216HuXXBH0aqlN6xviQ+hgZTHwMGTKEZj31B0pIctdT8SGxsQPAU8b4SE5O\npi1btrj0li1bRgBo8+bNtHzVWnrmjffc8qriw6DnFh+h6qnXH//4R4I+JTilq3oG+9zio4t6uzzj\nI5De3CVrO2gtXbqUsrKyCDwYKeuuttutje5WsRBYRoMHD6ZVq1aR1Wp1MV2IiOLi4shsNpOmaTRt\n2jQXOmD79u10+eWXU3l5OW3cuJHAt05vdlYvMTHRhQ6QICoDcMaf3meffUY333wzERElJCXTxx9/\n7NKLjY2l77//nuR2rRTAYAiKoDN6KsnJU0cw5ReMnsG+XQAe6oreJ598QjExMWohtApAQVf0Ro4c\nSatXryZpyNwcPlS9Z555hmJjY9U+/1YAv+2K3lVXXUWPPvooDRs2TO3LXwegVzfVxxkAW7tinzE+\noD/bw298KL25S9aSOTbFTU/FxxZ92+nvjfHRt29fqqysdOmVlJQQAMrIyKC4HokU2e8XbhgMFR8G\nPbf4CFVPJRkwdWA3dVbPYJ9bfHRG75NPPjGeS3KLj0B6UReMoKqqKje9kSNH0nPPPUfwwm7qrtd/\nrEMIFd5lhJWZTCY1qgoKVuZNzwgrW7JkiRqV+oSV1dXVUZ8+faikpISIOgZkqLCyQHoqBQsrC1ZP\n7OsAK+uiXkCYXyC9UGF+gfRChfkF0gsV5hdi+QWE+QXSCxXmZ9Tz1iGo+NiwYYOqXzeYnyecrb6+\nnrZv305tbW30/Nsfki1vMEVFRbnyqOLD0OC6xUeoeir56hA6qyf2BYT5hagXEObnqRfZ93K67rrr\nXDqBYH7d9erOJ6YFnTRNM5WUlCAsLAx/+ctfOrwfFRWFX1xzPaZPnw6Hg7cET506Fc3NzaipqcHG\njRsBHlVt7Kze3LlzYbPZkJ+fj9///vcAz7+V+dKbM2cO7r33XuTk5HjNU3R0NOrq6oz/igPQQETU\nGT1/qSt6cn0igJFE1NJVPUkt4N1X8zuj19jYiJkzZ2Lu3Lndlt/+/fsjIyMDZrMZ4CmUtyDo4c7o\nRURE4LbbbsOQIUMAbsieBXC5pmlxXSm/bdu2AbyutrIr+TXGB/hw1mr4iY9Aeio+7rnnHgDoA14s\ndcVHZWUlwsPDXXrR0dG45JJLYLFYEBsXj/grxqOxsRH19fWu9/3FR6h6/lJX9ARf3SE+umhfh/gI\npJdw7W/x5Zdfor6+PmB8dGvqrp4FwcPtSgHUhwrvMsLKpJdUW8caAfzcWRiYQU9tW6wCUO6pV1BQ\nQImJiUbgHGmaRiaTyRus7Cx4PlNth6RQ9TxgZWqaQj3z9kxn9BTMD/qWQDW90NBZ+yS/alqmBTxX\neiQUPS8wP89XSxfta4a+HZgA1IWq5wHza4O+g+ccgKbO2icwv3bw/PxPSjdUPQ+YnxO8C0ZNVfj1\nl5i4eJ/2/X/tnXl4nVW1/z87SdukTZu2aWlLB1poGQplKDJeQHBEQAEVUFDh/hD0KlxBEeGiXEBR\nJhELokJBEFCsCIjIfAUEkaEzLZ3btE3adE7TzNP+/fFdO++bk5Occ3IOCnrW87xPct7h++699t5r\nD+9a321tI7i37rR23pSa7K3MA37MmDF+5syZ/qijjvJ33XVXvK0Fl94twPZM8YKEb2ymv+Dp19EX\nvBiZnwc2S5V8FGjoa/osv50cX8jT7KFU5HbjLnrAA76mpsbPmzevJzK/DhTAOR+42ufCjucCJKMX\nmp96puRdcbKysK5O5OedMRlYnKzshhtuCIVWY8fGRLytW7d2IZwbOrzcz5o1y+/atct7342sbKd1\njossfbMzxUsgK/OIxuF+5Lm0MFO8OJkfmhI3ou8wvyMJOVsqvAQyvyYzkrdi23dmgpeEzG+nlcex\nGOFbpulLIPNbYjr8teV9U6Z4CWR+c6yebDe8bmR5qfC870LmV41cbANZXjdywFR4CWR+84gGZ08n\nax9xvB/c8StfOHhEF7zQPmx/gBADMBOLSUkkZ3v99df90qVLfXt7u//Rz3/tB+57rD/++OM7r4f2\nMWvWrPBRtJbI62ZjpnihfVjbbUbLUA3WoXYjo0uFl0Dmt9Xsy1TkvrsiU7wEMr9tRHs4LwKqMsFL\nQua30tL4PHJJz519zhlQV3K7K3q4Zw9rmO2ZknfFycqMoM2jPVP7hBcnK9trr73CCPc1w2tJxEuU\n0iFlnTODJGRlHWi0dzs9kJWlwouNpBLXhZPmNxVenMyPiM4gGMpu5Gep8JKQ+W2jFzK/VHhxIQrq\nOb2veAlkfiEA7UZ6ILdLJ30xMr82w8wKL+ZVtdzazva+5jeBzK8NzcTnppO+GQ8+7gsGDu2CF9rH\ngAEDQvn+gSigtBsZ3W9+8xs/ceJEP3DgQD+kbJgv3uMgv+eee3ZrH+aEEALzwiyrJVO8HtpHmPU2\nZ4qXQOYX2kdob02Z4iWQ+XnrrBaF8kiFN2j/E/zGjRu7lXvsG8IW4Emf6wF7TkC6k9stAKb2cO8m\nYFsOyOha0WbWTwI35QCvDY2WtwBDsiQX24WWjF6yRl6fJV5gS5xjuK9liRciPQ9AFOLLs8TzwDv6\nlwuBjizx2oB70Jr/S2RJXoiC3l61v63Ab7PEa7d6shaNTudkiTfbjONiFMeTrf7mof0VNqI9lRdl\nSW63jSja+SFgTZZkb+G40cpkSLZkdIZ3IZqhzs8Sr9byfIc3u5WD/DZafh1QkyW53WzgeLMtC9Es\ncH/fi41O93i3yO0eRptnJ5PvocjGHiVNMroORP0KiubMFg8UBHWf97422cUM8e5FDTxQa2SDF6K1\nz0DcUNOyxBsA4L1fZL/vzkF+x9jfe3vDygAvLj5LPIdmL0uQId8/GU6G6XsElW81UeRrX/HKUTTs\n24j3qzJLvL0RbcpW1E56lTTI2Qai5badKJr41izx5qI6fRnwo1TtLQ28EIj1LeBs732v3Fdp4IUR\nfVqSBl5Y6z8S1b1e63OaMhe5PR+IViIezwHmu89llHDfhURcPLuI1uxbiXh9AhdLfCQR1jB3Eq3v\nxT+uBH6QwE8TpnztRJuPtBFxtbSh3jV8AF9rz2wi4g7pIOIUmY1GW22xd4d0taF1bp9w7UHL78aE\nvLSj0cdsIn6UsAYfqDqaiXh76uw9f0eh7w2xfKy1/5st7YFbKHDxhPfFuWlmE42ovk3EpxLyW2//\nh3wvJOIcCnkOXEGz0YgsrONeGCu/hgS8+bH3VMUww+84V0wH8CLifgll2h4rj1AWIU0hXeHDe0fC\nuTY0u1oWOxd/LpRjWF5qoHv9Cyy+tXZuUywNzUT1r4Oe61+cHynUv+1E9SqUa3wZLnBarUKd2dKE\n9Iao3+2oHQSOqyvt3qrYuQpU16pRPQn7OdTG8BqJuHs6UL1pRfs/VFt5b7ZrgdNog+GusfyGPLYT\ncXgFvqAt9n+oF6tQnQk6DTQ4oQ6G54INCJ1csAP1RHVknuEttmdCe4jzda2ga71Zb/oL7bDJdFmN\nPLWCTQq6WWT3xnGD80i76TTePirs/1cRFUbQVVh2bI6Ve9jEqZaozgcbt42I62tLEttaQcIeNP/M\nGQKm6BNT3PN7pKSBiI9jMFLyA4iv5VDEDRJ60IeJSLSWId6SwCsS1sCrEK9KGxr5XoMaf+D8ALn1\nFSJOlVCRFyP+oqGIz6gdcRf93X4H7pcbUFTl8USh8UHuRQZ1E1GFCGk/E41YFhAZPyz/41GnWWBH\n4MO5m4jE7UVUKUN+Dybiqdllf0ejRthu+R6DeH/G2jOhUX/f7qu2vBVZmvqbDnaL5bcA2OTFGXOc\nlU9BDG8D4sxZhziTAg9SAfroHSp6oaVvqz13CAqWakccMKHM2i2v2w0j+KcfiFw7O4i4inZDo9P/\nROvuN5jusfQFg9GCGmYoj3Y0qzqXyJCHgcOXkBH7LyIisyCByDAMYgLnDqgBvmp4gZ8m8OnMQYbv\nRcQ1U2T6COURNhR63rDC+ncDEaFiK3LNfAQt/5xteSuxZwotbdXA/6Jvd6WWloFo8DAQlW+ot7tZ\nmpqBC4i+MQyyezYTtY3guTPE0n9JTKdBB6X2/PNYXIbpw9kROsnXiNriCMvXQLtnAaqToJlS0Os8\nVP/q7Fow4vWW56WW1oFEI/oplvY97Zky9LH+k4hyYxGqI45ILjD91RK1Q29puQ6VU5hRrrR8tqK6\nvJ9d60fEkXUxah/rEYfYOFTfDkP1o87070yflyBPpoVEnGeD7JkCe//TaGWlBbXxeufcaOecA3DO\nHW73biNbydEMIWz8MBEp/UrgyiT37Y8KM7BHttvfg1Fl8sjIfAwV9kRkKML2ghvRtLoB9bbr7doj\nqMMIBmAZ0e5Y96AlqmAYOojYFpvQTlphhBt67sWoMMNyT2Bb9Jael+38YqLRfYulLRDdvU40YmxG\njW+z4T9EV7bUPxGN5EM6Q17CiCvuZvsOsd3i7J4wkvq/2L3hqCGaWcSvBffJbXQdNQWDFB/Fx/E2\nEM2K4tfCSKY2lvdwLXwoXhN7NoyQArttfPS+JqajeJpCGbbQPV0rEu4Po69molF8GOkF9st2us6a\nwsiugYilM6QrsH+GdGwn2qayJfburcClRGvvNbE0dRCx99agjiPkbWGsvHcSfXgNS4Yh/SHvf7Dn\nqtEgYQtRXd1k6dhFNNIM5RnaXlyvYQbZTjSLCLoOW9+uRgZqSQJGg51bbeUWr2MBPwwg4jOgMDNa\nbOURL4NmIk/CpthzibP9JtQeamP3BNw/m/520LUuxY9kM8L4tSV0r2ceDQSX0r2NbLP0bIulMZRJ\nnekoeIGdQuTSG+rTz9DM/zw023nMdBPIF7ehTvMi09sCZGuOzoUtfzfI7X6JCuNs7/3ihPsuRMsK\nlJSUHDp27FjWrVtHeXk5xcXFrFmzhilTpuCco7WtHXwHVVVVTJw4ka1bt9La2sqYMWPo6Ohg5cqV\nW1E4/XsCLy7Lly8Pvft3MVbCTPE2bdpEQUEBI0eODHinoc6tsi94K1euZOzYsZSUlAS8/0Uj7Z3J\n8AYMGMC6deuYNGkShYWF3fDWrVvHkCFDGDp0aMD7iuV3e1/wgtTX11NVVYUXmV+P9SVdvFh5/Bx9\nRB+SI7yX0WiytK94ra2tVFVV0dLS0o4M8HV9zW+8Pi9fvrzNe98vXf11eGjzUER3vFGjRrFy5cqt\n3vuRcbzi4uJDJ0yYQEVFRVK8ppZWtm2u7qY7a2vb0cy8s31kihfaR3l5eWi758XbR6Z4oX0MGDAg\n4P0k3j4S8QYPHtyZp9a2dqqqKmlva2OvvfYC6GwfQ4YMCXjfibePVHgbNlRROmgQ5eVd97qpq6tj\nw4YNHd77xNWK3EguehXrVFKS28WPRG6VOJfH2LFj/RHHfsjPmTMnKTfNHnvs4YG12eDFuWmGDh0a\nRtE9ctOkwosLaXDTpMLLlJsmFV6cuwmNMLpw0yTi3X///f6AAw7wl1xyiS8vL/djxk3wd955Z5+5\nm1LhBUmXuyldvFj6euVuSgcvE+6mdPAy4W5KhZcpd1Mcb1DpYN9vxB5d8EL72H///eOz2M72kcjF\nk4hXNHysHz9+fLeyMC+ZqsT2kSleaB+///3vPZp9d2kfmeJlyt0Ul2tuu8vjnJ8yZUrnuUy5m7rj\nFfjVq1d3019PVB25OnL2DcF7/xSit17hvb+wbFM0AAAgAElEQVQ+jfu5/vrrOffcc9l3330ZMWIE\nb731FmvXrmXOnDk0NTVyySWXdN5/5plnsmTJErZs2cJll10GMMY59/m+4lVWVnL//ffz05/+lN/9\n7negjuy3fcVLkBLk7tfn/E6ZMoXx48czduxY0BrrfmgE2Se8E088kRtvvDGE1u9C30UG9oRXWVnJ\nokWLKCsrY8OGDXz23Au57LLLaG7WVgd1dXWUlZWFx7eiKXtpWNfMFC9BunllZYnXH83YfpUN3tln\nnx3oFxahAMFNfcV77LHHaG9v5/TTT4ckOwhmihdvH2ip4ere2kcc7/t33Mvwj361C15oHxdddBFo\nKWsNCe3jnHPO6RFv6DHnUFVVxZIlS5KVRzlJ2kcmeKF9nHHGGaDvU93aRyZ4oX00NDSA1vq7tY84\nXlzefOVF+o/eu5NqH7q1D0jSPnrDGzBuKpMm9bpT6bsi7xkuoziXx6hRozjj3At55ZVXaG+XbYhz\n0xxwwAGg9fgeuWlS4cW5aawgN9ALN00qvCDpctOkwsuUmyYVXpy7iSTcNIl4JSUl9OvXj+9+97v0\n79+fKfsdwJFHHkldXV3n+zLhbkqFl2l9SRcvXe6mDNOXkrupN7y+cDelSl+m3E1xvKKifhRPmNYF\nL7QPM1aeBO6mRC6eRLwBu+/DwIEDee6557rk7e2334Yk7SNTvNA+zL2zW/vIFC9T7qa4vPXqSwzc\n5+gu5zLlbkrEKz3gw93O/0MkV1MN0ucycsCvUnEPTZt+hIdeuYcqUSXoE16cm8bwQlRiWTbpM26a\nrdnmN4GbZjZac/XISyMr/RneD63MkqbvhRde8P369fOtra3ee/G1DBgwoCfuptnIE+b1vuIlcDd1\nTomzwYtxNy3MBV6C/r6APuhljJeEuyl4W1UjR4pcpO87vbWPON6MBx/3A/c7rgteaB+xJY+wM14Z\n8KtELp443vQjj/WFg4Z555wvKyvrkraTTz45afvIFG///ff3jz/+eDx9XdpHX9MXw+vSPnriHnr1\n1Vd9QUGBLxg41BcVFXWWRQJ3U7f20Rte/wED/PhLZnW75v27v2SUe0C5Sb5pjWUxcG3CtV2AT+RW\niXN5bN261R9yxH904QaJc9PceeedwaPlDuuAMsaLc9OYEQpuf74veN534aZpQR4VC9FySk2meAnc\nNN70VoWWKl7PFC/O3UTkXbPL0teeiNfS0uL32msvf9111/nW1lZ/ydU/8qWlpZ3rnQncTcGbZLN1\n1FszxUvC3bQwVh4Zpy+Bu8kTkZU9CbRkipfA3RS8nDo9hzLBS8LdFDyZgidMRnjed+Nu2mRlEfYp\n71af43g/uf8PftQ5N3XBC+3j7rvvDm1jHTHPrEQunlR4oX3EqERuQ8tQHYDPFC+0D9uAJngIdnoB\nZYqXwN3k0TLea9gey4l4QS644AJ/2DHHd4ssTuBuiqetDWjLFC+Bu6kDeViuQC6/w3xkY68xOzHf\njpN8JvY7k5t7BerKZXS1neuH/KCPtN9HBsUkcqvEuTxGjx7t9556YBdukDg3zfjx4xPdxZozxfM+\n4qYpLS0NBbUtFFpf8GLcNHOQK+KHg0HLFC+BmyYY8K8bXjdupFR4ce4ma0DV1mklTZ/33i9atMgf\neeSRfuDAgX5Y+Qg/ZsyY3rib2tHo1tMDd05veD1w06zrK14Cd1M4/mR/k3JV9YaXwN0UXAUr+5q+\nuFjjrkReeknbRyq8BO6mDrQEemY6eP0HDPCFg0d2S9+dd94Z2D/b0Br45w2vNZGLJx283/zmN2FG\nNMfqYAiM68ZllAovtA9zCAl7s18R8pspXhLuptdQnEuPeI2Njb6srMx//Ypr/W5nXOv33nvvnrib\nOpBjyJZ08RI7hB7ax0uW5xsTOoTL+mzHc9QZJOUyQh9l5gJH2H2PAAcBFTngHtpplfQs7+VFkSVe\nuxnICuCnOeCmudKOccDKLPEWmgHvjO3IEm8FGp09aY38T1niNaHR4+How21jDvAakJ/2emB1lngV\nyBc8V/prRoOHRYgmoiXb+uKjtnRhDvRXaTp0lveqLLmM4vW5Avh+ltw+K5An3kzL88Qs8WYbjjOb\n0JglXrOlb4zhrs8BN9KlaOY2CWjKksuoKZa2McAyn6MO4d3iMvod4sXfDDzvvX/DOXcqmsosQpuT\n9yhpcsm0o+WO+5xzLTnAa0GdVSFa78sWrxL5WC/vDSsDvFYUgZxSMuAKOho18kOzxOuPlgPHo0jv\nAVnihcjeYjT62yNLvOGog8mV/vqjOI5JaNbRLxlOBnj7OOeOtf9/SPb6a0EG7VQ0khzVG14aXDwQ\n1edxiIMoG7wQ8X2Gc64eRfZmg1dqf49F7aTX8kiTe2iQ936j/W7LEq8dRVGPRCso2drdAejDN2ig\nmFi+FzvnFjrn7nXODcsIua89SfygBy4j9KHnRURD8AbwDTTaaEYjysANMp+IF2QzXflo2lHPupMo\n2nQzUYTwGjQLCdGCFURLGGuIooPjka7e/q8hilxuR6PReJRwOxF3TOBcWZiQvjAC2BnD22U6WEi0\nrt6KjFJ4vsqutRBx/ITpZQVRlPR809V602XQU73dt4iID2iF6WUVsfVoojXl2pje30Fh9kuI6BKa\niCKKw3s2md5DXkOEbaNdC8t2rcjrZnbsvQ2x6yHN7UT8VSHPIVJ5bez+ZvS9ZAFdeYZW0pVDZicR\nn1J4brU9s8N035ZEf2Fj+xBRXWHPL6ErF1QjEX/Rrlj+NiAuqHdi716HlgU2WrmFOhp4mgJXVUjT\nLqJ6sR7N2IIuQr2fbZirrAxbibi3Qj0N0dNbTbdNKGJ+sWFXE333CFG/gaMncE/F6/M6S0dYQq1B\n9flFuz+kvwHVuXY08AlRvYlr5hVE0enrTedtpsNAdBfqSGDi7bBrcW6kEHUe8hCi7NtRwOHLRFHK\noQ43Wv5nEzEBJEblx21NS+zvhTE9eSI+oVD+geojjtdg50Oe6u3cc6itBm6oMNIPvFRt9sx201Go\nn4FFYCtyfKkg+oYwxOzvjpgtHoUGUwWIvuTef8YMAZJwGXnva1AlOhWNpi4F9kEjrBI0Evy7Fzvh\nTxCPUTkRdw729zLUCArtdxmqfAXAX+yZcO+tqAGBeuTSGF4REXlcqEQOGIYK+PPIP7o+lo1X7JlH\ngb3sCFwtcSki4pkpQR3iDsRLMjp2vRZVhJF29LPrAc/bMyWW7gPt/7GogSyw5zoQN00z+rD0Epo+\n9rd7iwyv3vQ3i4hIbQBa6uhHZGix32+jCh3KYxARDxPISF2LyiJ+rQhxsiwnquwQEdnNNPwCorJ3\nlp6bib5rBLxC5JwQiMq83f+iF8/ST+yewaa/ILVo9L7DrofR0zC0JLjA7g8cUZeixnY/Ub0Zb9ca\nLP3/Z+fvQvU44L1KxIrZjsrjRlROc1FHFSJKC5Dnzyrgx0SdfKHpoh7NhJ4n4gEajjhs5iKvphl2\n/26G+WeiDqcAtYMyVAeGA9ORp0zoOJzpcxvi9wl8RwWGE+hfbkKGJ/jhOxSJHjpULB21aEbYhLx7\ndrN7BxDRa7QQcfSAyiNwcT2NjFzgqhqAymEXqhc3mF6c/d2JOqs3UX0psXIA1Z0j7B6H6lqR/T3d\n6swPiSS883DE9bWeyDYEW7E7qi/BvvQzne9ADKMvxPQTpD8RoSGmw2LUNqYS8UaF/AZb0oJodDah\n9tvf9DfE7h2MBtVjiWzEfzjnxqD2CoD3fpP3vt1734G40Q4nA8lVh1CFlLUdZXYyUOWcK0GKmOe9\n3w0RTP3M7t8DFco5zrnDUA/6NKpgfyHqOdvRVLCciONlBeJF6iD6ZhG4Q36IKkk7muLeSsQlsw4Z\n/m2ooVQQjSJ3QzOdzeijVxjRfYHII2cvZOhnxdLnUUH+EDWMnahiDUTkdjejiv03VKkPQrOOT6HO\nah0yFGFUst7S/IphfwA14hA4tx51vGMtfQeixvgACgysBE6gK9HeOEvPWNP9ajteIvJIqLR3HAK0\ne+8fMp2MMtz4bOjDyLCN8t6XEhn/h5GxfQ24xXS7J6rwv0RGvMHyETplgGNMNwcSkZaBRuBjrDx+\navr+gnNuIqL7XYuM1EmxvNZbfgeZfo9Hs4qBqJGHD3sbUPnfhhrbctPjc6aTNqLG/BHT9adR+TXa\n+QtRp7kV1akqVA/GIZqMfZHBC5xHwQCPISrvJtQO90Iz6Eoij54diK9oP8O/1t59iuXhE6bb2wzv\nbdSGmlFn8CXDmI24b9oQb9hbqCN8ELlCBq6fva18TkXGLnAYfQXVixHIYN1nZbQDUbIvRXV9iN2/\niqiTegrV599Y+dSY3p1dr7L8gpYvLyYylA9aebXbM+VoMPQO0UrBOtQZPIXa6FOWl/nIaBcCE51z\nk1EdaiFiPMXKqAXZjGIrywbT4zDDnkU0Gz7N7jnf8v40Ed8a9u4BqJ1Wo7p7PWo3K9Gsd6fd96g9\nswC1mSMR/cZcVPeLUH1stTQfhWzBfahufw0RNnauT1kHEeR0NCtJW94NLqP7kOEJm5HM8t5fZ/dd\nizJaXFJS4uLcKnEuDxAXSHFxMQ0NDT3xoYQGVQy4kpIS+orX2tpKoH7AOsm+4MW4aaDrvqdtJSUl\ngzPBS+CmgWiJYAPQr6SkZPcs8Yilr7CkpKSgr/pLwGsDikpKSgozwUvgbgIZ+/DtwGdaXxK4m8Jt\nYXlqaGL5FhUVsWHDhk4umta2dtZWrKGgoIA999wzkbsJZASGB+BM8YIE7ia0/NE5ku4rnpVHBzK8\nA7HvEYn6i+O1trXT5qFqbXc8a2thGWgAah++uLi4IM7Fk4jX1NLKxsp1jB8/npISTZxbW1uprKyk\ntbU1LO0FFmKKi4vJBC+Ba6kJ2YFWbOaZJR7ItoTvEUV9yW9Mf21ELKre9FfYG16bhw3rKth9990p\nKSlJ5G5qR/Z1HPr+shH4qfd+tnPuAdTpB2eKr8S+haSWXHxDsE4lKy6juFRUVHjnCvxf//rXpNw0\nf/nLX8JaXY/cNOngJXDTtNILN006eJlw06TCy5SbJhVenLsJGe4u3DTZlAdpcDelwsuUuykVXqbc\nTW1tbf64447zTzzxhG9ra/NfvuQKP3r0aL/ffvt57zPnbkqFFyRd7qZ08WLp65W7KY5326//4Eee\nflU3vIceeii+RWoX7qZELp5EvOEf/7ovKirydXV1nXgnn3yyv+666zxamurSPjLFC+3D2m9jYvvI\nFC9T7qZ08ut9+txNycpj7NixnXipuJtydeQWzOiv07l3+vTpfvr06f7CCy/sVqGvu+46P3nf/f2a\nNWuSGqAvfelLoUMYkAu8Bx54ICwhjO4r3qOPPupPPPFE76WIbhU+U7zFixf7qqqqEEi2xEYBn+8r\n3pQpU3xZWZl/8803PZrGzgD+lgv9odHjednkt6amxp911lmdIygzaMP7infOOef4T3/60762ttaj\npZRVxCI8k+HNnDnTDxo0yBcWFvp+/fv7e+65pxOvoKCgsyNCSzBh3wbXF7wgiZHZfU1fQnlsQMsQ\nvbaPgFdQUOBd0YCkeGbQ3kbr7J3tY8qUKf7DH/5wL3j9/YQJEzqvhfbxYrQncJf2kSleaB8WSNat\nfWSKF9rHz3/+86TtI1O8BP11ax+p8Qb4J5+MXE9TtY9cHf+UDgEoGDZsmD/iiCN8S0tLNyVOnjzZ\nn33BxUkN0O233+5Hjx7tgQW5wIsV2irg0b7g1dXV+cmTJ/vly5d777t3CNmmzwzQFcAf+op34IEH\n+vPOOy+OV24Vqyyb9L3yyithxlGaTX7POeccf+qpp/pt27aFBvk94I2+4m3bts2fffbZIRCqEfgB\nsKonvOeff94PHz7cv/XWW769vd1/69qb/ciRI/2ee+7pvdcMITbjmI2+7ezqK16QZB1CNni33357\nWI4Zly7ebb9+1I/+0q1J8WJxCJ+Lt4/Bgwf7T3ziEz3ijfz0d31RUZGfN29el/aRrEPIFK+H9HVp\nH5nihfYRw+vSPvqavhkzZiRtH6nwRn/pVj969OhOvNA+/vjHP3ZrH7k8ctkZ5ITL6NVXX/UDBw70\nBx12lB89enQXfpDATfPQQw95ugaj9AkvoVKtphdumt7w+sJNk0n6SIObJhVenLuJJNw0fU1fptxN\nPeH1lbspA/31yt108803+9NOO63zd7bcTanwMuVuSgcvE+6mOF4yLqOQvpiB7MLdlMjFE8eLcwUN\nGTLEX3311b6oqMiPGjUqLIF2ax+Z4PXAPdSlfWSK94UvfCEj7qZ005cud1NieezxnSf9qaee6m++\n+Wbvve+Vu8mnsM2ZHDntXZK+oDu30WyScPEEueCCC/wXv/hFP+PBx7ucj3PTmFIqiPx3u3G/pMKL\nc9P86le/CtOwrcgbKKP0JeGmWYWm7XcbbjcumVTpS+CmCbERlX1Jn/dduZtshNFJC9GX9Hnfhbsp\njEqXWfqWZIqXwN0U97FvBxoyxUvgbloTS2PS/L700kt+xIgRnSOyb//gVj98+HD/7LPPeu+TcjcF\nP/q/AfMyxUvgbop7rCWtL6nwEribdhKjSSAJd1Mcb8aDj/sx5/20C15oH9bWwh7MnfsbJ3Lx9IYX\nbx9/+MMf4sseIbZiXiZ43kftw74nhqMR89/PFC+BuynsbtbZPjLF874Ld1MT8jAKMUUZ4yVwNzWh\nLYerfGRnK9DS3nxike+ZHrk0/HEuoyti5x02XUKudZ4k3EPeR1weL7zwgv/qt6/uwg0S56YpLi4O\nFWBzaJyZ4sW5aWxU34rctPqUvrigmdIWtLbpScI9lAovgZumGX2YOiWb9AXuJjM+ryE/ek8PXEap\n8GLcTe9Y+p4OBihTvATuphDU1trX8k3gbgqBaX8JjTwZ3u233+732msvX1pa6geXDfUjRozoibvJ\nIy+jj/VWHr3h9cBNsz78nyleAndT2FqyJh28AcXFvmDQsC54oX1YWwvbdYb8JuXi6Q0viOU7bIM5\nP+gvU7zQPmK2IFB+e3rgWkqVvhh3U+igm7LJb4y7aY3Zv8ND/UuFV1Q2yt9yyy2dekvC3VSH0fbE\nOoSsZwu56gySchklua+T2yhH3EPPoNHf+BzgVSI/6D8CH82SS2YV8l0OfuPDc8iNlIv0zUb++U9a\nh70hB3jPoCWUXKSvGXkaBXe9MVniVaPR7bWW3+os8VpQPMqVaC33RznQ3zIUn+CA+lxwI6E4k5ZU\n+U2Di6cS44EyI1mdA66gPwDzDTNbLqMQnNfJ75MlXlOsDu4N+Czxllu6phjuriy5jDrtgc9xh/Bu\ncRk9jIJbAHDOFTrn5hPjNuoNLAPuoWNR4NTbOcLbFwXgZJu+ZlSR9kAd5LYcpK8yh+kLcjSa0ZUm\nu5gB3oEoULAiR+krQrvEFaBAntpkOBngbbA87kJ1ZnMynAzwCg1rD+RuPSYZTgZ4U9Ds+YsomLIp\nGU4GePHydERRuUklDS6eFiIeKE8s/qKPeP2RfRhv+zT3KmngBaqGL9rvXqNz0+Ra6kDfr/6Ug/SN\nMa6qr6FI9F7bW5pSSVduLg+84Jybk45Oe5RsexTrnZJyGSW57xuocS9CI+fADbMCrXuG/0OPH0Lf\nGw17FzZCQb7vYW14J1EU7QY0sgwcM4FrJaxJe/QBOayzBq6VXUTRhHGuoPlo+WKlXV9LFLQS0tls\n5xYT8a0sQuvLu1DQXuBMWW348+yZarpy5wQunmY0LZxtGF/uJX1h7XSR5Tekr8NwGk1f7aaXXSiq\n9GLTa+CsqSDiLwrvqSTi4gn6qyFim601vEftXfH0Vdm5oKtwLSyNhDIOHEo7iNZum61sNhPx2wRO\npVa7tomofsT5aNqJOHBWEdWb3yfghfwGrqDZRLxO8TX9Bnt3yG8IENxheJti5dESeybw7IT8hzoQ\nqBlC3akznQYurV2GsdHeNx/VrfBMbQwznve3iaJ4h6A2GdrQLktfyGulnWtEs6c4d1PgnJpPRDN/\nB4rMrkP1OdSrJhSpPNvSu5NoqaUD1cdmosj0UN9uN/01EFFmrLS0Bf0FXqCO2NEUw6pP0Osqe19d\nDCNwE8V5jOJl+07sdwtq36GOzaIrV9Vc09ku0011DDO0tYBVT/TNqt7uXRzTZ4flfzURL1gtEVNt\nTSztqy1vtXbvFhJsLDDW/u6GVmiO+2fOECAJl1ES+YXdNxb16IMQb8xgFBHYQLRMUIAUEnr/QCLV\ngcLXf4qoMOrsAPXslUh5n0MVL3DmeFQp2lEo/fP2Poci/lqRAX+ZrlxBVyJul1+gNehBRDxBJ6MC\nWoUq8wAibppKu38DEXulQ3tOfwDRWWDP727pW47Wpq8l4l8K3E8/6iV9N6KIxXeIeFhAa/qbicL6\nC0zfA+wd95nuQ9j9KLQEUoxGrb+w8gj1ZIP9vdj093f7XYSoOJpj6fNo5Pxd03uj5XmoXQdRL4TO\nrMmuBV6YfnZuGaIQCA1vMzI+Sy39gRU18N1gOnza/h+LRmQDUHkVAxdYnrdaWfQHJtv/JxlWHeIu\n8ojGYXEsvw2oA7wJcTRd670fhAzra8gw3oVmyjuJaEsCWVszWq7rsHuWoro6DDX2kyx9ofM6BblB\nbkSdWo3hHYEMGECt936a936qXdvb7m8DtnvvB1v6QlDTeuAwVHd3ovobOont6IPlwfbMS/b7LtP9\n2ajzCfVuJuowKlCZb7M81SPKkkVEBJPrvPfllq8S02Fwvpjivd+LiLfsJkTdAKpvLYgGItiGQEcS\n+ICG2BEoUZqJ+M9OtPTU2/NhFrYbsgfE8r8eGet9iChRQB3iHKI6ut3+elTvtyGj3UxUJz1qf7XI\nzhSZLhzqJJ6xczNQ2Y5Hdf+/kWEH7WvxBVT/+nvvRxpWVUiY977K/m4GHiNDDqMg7waXEcQS65wb\n6ZwbaudL0TTsW6gyrEKF+r9EXB5h8/JgrJuJPIraEVvoWrQJ9ieI2AG3o0Z8MKrAF6HK8SpqdA4V\nfj1qYEWWzt8SjZjn0ZUrqBoV0unIaJyEuEbCx+wfEnEcDUXeVIHV8XFkVIchLpgtqEF80Lh4jjGd\nfQl9rAMtQTSjjq4WVcpDTBfDEb9RSN+BqMHPQhw7+xjOVUTeICFd0608Gs0wLEbG4DbT9dVE7sKz\nLK3fsPzNRI2vABnyVrTsMs903UDUgb9l6TvD3ldtGMOtDH+Ips3nW/ruQA0nUF5sxPaeJTIKBci4\n/NzSN8Ly8xcr49mmlxbTWQtRp4S9ay76vrHS0n0cXZehNlu5DEKbELXbuz9g/x+HSM2mWbqc6fpN\n0/0rzrmwo/oQS+NSK6ctyEgWWFkMRMZtsOluGqoH5ajTuhTVzzALqDe9BvK5WvsbiAH3sDQ2Ajjn\n9jSc1ZYeD5Q55wagOnwgWpb6nKUTwxllui9D7aXUOTfV8rMv8ITppx8RId6JqA0NQG6f55ve6lH7\n6Yfa/DTgCUtLuXNuOJrxrjY9Bkbijxuv2e6o/nwMtSMsnw5xCe1EhIOPmg63ErHw/j808OhA9a0N\nWO29f9vKYjURSypWflOJDH8Z6qwGIZ6gB4hmx6stv39Dker3EA2m+iMb9HlL33IissTVhvcjK6ez\nDS/Mxj1q60fZO5uAj1uZhIHVW6bHZudcfyu/JwCcc4Occ4PD/6a3jDiMgrwbXEa/RIVwtvd+sXPu\nQMRgGBr3LO/9dSNGjPAjR45k7dq1jBkzhv79+7Ny5UoOOuggQBwvK1asoLCwkGnTpjF//nymTJnC\noEGDqK+vZ+nSpYE8qw2gvLy8Xxyvvr6ejo4OJkyYAEBNTQ2rVq2if//+TJs2jaVLl1JeXs7IkSMD\nXhiBLgJmlJeX35MJXpDa2lpWrFgBaiSvIuNVUF5e7vqCBzBnzhxQRSpCs4A+p8/wwlR+ouW5KFP9\nATQ3N7Ny5UqamjqXvOtQJ7+6vLz89EzwNmzYQHNzM5MmTQr5DS7F7cC68vLy/TLBC5xShteByPb+\nCzVcV15e3n/kyJEMGjQoaZ2ur6+nqKiIlStXsv/++ycrj1BfPg7sXV5efmeWeF0k1M0+4oUlj/7p\n4NXX1zNo0KDO8ozjWdtoRB5zuyMj5crKypg8eXI3rFRpMzxQ2bYgo729rKzszizxIKKLriwrK5ue\nBZ5HncwwbIaRZX7bUWfwSTSo2a+srGzP3vB6K/elS5d2INLMA733J9kg4DG7pQj4jff++qQAKSQn\nHQKAc+4kNOIbC1yXLEH2seNCgFGjRh168803c/755zNz5kzGjh3LN7/5Tc466yyOOOIInnnuBe6d\neRelpaXcf//9nHrqqdxwww3st99+1NXV8clPfrIKFdiSZHh//vOfaWpq4tJLLwVg6YrV/NeF57Pb\nbrvxu9/9jlNOOYWbbrqJqVOnBrwlyEtqUV/wgsyZM4fLLrus3Xtf1Ft+08UDOOGEE9aiEcRiNFrr\nE94LL7zAT37yExoaGkAV/udo1tMnvCuvvJKTTjqJq6++ehNq4Nf1Nb/33XcfjzzyCAUFBezatasV\nzVDa+4r3zW9+k0mTJrFs2TIWL17cgabtf0Pfuxg1atShM2fOpLQ0+fe9zdt30tHSyJVXXsmvfvWr\nbuXhvZ+YWL7vV7zN23eyqRFGFnbHs7Yxx3v/gTjeyJEjD501a1aPaXtn3Sbu/+mPuqXtXcJbjpZT\nXstV+tDM4H/QjGHHyJEjR/8j87upEaaNLet2LY6X9OFsJRcflWMfNiaSBZdRb1wtcW6aBx98sNM1\nrCe8559/3o8YMcIvWLDANzQ0+KNP+Jh3zvlx48Z577ty08T4Wjw9cNOkwguSLjdNunheik3JTZMh\nXjdumkzxMuVuSoWXKXdTKrxU3E29uWF6L9e/3qhEEsv3/YwX3ByT4cW3qIwfqdwmx341Oc9SHi89\nvFRup4l4uTpy+VE5bXHOFaxZs4Z+/fpxxx13ABq9Xn755bz00ku0tLTw31ddzxVXXEFjYyMAM2bM\noLi4mClTpnDVVVeB1usqe8L7yEc+wtb++RIAACAASURBVDXXXMNnPvMZJk6cyPCRu1FaWkpRkb63\nlpaWUlvbxZuxEKjz3vu+4GWa33Tx7P5y4GTvfXO2eCbNaLZxZ1/w6uvrufzyy5kxY0bO8jt16lR2\n3313CgsLIQoy+mxf8UpKSjj99NM57LDDQB39tcDRsbX+vOQlL4mSq56FLLmM0uV+ifWSG+mFmyZR\nph50qHfO+cLCwm7cNDEqjB65aVLhZcpNkw5eJtw06abPC6QbN00meNlwN2WQvrS4m3rC6427yacx\nAp9+5LG9ciP5DEf072W8wGWUDK8vI9zA7dMLb1ge799ghtCIRtnLvPfjvPf3ADjnLnbO1Tjn2pxz\nW9C69cGTJ09mwIBoL/HDDjuMV199lfnz5wPw4VNOZ9CgQTz99NNUVlZy/PHHs23bNtrb23njjTdA\nO3OdiTwLPpOI19TUxKJFi/Des27dOlqam7jiiitoa2ujsrKSyy67jFtvvZWqqiq2bNkC8mI4zPBO\nzxTvP//zP2lqaqK1tRWg0Dl3knNuO/rweF6meMXFxfzP//wPzz//PMBk59w9zrm30ej+3EzxvPds\n3twZjzUBsX8egAx5RunbsGEDlZWVzJ8/P5RXcJENcRNnZZq+ESNGsGPHjtDpHYDcCz+FPDXOzBTv\n+uuv57HHHgvpc2hZC6DGORffEjSpnHfRt9i4cWPnpi7nn39+/PIBzrk651yTc261c67XQLw08KY7\n5xqcc+3OuWrn3DtZ4h3inGu29K10zj2fCm/kpy5Plb5W55y3dC7oCSekbdxFD/SEBXCoc26Lc67W\nMHv9kJkG3lTn3GyzM+2pyjcNvIOcc9c55xYZXi7St9I5t8DqTbYfbg9wzl0RP+Gcq3DOve2cm++c\nm93TgyklVz0L2re3AhmsK+zcCciH+8PI22YFfeR+iXPT7LXXXh4Zn+Be2Y3rZseOHX7atGmdXEWH\nHPEffsqUKUm5aWyTjVbkHuaB1kzxUnDTdOP2SYWXhJumjmh/4W76S4UX525CndTbREFU3biWUuHF\nBRHlVSJXyz6lL4G7qQmt+W/KJn0x7qbglvkicBZQm2oEnoKrqgK5Y/ZHsQA1WeLNRXELK1GU9+lZ\n4i1G25VuQ53rFalmCLudcW1SvBjVxGNosAfw9VQj3BR4YQOaDkRbfWOWeIuQQ0Md2u51apZ4Tchr\n8nLkzbM5S7w1prewne+vs5whzCGBHoj3KJfRsVYZFyDf3lnAR+yeidgHZ2BiDriHWoClhndYjvCe\nN7xsuXN2or2kj8f4grLEm2PpCnw3P84Srx75UHcgN7XyLPHmWfoC39JV2eKhQcQm4GPe977kkQZe\npRncJy2di3PBFWRYVwFLcsBl9GkzaHOB03LEjfSM1cNe63Oa3ENbgL9Ynn+WJbdPneFtRxHL2XIP\nrYt1CHsAV2aJt8Pax5XAeUBVjriMNlhd3PlvwWVENCIOXEZ7A8falPphFIyWUtLk4umHAn0gCgDJ\nFm+c/V+dJd5AIj6fo1FhZYO3t3GhjPLaH/WYLPFqiTY9b0aGNxu8yZa+SuTr/70s8Q5E5GdDgVuc\nc08nBUofrz+qI0c75xaigL1s8zvPOVdped2RJR4oKncQqoOXZIkXuJGOQPWw1/qcBhePQ4F0x1me\nP5gl3jY0w6pFAVbPZokXIncb0Sz/uizxBqLviWOTI2SMN8Y59wki6o4h6eCmkHeFyyhXHcJY1BkE\nCYktQh/zjkSBGeOdcw59EO5RqqqqGDp8ROfvcePGhY3I49IBDHHOfTiHeKOdcx/21uVmgVftva9F\no72PkYLMKg28fugDa5ApWeINQjEXz6CI33VZ4lWjDqYYRU92dcbOHO9tS99zyFDeniUeiPdqAvA7\nFF2eDd4gZBT/gJYrenU1SzN909HMcjHwSJZ4gc5iM3BYqvpcs2M7RUNGdv5OgleLjOxkZMwfyBIv\ndKBVKDL84CzxtqIAwcUoYr9Xsss08NaTgmAwQ7xaFPE/Ds1gUn5z6oMc40U18gng68654/oCkqtI\n5c+ij4DlyPhdgEYnk4EbvfcvGl3DCiKSrsCTElg8ByJaisGo4Qa+kEIiQq/S2N/AfVSDpp7TkGEr\nJ4qKXoToKsYig19ofwMZ3jB7R1g6CWRjK2N4W2NZ3R19zA5EYBBx6JSitdFC5GVU6py73PTSZul1\nqLJOs7RNtXc6os65zfI0mogPZh+0bngAapB7xnQzl4hWN5DljbY8e8Nusvf0I6JlCHxABxnWoURk\ncmVE5GDr7f2lRBwsgaIkUB60oEruiTqrUWjEVoGYUJdbXqahqfM4Ip6ZDUQzvl2Ir2Wwc+5eROOw\njoh+I8xmQpmOQQZwBBGp2nDTR4npe1/gGjRjPcPuKUV1z6F60EZU13aZDnbau1oMv9h+F6BBzpmI\n16nR8DYQ8TiF8NmQ3xGWnjbTaz/UsSxB9X1vtFQWCP/G2XMbLf39UF0sNj012jvG2jtriEbdQ62M\n5pru4+0j1Ocx9p5RpoMaO7+LiGrDo5nVWEvjSabHdkvfAsvPvpa3Taa3gfaetag+DLY0DDXcQBvh\n7f8WNCvsMB2MtvyWoXpzgF1bjup+g+luJ2rvu9CS9XB7zwFoVhg4mg4yXW619xSiTqnZ8h/SFjiR\nalBk+2GIOiYMbkP9252IbmW8/d1l+Z1OxK3WYritqH38h717ExEfWQ2qG4HzLGCsRbZrIKojzt47\n2HR2K4D3/kckiHPuGuRCf0vitVSSSy6jnYiHAyIuo8fRh2WASfb3ENSLXYwUciaq3O127QMowx1E\nnDdNKArxT6hStyLe/f7AV9Go9MeowBfb/Wu8ovl+gQxRCzLCLYbzuqVvATIas1AFORO5s96CKspZ\nKOT8g0QN+kXDOcHSWmzvfANV6KXG3/R1ZKCfQJxIbZbHx1GFqAa+jSgumlEnugYZl3eAAywPa1DD\nCq69dxMxVF5qadiM/PY/g5ZtICJ4a0bT8l12fwsy9AOtLP6XqPGvRo1oBzIMxxtGi/2/3crgbtOh\nt3zsbfd/HBmOelTZv2tlewDiMdqCXF6b0Ydj0HrtI2jAUA2scM6NRo3n+/ZcpeXzFLTevhM5MrQR\nccPMQIaiBpVxk/f+IMP9KjLiHyRiydyEvvF41CA/aO84y86dY79n2fVaVF/Cd5gGNOMoQB8gQ2DL\nf1u5/Y+942uoQQfmyp2IFG4TmvmtRHXlN3b9Lisrh9pAOxphrjasnchAnYaMzxIriz8Rddi/RstG\nX0X15mdWpqegTnE1Mir1qO4uRuvlqw1jARqgNaO21wY8hDr420wHV6CZw2q7t4KIHXStpbUR1dnX\niXi+xiG+seHI+D1jepttejvHdHoLauegevMR00uJpe93hvcBwywxXS608/dZXmtMH8Msje2onW60\n9L1DVxbVMWgp63sofqUNqPHej0cfrSvRoPcHqB7VAXMt7S8YxiuIEbUdkQBOsfe0WBm02/t/Zvg/\ntDzeYWl6jYgt9zuoQw9lNZb3AZdRjSW2HCnp+0gRYZQ3iIjh74/l5eVf6ytXSwJ/SRtQXV5ePi5L\nvEBfWwW0lZeX75slN80DiKLWI+6cbLhpQJWvGBG6bSovL98jC7wO1BFdj0YluUjfTlS+a4Hy8vLy\n3bPE24g6rHVAaXl5+aRUeD1ds/w2GV6QED8RnzWFEeujiCQtjGRHoc63AjXoQELTgOpLMRq5DSZi\n5g1snsMMP2DsRCP51Ygw7zS0jNCC9LcDdSz90egzTvD4WzRS34KMcdhqtANRN7Sij8jBSaLA7g1E\neVNQZxFmuS2WzqGo3bYQERWuRJ30N4iWeEbY/9VELsv9YjgPWLqa0Og+uH+GWfRAZCdq7NpYolmX\nAz7tvX/MOfcA6hRCOTWhAdD3UQd5JJF402UD6gAHGnbYB2KbvbuE7t8E6lGnPIloRQLLzx/Rd9B+\nsWsdRB1AGV2XCptR2zzI/sZlKdLlZ1A9GITKtwHpMrAkB5LDMHjYSsRsW2T3TUId0u/5V+My6itX\nS4xv5K73It4JJ5zQ4L0flENumjxeGni7DU8ehBzwEs8758KItJs454KLdCFwb7w+9/Tc+xXP/Nav\nRiP/Lnh9TFseL0u8vlzLiWTrphQ/yIDLKBuulmTReu8lPKAil/nN46WH15Mkw9NpLkx2vq/X3q94\n7+W05fHSv5aL45/CZfRvIFtT35LH+2fjeW34klT6cu39ivdeTlseL/1ruZB8h5CXvOQlL3kBctgh\nOOd+iz5s7eOcqzJukaXOuSXOuaOcczfb70XOuW5O2Ily3x0/5qijjmLZsmWMGzeOe+65J/GW6YZ9\nbTrpSxPPO+fWpsP9kgLvEOfcN51zz6TDhZIG3qGWtl3GJbM4S7zpxnOTq/zinFthfDfznXPzs8Q7\nyDm3xjm3M128FBK4fTqcuILmO+detP+9pXutc+5a59xEF/H2hKPJnnnDMOJHk3PuBefcU1Y2HU68\nXa32f6PlI5xvj11rds5tN4wV9nx4Z4tzbpW1o+/b73Btg32zwzn3suGH8txm79zunNvknFtuad9g\n11rs3VV2/tZYukJ+tjjnFjvnXjfsFtPPTic+nl32jHfiI1pkaalw4hPqsGs1Tvw64f5mF3EsLYrp\nJOT3bUtnXYIe1jsFXC2MXWuKXe8wndY557ZankN+vL1jSax8m+x6eMci08WXnHNvWjl04YJyzo22\naysNt930vNE5d6pdW2A63m5pPTJ2fo2L6tF859zVprMPGdbO+Hm7Vuic2+ycqze86bFrx8ee6fJc\nVvJurEOhHdK+bP/3R14MH0NfwO8Hns5mTTjGh3IP5t+bA7wmoMXS3Cv3Syo8or2f30JuaM/nAG89\n+kYzAajPEq+RaEP6qTnI73rk5taKXO3mZ4n3DnK1fdJbncpBfjuQe+8byEvlGuSe+CjyXnkJeX2s\nI9pXOOzpW2X5egd5+1QgV9adyGU5bBb/CXt+O/KLr0CR6rsQUd9tRBuod9i1BUS767Uh768a5P1S\noOTzZ+QtMxe5mM6JXVuO3A8nEm1oP9CubbZ8TkVUFovRrob1Vi8n23PrkVfRWuQC+gJyjV6Htg+d\nYWl6ErnHPkoUALaGiJKmytJwAPKEqUFcQFcQbal6C9pVcQfygHra9Hgd8kq8BgU2bsNYg+35W5EH\n0Tmm688ib6B7kZv29xAbwhvItboD2Zr9DesWu/YZ0/Nt9q7ngJvsPeNMnweRwAVlaStFrtfLTVdH\n2nPOrp1k+nsDeUO9AZTaPR+2PB/pu9rKn5lunvTd7ejdyHvrSVRn34hdOz7ZM9keuZwhnOicW+ac\nW4389u8B8N63eO9rvPfPIXer45A3T6/yzoK57LPPPkyePJkbbrgh2S21yP+4H2lsKJ0GXgg8AVWk\nbPCakKtZf1Qhv58lXhsy4HWoYfRK1ZEG3kqiPYzHkn1+y1B5b/eKlvxilnhtqZ7PUIJvukP1xaNg\nqu12vgPFDCxF8QQlyFskyE8sXycRBR1NAr7ovZ+GOpNi5E7aYelvtfs+gfRcggxkCEQM1x722udi\nf2QwJqAOpBTVb5CbaDGRj3tN7NogFGSIXfOIoqPI8rAFuU/ORrEA61CHsBrFc2DPzEFus/ejPamP\nRgZ3BXInDS67U4HHvPfLkMHcSURJM9Dys4gowPJMwxyI3GY/iYzZUlRvrrBrv0MuuCAjGAIhsedP\n9t43EG1y34ZWON7x3tehOI3pqHwPsefe9t4vRmV8pl07yJ6/0e75u+kHFLT2pvd+ASrrt1BMwv3A\nafYe7PngpoyNO+oM57f2nkVoIDzYngnuvJ2rBc65ccjQd2MKsGsnoQ4P7/3rwFDn3JjEe3MquehV\niMjt9kQVtQH5xc5DBTXI7jsYbUy+PgdkdCFS8EbgkhzgeeDl0ONnSS5WjRpVcA3syBIv+D+vQR9I\n23NAfuYNa0gO8vsO4t9pQY1vVZZ4Cy2vwWBnNaO0+hg61c1oVFiNGuImZCDrrC5NJIpED39rUMM8\nkCiozaNRd3CNDTOl8J5whACkYDx97J4mNNK80NIYRs2BhXaNXVtF1Mm02/UvKWssM9wQ9BRiE8Is\nY7FhPotGtp9Cs4gQ2/BRe3ezveMbyEg2o43sP2A6aSfalGqj6eJ109s6S0ut5WeO1a0GS8+9sWsh\nUjsElc23vyvs2p/sHUF/f0OkmTV2bwgyDcy6FXRtazeiAL9QL1+O6f1GNBPxlp5m1Pm2WxqvRJ3f\ns5aWHVj7sPcXosFFKNdaNFsKey/vtLzdaPr4PzRYnY9mqY1YfUYDgEesfGvteBrY3559BAW3XUxE\nyvh/wAd8NEPYHsfz76EZwuHASu/9avs9ANjovT8EFWDg7i5CFAmregNLkwysCRXi4USkdNnggXHc\nmBHJBq8WVaTdUNDR6Czx1qJKuYvIuGSDF0YtN3rva3OQ3wZkbP7Pe38gXQPA+oIHajwTDK9XLqM0\n5B006nwOGchAUTAJRbL/HS2RHI4aajUaCZ+IjMZgNHr9A4oY/a2dL0UNdgyRof8rqvNL7L3b0Ch+\nF4p4XmPnW+1c4PMJGwzVoUj4DtRhfcsw7iUymk1o6Qdk0O8lMlD1KLp2F1rS+REyWrtb2q5Fhu4l\nZKh+jgLgvmc6udl0lEwmoM7hdTR6nkxkmDH9XIVmPiVoiStEKlfbtTpkHwYDjV4zrzo0A6pDg8rz\niILptmIG3u6dhAaVW1HHUGhpL0YG+QhkLFu991OBb9o9Dah8Sy29e6Llo/6xNJ6OiCMDOVycC8p7\n79vRLOQg0+UV9q4f27VX0KzrcOfcAYbRYene23R2NqrPzxn+wygy/q92/nHn3Cl2LVCnJJO5dG0f\nj/dyb/qSi14FrefNtP9HYyHY9vtY4M/2/8Ooos9FFbYa9byzUeVsJgqXb0AVfz3qCRvsmRY7X48q\nehVqMKHy1Nl9NXZtPdEIq9nevdrubTW8MNVuNqwFRJGps2PHhti9gffIJzlCBGQIh4/ft4iImiOO\n0WE62GzPtxNFoYaRaohsbI090xrDbk347WP3hfzNIVrWuM3ylSy/O+g64mwhorbYTjSyCmmsNvww\nSgrfPVbE0hXwdlk5tRjeDqIR8wLL/yLUMEN6At5O+xvONxONGoOuWmLHAnvf62h0d7W9sw0Z3BA5\nuwFFg1eiiNlQt2dbfn9oZfMrK9tViF7g93Y9RMeeQjSSryaia/gx6gyes3P1ps9qtJa9GXUER6EO\npsl096TlucLSFurriiTt8Gem+6nI4F+Ntkm90tIZ2uSzaBnka4Y7Ahn67aizutfyuNLSchgJ+4Tb\nc3+z+1fYuU5KZqJvLWvt+WrT4TLU9sI3i4Ps3Gt2/zWIkmSZ5WVRuBZ791xkfMNzH4g9P9f0vAUY\nY/fvQAb/auBB02e4ts6uT0T16temn81oRvFt08GyhPxfbemsBJbYuV8Cn49dWxbeE3/G/g91rsJ0\n02Bpq0DtspLIjoVrXfCSlMd7hv66U7z31URkUaCPKe84507E1ujQ5umDUSO6234fjBrf06hTABnO\nH2Bc7ESNfaUdP0CN+1KM4gCtfbajPXlXI16fh9Eo4VPe++mIu2cJGtEU2/1PAqu99+X0zGUU+IMC\n189paBq3CBVqC6rkS5DBe9nww/aN7WiEOd/S28+uBdqMY5DBCeuvL6GKGWZeO02vLfa7Fo2AQFQL\nbyGeoMDFE1g9A3FZC9EmO5uAQq+ox2RcN4sQh8rOWN7OQ0ZqCxEH1BLUCL8GPI+MyX2oM96CpuiP\nWdpCA/qq5bkeDSA6o4i9uIdWodnk3Za+wGVUgpYaJsTOb0KNeRcyChvQx7wrLa8fsfIahQzFR9GI\nqhrNVn9h5fQaGqm/CCx0zg12zgWCRo8oBeYhLpsVqNynmb7eMN19HI1QgyGrQB+EO9Cgabg9X2R5\nGYJGyx83HR5kz92ARq3z7Zlv2HPHWD4bgDnOuSLn3HgnLpsxqO6Hbx9T0BLtEuApy3elc+4w9P3k\nCDS6L7K0l9s797T3vAwM8KJb/yxQ5yQDnZg0C1B76wCanTh0XgA+55wbhjqZQagjug7Vl/9EH10D\nKeIjaEbyNzQzewo4GRnmP6HR+xr0YfcZAOfcDDRT+QUaVR9gutwf1bcJaNbjgYucvLGGoDr4UctX\nE/Bl59zRyNi/gmYHb9r7V6CO4dOowzsXeM45N9S8jSYZVrvlMXCXPYHaSKAPqTed4kTu+VG793DU\ndseibwifQ/Zthun1Uu/9ODu/zK7dAey08sDS4ez/w+lKGth3ydEM4Sjg2djvGagBL0RTmWFE+yWE\nnn8bqoj7oca5CBV+G9Ea6haiqfVGVJBh/TWMOK9GlTg+Mg7rnVejbxjb7fx2VED1qKAfAr6CjPen\nkMFfQUQWttLyEUbBLfaeuywtK4lmL2Ek+DPUKbyCGlwTEQvnSrS8FYx1IxETaVgiCKPpMDKvJhqJ\nt9rvzaab+OxhB5qSvxjLb61d32TPPxfTT52lucryu9DSUxV7V72VyU47t5BoxFKNRlJzUUf+U8tT\nq+Xjl3bfRkvDZtPpT1AH3Y6MxHZ7TwcaQb5FNApuRXVmtZWVt/SF9dilyBPoPqIZUJj9VKAOaxNd\nZznVlo5niGYvTZbHvyOjVhfTa0j7E6ijCvUueC7VIg+hOiJyNE9UV1osf2Hf6fissMXKbZvpuT32\n3lbLw1VoJhy/9jIyZGcQzUTDuzbb7y2m/7dRJ7nK0hy+RWwnInhsoGt+a4g8quKkb62GFb43hLrU\nYeVcZ/qIt8NdRCRtYda21fIbx2m28pxnZdERw9mK6l7YbyXM9sMMObTNYDPWxM4HXW5B9uAF01F7\nDGuxlW9oH6HNttC1fSyle/tYgRw95tk920y3b6PB7jo7v8HeuwDV5+DZdJGlt9bK4ybgq3bNoY6s\n3vB+GLt2EdFmZK8DR+fElueoQyiyTE5CvfoC0vjIQQ+bRRtec094vTz3j8b7HBpJdOKhkUYL0cYz\nz9r53RFjIqgDW2DHYrQGGtJ3BlHD2YSM7uyE91+DRtuz7feHkCFdaMdpSdLXkG767PcryCgsQCOb\nkL5HkWFZhkZxY2PpOB54PVF/vaQvXh4NfS2PcM3ef1/C+d7y+5T9fwwyBAsxt1ngJLv2AGqMC5HR\nCEsN30UNdX7s2K2H/H4qSXrrctH28kf+yOWROyC5SC1HI5Gr0nymN86OGT3h9fRcHu/fGu+fxv+S\nP/LHv8qRM7bTvOQlL3nJy/tb8lxGeclLXvKSFyDfIeQlL3nJS15MctohGBnTPOfck/b7GheRaM13\nIut60kUEYi1GELU1XLPnfm7kUoFkLBB2VTiRiL3uRKrVbsRVgZTrt865YufcLww7EIK1Oud2OBFk\nHWzPz3ciwdpk9211zl0Ry0uFkVQF8q1EUqp7jXhqmxPh1ULn3PTY+S7kbM65q81FcJ6LiLEq7Xyx\n3VNn+WhwIhjbbEcg6droIoKsbS4iSWt3IhKrdyIC22z3Bf012lHtIrKzRrtWZXn8nuWr2nUlb9vo\nnDvJ9PG2PRtI3DoJt5zItmqdiMyaTSefdaIzWZlEt4Hsq810n0p/tzgR0lVYfisT9Pqic+4d08sW\n948iA8vLv4w4585wIvXrcM71uAlNQluYnXDtYicSz8XOuZvs3ERrb6Hu/SKDNM1wztWlvjNHkssP\nEsgn/jdEodbXEAVidF5DfuObe7hWhLxBfmzP34zcskrsWg3yyrkJuTCuQW5/r6BI0PPQh8qH7Nom\n5Occrj2HIikLiVxZ9yFyK5tqaapA8QcXALVJ8nocilINe98eifzRj0PRjGtIIJ9CroIXWD4Ho4+q\nU+35vezaC5bHdWiLywWIm2Ud8jjagTbrXm3vKUUun79F3jCriIgE70QucrPs/Seh2IZv2n0tyBd8\nIfAXu6caucdelpD2CuRb/t/I9a6OGOEW8vBZCVxhv6+0tO5J5EkU1+0aFDvxjF37XG/6M91Nt/c8\nHXSXcO0kK9/lAS/2/PGJ5ZE//n0Pknuk7We24CWMIqKHZytIEgSG9lh/AcVvQOR1NpE0Nw5LwPsA\n8nL7h3mk5ZLcbhwK6piZ5bWPISO+0X43EgXynIgM0RuISOrXdm1P5P44EPn7ehTwEwi+NiVcG4LC\n2OuAHV5EXb9FBR2IrkB+6btIIt77vyKDWOMlr6OgsRXIDznZMxvtOt77Xci/eaw9v8quTUCGdA0R\naVo18lMejmZ1hwOvee/nepFqvUREx4D3/jnvfRsKnGpABhNEJrbD9D0i9p7nUIBPkIEoqGeuc+73\nzrmwb2UBClb7gb0nkXBrNBFR3ttAf+/9au99C4o9CLodYO89wu5/GLmg9qg/7/1G7/1c+9kedJdw\n7VQU2LcE+YO/+2RgefmXEe/9ErMFfZX/Am7wIivEe7851QPOuY855/6e2Nacc4VoMHx5FunJWHK5\nZHQbSnxHwvmLUdBZHV05boaiQIsKNIIM1/ZGRvv7KCjnO0SzhofRqPZbaET9URS4MQr1wju9WFWf\nQb7uY1AE5qdQ8NxERMJ2M/KhH4+CfEABaIVEG3F71NtfD5TaEsTTzrn4jvCjiVgtA0Z8I++je3ju\naOfcEtT57YTOCjATRcYuQAbyDcMcj6JLDzZdfRs4wTn3qnNuLurMTkOzpMbYe/a3+4+yaeePkTG+\nHM0sihG3TR0Rv5EzvU1AI6Z9EROoR0Fowy1tyfI8EHjWOfe05aOoh/sKUFTuWWj2E66lq79j0Czv\n2wnnx6LyOCSmu3Tw8pKXTMQDLzgtQV8YO783cKzTvhkvO0WFB5lky0UvO+eOBXDOjUDxLB/xYlCY\nTcQ8cBHwhA0i/2FSlPqW1OKMjMl7P8c5d3zs0s+RETkRjUa/RhTVeCBS4BdR6P5/I2NfhDqLPVBY\n+KvIcN2BDMFUtFx0BAqpn4SM2qXAIOfcF5DR+zIiAOtAhm0B6mDesnsdilw8todsHeO9r7LCex4V\nUCmK7JyShloagWne+zqn8Pnwx1D44gAACAxJREFU3FxLz1PIQD/knNtuaQZFTn7SdHSUnTvP9Hce\n6iDfRBQVY9CUcg9EC3BlwHGiXWhDRvE7yEiegmYNRZb/lZY+iAjKrkYd9UZL736G+RkUO1CCqBT6\nOW2MMhF11N9FS2sHWn677UAUk++hWc4ERA/QkuSenvS3DM0uvo+W++LlUYhmL5d472stX0ECGVgi\nXl7+jcQ59waqz6XAcBdtvvQd7/2zacIE27Ab8LxzbqmtGBShAdORiL9plnNuT9SWJnjvtznnDkUE\ndvvbfVOBv1ld7Q/83Tm3OwpQPT4HWc5IcjVD+A/gU865CmQcPuSce9B7vwkZtU8hPpCPoijOe7z3\nS+3aUaiAPmTHqcBL3vvN3vs1qCGXoHXy0fb7QKJlpTK0Jr8ZRdEejbjPA+3Cl+25h+2ek4lI8RrQ\n7AJEKdFu5/Heh70R1qPlmsO9908hQzjCrlUjPiJiGOG5DlvOIeG5RrTU9ZD3/rv2/MleXE5fQSPc\nt4HbrIJORcZzieX5BcvPs5b3kxFdxj5oiSss71yHDOYIxNl0N4rOHYuWiBzaSawOdZADnHMXYayg\n3vsDEFNmnellDBr19yfiqS+2/B7rvX8Y2OScG2P5LbDnkunmbTTrWYMifKfbtVT6G41oKh7w3j8a\n16tzrh9a9nrLe/9o4ju9WF2TlUde/o3Ee3+EtbUvoxH4wXak2xl02gZbEnqMaD+WSuBRWwJ+Ew1G\nR3jvm7332+yZOej73d6oDT4fS8NU7/35aPA2GVhpNnWgc25lDrKfUnLSIXjvr/Tej/PeT8SImrz3\nXzDjcKUXUdOP0ZLGX4BLnXNjvfdXoo+IjYhc7C+o8zjEiURrHFpa8MjIv4kM5Eq05HMUUvrJwB8R\nkd4SZPhH2PWr0bLMx9Hy0RZEVPcWGqF2OOf2RiyFE4EnnMjCwhLKOGRkF7nuJFIvoHVq55w7khj5\nFFDkXFLyqYcQO+KtsfPOiRzrcrR+PgGRYF2CZktnWT7HoE71T6jzLEM7SB1tOjkGaHHOfcKuX4AM\n/AcRhcMixLlShzrRZd77UtPFX9H6ezWa9k5Do/dCxM30V+/9aNSxByqIL4Q8m7F+AjjX8jXA0jLJ\nOdff6sUTTiRoS9DofJ6V2b5o6TCV/m6ytPwkiV7vQUyuI5KVh3u3yMDy8m8lcdtgdfljqF2BZp0n\n2LW90eBpq3NupC0JYzOGKcgp5HXgP5xzk2PYe3vv/+y9H+29n2g2tcF7P/kfksFcf6Um5s1Bdx6Y\nTyND9BnkSRNIyt6IXZuJRo2B9K0ZGfFABRu2MgyMooFKuAYxS060+wORVyDOCtv5/R0ZjgXIq2gT\nEcnYMkv3d1Fvv4OuZGWrMPIptG6/kWijnkoicrZwPk7OdhMRZ04TEcnet9DyxyYisq16S3Mr0XaX\nnogVdQEReVcg+AqbfQQitzbUuQScnYgk7HpUEeNkfluQoT2ViMwvvK8OeSJVEnEvvW7XqoAfms4u\nMn0G0r63UCe+HBn7RN2utrQ1IcMcyLt60l9v5HY3E3ERbSHa/OVdJwPLH+/Pg555ryrpnfcqkYfs\nqtjz/RFN9SK0kvEhO/8Zu3e+nf9k7Jn3FO9VnroiL3nJS17yAuQjlfOSl7zkJS8m+Q4hL3nJS17y\nAuQ7hLzkJS95yYtJvkPIS17ykpe8AP9CHYJLn5jqUrtvkTMyvNi1bsRUabx3iBPR2h25yMc/U9LN\nv0sgMbRziUSGJ6V41x5O4frz7X1fzWVe8pKXvGQuOYlU/keLRUOf570/L3Z6EXJd/WUvz41FEdFT\nvfeNzrlZyD/+PufcCcjt8iDvfbNFIaYj30c+/O8bSaa/DPP/DeT2OyTh/E+897ekmYyNwFH2rhDn\n8YT3fkO6+chLXvKSW/mXmSH49ImpioAS51wREeEd9EBMZaPhm51zbznx4HwlAFkY+igU+ft+l7SI\nuVwvRIU93J9Uf977lvAuFMT2L1MX85KX96v8WzVCr5DzWxCV9EYiMjzomZjqfLvvMMRPcoFF3xag\n6OvL/rG5eNekN2KuuPREYghwsRn9e51zw+xcUv0BOO1jsBAFn92Ynx3kJS//XHlfdQhmrOaj0emn\nYuvVH0/z+WFoWWQSij4MZHjQlZjq24iYyqHQ9C/Ze99A9BdTEFHfU977ytzl8N2VFPrrKf/x5ztJ\nDJPA/xxFcR6MOtsf2/me9If3fr33/kDE23Kuc25UN9S85CUv/zB5X31D8N4fAT1+Q0hHPgKs8d5v\nMZxAhvcgMWIq4E3nXAfiQ3LAxT6B/Mo5dy4aUX8NcR31d87Vee+v4D0qvenPOXcpyfO/JQYRSAxP\nQsR2Q5xIDL/gRWQYsO5GNCTQg/4S0rXBiTn1WESTkZe85OWfIO+rGUIOZB1wpBNxniMiw4MeiKkQ\nq+h/ObFp4pzb2zk3yHt/jvd+ghf51GXAr9/LnUEa0lP+O8X3QGJoz8Q3oglEetCD/pxz45xzJXZu\nGOJ5ymZzkrzkJS9Zyr9Mh+CcO905V4kYTv/snHvWzu/unHsKwHv/BhqBzkVkagXAXQZxL7CnjVQf\nBs610fJMRKY31679kvfZzCpNSZr/uP5SyE1O+8wuRB3LpXa+J/3tB7zhnFuANim6xXv/do7zlJe8\n5CUDyZPb5SUveclLXoB/oRlCXvKSl7zkJTvJdwh5yUte8pIXIN8h5CUveclLXkzyHUJe8pKXvOQF\nyHcIeclLXvKSF5N8h5CXvOQlL3kB8h1CXvKSl7zkxeT/A+KTEX79+ZkUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fb499d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\xac\\x81cient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-356-79299e220b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\xac\\x81cient'"
     ]
    }
   ],
   "source": [
    "X = [[word2idx[token] for token in d.split()] for d in desc]\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token not found: ��cient\n",
      "Token not found: erence,\n",
      "Token not found: umb”\n",
      "Token not found: ed,\n",
      "Token not found: ssed.\n",
      "Token not found: ng\n"
     ]
    }
   ],
   "source": [
    "# Um, just remove the last token in the first one. \n",
    "X = []\n",
    "for d in desc:\n",
    "    item_list = []\n",
    "    for token in d.split():\n",
    "        try:\n",
    "            item_list.append(word2idx[token])\n",
    "        except:\n",
    "            print('Token not found: ' + token )\n",
    "    X.append(item_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump((embedding, idx2word, word2idx, glove_idx2idx),open('/Users/fm61/Documents/Tutorials/How_to_make_a_text_summarizer/data/vocabulary-embedding.pkl','wb'),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-253bcac7dd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/vocabulary-embedding.data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-47fc0e9c4e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/vocabulary-embedding.data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/vocabulary-embedding.data.pkl'"
     ]
    }
   ],
   "source": [
    "pickle.dump((X,Y),open('data/vocabulary-embedding.data.pkl','wb'),-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN0 = 'vocabulary-embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN1 = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlend=25 # 0 - if we dont want to use description at all\n",
    "maxlenh=25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512 # must be same as 160330-word-gen\n",
    "rnn_layers = 3  # match FN1\n",
    "batch_norm=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed=73\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "LR = 1e-4\n",
    "batch_size=64\n",
    "nflips=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = 30000\n",
    "nb_val_samples = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/vocabulary-embedding.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-329-1248b6e963f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_idx2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/vocabulary-embedding.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/vocabulary-embedding.pkl'"
     ]
    }
   ],
   "source": [
    "embedding, idx2word, word2idx, glove_idx2idx = pickle.load(open('data/vocabulary-embedding.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/vocabulary-embedding.data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-331-f95880c672be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/%s.data.pkl'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mFN0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/vocabulary-embedding.data.pkl'"
     ]
    }
   ],
   "source": [
    "with open('data/%s.data.pkl'%FN0, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 14 14\n",
      "dimension of embedding space for words 100\n",
      "vocabulary size 19722 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 19722 19722\n",
      "number of words outside vocabulary which we can substitue using glove similarity 18\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) -18\n"
     ]
    }
   ],
   "source": [
    "print 'number of examples',len(X),len(Y)\n",
    "print 'dimension of embedding space for words',embedding_size\n",
    "print 'vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words\n",
    "print 'total number of different words',len(idx2word), len(word2idx)\n",
    "print 'number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx)\n",
    "print 'number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov0 = vocab_size-nb_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print label+':',\n",
    "    for w in x:\n",
    "        print idx2word[w],\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: We study response selection for multi-turn conversation in retrieval based chatbots. Existing works either ignores relationships among utterances, or misses important in- formation in context when matching a re- sponse with a highly abstract context vec- tor ﬁnally. We propose a new session based matching model to address both problems. The model ﬁrst matches a re- sponse with each utterance on multiple granularities, and distills important match- ing information from each pair as a vec- tor with\n",
      "D: opera- tions. The vectors are then accumulated in a chronological order through a recur- rent neural network (RNN) which mod- els the relationships among the utterances. The ﬁnal matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that our model can signiﬁcantly outperform the state-of-the-art methods for response se- lection in multi-turn conversation. 1 Introduction Traditional research in human-computer conversa- tion focused on building task-oriented dialog sys- tems in vertical domains to help people complete speciﬁc tasks such as ordering and tutoring etc (Boden, 2006; Wallace, 2009; Young et al., 2010). Recently, with the large amount of conversation data available on Internet, there has been a surge of interest in building non-task-oriented chatbots that can naturally and meaningfully converse with humans on open domain topics (Jafarpour et al., 2010; Ritter et al., 2011). Existing work on build- ing chatbots includes generation based methods ∗The work was done when the ﬁrst author was an intern in Microsoft Research Asia. Table 1: An example of multi-turn conversation Context 1: I am going to hold a drum class in Shanghai. Anyone wants to join? The location is near the Bound. Context 2: Interesting! Do you have coaches who can help me practice drum? Context 3: Of course. Message: Can I have a free ﬁrst lesson? and retrieval based methods. In this work, we study retrieval based chatbots, because they select responses from an index of existing conversation and thus can leverage the existing search power and always return ﬂuent responses. While most existing work on retrieval based chatbots stud- ies response selection for single-turn conversation (Wang et al., 2013; Wang et al., 2015), we con- sider the problem in a multi-turn scenario which is the nature of conversation but has not been well explored yet. Different from response selection in single-turn conversation in which one only needs to match a response with a single input message, response se- lection in multi-turn conversation requires match- ing between a response and a conversation session in which one needs to consider not only the match- ing between the response and the input message but also the matching between the response and the utterances in previous turns as context. The challenges of the task include (1) how to iden- tify important information (words, phrases, and sentences) in context that is crucial to selecting a proper response for the session and how to lever- age the information in matching; and (2) how to model relationships among the utterances. Ta- ble 1 illustrates the challenges with an example. First, “hold a drum class” and “drum” in the con- text are very important. Without them, one may ﬁnd responses relevant to the message but non- sense in the session (e.g., “what lessons do you want?”). On the other hand, although “Shanghai”, “the Bund”, and “coaches” are also keywords in their utterances, they are useless and even noise to response selection. It is crucial yet non-trivial to extract the important information from the context and leverage them in matching while circumvent the noise. Second, the message highly depends on Context 1, and the order of the utterances matters in response selection: exchanging Context 2 and the message may lead to different responses. Ex- isting work, however, either ignores relationships among utterances (Lowe et al., 2015; Yan et al., 2016), or loses important information in context in the process of converting the whole session to a vector without enough supervision from responses (Lowe et al., 2015; Zhou et al., 2016). We propose a new session based matching model which can tackle both challenges in an end- to-end way. One major problem suffered by the existing models is that responses in matching can- not meet the session until the ﬁnal step, which re- sults in information loss. To overcome this draw- back, our model matches a response with each ut- terance in the session (message and context) at the very beginning. For each utterance-response pair, the model constructs a word-word similarity ma- trix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated unites (GRU) (Chung et al., 2014) respectively. The two matri- ces capture important matching information in the pair on a word level and a segment level respec- tively, and the information is distilled and fused as a matching vector through an alternation of con- volution and pooling operations on the matrices. By this means, important information in context is recognized under sufﬁcient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to a GRU to form a matching score for the session and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the session. It models the relationships and the dependencies among the utterances in a matching fashion and has the ut- terance order supervise the accumulation of pair matching. The gate mechanism of the GRU helps select important pairs and ﬁlter out noise. The matching degree of the session and the response is computed by a logit model with the hidden states of the GRU. Our model extends the pow- erful “2D” matching paradigm in text pair match- ing for single-turn conversation to session based matching for multi-turn conversation, and enjoys the advantage that both important information in utterance-response pairs and relationships among utterances are sufﬁciently preserved and leveraged in matching. We test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large public English data set for research in multi-turn conversation. The results show that our model can signiﬁcantly outperform the state-of-the-art methods, and improvement to the best baseline model on R10@1 is over 6%. One problem with the Ubuntu data is that negative examples are randomly sampled which might oversimplify the multi-turn problem in a real retrieval based chatbot. To further verify the efﬁcacy of the proposed model in a real situation, we simulate the procedure of a retrieval based chatbot and create a large scale Chinese test set. Instead of negative sampling, labels in the data are generated by 3 human judges. On this data, our model im- proves the best baseline model over 4% on P@1 (equivalent to R10@1). We published the data at https://github.com/MarkWuNLP/ MultiTurnResponseSelection. Our contributions in this paper are three-folds: (1) proposal of a new session based matching model for multi-turn response selection in retrieval based chatbots; (2) empirical veriﬁcation of the ef- fectiveness of the model on public data sets; (3) publication of a large human labeled data set to research communities. 2 Related Work Early work (Weizenbaum, 1966) on chatbots exploits hand crafted templates to generate re- sponses, which requires huge human effort and is not scalable. Recently, data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) have drawn a lot of attention. Existing work along this line includes retrieval based methods and genera- tion based methods. The former selects a proper response from an index based on matching be- tween the response and an input message with or without context (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016; Zhou et al., 2016), while the latter employs statistical machine translation techniques (Ritter et al., 2011) or the sequence to sequence framework (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015; Li et al., 2016; Xing et al., 2016; Serban et al., 2016) to gener- ate responses. Our work belongs to retrieval based methods, and we study response selection with context information. Early studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016). Recently, researchers be- gin to pay attention to multi-turn conversation. For example, Lowe et al. (Lowe et al., 2015) match a response with the literal concatenation of context utterances. Yan et al. (Yan et al., 2016) concate- nate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (Zhou et al., 2016) improve multi-turn response selection with a multi-view model including an ut- terance view and a word view. The stark difference between our model and the existing models is that our model matches a response with each utterance at the very ﬁrst and matching information instead of sentences is accumulated in a temporal manner through a GRU. 3 Matching Approach 3.1 Problem Formalization Suppose that we have a data set D = {(yi, si, ri)}N i=1, where si = {ui,1, . . . , ui,ni} represents with a conversation {ui1, . . . , ui,ni−1} utterances in context and ui,ni an input message. ri is a response candidate and yi ∈ {0, 1} denotes a label. yi = 1 means ri is a proper response for si, otherwise yi = 0. Our goal is to learn a matching model g(·,·) with D. For any session-response pair (s, r), g(s, r) measures the matching degree between s and r. session 3.2 Model Overview Figure 1 gives the architecture of our model. The model ﬁrst decomposes session-response match- ing into several utterance-response pair matching and then all pair matching are accumulated as a session based matching through a recurrent neu- ral network. Speciﬁcally, the model consists of two layers. The ﬁrst layer matches a response candidate with each utterance (context and mes- sage) in the session on a word level and a segment level. An utterance-response pair is transformed to a word-word similarity matrix and a sequence- sequence similarity matrix, and important match- ing information in the pair is distilled from the two matrices and encoded in a matching vector. The matching vectors are then fed to the second layer where they are accumulated in the hidden states of a recurrent neural network with gated unites (GRU) following the chronological order of the ut- terances in the session. The matching degree of the session and the response is calculated with the hidden states of the GRU. Our model enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the session at the very be- ginning of the whole matching procedure, thus matching information in every utterance-response pair can be sufﬁciently extracted and carried to the ﬁnal matching score with minimal loss. Second, information extraction from each utterance is con- ducted on different granularities and under sufﬁ- cient supervision from the response, thus seman- tic structures that are useful to response selection in each utterance can be well identiﬁed and ex- tracted. Third, matching and utterance relation- ships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score. By taking utterance relationships into consid- eration, our model extends the “2D” matching which has proven effective in text pair match- ing for single-turn response selection to sequen- tial “2D” matching for session based matching in response selection for multi-turn conversation. We name our model “Sequential Match Network” (SMN). In the following sections, we will describe details of the two layers. 3.3 Utterance-Response Matching At the ﬁrst layer, given an utterance u in a ses- sion s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, . . . , eu,nu] and R = [er,1, . . . , er,nr ] respectively, where eu,i, er,i ∈ Rd are the em- beddings of the i-th word of u and r respec- tively. U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input chan- nels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a Figure 1: Architecture of SMN matching vector v. Speciﬁcally, ∀i, j, the (i, j)-th element of M1 is deﬁned by e1,i,j = e(cid:62) u,i · er,j. (1) M1 models the matching between u and r on a word level. To construct M2, we ﬁrst employ a recurrent neural network with gated units (GRU) (Chung et al., 2014) to transform U and R to hidden vectors. Suppose that Hu = [hu,1, . . . , hu,nu] is the hidden vectors of U, then ∀i, hu,i ∈ Rm is deﬁned by zi = σ(Wzeu,i + Uzhu,i−1) ri = σ(Wreu,i + Urhu,i−1) (cid:101)hu,i = tanh(Wheu,i + Uh(ri (cid:12) hu,i−1)) hu,i = zi (cid:12)(cid:101)hu,i + (1 − zi) (cid:12) hu,i−1, (2) where hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters. Similarly, we have Hr = [hr,1, . . . , hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th ele- ment of M2 is deﬁned by e2,i,j = h(cid:62) (3) where W1 ∈ Rm×m is a linear transformation. ∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector. Therefore, M2 models the match- ing between u and r on a segment level. u,iW1hr,j, M1 and M2 are then processed by a CNN ∀i = 1, 2, CNN regards Mi as to form v. (cid:104) (cid:105) I (l,f )×J (l,f ) an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f ) = z(l,f ) denotes the output of feature i,j maps of type-f on layer-l, where z(0,f ) = Mf , ∀f = 1, 2. On the convolution layer, we employ a 2D convolution operation with a window size w × r(l,f ) r(l,f ) Fl−1(cid:88) , and deﬁne z(l,f ) w(cid:88) h(cid:88) as (l,f ) (l,f ) i,j · z(l−1,f(cid:48)) i+s,j+t + bl,k), (4) z(l,f ) i,j = σ( W(l,f ) s,t h r r f(cid:48)=0 s=0 t=0 w ×r(l,f ) where σ(·) is a ReLU, W(l,f ) ∈ Rr(l,f ) and bl,k are parameters, and Fl−1 is the number of fea- ture maps on the (l − 1)-th layer. An max pooling operation follows a convolution operation and can be formulated as h z(l,f ) i,j = max (l,f ) w >s≥0 p max h >t≥0 (l,f ) p zi+s,j+t, (5) h w and p(l,f ) where p(l,f ) are the width and the height of the 2D pooling respectively. The output of the ﬁnal feature maps are concatenated and mapped to a low dimensional space with a linear transforma- tion as the matching vector v ∈ Rq. From Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful to recognize the appro- priateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matri- ces. These areas will be transformed and selected by convolution and pooling operations and carry .... .... ....Score12,MMConvolutionPoolingMatching Acclumulation()L ....1,,,,Tijuirjeee2,,,1,TijuirjehWh .... ....1u1nunurWord EmbeddingGRU1Sim MatricesGRU2CNN the important information in the utterance to the matching vector. This is how our model identiﬁes important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multi- ple granularities of text. 3.4 Matching Accumulation Suppose that [v1, . . . , vn] is the output of the ﬁrst layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . . , vn] as an input and en- codes the matching sequence into its hidden states Hm = [h1, . . . , hn] ∈ Rq×n with a detailed pa- rameterization similar to Equation (2). This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the session; (2) it leverages the temporal relationship to supervise the accumulation of the pair match- ing as a session based matching. Moreover, from Equation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input ﬂows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be ﬁltered out. With Hm, we deﬁne g(s, r) as g(s, r) = sof tmax(W2L[h1, . . . , hn] + b2), (6) where W2 and b2 are parameters. We con- sider three parameterizations for L[h1, . . . , hn]: (1) only the last hidden state is used. Then (cid:80)n L[h1, . . . , hn] = hn. (2) the hidden states are linearly combined. Then, L[h1, . . . , hn] = i=1 wihi, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h1, . . . , hn] is deﬁned as ti = tanh(W3hi + b3), (cid:80) exp(t(cid:62) i ts) n(cid:88) i(exp(t(cid:62) i ts)) , αi = L[h1, . . . , hn] = αihi, i=1 where W3 ∈ Rq×q and b3 ∈ Rq are parameters. ts ∈ Rq is a high level virtual context vector which is randomly initialized and jointly learned in train- ing. i=1 [yilog(g(si, ri)) + (1 − yi)log(1 − g(si, ri))] , (7) Both (2) and (3) aim to learn weights for {h1, . . . , hn} from training data and dynamically highlight the effect of important matching vec- tors in the ﬁnal matching score. The difference is that weights in (2) are nonparametric and un- normalized, while in (3) they are parametric and normalized. We denote our model with the three parameterizations of L[h1, . . . , hn] as SMNlast, SMNnon−para, and SMNpara respectively, and empirically compare them in experiments. We learn g(·,·) by minimizing cross entropy with D. Let Θ denote the parameters of our model, then the objective function L(D, Θ) of learning can be formulated as − N(cid:88) where N in the number of instances in D. 4 Response Candidate Retrieval In practice of a retrieval based chatbot, to apply the matching approach to response selection, one needs to retrieve a bunch of response candidates from an index beforehand. While candidate re- trieval is not the focus of the paper, it is an im- portant step in a real system. In this work, we ex- ploit a heuristic method to obtain response can- didates from index. Given a message un with {u1, . . . , un−1} utterances in its previous turns, we extract top 5 keywords based on their tf-idf values1 and expand un with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline re- trieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the session. 5 Experiment We tested our model on a public English data set and a Chinese data set we publish with this paper. 5.1 Experiment setup The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains large scale multi-turn dialogues collected from chat logs of Ubuntu Fo- rum. The data set consists of 1 million session- response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Posi- tive responses are true responses from human, and 1Tf is word frequency in the session, while idf is calcu- lated using the entire index. negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016)2 in which numbers, urls, and paths are replaced by special placeholders. One problem with the Ubuntu data is that nega- tive examples is much easier to identify than those in a real chatbot, because they are randomly sam- pled and most of them are far from the semantics of the context. A better data set that can simulate the real scenario of a retrieval based chatbot must have responses generated following the procedure of information retrieval and labels annotated by humans. As far as we know, however, there are no such data sets publicly available. To test our model in a setting closer to the real case and facil- itate the research of multi-turn response selection, we created a new data set and publish it to research communities with the paper. We crawled 15 mil- lion post-reply pairs from Sina Weibo3 which is the largest microblogging service in China and in- dexed the pairs with an open source Lucene4. We then crawled 1.1 million dyadic dialogues (con- versation between two people) longer than 2 turns from Douban group5 which is a popular forum in China. From the data, we randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap among the three sets. For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a session and randomly sam- pled another response from the 1.1 million data as a negative response. In total, there are 1 mil- lion session-response pairs in the training set and 50 thousand pairs in the validation set. To cre- ate the test set, we took the last turn of each dia- logue as a message, retrieved 10 response candi- dates from the index following the method in Sec- tion 4, and ﬁnally formed a test set with 10, 000 session-response pairs. We recruited three label- ers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the con- text. Each pair received three labels and the ma- jority of the labels was taken as the ﬁnal decision. 2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0 3http://weibo.com/ 4https://lucenenet.apache.org/ 5https://www.douban.com/group # response candidates per session Avg. # positive responses per session # session-response pairs Table 2: Statistics of the Chinese data set val 50k 2 1 3 40 5.81 17.22 Min. # turns per session Max. # turns per session Avg. # turns per session train 1M 2 1 3 53 6.03 16.75 Avg. # words per utterance test 10k 10 1.18 3 44 5.95 17.17 Table 2 gives the statistics of the three sets. Note that the Fleiss’ kappa (Fleiss, 1971) of the label- ing is 0.41, which indicates that the three labelers reached a relatively high agreement. On the Ubuntu data, we followed (Lowe et al., 2015) and employed recall at position k in n can- didates (Rn@k) as evaluation metrics, and on the human labeled data, we followed the convention of information retrieval and employed mean aver- age precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees and oth- ers, 1999), and precision at position 1 (P@1) as metrics. Note that when using the labeled set, we removed sessions with all negative responses or all positive responses, as models make no differ- ence on them. After that there are 6, 670 session- response pairs left in test. 5.2 Baseline We considered the following baselines: Basic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM. Multi-view: the model proposed by Zhou et al. (2016) who utilize a hierarchical recurrent neural network to model utterance relationships. Deep learning to respond (DL2R): the model proposed by Yan et al. (2016). Advanced single-turn matching models: since LSTM and BiLSTM do not represent the state-of-the-art matching models, we concatenated the utterances in a session and matched the long text with a response candidate using more pow- erful models including MV-LSTM (Wan et al., 2016), Match-LSTM (Wang and Jiang, 2015), and Multi-Channel which is described in Section 3.3. Multi-Channel is a simple version of our model without considering utterance relationships. 5.3 Parameter Tuning For baseline models, if their results are available in the existing literatures (e.g., those on Ubuntu Corpus), we just copied the numbers, otherwise Table 3: Evaluation results on the two data sets Ubuntu data Chinese data TF-IDF (Lowe et al., 2015) RNN (Lowe et al., 2015) CNN (Kadlec et al., 2015) LSTM (Kadlec et al., 2015) BiLSTM (Kadlec et al., 2015) Multi-View (Zhou et al., 2016) DL2R (Yan et al., 2016) MV-LSTM (Wan et al., 2016) Match-LSTM (Wang and Jiang, 2015) Multi-Channel SMNlast SMNnon−para SMNpara R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 0.179 0.659 0.768 0.208 0.226 0.848 0.320 0.901 0.313 0.895 0.908 0.342 0.330 0.899 0.348 0.906 0.904 0.345 0.349 0.904 0.392 0.923 0.387 0.927 0.926 0.372 0.359 0.422 0.440 0.527 0.514 0.543 0.527 0.538 0.537 0.543 0.571 0.572 0.561 0.708 0.819 0.896 0.949 0.944 0.951 0.944 0.946 0.944 0.942 0.956 0.962 0.960 0.545 0.547 0.684 0.784 0.780 0.801 0.783 0.804 0.799 0.809 0.842 0.838 0.848 0.331 0.390 0.417 0.485 0.479 0.505 0.488 0.498 0.500 0.506 0.526 0.523 0.517 0.410 0.403 0.549 0.638 0.630 0.662 0.626 0.653 0.653 0.656 0.723 0.725 0.726 Table 4: Evaluation results of model ablation Ubuntu data Chinese data ReplaceM ReplaceS Only M1 Only M2 SMNlast R2@1 0.905 0.918 0.919 0.921 0.923 R10@1 0.661 0.716 0.704 0.715 0.723 R10@2 0.799 0.832 0.832 0.836 0.842 R10@5 MAP MRR 0.541 0.950 0.565 0.954 0.562 0.955 0.565 0.956 0.956 0.571 0.503 0.522 0.518 0.521 0.526 P@1 0.343 0.376 0.370 0.382 0.392 (a) M1 of u1 and r (b) M1 of u3 and r (c) Update gate Figure 2: Model visualization we implemented the models following the settings in the literatures. All models were implemented using Theano (Theano Development Team, 2016). Word embeddings were initialized by the results of word2vec 6 run on the training data, and the dimension of word vectors is 200. For Multi- Channel and layer one of our model, we set the di- mension of the hidden states of GRU as 200. We tuned the window size in convolution and pool- ing in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) ﬁnally. The number of feature maps is 8. In layer two, 6https://code.google.com/archive/p/ word2vec/ we set the dimensions of matching vectors and the hidden states of GRU as 50. We optimized the objective function using back-propagation and the parameters were updated by stochastic gradi- ent descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU. The ini- tial learning rate is 0.001, and the parameters of Adam, β1 and β2, that control exponential de- cay are 0.9 and 0.999 respectively. We employed early-stopping (Lawrence and Giles, 2000) as a regularization strategy. Models were trained in mini-batches with a batch size 200, and we padded zeros if the length of an utterance exceeds 50. thenthecommandglebihanshouldextractthemallfrom/tothatdirectoryhowcanunzipmanyrar(_number_forexample)filesatonceValue of M_1 (u_1 and r)0.000.150.300.450.600.750.901.051.201.351.50valuethenthecommandglebihanshouldextractthemallfrom/tothatdirectoryokayhowValue of M_1 (u_3 and r)0.000.150.300.450.600.750.901.051.201.351.50value010203040u_1u_2u_3u_4u_5Value of update gate0.00.10.20.30.40.50.60.70.80.91.0value 5.4 Evaluation Results Table 3 shows the evaluation results on the two data sets. Our models outperform baselines greatly in terms of all metrics on both data sets, and the improvements are statistically signiﬁcant (t-test with p-value ≤ 0.01). Even the state-of- the-art single-turn matching models perform much worse than our models. The results demonstrate that one cannot neglect utterance relationships and simply perform multi-turn response selection by transforming it to a single-turn problem. Our mod- els achieve signiﬁcant improvements over Multi- View, which justiﬁed our “matching ﬁrst” strategy. DL2R is also worse than our models, indicating that utterance reformulation with heuristic rules is not a good method to utilize context information. Numbers on the Ubuntu data are much higher than those on the Chinese data (R10@1 and P@1 are equivalent). The results showed the merit of our new data and supported our claim that the Ubuntu data oversimpliﬁed the problem of multi-turn re- sponse selection. There is no signiﬁcant differ- ence among our three models. The reason might be GRU has already selected useful signals from the matching sequence and accumulated them in the ﬁnal state with its gate mechanism, especially when the sequence is not long, and there is no need to equip another attention mechanism on top of it. 5.5 Further Analysis Visualization: we visualize the similarity matri- ces and the gates of GRU in layer two using an example from the Ubuntu Corpus to further clar- ify how our model identiﬁes important informa- tion in context and how it selects important match- ing vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4. The ex- ample is {u1: how can unzip many rar ( number for example ) ﬁles at once; u2: sure you can do that in bash; u3: okay how? u4: are the ﬁles all in the same directory? u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct re- sponse to the top position. Due to space limitation, we only visualized M1 of u1 and r in Figure 2(a), M1 of u3 and r in Figure 2(b), and the update gate (i.e. z) in Figure 2(c). They are already enough to support our analysis. In all pictures, darker ar- eas mean larger values. We can see that in u1 important words including “unzip”, “rar”, “ﬁles” are recognized and carried to matching by “com- mand”, “extract”, and “directory” in r, while u3 is almost useless and thus little information is ex- tracted from it. u1 is crucial to response selection and nearly all information from u1 and r ﬂows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the ﬁnal state. Figure 3: Comparison across session length Model ablation: we investigate the effect of different parts of our model by removing them one by one. Table 4 reports the results. First, replac- ing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (de- noted as ReplaceM ) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vec- tor and misses important information in the pair. Together with the visualization, we can conclude that “2D” matching plays a key role in the “match- ing ﬁrst” strategy as it can capture the impor- tant matching information in each pair with mini- mal loss. Second, the performance slightly drops when replacing the GRU for matching accumu- lation with a multi-layer perceptron (denoted as ReplaceS). This indicates that utterance relation- ships are also useful. Finally, we left only one channel in matching and found that M2 is a little more powerful than M1 and we can achieve the best results with both of them. Session length: we ﬁnally study how our model (SMNlast) performs with respect to the length of sessions. Figure 3 shows the comparison on MAP in different length intervals on the Chinese data. We can see that our model consistently performs better than the baselines, and when sessions be- come longer, the gap becomes larger. The re- sults demonstrate that our model can well capture the dependencies, especially long dependencies, among utterances in sessions. (2,5](5,10](10,)session length4045505560MAPLSTMMV-LSTMMulti-ViewSMN 6 Conclusion We present a new model for multi-turn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can signiﬁcantly outperform the state-of-the-art meth- ods. References [Baeza-Yates et al.1999] Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information re- trieval, volume 463. ACM press New York. [Boden2006] Margaret Ann Boden. 2006. Mind as ma- chine: A history of cognitive science. Clarendon Press. [Chung et al.2014] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Em- pirical evaluation of gated recurrent neural net- arXiv preprint works on sequence modeling. arXiv:1412.3555. [Fleiss1971] Joseph L Fleiss. 1971. Measuring nomi- nal scale agreement among many raters. Psycholog- ical bulletin, 76(5):378. [Higashinaka et al.2014] Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, No- zomi Kobayashi, Hiroaki Sugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro Matsuo. 2014. To- wards an open-domain conversational system fully based on natural language processing. In COLING, pages 928–939. [Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Pro- cessing Systems, pages 2042–2050. [Jafarpour et al.2010] Sina Jafarpour, Christopher JC Burges, and Alan Ritter. 2010. Filter, rank, and transfer the knowledge: Learning to chat. Advances in Ranking, 10. [Ji et al.2014] Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text conversation. arXiv preprint arXiv:1408.6988. [Kadlec et al.2015] Rudolf Kadlec, Martin Schmid, and Jan Kleindienst. 2015. Improved deep learning baselines for ubuntu corpus dialogs. arXiv preprint arXiv:1510.03753. [Kingma and Ba2014] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980. [Lawrence and Giles2000] Steve Lawrence and C Lee Giles. 2000. Overﬁtting and neural networks: con- jugate gradient and backpropagation. In Neural Net- works, 2000. IJCNN 2000, Proceedings of the IEEE- INNS-ENNS International Joint Conference on, vol- ume 1, pages 114–119. IEEE. [Li et al.2015] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity- promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055. [Li et al.2016] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A persona- based neural conversation model. arXiv preprint arXiv:1603.06155. [Lowe et al.2015] Ryan Lowe, Nissan Pow, Iulian Ser- ban, and Joelle Pineau. 2015. The ubuntu dia- logue corpus: A large dataset for research in unstruc- tured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909. [Ritter et al.2011] Alan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven response gen- eration in social media. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing, pages 583–593. Association for Com- putational Linguistics. [Serban et al.2015] Iulian V Serban, Alessandro Sor- doni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2015. Building end-to-end dialogue sys- tems using generative hierarchical neural network models. arXiv preprint arXiv:1507.04808. [Serban et al.2016] Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Bengio, and Aaron Courville. 2016. Mul- tiresolution recurrent neural networks: An applica- tion to dialogue response generation. arXiv preprint arXiv:1606.00776. [Shang et al.2015] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for In ACL 2015, July 26-31, short-text conversation. 2015, Beijing, China, Volume 1: Long Papers, pages 1577–1586. [Socher et al.2013] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowl- In Advances in Neural edge base completion. Information Processing Systems, pages 926–934. [Theano Development Team2016] Theano Develop- ment Team. 2016. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May. [Vinyals and Le2015] Oriol Vinyals and Quoc Le. arXiv 2015. A neural conversational model. preprint arXiv:1506.05869. [Voorhees and others1999] Ellen M Voorhees et al. 1999. The trec-8 question answering track report. In Trec, volume 99, pages 77–82. [Wallace2009] Richard S Wallace. 2009. The anatomy of ALICE. Springer. [Wan et al.2016] Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, and Xueqi Cheng. 2016. Match-srnn: Modeling the recursive match- arXiv preprint ing structure with spatial rnn. arXiv:1604.04378. [Xu et al.2016] Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, and Xiaolong Wang. 2016. Incorpo- rating loose-structured knowledge into lstm with re- call gate for conversation modeling. arXiv preprint arXiv:1605.05110. [Wang and Jiang2015] Shuohang Wang and Jing Jiang. 2015. Learning natural language inference with lstm. arXiv preprint arXiv:1512.08849. [Wang et al.2013] Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen. 2013. A dataset for research on short-text conversations. In EMNLP, pages 935– 945. [Wang et al.2015] Mingxuan Wang, Zhengdong Lu, Syntax-based arXiv preprint Hang Li, and Qun Liu. deep matching of short arXiv:1503.02427. 2015. texts. [Weizenbaum1966] Joseph Weizenbaum. 1966. Eliza?a computer program for the study of nat- ural language communication between man and machine. Communications of the ACM, 9(1):36–45. [Wu et al.2016] Yu Wu, Wei Wu, Zhoujun Li, and Ming Zhou. 2016. Topic augmented neural network for short text conversation. CoRR, abs/1605.00090. [Xing et al.2016] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2016. Topic augmented neural response generation with a joint attention mechanism. arXiv preprint arXiv:1606.08340. [Yan et al.2016] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural net- works for retrieval-based human-computer conver- In Proceedings of the 39th Inter- sation system. national ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016, pages 55–64. [Yang et al.2016] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document clas- siﬁcation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies. [Young et al.2010] Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden in- formation state model: A practical framework for pomdp-based spoken dialogue management. Com- puter Speech & Language, 24(2):150–174. [Zhou et al.2016] Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao, R Yan, D Yu, Xuan Liu, and H Tian. 2016. Multi-view response selection for human-computer conversation. EMNLP16.\n"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "prt('H',Y_train[i])\n",
    "prt('D',X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 334\n",
    "prt('H',Y_test[i])\n",
    "prt('D',X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, recurrent_dropout=0, name=\"lstm_1\", dropout=0, return_sequences=True)`\n",
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, recurrent_dropout=0, name=\"lstm_2\", dropout=0, return_sequences=True)`\n",
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, recurrent_dropout=0, name=\"lstm_3\", dropout=0, return_sequences=True)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlen,dropout=p_emb, weights=[embedding], mask_zero=True,\n",
    "                    name='embedding_1'))\n",
    "for i in range(rnn_layers):\n",
    "    lstm = LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,\n",
    "                dropout_W=p_W, dropout_U=p_U,\n",
    "                name='lstm_%d'%(i+1)\n",
    "                  )\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(p_dense,name='dropout_%d'%(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
    "    return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context,**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_rnn_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop # usually I prefer Adam but article used rmsprop\n",
    "# opt = Adam(lr=LR)  # keep calm and reduce learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,np.float32(LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_shape(x):\n",
    "    return 'x'.join(map(str,x.shape))\n",
    "    \n",
    "def inspect_model(model):\n",
    "    for i,l in enumerate(model.layers):\n",
    "        print i, 'cls=%s name=%s'%(type(l).__name__, l.name)\n",
    "        weights = l.get_weights()\n",
    "        for weight in weights:\n",
    "            print str_shape(weight),\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls=Embedding name=embedding_1\n",
      "19722x100\n",
      "1 cls=LSTM name=lstm_1\n",
      "100x2048 512x2048 2048\n",
      "2 cls=Dropout name=dropout_1\n",
      "\n",
      "3 cls=LSTM name=lstm_2\n",
      "512x2048 512x2048 2048\n",
      "4 cls=Dropout name=dropout_2\n",
      "\n",
      "5 cls=LSTM name=lstm_3\n",
      "512x2048 512x2048 2048\n",
      "6 cls=Dropout name=dropout_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to open file (Unable to open file: name = 'data/train.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-410-d54ff88f5506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFN1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/%s.hdf5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mFN1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/travis/miniconda3/conda-bld/recipe_1486533883490/work/h5py-2.6.0/h5py/_objects.c:2849)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/travis/miniconda3/conda-bld/recipe_1486533883490/work/h5py-2.6.0/h5py/_objects.c:2807)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/travis/miniconda3/conda-bld/recipe_1486533883490/work/h5py-2.6.0/h5py/h5f.c:2102)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to open file (Unable to open file: name = 'data/train.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "if FN1:\n",
    "    model.load_weights('data/%s.hdf5'%FN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\n",
    "    \"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [lpadd([3]*26)]\n",
    "# pad from right (post) so the first maxlend will be description followed by headline\n",
    "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(data[:,maxlend] == eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50), [26])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,map(len, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 512)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(data, verbose=0, batch_size=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    "def beamsearch(predict, start=[empty]*maxlend + [eos],\n",
    "               k=1, maxsample=maxlen, use_unk=True, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in xrange(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        cand_scores[:,empty] = 1e20\n",
    "        if not use_unk:\n",
    "            for i in range(nb_unknown_words):\n",
    "                cand_scores[:,vocab_size - 1 - i] = 1e20\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        ranks_dead = [r for r in ranks if r < n]\n",
    "        ranks_live = [r - n for r in ranks if r >= n]\n",
    "        \n",
    "        dead_scores = [dead_scores[r] for r in ranks_dead]\n",
    "        dead_samples = [dead_samples[r] for r in ranks_dead]\n",
    "        \n",
    "        live_scores = [live_scores[r] for r in ranks_live]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_live]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        zombie = [s[-1] == eos or len(s) > maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    sample_lengths = map(len, samples)\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right (post) so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
    "    return np.array([prob[sample_length-maxlend-1] for prob, sample_length in zip(probs, sample_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x,x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_unfold(desc,xs):\n",
    "    # assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= oov0:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x,x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named Levenshtein",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-426-56f2922253b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named Levenshtein"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import Levenshtein\n",
    "\n",
    "def gensamples(skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    i = random.randint(0,len(X_test)-1)\n",
    "    print 'HEAD:',' '.join(idx2word[w] for w in Y_test[i][:maxlenh])\n",
    "    print 'DESC:',' '.join(idx2word[w] for w in X_test[i][:maxlend])\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print 'HEADS:'\n",
    "    x = X_test[i]\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print score, ' '.join(words)\n",
    "        #         print '%s (%.2f) %f'%(' '.join(words), score, distance)\n",
    "        else:\n",
    "                print score, ' '.join(words)\n",
    "        codes.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/pandas']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensamples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-429-f97810ccf9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensamples' is not defined"
     ]
    }
   ],
   "source": [
    "gensamples(skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_headline(x, nflips=None, model=None, debug=False):\n",
    "    \"\"\"given a vectorized input (after `pad_sequences`) flip some of the words in the second half (headline)\n",
    "    with words predicted by the model\n",
    "    \"\"\"\n",
    "    if nflips is None or model is None or nflips <= 0:\n",
    "        return x\n",
    "    \n",
    "    batch_size = len(x)\n",
    "    assert np.all(x[:,maxlend] == eos)\n",
    "    probs = model.predict(x, verbose=0, batch_size=batch_size)\n",
    "    x_out = x.copy()\n",
    "    for b in range(batch_size):\n",
    "        # pick locations we want to flip\n",
    "        # 0...maxlend-1 are descriptions and should be fixed\n",
    "        # maxlend is eos and should be fixed\n",
    "        flips = sorted(random.sample(xrange(maxlend+1,maxlen), nflips))\n",
    "        if debug and b < debug:\n",
    "            print b,\n",
    "        for input_idx in flips:\n",
    "            if x[b,input_idx] == empty or x[b,input_idx] == eos:\n",
    "                continue\n",
    "            # convert from input location to label location\n",
    "            # the output at maxlend (when input is eos) is feed as input at maxlend+1\n",
    "            label_idx = input_idx - (maxlend+1)\n",
    "            prob = probs[b, label_idx]\n",
    "            w = prob.argmax()\n",
    "            if w == empty:  # replace accidental empty with oov\n",
    "                w = oov0\n",
    "            if debug and b < debug:\n",
    "                print '%s => %s'%(idx2word[x_out[b,input_idx]],idx2word[w]),\n",
    "            x_out[b,input_idx] = w\n",
    "        if debug and b < debug:\n",
    "            print\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_seq_labels(xds, xhs, nflips=None, model=None, debug=False):\n",
    "    \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "    batch_size = len(xhs)\n",
    "    assert len(xds) == batch_size\n",
    "    x = [vocab_fold(lpadd(xd)+xh) for xd,xh in zip(xds,xhs)]  # the input does not have 2nd eos\n",
    "    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    x = flip_headline(x, nflips=nflips, model=model, debug=debug)\n",
    "    \n",
    "    y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "    for i, xh in enumerate(xhs):\n",
    "        xh = vocab_fold(xh) + [eos] + [empty]*maxlenh  # output does have a eos at end\n",
    "        xh = xh[:maxlenh]\n",
    "        y[i,:,:] = np_utils.to_categorical(xh, vocab_size)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, seed=seed):\n",
    "    \"\"\"yield batches. for training use nb_batches=None\n",
    "    for validation generate deterministic results repeating every nb_batches\n",
    "    \n",
    "    while training it is good idea to flip once in a while the values of the headlines from the\n",
    "    value taken from Xh to value generated by the model.\n",
    "    \"\"\"\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxint)\n",
    "        random.seed(c+123456789+seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0,len(Xd)-1)\n",
    "\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend,len(xd)), max(maxlend,len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "            \n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh,len(xh)), max(maxlenh,len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs, nflips=nflips, model=model, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 50), (64, 25, 19722), 2)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_gen(gen, n=5):\n",
    "    Xtr,Ytr = next(gen)\n",
    "    for i in range(n):\n",
    "        assert Xtr[i,maxlend] == eos\n",
    "        x = Xtr[i,:maxlend]\n",
    "        y = Xtr[i,maxlend:]\n",
    "        yy = Ytr[i,:]\n",
    "        yy = np.where(yy)[1]\n",
    "        prt('L',yy)\n",
    "        prt('H',y)\n",
    "        if maxlend:\n",
    "            prt('D',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: We give an o ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H: ~ We give an o _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D: The mechanisms of attention and memory (Graves et al., 2016) has been attracting much attention. Novel architectures and applications using deep RL were recognized in\n",
      "L: We study reinforcement learning of chatbots with recurrent neural network archi- tectures when the rewards are noisy and expensive to obtain. For instance, a chat-\n",
      "H: ~ We study reinforcement learning of chatbots with recurrent neural network archi- tectures when the rewards are noisy and expensive to obtain. For instance, a\n",
      "D: our policy. RELATED WORK Recently there has been a surge of interest in deep learning approaches to reinforcement learning, many of them adopting Q-learning, e.g.\n",
      "L: We give an o ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H: ~ We give an o _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D: list is boundless. New learning mechanisms have emerged, e.g., using unsupervised/semi-supervised/tranfer learning to improve the quality and speed of learn- ing, and more new mechanisms\n",
      "L: While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider\n",
      "H: ~ While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to\n",
      "D: no large scale annotations available for building a supervised learning procedure. We consider leveraging the large amount of human-human conversation data available on the web\n",
      "L: We consider incorporating topic information into message- response matching to boost responses with rich content in retrieval-based chatbots. To this end, we propose a topic-\n",
      "H: ~ We consider incorporating topic information into message- response matching to boost responses with rich content in retrieval-based chatbots. To this end, we propose a\n",
      "D: gap between the message and the response. We also compared our model with CNTN on short (less than 10 words) and long (more than or\n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: We give an o ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H: ~ We i an o _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D: The mechanisms of attention and memory (Graves et al., 2016) has been attracting much attention. Novel architectures and applications using deep RL were recognized in\n",
      "L: We study reinforcement learning of chatbots with recurrent neural network archi- tectures when the rewards are noisy and expensive to obtain. For instance, a chat-\n",
      "H: ~ Deep study reinforcement learning of chatbots with recurrent there network archi- tectures when the rewards are noisy e.g., expensive to ﬁrst ﬁrst ﬁrst a\n",
      "D: our policy. RELATED WORK Recently there has been a surge of interest in deep learning approaches to reinforcement learning, many of them adopting Q-learning, e.g.\n",
      "L: We give an o ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "H: ~ turns give 10 o _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "D: list is boundless. New learning mechanisms have emerged, e.g., using unsupervised/semi-supervised/tranfer learning to improve the quality and speed of learn- ing, and more new mechanisms\n",
      "L: While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider\n",
      "H: ~ While e.g., response generation for building chatbot systems has drawn a lot of attention recently, there e.g., limited e.g., on when e.g., should should\n",
      "D: no large scale annotations available for building a supervised learning procedure. We consider leveraging the large amount of human-human conversation data available on the web\n",
      "L: We consider incorporating topic information into message- response matching to boost responses with rich content in retrieval-based chatbots. To this end, we propose a topic-\n",
      "H: ~ We consider incorporating 4 information into message- response matching to boost responses with a content a retrieval-based e.g., To e.g., end, we propose e.g.,\n",
      "D: gap between the message and the response. We also compared our model with CNTN on short (less than 10 words) and long (more than or\n"
     ]
    }
   ],
   "source": [
    "# Test flipping\n",
    "test_gen(gen(X_train, Y_train, nflips=6, model=model, debug=False, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valgen = gen(X_test, Y_test,nb_batches=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for\n",
      "H: ~ In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots\n",
      "D: j are assigned so that y >= 0 for the majority of the ij interaction time. We analyze three properties of the trajectories: • Latency.\n",
      "L: In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for\n",
      "H: ~ In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots\n",
      "D: decreasing (error bars correspond to one standard error). This suggests that disagreement between bots is not becoming less common. In general, bots revert each other\n",
      "L: We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natu- ral, conversational language about\n",
      "H: ~ We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natu- ral, conversational language\n",
      "D: previous questions asked by the user nor consistency with respect to previous answers provided by the system (Q: ‘How many people on wheelchairs?’, A: ‘Two’;\n",
      "L: In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for\n",
      "H: ~ In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots\n",
      "D: j are assigned so that y >= 0 for the majority of the ij interaction time. We analyze three properties of the trajectories: • Latency.\n"
     ]
    }
   ],
   "source": [
    "# Check that valgen repeats itself after nb_batches\n",
    "for i in range(4):\n",
    "    test_gen(valgen, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nflips=nflips, model=model)\n",
    "valgen = gen(X_test, Y_test, nb_batches=nb_val_samples//batch_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 50), (64, 25, 19722), 2)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(traingen)\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., steps_per_epoch=30000, epochs=1, validation_steps=3000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected dropout_3 to have shape (None, 50, 512) but got array with shape (64, 25, 19722)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-843ff3f3d098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     h = model.fit_generator(traingen, samples_per_epoch=nb_train_samples,\n\u001b[0;32m----> 4\u001b[0;31m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                            )\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1300\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1301\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Users/fm61/anaconda3/envs/txt_summ/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    131\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected dropout_3 to have shape (None, 50, 512) but got array with shape (64, 25, 19722)"
     ]
    }
   ],
   "source": [
    "for iteration in range(500):\n",
    "    print 'Iteration', iteration\n",
    "    h = model.fit_generator(traingen, samples_per_epoch=nb_train_samples,\n",
    "                        nb_epoch=1, validation_data=valgen, nb_val_samples=nb_val_samples\n",
    "                           )\n",
    "    for k,v in h.history.iteritems():\n",
    "        history[k] = history.get(k,[]) + v\n",
    "    with open('data/%s.history.pkl'%FN,'wb') as fp:\n",
    "        pickle.dump(history,fp,-1)\n",
    "    model.save_weights('data/%s.hdf5'%FN, overwrite=True)\n",
    "    gensamples(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict: Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN = 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN0 = 'vocabulary-embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN1 = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlend=50 # 0 - if we dont want to use description at all\n",
    "maxlenh=25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512\n",
    "rnn_layers = 3  # match FN1\n",
    "batch_norm=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed=73\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = 30000\n",
    "nb_val_samples = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/vocabulary-embedding.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-452-afa981a0a56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/%s.pkl'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mFN0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_idx2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/vocabulary-embedding.pkl'"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "with open('data/%s.pkl'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of embedding space for words 100\n",
      "vocabulary size 19722 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 19722 19722\n",
      "number of words outside vocabulary which we can substitue using glove similarity 18\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) -18\n"
     ]
    }
   ],
   "source": [
    "print 'dimension of embedding space for words',embedding_size\n",
    "print 'vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words\n",
    "print 'total number of different words',len(idx2word), len(word2idx)\n",
    "print 'number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx)\n",
    "print 'number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(vocab_size-nb_unknown_words, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print label+':',\n",
    "    for w in x:\n",
    "        print idx2word[w],\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [txt_summ]",
   "language": "python",
   "name": "Python [txt_summ]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
